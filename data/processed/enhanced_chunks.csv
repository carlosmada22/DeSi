id,title,source,source_type,content,section_title,chunk_index,chunk_id,content_type,file_path,processed_at,source_priority
docs:openbis:en_20.10.0-11_index:0,OpenBIS Documentation,https://openbis.readthedocs.io/en/20.10.0-11/index.html,openbis,"## OpenBIS Documentation

The complete solution for managing your research data.
## User Documentation

## General Users
## General Admin Users
## Advance Features
## Legacy Advance Features
## Software Developer Documentation

## Development Environment
## APIS
## Server-Side Extensions
## Client-Side Extensions
## Legacy Server-Side Extensions
## System Documentation

## Standalone
## Docker
Advanced configuration
## Change Log",OpenBIS Documentation,0,en_20.10.0-11_index_0,reference,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_index.txt,2025-09-30T12:09:00.817796Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_index:0,APIS,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/index.html,openbis,"## APIS

Java / Javascript (V3 API) - openBIS V3 API
## I. Architecture
One AS, one or more DSS
The Java API
The Javascript API
II. API Features
Current Features - AS
Current Features - DSS
## Missing/Planned Features
III. Accessing the API
Connecting in Java
Connecting in Javascript
AMD / RequireJS
AMD / RequireJS bundle
VAR bundle
ESM bundle
Synchronous Java vs Asynchronous Javascript
IV. AS Methods
## Login
## Example
## Personal Access Tokens
## Session Information
## Example
Creating entities
## Example
Properties example
Different ids example
Parent child example
Updating entities
## Example
Properties example
Parents example
Getting authorization rights for entities
Freezing entities
## Space
## Project
## Experiment
## Sample
Data Set
Searching entities
## Example
Example with pagination and sorting
Example with OR operator
Example with nested logical operators
Example with recursive fetch options
Global search
Getting entities
## Example
Deleting entities
## Example
Searching entity types
## Modifications
Custom AS Services
Search for custom services
Execute a custom service
Archiving / unarchiving data sets
Archiving data sets
Unarchiving data sets
## Executing Operations
Method executeOperations
Method getOperationExecutions / searchOperationExecutions
Method updateOperationExecutions / deleteOperationExecutions
### Configuration
## Semantic Annotations
## Web App Settings
## Imports
V. DSS Methods
Search files
## Example
Downloading files, folders, and datasets
## Simple Downloading
Download a single file located inside a dataset
Download a folder located inside a dataset
Search for a dataset and download all its contents, file by file
Download a whole dataset recursively
Search and list all the files inside a data store
## Fast Downloading
What happens under the hood?
## Customizing Fast Dowloading
Register Data Sets
VI. Web application context
Python (V3 API) - pyBIS!
Dependencies and Requirements
### Installation
### General Usage
TAB completition and other hints in Jupyter / IPython
Checking input
## Glossary
connect to OpenBIS
login
Verify certificate
Check session token, logout()
Authentication without user/password
Personal access token (PAT)
## Caching
Mount openBIS dataStore server
Prerequisites: FUSE / SSHFS
Mount dataStore server with pyBIS
## Masterdata
browse masterdata
create property types
create sample types / object types
assign and revoke properties to sample type / object type
create a dataset type
create an experiment type / collection type
create material types
create plugins
Users, Groups and RoleAssignments
## Spaces
## Projects
## Experiments / Collections
create a new experiment
search for experiments
Experiment attributes
Experiment properties
## Samples / Objects
create/update/delete many samples in a transaction
parents, children, components and container
sample tags
Sample attributes and properties
search for samples / objects
freezing samples
## Datasets
working with existing dataSets
download dataSets
link dataSets
dataSet attributes and properties
search for dataSets
freeze dataSets
create a new dataSet
create dataSet with zipfile
create dataSet with mixed content
create dataSet container
get, set, add and remove parent datasets
get, set, add and remove child datasets
dataSet containers
## Semantic Annotations
## Tags
Vocabulary and VocabularyTerms
Change ELN Settings via pyBIS
## Main Menu
## Storages
## Templates
## Custom Widgets
Things object
JSON response
DataFrame
## Objects
Best practices
## Logout
Iteration over tree structure
Iteration over raw data
Matlab (V3 API) - How to access openBIS from MATLAB
## Preamble
## Setup
macOS
## Windows 10
### Usage
## Notes
## Personal Access Tokens
## Background
What are “Personal access tokens” ?
Who can create a “Personal access token” ?
Where can I use “Personal access tokens” ?
Where “Personal access tokens” are stored ?
How long should my “Personal Access Tokens” be valid ?
### Configuration
## Typical Application Workflow
## V3 API",APIS,0,en_20.10.0-11_software-developer-documentation_apis_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_index.txt,2025-09-30T12:09:00.910674Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_java-javascript-v3-api:0,Java / Javascript (V3 API) - openBIS V3 API,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/java-javascript-v3-api.html,openbis,"Java / Javascript (V3 API) - openBIS V3 API

## I. Architecture

Open BIS consists of two main components: an Application Server and one
or more Data Store Servers. The Application Server manages the system’s
meta data, while the Data Store Server(s) manage the file store(s). Each
Data Store Server manages its own file store. Here we will refer to the
Application Server as the “AS” and the Data Store Server as the “DSS.”
One AS, one or more DSS

Why is there only one Application Server but multiple Data Store
Servers? It is possible to have only one Data Store Server, but in a
complex project there might be many labs using the same OpenBIS instance
and therefore sharing the same meta data. Each lab might have its own
Data Store Server to make file management easier and more efficient. The
Data Store Servers are on different Java virtual machines, which enables
the files to be processed faster. It is also more efficient when the
physical location of the Data Store Server is closer to the lab that is
using it. Another reason is that the meta data tends to be relatively
small in size, whereas the files occupy a large amount of space in the
system.
The Java API

The Java V3 API consists of two interfaces:
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerAPI
ch.ethz.sis.openbis.generic.dssapi.v3.IDatastoreServerAPI
Please check our JavaDoc for more
## details:
https://openbis.ch/javadoc/20.10.x/javadoc-api-v3/index.html
All V3 API jars are packed in openBIS-API-V3-
.zip which
is part of openBIS-clients-and-APIs-
.zip (the latest version can be downloaded at
https://unlimited.ethz.ch/display/openbis/Production+Releases
)
The Javascript API

The Javascript V3 API consists of a module hosted at
<OPENBIS_URL>/resources/api/v3/openbis.js, for instance
http://localhost/openbis
/ resources/api/v3/openbis.js. Please check
the openbis.js file itself for more details.
II. API Features

Current Features - AS

The current implementation of the V3 openBIS API contains the following
## features:
Creation:  Create spaces, projects, experiments and experiment
types, samples and sample types, materials and material types,
vocabulary terms, tags
Associations: Associate spaces, project, experiments, samples,
datasets, materials to each other
Tags: Add/Remove/Set tags for experiments, samples, datasets and
materials
Properties: Set properties for experiments, samples, datasets and
materials
Search: Search & get spaces, project, experiments, samples,
datasets, materials, vocabulary terms, tags
Update: Update spaces, project, experiments, samples, datasets,
materials, vocabulary terms, tags
Deletion: Delete spaces, project, experiments, samples, datasets,
materials, vocabulary terms, tags
Authentication: Login as user, login as another user, login as an
anonymous user
Transactional features: performing multiple operations in one
transaction (with executeOperations method)
Queries: create/update/get/search/delete/execute queries
Generating codes/permids
Current Features - DSS

Search data set files
Download data set files
## Missing/Planned Features

The current implementation of the V3 openBIS API does not yet include
the following features:
Management features: Managing data stores
Search features: Searching experiments having samples/datasets,
searching datasets (oldest, deleted, for archiving etc.)
Update features: Updating datasets share id, size, status, storage
confirmation, post registration status
III. Accessing the API

In order to use V3 API you have to know the url of an openBIS instance
you want to connect to. Moreover, before calling any of the API methods
you have to login to the system to receive a sessionToken. All the login
methods are part of the AS API. Once you successfully authenticate in
openBIS you can invoke other methods of the API (at both AS and DSS). In
each call you have to provide your sessionToken. When you have finished
working with the API you should call logout method to release all the
resources related with your session.
Note: If the openBIS instance you are connecting to uses SSL and does
not have a real certificate (it is using the self-signed certificate
that comes with openBIS), you need to tell the java client to use the
trust store that comes with openBIS. This can be done by setting the
property
javax.net
## .ssl.trustStore. Example:
Using openBIS trust store in Java clients
java
-Djavax.net.ssl.trustStore
=
/home/openbis/openbis/servers/openBIS-server/jetty/etc/openBIS.keystore
-jar
the-client.jar
Connecting in Java

In order to connect to openBIS V3 API in Java you can:
use IApplicationServerApi (AS) and IDataStoreServerApi (DSS) interfaces directly
use OpenBIS facade (that talks to IApplicationServerApi and IDataStoreServerApi interfaces internally)
Using the OpenBIS facade has some advantages over using the AS and DSS interfaces directly:
it hides the details of the protocol and the data serialization format used between the client and the server
it does not require you to know V3 API endpoints for both AS and DSS and their URLs
it provides additional utility methods (e.g. getManagedPersonalAccessToken)
Because of these reasons, OpenBIS facade is the recommended way of connecting to V3 API in Java.
Code examples for both approaches are presented below.
V3ConnectionExampleUsingASAndDSSInterfaces.java
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.Space
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.fetchoptions.SpaceFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3ConnectionExampleUsingASAndDSSInterfaces
{
private
static
final
## String
## URL
=
""http://localhost:8888/openbis/openbis""
+
IApplicationServerApi
.
## SERVICE_URL
;
private
static
final
int
## TIMEOUT
=
10000
;
public
static
void
main
(
## String
[]
args
)
{
// get a reference to AS API
IApplicationServerApi
v3
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
## URL
,
## TIMEOUT
);
// login to obtain a session token
## String
sessionToken
=
v3
.
login
(
""admin""
,
""password""
);
// invoke other API methods using the session token, for instance search for spaces
SearchResult
<
## Space
>
spaces
=
v3
.
searchSpaces
(
sessionToken
,
new
SpaceSearchCriteria
(),
new
SpaceFetchOptions
());
## System
.
out
.
println
(
""Number of spaces: ""
+
spaces
.
getObjects
().
size
());
// logout to release the resources related with the session
v3
.
logout
(
sessionToken
);
}
}
V3ConnectionExampleUsingOpenBISFacade.java
import
ch.ethz.sis.openbis.generic.OpenBIS
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.Space
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.fetchoptions.SpaceFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria
;
public
class
V3ConnectionExampleUsingOpenBISFacade
{
private
static
final
## String
## URL
=
""http://localhost:8888""
;
private
static
final
int
## TIMEOUT
=
10000
;
public
static
void
main
(
## String
[]
args
)
{
// create OpenBIS facade (it aggregates methods from both IApplicationServerApi and IDataStoreServerApi)
OpenBIS
v3
=
new
OpenBIS
(
## URL
,
## TIMEOUT
);
// login to obtain a session token (the token is stored in the facade and used for subsequent calls)
v3
.
login
(
""admin""
,
""password""
);
// invoke other API methods, for instance search for spaces
SearchResult
<
## Space
>
spaces
=
v3
.
searchSpaces
(
new
SpaceSearchCriteria
(),
new
SpaceFetchOptions
());
## System
.
out
.
println
(
""Number of spaces: ""
+
spaces
.
getObjects
().
size
());
// logout to release the resources related with the session
v3
.
logout
();
}
}
Connecting in Javascript

We have put a lot of effort to make the use of the API in Javascript and Java almost identical. The DTOs which are a big part of the API are exactly
the same in both languages. The methods you can invoke via the Javascript and Java APIs are also exactly the same. This makes the switch from
Javascript to Java or the other way round very easy. Because of some major differences between Javascript and Java development still some things had
to be done a bit differently. But even then we tried to be conceptually consistent.
Before we go into details let’s mention that there are actually 4 different ways the Javascript V3 API can be loaded and used. These are:
AMD / RequireJS
AMD / RequireJS bundle
VAR bundle
ESM bundle
IMPORTANT: VAR and ESM bundles are currently the recommended way of using the Javascript V3 API. AMD / RequireJS approach is still supported but no
longer recommended.
AMD / RequireJS

Initially, the only way to load and use the V3 API in Javascript was based on AMD modules and RequireJS (see code example below). In that approach,
what we had to do first was to load RequireJS library itself and its config. Once that was done, we could start loading all the necessary V3 API
classes and the V3 API facade to make our V3 API calls.
This approach worked fine, but there were also some drawbacks:
each V3 API class we wanted to use had to be explicitly “required” and its full class name had to be provided (e.g. as/dto/space/Space)
classes were loaded asynchronously making the code using the V3 API more complex
every V3 API class was loaded with a separate HTTP request to the server (loading multiple classes resulted in multiple requests to the server)
it required a third party dependency manager (here RequireJS)
Because of these shortcomings this approach is no longer recommended, but still fully supported. Please use VAR or ESM bundles instead
(depending on your use case).
V3ConnectionExampleUsingRequireJS.html
<!DOCTYPE html>
<
html
>
<
head
>
<
meta
charset
=
""utf-8""
>
<
title
>
V3ConnectionExampleUsingRequireJS
</
title
>
<!--
These two js files, i.e. config.js and require.js are RequireJS configuration and RequireJS library itself.
Please check http://requirejs.org/ for more details on how RequireJS makes loading dependencies in Javascript easier.
-->
<
script
type
=
""text/javascript""
src
=
""http://localhost:8888/openbis/resources/api/v3/config.js""
></
script
>
<
script
type
=
""text/javascript""
src
=
""http://localhost:8888/openbis/resources/api/v3/require.js""
></
script
>
</
head
>
<
body
>
<
script
>
// With ""require"" call we asynchronously load ""openbis"", ""SpaceSearchCriteria"" and ""SpaceFetchOptions"" classes that we will need for our example.
// The function that is passed as a second parameter of the require call is a callback that gets executed once requested classes are loaded.
// In Javascript we work with exactly the same classes as in Java. For instance, ""ch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria""
// Java class and ""as/dto/space/search/SpaceSearchCriteria"" Javascript class have exactly the same methods. In order to find a Javascript class name please
// check our Javadoc (https://openbis.ch/javadoc/20.10.x/javadoc-api-v3/index.html). The Javascript class name is defined in @JsonObject annotation of each V3 API Java DTO.
require
([
""openbis""
,
""as/dto/space/search/SpaceSearchCriteria""
,
""as/dto/space/fetchoptions/SpaceFetchOptions""
],
function
(
openbis
,
SpaceSearchCriteria
,
SpaceFetchOptions
)
{
// get a reference to AS API
var
v3
=
new
openbis
();
// login to obtain a session token (the token it is automatically stored in openbis object and will be used for all subsequent API calls)
v3
.
login
(
""admin""
,
""password""
).
done
(
function
()
{
// invoke other API methods, for instance search for spaces
v3
.
searchSpaces
(
new
SpaceSearchCriteria
(),
new
SpaceFetchOptions
()).
done
(
function
(
result
)
{
alert
(
""Number of spaces: ""
+
result
.
getObjects
().
length
);
// logout to release the resources related with the session
v3
.
logout
();
});
});
});
</
script
>
</
body
>
</
html
>
AMD / RequireJS bundle

To improve the performance of AMD / RequireJS approach, we started to also provide a bundled version of the “config.js” called “config.bundle.js”
(found in the same folder).
Using the bundled version of the config makes RequireJS issue only one request to the server to load all DTOs at once instead of separate requests for
each DTO. This will significantly reduce the loading times of your webapp. What is also really nice is the fact it is so easy to start using this
improvement. Just load “config.bundle.js” instead of “config.js” and that’s it!
Even though AMD / RequireJS solution is not recommended anymore, if you have a lot of existing code written with AMD / RequireJS approach then it
makes perfect sense to use this improvement before migrating to either VAR or ESM.
VAR bundle

VAR bundle (bundle assigned to window.openbis variable) allows you to overcome the shortcomings of AMD / RequireJS solution. First, the VAR bundle
consists of V3 API facade and all V3 API classes. Therefore, once the bundle is loaded, no further calls to the server are needed. Second, the bundle
exposes the V3 API classes both via their simple names and their full names (see code example below) which makes it far easier for developers to use.
Third, it does not require any additional library.
VAR bundle can be loaded at an HTML page using a standard script tag.
V3ConnectionExampleUsingVARBundle.html
<!DOCTYPE html>
<
html
>
<
head
>
<
meta
charset
=
""utf-8""
/>
<
title
>
V3ConnectionExampleUsingVARBundle
</
title
>
<!--
Import VAR openBIS V3 API Javascript bundle as ""openbis"".
The bundle contains V3 API Javascript facade and all V3 API Javascript classes.
The facade can be accessed via:
- ""openbis"" name (e.g. var v3 = new openbis.openbis())
The classes can be accessed via:
- simple name (e.g. var space = new openbis.Space()) - works for classes with a unique simple name (see details below)
- full name (e.g. var space = new opebis.as.dto.space.Space()) - works for all classes
Classes with a unique simple name (e.g. Space) can be accessed using both their simple name (e.g. openbis.Space)
and their full name (e.g. openbis.as.dto.space.Space).
Classes with a non-unique simple name (e.g. ExternalDmsSearchCriteria) can be accessed only using their full name
(i.e. as.dto.dataset.search.ExternalDmsSearchCriteria and as.dto.externaldms.search.ExternalDmsSearchCriteria).
List of classes with duplicated simple names (i.e. accessible only via their full names):
- as.dto.dataset.search.ExternalDmsSearchCriteria
- as.dto.externaldms.search.ExternalDmsSearchCriteria
- as.dto.pat.search.PersonalAccessTokenSessionNameSearchCriteria
- as.dto.session.search.PersonalAccessTokenSessionNameSearchCriteria
-->
<!-- Import the bundle as ""openbis"" (the bundle content is assigned to window.openbis field).
In case window.openbis field is already used to store something different, then please
call openbis.noConflict() function right after the VAR bundle is loaded. It will bring back
the original value of window.openbis field and return the loaded VAR bundle for it to be
assigned to a different field (works similar to jquery.noConflict() function).
-->
<
script
src
=
""http://localhost:8888/openbis/resources/api/v3/openbis.var.js""
></
script
>
</
head
>
<
body
>
<
script
>
// create an instance of the Javascript facade
var
v3
=
new
openbis
.
openbis
();
// login to obtain a session token (the token it is automatically stored in openbis object and will be used for all subsequent API calls)
v3
.
login
(
""admin""
,
""admin""
).
done
(
function
()
{
// invoke other API methods, for instance search for spaces
v3
.
searchSpaces
(
new
openbis
.
SpaceSearchCriteria
(),
new
openbis
.
SpaceFetchOptions
()).
done
(
function
(
result
)
{
alert
(
""Number of spaces: ""
+
result
.
getObjects
().
length
);
// logout to release the resources related with the session
v3
.
logout
();
});
});
</
script
>
</
body
>
</
html
>
ESM bundle

Similar to VAR bundle, ESM bundle (ECMAScript module) is a bundle that contains the V3 API facade and all V3 API classes. It also exposes the V3 API
classes via both their simple names and their full names. The main difference between VAR and ESM is the format of the bundle and how and where it can
be imported.
ESM bundle can be loaded at an HTML page using a standard script tag with type=”module”. It is also well suited for webapps that bundle all their
resources with tools like Webpack.
V3ConnectionExampleUsingESMBundle.html
<!DOCTYPE html>
<
html
>
<
head
>
<
meta
charset
=
""utf-8""
/>
<
title
>
V3ConnectionExampleUsingESMBundle
</
title
>
</
head
>
<
body
>
<!--
Import ESM (ECMAScript module) openBIS V3 API Javascript bundle as ""openbis"".
The bundle contains V3 API Javascript facade and all V3 API Javascript classes.
The facade can be accessed via:
- ""openbis"" name (e.g. var v3 = new openbis.openbis())
The classes can be accessed via:
- simple name (e.g. var space = new openbis.Space()) - works for classes with a unique simple name (see details below)
- full name (e.g. var space = new opebis.as.dto.space.Space()) - works for all classes
Classes with a unique simple name (e.g. Space) can be accessed using both their simple name (e.g. openbis.Space)
and their full name (e.g. openbis.as.dto.space.Space).
Classes with a non-unique simple name (e.g. ExternalDmsSearchCriteria) can be accessed only using their full name
(i.e. as.dto.dataset.search.ExternalDmsSearchCriteria and as.dto.externaldms.search.ExternalDmsSearchCriteria).
List of classes with duplicated simple names (i.e. accessible only via their full names):
- as.dto.dataset.search.ExternalDmsSearchCriteria
- as.dto.externaldms.search.ExternalDmsSearchCriteria
- as.dto.pat.search.PersonalAccessTokenSessionNameSearchCriteria
- as.dto.session.search.PersonalAccessTokenSessionNameSearchCriteria
-->
<
script
type
=
""module""
>
// import the bundle as ""openbis"" (the bundle can be imported with a different name)
import
openbis
from
""http://localhost:8888/openbis/resources/api/v3/openbis.esm.js""
// create an instance of the Javascript facade
var
v3
=
new
openbis
.
openbis
();
// login to obtain a session token (the token it is automatically stored in openbis object and will be used for all subsequent API calls)
v3
.
login
(
""admin""
,
""admin""
).
done
(
function
()
{
// invoke other API methods, for instance search for spaces
v3
.
searchSpaces
(
new
openbis
.
SpaceSearchCriteria
(),
new
openbis
.
SpaceFetchOptions
()).
done
(
function
(
result
)
{
alert
(
""Number of spaces: ""
+
result
.
getObjects
().
length
);
// logout to release the resources related with the session
v3
.
logout
();
});
});
</
script
>
</
body
>
</
html
>
Synchronous Java vs Asynchronous Javascript

Even though the V3 API code examples in both Java and Javascript look similar there is one major difference between them. All the methods in the Java
API that connect to the openBIS server are synchronous, while all their Javascript counterparts are asynchronous. Let’s compare how that looks.
V3JavaCallsAreSynchronous.java
public
class
V3JavaCallsAreSynchronous
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created (please check ""Accessing the API"" section for more details)
// this makes a synchronous (blocking) call to the server
SearchResult
<
## Space
>
spaces
=
v3
.
searchSpaces
(
new
SpaceSearchCriteria
(),
new
SpaceFetchOptions
());
// this loop will execute only after searchSpaces method call is finished and spaces have been fetched from the server
for
(
## Space
space
## :
spaces
)
{
## System
.
out
.
println
(
space
.
getCode
());
}
}
}
V3JavascriptCallsAreAsynchronous.java
<!DOCTYPE html>
<
html
>
<
head
>
<
meta
charset
=
""utf-8""
/>
<
title
>
V3JavascriptCallsAreAsynchronous
</
title
>
</
head
>
<
body
>
<
script
>
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// this makes a non-blocking (asynchronous) call to the server
v3
.
searchSpaces
(
new
SpaceSearchCriteria
(),
new
SpaceFetchOptions
()).
done
(
// we need to put the loop in ""done"" callback for it to be executed once spaces have been fetched from the server
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
space
){
console
.
log
(
space
.
getCode
());
});
}
);
</
script
>
</
body
>
</
html
>
What Javascript V3 API asynchronous functions actually return is a jQuery Promise object, which offers methods like: then, done, fail (
see: https://api.jquery.com/category/deferred-object/). These methods can be used for registering different callbacks that will be either executed
when a call succeeds or fails (e.g. due to a network problem).
A more modern and an easier to understand way of working with Promise objects is the async / await syntax (
see: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/async_function). Our asynchronous Javascript V3 API methods support
it as well.
IV. AS Methods

The sections below describe how to use different methods of the V3 API. Each section describes a group of similar methods. For instance, we have one
section that describes creation of entities. Even though the API provides us methods for creation of spaces, projects, experiments, samples and
materials, vocabulary terms, tags we only concentrate here on creation of samples. Samples are the most complex entity kind. Once you understand how
creation of samples works you will also know how to create other kinds of entities as all creation methods follow the same patterns. The same applies
for other methods like updating of entities, searching or getting entities. We will introduce them using the sample example.
Each section will be split into Java and Javascript subsections. We want to keep Java and Javascript code examples close to each other so that you can
easily see what are the similarities and differences in the API usage between these two languages.
NOTE: The following code examples assume that we have already got a reference to the V3 API and we have already authenticated to get a session token.
Moreover in Javascript example we do not include the html page template to make them shorter and more readable. Please check “Accessing the API”
section for examples on how to get a reference to V3 API, authenticate or build a simple html page.
## Login

OpenBIS provides the following login methods:
login(user, password) - login as a given user
loginAs(user, password, asUser) - login on behalf of a different
user (e.g. I am an admin but I would like to see only things user
“x” would normally see)
loginAsAnonymousUser() - login as an anonymous user configured in AS
service.properties
All login methods return a session token if the provided parameters were
correct. In case a given user does not exist or the provided password
was incorrect the login methods return null.
## Example

V3LoginExample.java
public
class
V3LoginExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created (please check ""Accessing the API"" section for more details)
// login as a specific user
## String
sessionToken
=
v3
.
login
(
""admin""
,
""password""
);
## System
.
out
.
println
(
sessionToken
);
// login on behalf of a different user (I am an admin but I would like to see only things that some other user would normally see)
sessionToken
=
v3
.
loginAs
(
""admin""
,
""password""
,
""someotheruser""
);
## System
.
out
.
println
(
sessionToken
);
// login as an anonymous user (anonymous user has to be configured in service.properties first)
sessionToken
=
v3
.
loginAsAnonymousUser
();
## System
.
out
.
println
(
sessionToken
);
}
}
V3LoginExample.html
<
script
>
// we assume here that v3 object has been already created (please check ""Accessing the API"" section for more details)
// login as a specific user
v3
.
login
(
""admin""
,
""password""
).
done
(
function
(
sessionToken
)
{
alert
(
sessionToken
);
// login on behalf of a different user (I am an admin but I would like to see only things that some other user would normally see)
v3
.
loginAs
(
""admin""
,
""password""
,
""someotheruser""
).
done
(
function
(
sessionToken
)
{
alert
(
sessionToken
);
// login as an anonymous user (anonymous user has to be configured in service.properties first)
v3
.
loginAsAnonymousUser
().
done
(
function
(
sessionToken
)
{
alert
(
sessionToken
);
});
});
});
</
script
>
## Personal Access Tokens

A personal access token (in short: PAT) can be thought of as a longer lived session token which can be used for integrating openBIS with external systems. If you would like to learn more about the idea behind PATs please read:
## Personal Access Tokens
).
Example of how to create and use a PAT:
import
java.util.Arrays
;
import
java.util.Date
;
import
java.util.List
;
import
java.util.Map
;
import
org.apache.commons.lang.time.DateUtils
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.PersonalAccessToken
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.create.PersonalAccessTokenCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.fetchoptions.PersonalAccessTokenFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.id.IPersonalAccessTokenId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.id.PersonalAccessTokenPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.fetchoptions.SpaceFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria
;
public
class
V3PersonalAccessTokenExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
PersonalAccessTokenCreation
creation
=
new
PersonalAccessTokenCreation
();
creation
.
setSessionName
(
""test session""
);
creation
.
setValidFromDate
(
new
## Date
(
## System
.
currentTimeMillis
()
-
DateUtils
.
## MILLIS_PER_DAY
));
creation
.
setValidToDate
(
new
## Date
(
## System
.
currentTimeMillis
()
+
DateUtils
.
## MILLIS_PER_DAY
));
// create and get the new PAT
## List
<
PersonalAccessTokenPermId
>
ids
=
v3api
.
createPersonalAccessTokens
(
sessionToken
,
## Arrays
.
asList
(
creation
));
## Map
<
IPersonalAccessTokenId
,
PersonalAccessToken
>
map
=
v3api
.
getPersonalAccessTokens
(
sessionToken
,
ids
,
new
PersonalAccessTokenFetchOptions
());
PersonalAccessToken
pat
=
map
.
get
(
ids
.
get
(
0
));
// use the new PAT to list spaces
v3api
.
searchSpaces
(
pat
.
getHash
(),
new
SpaceSearchCriteria
(),
new
SpaceFetchOptions
());
}
}
## Session Information

OpenBIS provides a method to obtain the session information for an
## already log in user:
## Example

V3CreationExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.session.SessionInformation
;
public
class
V3SessionInformationExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SessionInformation
sessionInformation
=
v3
.
getSessionInformation
(
sessionToken
);
## System
.
out
.
println
(
## ""User Name: ""
+
sessionInformation
.
getUserName
());
## System
.
out
.
println
(
## ""Home Group: ""
+
sessionInformation
.
getHomeGroupCode
());
## System
.
out
.
println
(
## ""Person: ""
+
sessionInformation
.
getPerson
());
## System
.
out
.
println
(
""Creator Person: ""
+
sessionInformation
.
getCreatorPerson
());
}
}
Creating entities

The methods for creating entities in V3 API are called: createSpaces,
createProjects, createExperiments, createSamples, createMaterials,
createVocabularyTerms, createTags. They all allow to create one or more
entities at once by passing one or more entity creation objects (i.e.
SpaceCreation, ProjectCreation, ExperimentCreation, SampleCreation,
MaterialCreation, VocabularyTermCreation, TagCreation). All these
methods return as a result a list of the new created entity perm ids.
NOTE: Creating data sets via V3 API is not available yet. The new V3
dropboxes are planned but not implemented yet. Please use V2 dropboxes
until V3 version is out.
## Example

V3CreationExample.java
import
java.util.List
;
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SamplePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3CreationExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
// you can also pass more than one creation object to create multiple entities at once
## List
<
SamplePermId
>
permIds
=
v3
.
createSamples
(
sessionToken
,
## Arrays
.
asList
(
sample
));
## System
.
out
.
println
(
""Perm ids: ""
+
permIds
);
}
}
V3CreationExample.html
<
script
>
require
([
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
],
function
(
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
// you can also pass more than one creation object to create multiple entities at once
v3
.
createSamples
([
sample
]).
done
(
function
(
permIds
)
{
alert
(
""Perm ids: ""
+
## JSON
.
stringify
(
permIds
));
});
});
</
script
>
Properties example

V3CreationWithPropertiesExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3CreationWithPropertiesExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
// examples of value formats that should be used for different types of properties
sample
.
setProperty
(
## ""MY_VARCHAR""
,
""this is a description""
);
sample
.
setProperty
(
## ""MY_INTEGER""
,
""123""
);
sample
.
setProperty
(
## ""MY_REAL""
,
""123.45""
);
sample
.
setProperty
(
## ""MY_BOOLEAN""
,
""true""
);
sample
.
setProperty
(
## ""MY_MATERIAL""
,
## ""MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)""
);
sample
.
setProperty
(
## ""MY_VOCABULARY""
,
## ""MY_TERM_CODE""
);
v3
.
createSamples
(
sessionToken
,
## Arrays
.
asList
(
sample
));
}
}
V3CreationWithPropertiesExample.html
<
script
>
require
([
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
],
function
(
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
// examples of value formats that should be used for different types of properties
sample
.
setProperty
(
## ""MY_VARCHAR""
,
""this is a description""
);
sample
.
setProperty
(
## ""MY_INTEGER""
,
""123""
);
sample
.
setProperty
(
## ""MY_REAL""
,
""123.45""
);
sample
.
setProperty
(
## ""MY_BOOLEAN""
,
""true""
);
sample
.
setProperty
(
## ""MY_MATERIAL""
,
## ""MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)""
);
sample
.
setProperty
(
## ""MY_VOCABULARY""
,
## ""MY_TERM_CODE""
);
v3
.
createSamples
([
sample
]).
done
(
function
(
permIds
)
{
alert
(
""Perm ids: ""
+
## JSON
.
stringify
(
permIds
));
});
});
});
</
script
>
Different ids example

V3CreationWithDifferentIdsExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3CreationWithDifferentIdsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
// as an experiment id we can use any class that implements IExperimentId interface. For instance, experiment identifier:
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
// or experiment perm id:
sample
.
setExperimentId
(
new
ExperimentPermId
(
""20160115170718361-98668""
));
v3
.
createSamples
(
sessionToken
,
## Arrays
.
asList
(
sample
));
}
}
V3CreationWithDifferentIdsExample.html
<
script
>
require
([
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/experiment/id/ExperimentPermId""
],
function
(
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
ExperimentPermId
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
// as an experiment id we can use any class that implements IExperimentId interface. For instance, experiment identifier:
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
// or experiment perm id:
sample
.
setExperimentId
(
new
ExperimentPermId
(
""20160115170718361-98668""
));
v3
.
createSamples
([
sample
]).
done
(
function
(
permIds
)
{
alert
(
""Perm ids: ""
+
## JSON
.
stringify
(
permIds
));
});
});
</
script
>
Parent child example

The following example creates parent and child samples for a sample type
## which allow automatic code generation:
V3CreationParentAndChildExample
import
java.util.Arrays
;
import
java.util.List
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.id.CreationId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SamplePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3CreationParentAndChildExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
parentSample
=
new
SampleCreation
();
parentSample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
parentSample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
parentSample
.
setCreationId
(
new
CreationId
(
""parent""
));
SampleCreation
childSample
=
new
SampleCreation
();
childSample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
childSample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
childSample
.
setParentIds
(
## Arrays
.
asList
(
parentSample
.
getCreationId
()));
## List
<
SamplePermId
>
permIds
=
v3
.
createSamples
(
sessionToken
,
## Arrays
.
asList
(
parentSample
,
childSample
));
## System
.
out
.
println
(
""Perm ids: ""
+
permIds
);
}
}
V3CreationParentAndChildExample.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/common/id/CreationId""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
CreationId
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
parentSample
=
new
SampleCreation
();
parentSample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
parentSample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
parentSample
.
setCreationId
(
new
CreationId
(
""parent""
));
var
childSample
=
new
SampleCreation
();
childSample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
childSample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
childSample
.
setParentIds
([
parentSample
.
getCreationId
()]);
v3
.
createSamples
([
parentSample
,
childSample
]).
done
(
function
(
permIds
)
{
alert
(
""Perm ids: ""
+
## JSON
.
stringify
(
permIds
));
});
});
</
script
>
Updating entities

The methods for updating entities in V3 API are called: updateSpaces,
updateProjects, updateExperiments, updateSamples, updateDataSets,
updateMaterials, updateVocabularyTerms, updateTags. They all allow to
update one or more entities at once by passing one or more entity update
objects (i.e. SpaceUpdate, ProjectUpdate, ExperimentUpdate,
SampleUpdate, MaterialUpdate, VocabularyTermUpdate, TagUpdate). With
update objects you can update entities without fetching their state
first, i.e. the update objects contain only changes - not the full state
of entities. All update objects require an id of an entity that will be
updated. Please note that some of the entity fields cannot be changed
once an entity is created, for instance sample code becomes immutable
after creation.
## Example

V3UpdateExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.update.SampleUpdate
;
public
class
V3UpdateExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// here we update a sample and attach it to a different experiment
SampleUpdate
sample
=
new
SampleUpdate
();
sample
.
setSampleId
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_OTHER_EXPERIMENT_CODE""
));
// you can also pass more than one update object to update multiple entities at once
v3
.
updateSamples
(
sessionToken
,
## Arrays
.
asList
(
sample
));
## System
.
out
.
println
(
## ""Updated""
);
}
}
V3UpdateExample.html
<
script
>
require
([
""as/dto/sample/update/SampleUpdate""
,
""as/dto/sample/id/SampleIdentifier""
,
""as/dto/experiment/id/ExperimentIdentifier""
],
function
(
SampleUpdate
,
SampleIdentifier
,
ExperimentIdentifier
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// here we update a sample and attach it to a different experiment
var
sample
=
new
SampleUpdate
();
sample
.
setSampleId
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_OTHER_EXPERIMENT_CODE""
));
// you can also pass more than one update object to update multiple entities at once
v3
.
updateSamples
([
sample
]).
done
(
function
()
{
alert
(
## ""Updated""
);
});
});
</
script
>
Properties example

V3UpdateWithPropertiesExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.update.SampleUpdate
;
public
class
V3UpdateWithPropertiesExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleUpdate
sample
=
new
SampleUpdate
();
sample
.
setSampleId
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
));
// examples of value formats that should be used for different types of properties
sample
.
setProperty
(
## ""MY_VARCHAR""
,
""this is a description""
);
sample
.
setProperty
(
## ""MY_INTEGER""
,
""123""
);
sample
.
setProperty
(
## ""MY_REAL""
,
""123.45""
);
sample
.
setProperty
(
## ""MY_BOOLEAN""
,
""true""
);
sample
.
setProperty
(
## ""MY_MATERIAL""
,
## ""MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)""
);
sample
.
setProperty
(
## ""MY_VOCABULARY""
,
## ""MY_TERM_CODE""
);
v3
.
updateSamples
(
sessionToken
,
## Arrays
.
asList
(
sample
));
## System
.
out
.
println
(
## ""Updated""
);
}
}
V3UpdateWithPropertiesExample.html
<
script
>
require
([
""as/dto/sample/update/SampleUpdate""
,
""as/dto/sample/id/SampleIdentifier""
],
function
(
SampleUpdate
,
SampleIdentifier
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleUpdate
();
sample
.
setSampleId
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
));
// examples of value formats that should be used for different types of properties
sample
.
setProperty
(
## ""MY_VARCHAR""
,
""this is a description""
);
sample
.
setProperty
(
## ""MY_INTEGER""
,
""123""
);
sample
.
setProperty
(
## ""MY_REAL""
,
""123.45""
);
sample
.
setProperty
(
## ""MY_BOOLEAN""
,
""true""
);
sample
.
setProperty
(
## ""MY_MATERIAL""
,
## ""MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)""
);
sample
.
setProperty
(
## ""MY_VOCABULARY""
,
## ""MY_TERM_CODE""
);
v3
.
updateSamples
([
sample
]).
done
(
function
()
{
alert
(
## ""Updated""
);
});
});
</
script
>
Parents example

V3UpdateWithParentsExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.update.SampleUpdate
;
public
class
V3UpdateWithParentsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// Let's assume the sample we are about to update has the following parents:
## // - MY_PARENT_CODE_1
## // - MY_PARENT_CODE_2
SampleUpdate
sample
=
new
SampleUpdate
();
sample
.
setSampleId
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
));
// We can add and remove parents from the existing list. For instance, here we are adding: MY_PARENT_CODE_3 and removing: MY_PARENT_CODE_1.
// The list of parents after such change would be: [MY_PARENT_CODE_2, MY_PARENT_CODE_3]. Please note that we don't have to fetch the existing
// list of parents, we are just defining what changes should be made to this list on the server side. Updating lists of children or contained
// samples works exactly the same.
sample
.
getParentIds
().
add
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_PARENT_CODE_3""
));
sample
.
getParentIds
().
remove
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_PARENT_CODE_1""
));
// Instead of adding and removing parents we can also set the list of parents to a completely new value.
sample
.
getParentIds
().
set
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_PARENT_CODE_2""
),
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_PARENT_CODE_3""
));
v3
.
updateSamples
(
sessionToken
,
## Arrays
.
asList
(
sample
));
## System
.
out
.
println
(
## ""Updated""
);
}
}
V3UpdateWithParentsExample.html
<
script
>
require
([
""as/dto/sample/update/SampleUpdate""
,
""as/dto/sample/id/SampleIdentifier""
],
function
(
SampleUpdate
,
SampleIdentifier
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// Let's assume the sample we are about to update has the following parents:
## // - MY_PARENT_CODE_1
## // - MY_PARENT_CODE_2
var
sample
=
new
SampleUpdate
();
sample
.
setSampleId
(
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
));
// We can add and remove parents from the existing list. For instance, here we are adding: MY_PARENT_CODE_3 and removing: MY_PARENT_CODE_1.
// The list of parents after such change would be: [MY_PARENT_CODE_2, MY_PARENT_CODE_3]. Please note that we don't have to fetch the existing
// list of parents, we are just defining what changes should be made to this list on the server side. Updating lists of children or contained
// samples works exactly the same.
sample
.
getParentIds
().
add
(
new
SampleIdentifier
(
## ""/MY_SPACE/MY_PARENT_CODE_3""
));
sample
.
getParentIds
().
remove
(
new
SampleIdentifier
(
## ""/MY_SPACE/MY_PARENT_CODE_1""
));
// Instead of adding and removing parents we can also set the list of parents to a completely new value.
sample
.
getParentIds
().
set
(
new
SampleIdentifier
(
## ""MY_SPACE/MY_PARENT_CODE_2""
),
new
SampleIdentifier
(
## ""MY_SPACE/MY_PARENT_CODE_3""
));
v3
.
updateSamples
([
sample
]).
done
(
function
()
{
alert
(
## ""Updated""
);
});
});
</
script
>
Getting authorization rights for entities

If the user isn’t allowed to create or update an entity an exception is
thrown. But often a client application wants to know in advance whether
such operations are allowed or not. With the API method
getRights()
authorizations rights for specified entities can be requested. Currently
only creation and update authorization rights for projects, experiments,
samples and data sets (only update right) are returned.
In order to check whether an entity can be created or not a dummy
identifier has to be provided when calling
getRights()
## . This
identifier should be a wellformed identifier which specifies the entity
to which such a new entity belongs. For example, calling
getRights()
with
new
ExperimentIdentifier(""/MY-SPACE/PROJECT1/DUMMY"")
would return
rights containing
## CREATE
if the user is allowed to create an
experiment in the project
## /MY-SPACE/PROJECT1
.
Freezing entities

An entity (Space, Project, Experiment, Sample, Data Set) can be frozen.
## There are two types of frozen:
## Core
and
surface
. A frozen core means
that certain attributes of the entity can not be changed but still
connections between entities can be added or removed. A frozen surface
implies a frozen core and frozen connections of particular types. To
freeze an entity it has to be updated by invoking at least one freeze
method on the update object. Example:
SampleUpdate sample = new SampleUpdate();
sample.setSampleId(new SampleIdentifier(""/MY_SPACE_CODE/MY_SAMPLE_CODE""));
sample.freezeForChildren();
v3.updateSamples(sessionToken, Arrays.asList(sample));
## Warning
Freezing can not be reverted.
The timestamp of freezing, the types of freezing, the user and the
identifier of the frozen entity will be stored in the database as a
freezing event.
The following tables show all freezing possibilities and what is actual
frozen.
## Space

Freezing method
## Description
freeze
The specified space can not be deleted.
The description can not be set or changed.
freezeForProjects
Same as freeze() plus no projects can be added to or removed from the specified space.
freezeForSamples
Same as freeze() plus no samples can be added to or removed from the specified space.
## Project

Freezing method
## Description
freeze
The specified project can not be deleted.
The description can not be set or changed.
No attachments can be added or removed.
freezeForExperiments
Same as freeze() plus no experiments can be added to or removed from the specified project.
freezeForSamples
Same as freeze() plus no samples can be added to or removed from the specified project.
## Experiment

Freezing method
## Description
freeze
The specified experiment can not be deleted.
No properties can be added, removed or modified.
No attachments can be added or removed.
freezeForSamples
Same as freeze() plus no samples can be added to or removed from the specified experiment.
freezeForDataSets
Same as freeze() plus no data sets can be added to or removed from the specified experiment.
## Sample

Freezing method
## Description
freeze
The specified sample can not be deleted.
No properties can be added, removed or modified.
No attachments can be added or removed.
freezeForComponents
Same as freeze() plus no component samples can be added to or removed from the specified sample.
freezeForChildren
Same as freeze() plus no child samples can be added to or removed from the specified sample.
freezeForParents
Same as freeze() plus no parent samples can be added to or removed from the specified sample.
freezeForDataSets
Same as freeze() plus no data sets can be added to or removed from the specified sample.
Data Set

Freezing method
## Description
freeze
The specified data set can not be deleted.
No properties can be added, removed or modified.
Content copies can be still added or removed for frozen link data sets.
freezeForChildren
Same as freeze() plus no child data sets can be added to or removed from the specified data set.
freezeForParents
Same as freeze() plus no parent data sets can be added to or removed from the specified data set.
freezeForComponents
Same as freeze() plus no component data sets can be added to or removed from the specified data set.
freezeForContainers
Same as freeze() plus no container data sets can be added to or removed from the specified data set.
Searching entities

The methods for searching entities in V3 API are called:
searchSpaces
,
searchProjects
,
searchExperiments
,
searchSamples
,
searchDataSets
,
searchMaterials
,
searchVocabularyTerms,
searchTags
,
searchGlobally
.
They all take criteria and fetch options objects as an input. The
criteria object allows you to specify what entities you are looking for.
For instance, only entities from a given space, entities of a given
type, entities with a property X that equals Y and much much more.
The fetch options object allows you to tell the API which parts of the
entities found should be fetched and returned as a result of the method
call. For instance, you can tell the API to return the results only with
properties because this is all what you will need for your processing.
This gives you a very fine grained control over how much data you
actually fetch from the server. The less you ask for via fetch options
the less data the API has to load from the database and the less data it
will have to transfer over the network. Therefore by default, the fetch
options object is empty, i.e. it tells the API only to fetch the basic
information about a given entity, i.e. its id, attributes and creation
and registration dates. If you want to fetch anything more then you have
to let the API know via fetch options which parts you are also
interested in.
Another functionality that the fetch options object provides is
pagination (see FetchOptions.from(Integer) and
FetchOptions.count(Integer) methods). With pagination a user can control
if a search method shall return all found results or just a given
subrange. This is especially useful for handling very large numbers of
results e.g. when we want to build a UI to present them. In such a
situation, we can perform the search that returns only the first batch
of results (e.g. the first 100) for the UI to be responsive and ask for
another batch only if a user requests that (e.g. via clicking on a next
page button in the UI). The pagination is available in all the search
methods including the global search (i.e. searchGlobally method). A code
example on how to use the pagination methods is presented below.
Apart from the pagination the fetch options also provides the means to
sort the results (see FetchOptions.sortBy() method). What fields can be
used for sorting depends on the search method and the returned objects.
Results can be sorted ascending or descending. Sorting by multiple
fields is also possible (e.g. first sort by type and then by
identifier). A code example on how to use sorting is presented below.
## Example

V3SearchExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria
;
public
class
V3SearchExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// search for samples that are in space with code MY_SPACE_CODE and are of sample type with code MY_SAMPLE_TYPE_CODE
SampleSearchCriteria
criteria
=
new
SampleSearchCriteria
();
criteria
.
withSpace
().
withCode
().
thatEquals
(
## ""MY_SPACE_CODE""
);
criteria
.
withType
().
withCode
().
thatEquals
(
## ""MY_SAMPLE_TYPE_CODE""
);
// tell the API to fetch properties for each returned sample
SampleFetchOptions
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withProperties
();
SearchResult
<
## Sample
>
result
=
v3
.
searchSamples
(
sessionToken
,
criteria
,
fetchOptions
);
for
(
## Sample
sample
## :
result
.
getObjects
())
{
// because we asked for properties via fetch options we can access them here, otherwise NotFetchedException would be thrown by getProperties method
## System
.
out
.
println
(
## ""Sample ""
+
sample
.
getIdentifier
()
+
"" has properties: ""
+
sample
.
getProperties
());
}
}
}
V3SearchExample.html
<
script
>
require
([
""as/dto/sample/search/SampleSearchCriteria""
,
""as/dto/sample/fetchoptions/SampleFetchOptions""
],
function
(
SampleSearchCriteria
,
SampleFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// search for samples that are in space with code MY_SPACE_CODE and are of sample type with code MY_SAMPLE_TYPE_CODE
var
criteria
=
new
SampleSearchCriteria
();
criteria
.
withSpace
().
withCode
().
thatEquals
(
## ""MY_SPACE_CODE""
);
criteria
.
withType
().
withCode
().
thatEquals
(
## ""MY_SAMPLE_TYPE_CODE""
);
// tell the API to fetch properties for each returned sample
var
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withProperties
();
v3
.
searchSamples
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
sample
)
{
// because we asked for properties via fetch options we can access them here, otherwise NotFetchedException would be thrown by getProperties method
alert
(
## ""Sample ""
+
sample
.
getIdentifier
()
+
"" has properties: ""
+
## JSON
.
stringify
(
sample
.
getProperties
()));
});
});
});
</
script
>
Example with pagination and sorting

V3SearchWithPaginationAndSortingExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria
;
public
class
V3SearchWithPaginationAndSortingExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleSearchCriteria
criteria
=
new
SampleSearchCriteria
();
SampleFetchOptions
fetchOptions
=
new
SampleFetchOptions
();
// get the first 100 results
fetchOptions
.
from
(
0
);
fetchOptions
.
count
(
100
);
// sort the results first by a type (ascending) and then by an identifier (descending)
fetchOptions
.
sortBy
().
type
().
asc
();
fetchOptions
.
sortBy
().
identifier
().
desc
();
SearchResult
<
## Sample
>
result
=
v3
.
searchSamples
(
sessionToken
,
criteria
,
fetchOptions
);
// because of the pagination the list contains only the first 100 objects (or even less if there are fewer results found)
## System
.
out
.
println
(
result
.
getObjects
());
// returns the number of all found results (i.e. potentially more than 100)
## System
.
out
.
println
(
result
.
getTotalCount
());
}
}
V3SearchWithPaginationAndSortingExample.html
<
script
>
require
([
""as/dto/sample/search/SampleSearchCriteria""
,
""as/dto/sample/fetchoptions/SampleFetchOptions""
],
function
(
SampleSearchCriteria
,
SampleFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
criteria
=
new
SampleSearchCriteria
();
var
fetchOptions
=
new
SampleFetchOptions
();
// get the first 100 results
fetchOptions
.
from
(
0
);
fetchOptions
.
count
(
100
);
// sort the results first by a type (ascending) and then by an identifier (descending)
fetchOptions
.
sortBy
().
type
().
asc
();
fetchOptions
.
sortBy
().
identifier
().
desc
();
v3
.
searchSamples
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
// because of pagination the list contains only the first 100 objects (or even less if there are fewer results found)
console
.
log
(
result
.
getObjects
());
// returns the number of all found results (i.e. potentially more than 100)
console
.
log
(
result
.
getTotalCount
());
});
});
</
script
>
Example with OR operator

By default all specified search criteria have to be fulfilled. If only
one criteria needs to be fulfilled use
criteria.withOrOperator()
as in
the following example:
V3SearchWithOrOperatorExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria
;
public
class
V3SearchWithOrOperatorExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// search for samples that are either in space with code MY_SPACE_CODE or of sample type with code MY_SAMPLE_TYPE_CODE
SampleSearchCriteria
criteria
=
new
SampleSearchCriteria
();
criteria
.
withOrOperator
();
criteria
.
withSpace
().
withCode
().
thatEquals
(
## ""MY_SPACE_CODE""
);
criteria
.
withType
().
withCode
().
thatEquals
(
## ""MY_SAMPLE_TYPE_CODE""
);
// tell the API to fetch the type for each returned sample
SampleFetchOptions
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withType
();
SearchResult
<
## Sample
>
result
=
v3
.
searchSamples
(
sessionToken
,
criteria
,
fetchOptions
);
for
(
## Sample
sample
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
## ""Sample ""
+
sample
.
getIdentifier
()
+
"" [""
+
sample
.
getType
().
getCode
()
+
""]""
);
}
}
}
V3SearchWithOrOperatorExample.html
<
script
>
require
([
""as/dto/sample/search/SampleSearchCriteria""
,
""as/dto/sample/fetchoptions/SampleFetchOptions""
],
function
(
SampleSearchCriteria
,
SampleFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// search for samples that are in space with code MY_SPACE_CODE and are of sample type with code MY_SAMPLE_TYPE_CODE
var
criteria
=
new
SampleSearchCriteria
();
criteria
.
withOrOperator
();
criteria
.
withSpace
().
withCode
().
thatEquals
(
## ""MY_SPACE_CODE""
);
criteria
.
withType
().
withCode
().
thatEquals
(
## ""MY_SAMPLE_TYPE_CODE""
);
// tell the API to fetch type for each returned sample
var
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withType
();
v3
.
searchSamples
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
sample
)
{
alert
(
## ""Sample ""
+
sample
.
getIdentifier
()
+
"" [""
+
sample
.
getType
().
getCode
()
+
""]""
);
});
});
});
</
script
>
Example with nested logical operators

The following code finds samples with perm ID that ends with “6” AND
(with code that contains “-” OR that starts with “C”) AND (with
experiment OR of type whose code starts with “MASTER”).
V3SearchWithNestedLogicalOperatorsExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria
;
public
class
V3SearchWithRecursiveFetchOptionsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleSearchCriteria
criteria
=
new
SampleSearchCriteria
().
withAndOperator
();
criteria
.
withPermId
().
thatEndsWith
(
""6""
);
SampleSearchCriteria
subcriteria1
=
criteria
.
withSubcriteria
().
withOrOperator
();
subcriteria1
.
withCode
().
thatContains
(
""-""
);
subcriteria1
.
withCode
().
thatStartsWith
(
## ""C""
);
SampleSearchCriteria
subcriteria2
=
criteria
.
withSubcriteria
().
withOrOperator
();
subcriteria2
.
withExperiment
();
subcriteria2
.
withType
().
withCode
().
thatStartsWith
(
## ""MASTER""
);
// tell the API to fetch all descendents for each returned sample
SampleFetchOptions
fetchOptions
=
new
SampleFetchOptions
();
SearchResult
<
## Sample
>
result
=
v3
.
searchSamples
(
sessionToken
,
criteria
,
fetchOptions
);
for
(
## Sample
sample
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
## ""Sample ""
+
sample
.
getIdentifier
()
+
"" [""
+
sample
.
getType
().
getCode
()
+
""]""
);
}
}
}
V3SearchWithNestedLogicalOperatorsExample.html
<
script
>
require
([
""as/dto/sample/search/SampleSearchCriteria""
,
""as/dto/sample/fetchoptions/SampleFetchOptions""
],
function
(
SampleSearchCriteria
,
SampleFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
criteria
=
new
SampleSearchCriteria
().
withAndOperator
();
criteria
.
withPermId
().
thatEndsWith
(
""6""
);
var
subcriteria1
=
criteria
.
withSubcriteria
().
withOrOperator
();
subcriteria1
.
withCode
().
thatContains
(
""-""
);
subcriteria1
.
withCode
().
thatStartsWith
(
## ""C""
);
var
subcriteria2
=
criteria
.
withSubcriteria
().
withOrOperator
();
subcriteria2
.
withExperiment
();
subcriteria2
.
withType
().
withCode
().
thatStartsWith
(
## ""MASTER""
);
// tell the API to fetch type for each returned sample
var
fetchOptions
=
new
SampleFetchOptions
();
v3
.
searchSamples
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
sample
)
{
alert
(
## ""Sample ""
+
sample
.
getIdentifier
()
+
"" [""
+
sample
.
getType
().
getCode
()
+
""]""
);
});
});
});
</
script
>
Example with recursive fetch options

In order to get all descendent/acsendents of a sample fetch options can
be used recursively by
using
fetchOptions.withChildrenUsing(fetchOptions)
as in the following
## example:
V3SearchWithRecursiveFetchOptionsExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria
;
public
class
V3SearchWithRecursiveFetchOptionsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleSearchCriteria
criteria
=
new
SampleSearchCriteria
();
criteria
.
withType
().
withCode
().
thatEquals
(
## ""MY_SAMPLE_TYPE_CODE""
);
// tell the API to fetch all descendents for each returned sample
SampleFetchOptions
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withChildrenUsing
(
fetchOptions
);
SearchResult
<
## Sample
>
result
=
v3
.
searchSamples
(
sessionToken
,
criteria
,
fetchOptions
);
for
(
## Sample
sample
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
## ""Sample ""
+
renderWithDescendants
(
sample
));
}
}
private
static
## String
renderWithDescendants
(
## Sample
sample
)
{
StringBuilder
builder
=
new
StringBuilder
();
for
(
## Sample
child
## :
sample
.
getChildren
())
{
if
(
builder
.
length
()
>
0
)
{
builder
.
append
(
"", ""
);
}
builder
.
append
(
renderWithDescendants
(
child
));
}
if
(
builder
.
length
()
==
0
)
{
return
sample
.
getCode
();
}
return
sample
.
getCode
()
+
"" -> (""
+
builder
.
toString
()
+
"")""
;
}
}
V3SearchWithRecursiveFetchOptionsExample.html
<
script
>
require
([
""as/dto/sample/search/SampleSearchCriteria""
,
""as/dto/sample/fetchoptions/SampleFetchOptions""
],
function
(
SampleSearchCriteria
,
SampleFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
criteria
=
new
SampleSearchCriteria
();
criteria
.
withType
().
withCode
().
thatEquals
(
## ""MY_SAMPLE_TYPE_CODE""
);
// tell the API to fetch all descendents for each returned sample
var
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withChildrenUsing
(
fetchOptions
);
v3
.
searchSamples
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
sample
)
{
alert
(
## ""Sample ""
+
renderWithDescendants
(
sample
));
});
});
function
renderWithDescendants
(
sample
)
{
var
children
=
sample
.
getChildren
();
var
list
=
""""
;
for
(
var
i
=
0
;
i
<
children
.
length
;
i
++
)
{
if
(
list
.
length
>
0
)
{
list
+=
"", ""
;
}
list
+=
renderWithDescendants
(
children
[
i
]);
}
if
(
children
.
length
==
0
)
{
return
sample
.
getCode
();
}
return
sample
.
getCode
()
+
"" -> (""
+
list
+
"")""
}
});
</
script
>
Global search

There are two kinds or global search:
Using thatContains() and thatContainsExactly() methods of
GlobalSearchTextCriteria. This type of search performs the substring
search in any field of any entity.
Using thatMatches() method of GlobalSearchTextCriteria. This type of
search performs lexical match using English dictionaly. If a
matching string is not a word it is matched as a whole (i.e. code
will match code only if a whole code string is provided).
Global search searches for experiments, samples, data sets and materials
by specifying a text snippet (or complete words) to be found in any type
of meta data (entity attribute or property). Example:
V3GlobalSearchExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.global.GlobalSearchObject
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.global.fetchoptions.GlobalSearchObjectFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.global.search.GlobalSearchCriteria
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.global.search.GlobalSearchObjectKind
;
public
class
V3GlobalSearchExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// search for any text matching 'default' but only among samples
GlobalSearchCriteria
criteria
=
new
GlobalSearchCriteria
();
criteria
.
withObjectKind
().
thatIn
(
GlobalSearchObjectKind
.
## SAMPLE
);
criteria
.
withText
().
thatMatches
(
""default""
);
// Fetch also the sample type
GlobalSearchObjectFetchOptions
fetchOptions
=
new
GlobalSearchObjectFetchOptions
();
fetchOptions
.
withSample
().
withType
();
SearchResult
<
GlobalSearchObject
>
result
=
v3
.
searchGlobally
(
sessionToken
,
criteria
,
fetchOptions
);
for
(
GlobalSearchObject
object
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
object
.
getObjectKind
()
+
"": ""
+
object
.
getObjectIdentifier
()
+
"" [""
+
object
.
getSample
().
getType
().
getCode
()
+
""], score:""
+
object
.
getScore
()
+
"", match:""
+
object
.
getMatch
());
}
}
}
V3GlobalSearchExample.html
<
script
>
require
([
""as/dto/global/search/GlobalSearchCriteria""
,
""as/dto/global/search/GlobalSearchObjectKind""
,
""as/dto/global/fetchoptions/GlobalSearchObjectFetchOptions""
],
function
(
GlobalSearchCriteria
,
GlobalSearchObjectKind
,
GlobalSearchObjectFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// search for any text matching 'default' but only among samples
var
criteria
=
new
GlobalSearchCriteria
();
criteria
.
withObjectKind
().
thatIn
([
GlobalSearchObjectKind
.
## SAMPLE
]);
criteria
.
withText
().
thatMatches
(
""default""
);
// Fetch also the sample type
var
fetchOptions
=
new
GlobalSearchObjectFetchOptions
();
fetchOptions
.
withSample
().
withType
();
v3
.
searchGlobally
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
object
)
{
alert
(
object
.
getObjectKind
()
+
"": ""
+
object
.
getObjectIdentifier
()
+
"" [""
+
object
.
getSample
().
getType
().
getCode
()
+
""], score:""
+
object
.
getScore
()
+
"", match:""
+
object
.
getMatch
());
});
});
});
</
script
>
Getting entities

The methods for getting entities in V3 API are called: getSpaces,
getProjects, getExperiments, getSamples, getDataSets, getMaterials,
getVocabularyTerms, getTags. They all take a list of entity ids and
fetch options as an input (please check “Searching entities” section for
more details on the fetch options). They return a map where the passed
entity ids become the keys and values are the entities found for these
ids. If no entity was found for a given id or entity exists but you
don’t have access to it then there is no entry for such an id in the
returned map.
## Example

V3GetExample.java
import
java.util.Arrays
;
import
java.util.Map
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.ISampleId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SamplePermId
;
public
class
V3GetExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
ISampleId
id1
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
);
ISampleId
id2
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE_2""
);
ISampleId
id3
=
new
SamplePermId
(
""20160115170726679-98669""
);
// perm id of sample /MY_SPACE_CODE/MY_SAMPLE_CODE
ISampleId
id4
=
new
SamplePermId
(
""20160118115737079-98672""
);
// perm id of sample /MY_SPACE_CODE/MY_SAMPLE_CODE_3
ISampleId
id5
=
new
SamplePermId
(
## ""I_DONT_EXIST""
);
SampleFetchOptions
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withProperties
();
## Map
<
ISampleId
,
## Sample
>
map
=
v3
.
getSamples
(
sessionToken
,
## Arrays
.
asList
(
id1
,
id2
,
id3
,
id4
,
id5
),
fetchOptions
);
map
.
get
(
id1
);
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE
map
.
get
(
id2
);
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_2
map
.
get
(
id3
);
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE
map
.
get
(
id4
);
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_3
map
.
get
(
id5
);
// returns null
}
}
V3GetExample.html
<
script
>
require
([
""as/dto/sample/id/SampleIdentifier""
,
""as/dto/sample/id/SamplePermId""
,
""as/dto/sample/fetchoptions/SampleFetchOptions""
],
function
(
SampleIdentifier
,
SamplePermId
,
SampleFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
id1
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
);
var
id2
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE_2""
);
var
id3
=
new
SamplePermId
(
""20160115170726679-98669""
);
// perm id of sample /MY_SPACE_CODE/MY_SAMPLE_CODE
var
id4
=
new
SamplePermId
(
""20160118115737079-98672""
);
// perm id of sample   /MY_SPACE_CODE/MY_SAMPLE_CODE_3
var
id5
=
new
SamplePermId
(
## ""I_DONT_EXIST""
);
var
fetchOptions
=
new
SampleFetchOptions
();
fetchOptions
.
withProperties
();
v3
.
getSamples
([
id1
,
id2
,
id3
,
id4
,
id5
],
fetchOptions
).
done
(
function
(
map
)
{
map
[
id1
];
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE
map
[
id2
];
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_2
map
[
id3
];
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE
map
[
id4
];
// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_3
map
[
id5
];
// returns null
});
});
</
script
>
Deleting entities

The methods for deleting entities in V3 API are called: deleteSpaces,
deleteProjects, deleteExperiments, deleteSamples, deleteDataSets,
deleteMaterials, deleteVocabularyTerms, deleteTags. The delete methods
for spaces, projects, materials, vocabulary terms, tags perform a
permanent deletion (there is no trash can for these entities - deletion
cannot be reverted). The delete methods for experiments, samples and
data sets perform a logical deletion (move entities to the trash can)
and return a deletion id. This deletion id can be used for either
confirming the logical deletion to remove the entities permanently or
reverting the logical deletion to take the entities out from the trash
can.
## Example

V3DeleteExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.deletion.id.IDeletionId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.delete.SampleDeletionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.ISampleId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier
;
public
class
V3DeleteExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
ISampleId
id1
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
);
ISampleId
id2
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE_2""
);
SampleDeletionOptions
deletionOptions
=
new
SampleDeletionOptions
();
deletionOptions
.
setReason
(
""Testing logical deletion""
);
// logical deletion (move objects to the trash can)
IDeletionId
deletionId
=
v3
.
deleteSamples
(
sessionToken
,
## Arrays
.
asList
(
id1
,
id2
),
deletionOptions
);
// you can use the deletion id to confirm the deletion (permanently delete objects)
v3
.
confirmDeletions
(
sessionToken
,
## Arrays
.
asList
(
deletionId
));
// you can use the deletion id to revert the deletion (get the objects out from the trash can)
v3
.
revertDeletions
(
sessionToken
,
## Arrays
.
asList
(
deletionId
));
}
}
V3DeleteExample.html
<
script
>
require
([
""as/dto/sample/id/SampleIdentifier""
,
""as/dto/sample/delete/SampleDeletionOptions""
],
function
(
SampleIdentifier
,
SampleDeletionOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
id1
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE""
);
var
id2
=
new
SampleIdentifier
(
## ""/MY_SPACE_CODE/MY_SAMPLE_CODE_2""
);
var
deletionOptions
=
new
SampleDeletionOptions
();
deletionOptions
.
setReason
(
""Testing logical deletion""
);
// logical deletion (move objects to the trash can)
v3
.
deleteSamples
([
id1
,
id2
],
deletionOptions
).
done
(
function
(
deletionId
)
{
// you can use the deletion id to confirm the deletion (permanently delete objects)
v3
.
confirmDeletions
([
deletionId
]);
// you can use the deletion id to revert the deletion (get the objects out from the trash can)
v3
.
revertDeletions
([
deletionId
]);
});
});
</
script
>
Searching entity types

The following search methods allows to search for entity types including
all assigned property
## types:
searchDataSetTypes
,
searchExperimentTypes
,
searchMaterialTypes
and
searchSampleTypes
. Here is an example which will search for all
sample types and assigned property types:
V3SearchTypesExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.property.PropertyAssignment
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.SampleType
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleTypeFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleTypeSearchCriteria
;
public
class
V3SearchTypesExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleTypeSearchCriteria
searchCriteria
=
new
SampleTypeSearchCriteria
();
SampleTypeFetchOptions
fetchOptions
=
new
SampleTypeFetchOptions
();
fetchOptions
.
withPropertyAssignments
().
withPropertyType
();
SearchResult
<
SampleType
>
result
=
v3
.
searchSampleTypes
(
sessionToken
,
searchCriteria
,
fetchOptions
);
for
(
SampleType
sampleType
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
sampleType
.
getCode
());
for
(
PropertyAssignment
assignment
## :
sampleType
.
getPropertyAssignments
())
{
## System
.
out
.
println
(
""  ""
+
assignment
.
getPropertyType
().
getCode
()
+
(
assignment
.
isMandatory
()
?
""*""
## :
""""
));
}
}
}
}
V3SearchTypesExample.html
<
script
>
require
([
""as/dto/sample/search/SampleTypeSearchCriteria""
,
""as/dto/sample/fetchoptions/SampleTypeFetchOptions""
],
function
(
SampleTypeSearchCriteria
,
SampleTypeFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// here we are interested only in the last updates of samples and projects
var
criteria
=
new
SampleTypeSearchCriteria
();
var
fetchOptions
=
new
SampleTypeFetchOptions
();
fetchOptions
.
withPropertyAssignments
().
withPropertyType
();
v3
.
searchSampleTypes
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
sampleType
)
{
var
msg
=
sampleType
.
getCode
();
var
assignments
=
sampleType
.
getPropertyAssignments
();
for
(
var
i
=
0
;
i
<
assignments
.
length
;
i
++
)
{
msg
+=
""\n  ""
+
assignments
[
i
].
getPropertyType
().
getCode
();
}
alert
(
msg
);
});
});
});
</
script
>
## Modifications

The API allows to ask for the latest modification (UPDATE or
CREATE_OR_DELETE) for groups of objects of various kinds (see
class
ch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.ObjectKind
for
a complete list). This feature of the openBIS API helps GUI clients to
update views automatically. Here is an example which asks for the latest
project and sample update:
V3SearchObjectKindModificationsExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.ObjectKind
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.ObjectKindModification
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.OperationKind
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.fetchoptions.ObjectKindModificationFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.search.ObjectKindModificationSearchCriteria
;
public
class
V3SearchObjectKindModificationsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// here we are interested only in the last updates of samples and projects
ObjectKindModificationSearchCriteria
criteria
=
new
ObjectKindModificationSearchCriteria
();
criteria
.
withObjectKind
().
thatIn
(
ObjectKind
.
## PROJECT
,
ObjectKind
.
## SAMPLE
);
criteria
.
withOperationKind
().
thatIn
(
OperationKind
.
## UPDATE
);
ObjectKindModificationFetchOptions
fetchOptions
=
new
ObjectKindModificationFetchOptions
();
SearchResult
<
ObjectKindModification
>
result
=
v3
.
searchObjectKindModifications
(
sessionToken
,
criteria
,
fetchOptions
);
for
(
ObjectKindModification
modification
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
""The last ""
+
modification
.
getOperationKind
()
+
"" of an entity of kind ""
+
modification
.
getObjectKind
()
+
"" occured at ""
+
modification
.
getLastModificationTimeStamp
());
}
}
}
V3SearchObjectKindModificationsExample.html
<
script
>
require
([
""as/dto/objectkindmodification/search/ObjectKindModificationSearchCriteria""
,
""as/dto/objectkindmodification/ObjectKind""
,
""as/dto/objectkindmodification/OperationKind""
,
""as/dto/objectkindmodification/fetchoptions/ObjectKindModificationFetchOptions""
],
function
(
ObjectKindModificationSearchCriteria
,
ObjectKind
,
OperationKind
,
ObjectKindModificationFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
// here we are interested only in the last updates of samples and projects
var
criteria
=
new
ObjectKindModificationSearchCriteria
();
criteria
.
withObjectKind
().
thatIn
([
ObjectKind
.
## PROJECT
,
ObjectKind
.
## SAMPLE
]);
criteria
.
withOperationKind
().
thatIn
([
OperationKind
.
## UPDATE
]);
var
fetchOptions
=
new
ObjectKindModificationFetchOptions
();
v3
.
searchObjectKindModifications
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
modification
)
{
alert
(
""The last ""
+
modification
.
getOperationKind
()
+
"" of an entity of kind ""
+
modification
.
getObjectKind
()
+
"" occured at ""
+
modification
.
getLastModificationTimeStamp
());
});
});
});
</
script
>
Custom AS Services

In order to extend openBIS API new custom services can be established by core plugins of type
services
(see
## Custom Application Server Services
). The API offers a method to search for a service and to execute a service.
Search for custom services

As with any other search method
searchCustomASServices()
needs a search criteria
CustomASServiceSearchCriteria
and fetch options
CustomASServiceFetchOptions
. The following example returns all available custom AS services.
## Example

V3SearchCustomASServicesExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.service.CustomASService
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.service.fetchoptions.CustomASServiceFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.service.search.CustomASServiceSearchCriteria
;
public
class
V3SearchCustomASServicesExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
CustomASServiceSearchCriteria
criteria
=
new
CustomASServiceSearchCriteria
();
CustomASServiceFetchOptions
fetchOptions
=
new
CustomASServiceFetchOptions
();
SearchResult
<
CustomASService
>
result
=
v3
.
searchCustomASServices
(
sessionToken
,
criteria
,
fetchOptions
);
for
(
CustomASService
service
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
service
.
getCode
()
+
"": ""
+
service
.
getLabel
()
+
"" (""
+
service
.
getDescription
()
+
"")""
);
}
}
}
V3SearchCustomASServicesExample.html
<
script
>
require
([
""as/dto/service/search/CustomASServiceSearchCriteria""
,
""as/dto/service/fetchoptions/CustomASServiceFetchOptions""
],
function
(
CustomASServiceSearchCriteria
,
CustomASServiceFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
criteria
=
new
CustomASServiceSearchCriteria
();
var
fetchOptions
=
new
CustomASServiceFetchOptions
();
v3
.
searchCustomASServices
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
service
)
{
alert
(
service
.
getCode
()
+
"": ""
+
service
.
getLabel
()
+
"" (""
+
service
.
getDescription
()
+
"")""
);
});
});
});
</
script
>
Execute a custom service

In order to execute a custom AS service its code is needed. In addition
a set of key-value pairs can be provided. The key has to be a string
whereas the value can be any object. Note, that in case of Java the
object has be an instance of class which Java serializable. The
key-value pairs are added to
CustomASServiceExecutionOptions
object by
invoking
withParameter()
for each pair.
The result can be any object (again it has to be Java serializable in
the Java case). In a Java client the result will usually be casted for
further processing.
## Example

V3ExecuteCustomASServiceExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.service.CustomASServiceExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.service.id.CustomASServiceCode
;
public
class
V3ExecuteCustomASServiceExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
CustomASServiceCode
id
=
new
CustomASServiceCode
(
""example-service""
);
CustomASServiceExecutionOptions
options
=
new
CustomASServiceExecutionOptions
().
withParameter
(
""space-code""
,
## ""TEST""
);
## Object
result
=
v3
.
executeCustomASService
(
sessionToken
,
id
,
options
);
## System
.
out
.
println
(
## ""Result: ""
+
result
);
}
}
V3ExecuteCustomASServiceExample.html
<
script
>
require
([
""as/dto/service/id/CustomASServiceCode""
,
""as/dto/service/CustomASServiceExecutionOptions""
],
function
(
CustomASServiceCode
,
CustomASServiceExecutionOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
id
=
new
CustomASServiceCode
(
""example-service""
);
var
options
=
new
CustomASServiceExecutionOptions
().
withParameter
(
""space-code""
,
## ""TEST""
);
v3
.
executeCustomASService
(
id
,
options
).
done
(
function
(
result
)
{
alert
(
result
);
});
});
</
script
>
Archiving / unarchiving data sets

The API provides the following methods for handling the data set
archiving: archiveDataSets and unarchiveDataSets. Both methods schedule
the operation to be executed asynchronously, i.e. once
archiveDataSets/unarchiveDataSets method call finishes the requested
data sets are only scheduled for the archiving/unarchiving but are not
in the archive/store yet.
Archiving data sets

## Example

V3ArchiveDataSetsExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.archive.DataSetArchiveOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.IDataSetId
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3ArchiveDataSetsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
IDataSetId
id1
=
new
DataSetPermId
(
""20160524154020607-2266""
);
IDataSetId
id2
=
new
DataSetPermId
(
""20160524154020607-2267""
);
DataSetArchiveOptions
options
=
new
DataSetArchiveOptions
();
// With removeFromDataStore flag set to true data sets are moved to the archive.
// With removeFromDataStore flag set to false data sets are copied to the archive.
// Default value is true (move to the archive).
options
.
setRemoveFromDataStore
(
false
);
// Schedules archiving of the specified data sets. Archiving itself is executed asynchronously.
v3
.
archiveDataSets
(
sessionToken
,
## Arrays
.
asList
(
id1
,
id2
),
options
);
## System
.
out
.
println
(
""Archiving scheduled""
);
}
}
V3ArchiveDataSetsExample.html
<
script
>
require
([
""openbis""
,
""as/dto/dataset/id/DataSetPermId""
,
""as/dto/dataset/archive/DataSetArchiveOptions""
],
function
(
openbis
,
DataSetPermId
,
DataSetArchiveOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
id1
=
new
DataSetPermId
(
""20160524154020607-2266""
)
var
id2
=
new
DataSetPermId
(
""20160524154020607-2267""
)
var
options
=
new
DataSetArchiveOptions
();
// With removeFromDataStore flag set to true data sets are moved to the archive.
// With removeFromDataStore flag set to false data sets are copied to the archive.
// Default value is true (move to the archive).
options
.
setRemoveFromDataStore
(
false
);
// Schedules archiving of the specified data sets. Archiving itself is executed asynchronously.
v3
.
archiveDataSets
([
id1
,
id2
],
options
).
done
(
function
()
{
alert
(
""Archiving scheduled""
);
});
});
});
</
script
>
Unarchiving data sets

## Example

V3UnarchiveDataSetsExample.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.IDataSetId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.unarchive.DataSetUnarchiveOptions
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3UnarchiveDataSetsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
IDataSetId
id1
=
new
DataSetPermId
(
""20160524154020607-2266""
);
IDataSetId
id2
=
new
DataSetPermId
(
""20160524154020607-2267""
);
DataSetUnarchiveOptions
options
=
new
DataSetUnarchiveOptions
();
// Schedules unarchiving of the specified data sets. Unarchiving itself is executed asynchronously.
v3
.
unarchiveDataSets
(
sessionToken
,
## Arrays
.
asList
(
id1
,
id2
),
options
);
## System
.
out
.
println
(
""Unarchiving scheduled""
);
}
}
V3UnarchiveDataSetsExample.html
<
script
>
require
([
""openbis""
,
""as/dto/dataset/id/DataSetPermId""
,
""as/dto/dataset/unarchive/DataSetUnarchiveOptions""
],
function
(
openbis
,
DataSetPermId
,
DataSetUnarchiveOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
id1
=
new
DataSetPermId
(
""20160524154020607-2266""
)
var
id2
=
new
DataSetPermId
(
""20160524154020607-2267""
)
var
options
=
new
DataSetUnarchiveOptions
();
// Schedules unarchiving of the specified data sets. Unarchiving itself is executed asynchronously.
v3
.
unarchiveDataSets
([
id1
,
id2
],
options
).
done
(
function
()
{
alert
(
""Unarchiving scheduled""
);
});
});
});
</
script
>
## Executing Operations

The V3 API provides you with methods that allow you to create, update,
get, search and delete entities, archive and unarchive datasets, execute
custom services and much more. With these methods you can
programmatically access most of the openBIS features to build your own
webapps, dropboxes or services. Even though these methods are quite
different, there are some things that they all have in common:
each method is executed in its own separate transaction
each method is executed synchronously
Let’s think about what it really means. Separate transactions make two
(even subsequent) method calls completely unrelated. For instance, when
you make a call to create experiments and then another call to create
samples, then even if the sample creation fails the experiments, that
had been already created, would remain in the system. Most of the time
this is exactly what we want but not always. There are times when we
would like to create either both experiments and samples or nothing if
something is wrong. A good example would be an import of some file that
contains both experiments and samples. We would like to be able to
import the file, fail if it is wrong, correct the file and import it
again. With separate transactions we would end up with some things
already created after the first failed import and we wouldn’t be able to
reimport the corrected file again as some things would be already in the
system.
Synchronous method execution is also something what we expect most of
the time. You call a method and it returns once all the work is done.
For instance, when we call a method to create samples we know that once
the method finishes all the samples have been created in the
system. This makes perfect sense when we need to execute operations that
depend on each other, e.g. we can create data sets and attach them to
samples only after the samples had been created. Just as with the
separate transactions, there are cases when synchronous method execution
is limiting. Let’s use the file import example again. What would happen
if a file we wanted to import contained hundreds of thousands of
entities? The import would probably take a very long time. Our
synchronous method call would not return until all the entities have
been created which means we would also block a script/program that makes
this method call for a very long time. We could of course create a
separate thread in our script/program to overcome this problem but that
would add up more complexity. It would be also nice to notify a user
once such an operation finishes or fails, e.g. by sending an email.
Unfortunately that would mean we have to keep our script/program running
until the operation finishes or fails to send such an email. What about
a progress information for running executions or a history of previous
operations and their results? That would be nice but it would increase
the complexity of our script/program even more.
## Therefore, if you want to:
execute multiple operations in a single transaction
execute operations asynchronously
monitor progress of operations
receive notifications about finished/failed operations
keep history of operations and their results
## you should use:
executeOperations method to execute your operations
getOperationExecutions and searchOperationExecutions methods to
retrieve information about operation executions (e.g. progress,
results or errors)
updateOperationExecutions and deleteOperationExecutions methods to
control what information should be still kept for a given operation
execution and what information can be already removed
More details on each of these methods in presented in the sections
below. Please note that all of the described methods are available in
both Javascript and Java.
Method executeOperations

This method can be used to execute one or many operations either
synchronously or asynchronously. Operations are always executed in a
single transaction (a failure of a single operation triggers a rollback
of all the operations). The executeOperations method can be used to
execute any of the IApplicationServerApi methods (except for
login/logout and executeOperations itself), i.e. for each
IApplicationServerApi method there is a corresponding operation class
(class that implements IOperation interface). For instance,
IApplicationServerApi.createSpaces method is represented by
CreateSpacesOperation class, IApplicationServerApi.updateSpaces method
by UpdateSpacesOperation class etc.
Asynchronous operation execution

An asynchronous executeOperations invocation only schedules operations
for the execution and then immediately returns. Results of the scheduled
operations can be retrieved later with getOperationExecutions or
searchOperationExecutions methods.
Because the operations are scheduled to be executed later (in a separate
thread) a regular try/catch block around executeOperations method will
only catch exceptions related with scheduling the operations for the
execution, but NOT the exceptions thrown by the operations during the
execution. To check for errors that occurred during the execution please
use getOperationExecutions and searchOperationExecutions methods once
the execution finishes.
In order to execute operations asynchronously, executeOperations has to
be used with AsynchronousOperationExecutionOptions. With such options,
the method returns AsynchronousOperationExecutionResults object.
AsynchronousOperationExecutionResults object contains automatically
generated executionId that can be used for retrieving additional
information about the execution, fetching the results or errors.
During its life an asynchronous execution goes through the following
## states:
NEW - execution has been just created with executeOperations method
SCHEDULED - execution has been added to a thread pool queue and is
waiting for a free thread
RUNNING - execution has been picked from a thread pool queue by a
free thread and is currently executing
FINISHED/FAILED - if execution finishes successfully then execution
state is changed to FINISHED, if anything goes wrong it is changed
to FAILED
V3ExecuteOperationsAsynchronous.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3ExecuteOperationsAsynchronous
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
CreateSamplesOperation
operation
=
new
CreateSamplesOperation
(
sample
);
AsynchronousOperationExecutionResults
results
=
(
AsynchronousOperationExecutionResults
)
v3
.
executeOperations
(
sessionToken
,
## Arrays
.
asList
(
operation
),
new
AsynchronousOperationExecutionOptions
());
## System
.
out
.
println
(
""Execution id: ""
+
results
.
getExecutionId
());
}
}
V3ExecuteOperationsAsynchronous.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/sample/create/CreateSamplesOperation""
,
""as/dto/operation/AsynchronousOperationExecutionOptions""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
CreateSamplesOperation
,
AsynchronousOperationExecutionOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
var
operation
=
new
CreateSamplesOperation
([
sample
]);
v3
.
executeOperations
([
operation
],
new
AsynchronousOperationExecutionOptions
()).
done
(
function
(
results
)
{
console
.
log
(
""Execution id: ""
+
results
.
getExecutionId
());
});
});
</
script
>
Synchronous operation execution

A synchronous executeOperations invocation immediately executes all the
operations. Any exceptions thrown by the executed operations can be
caught with a regular try/catch block around executeOperations method.
In order to execute operations synchronously, executeOperations has to
be used with SynchronousOperationExecutionOptions. With such options,
the method returns SynchronousOperationExecutionResults object.
SynchronousOperationExecutionResults object contains the results for all
the executed operations.
In contrast to the asynchronous version, the synchronous call requires
executionId to be explicitly set in SynchronousOperationExecutionOptions
for the additional information to be gathered about the execution.
During its life a synchronous execution goes through the following
## states:
NEW - execution has been just created with executeOperations method
RUNNING - execution is being executed by the same thread as
executeOperations method
FINISHED/FAILED - if execution finishes successfully then execution
state is changed to FINISHED, if anything goes wrong it is changed
to FAILED
V3ExecuteOperationsSynchronous.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.SynchronousOperationExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.SynchronousOperationExecutionResults
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperationResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3ExecuteOperationsSynchronous
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
CreateSamplesOperation
operation
=
new
CreateSamplesOperation
(
sample
);
SynchronousOperationExecutionResults
results
=
(
SynchronousOperationExecutionResults
)
v3
.
executeOperations
(
sessionToken
,
## Arrays
.
asList
(
operation
),
new
SynchronousOperationExecutionOptions
());
CreateSamplesOperationResult
result
=
(
CreateSamplesOperationResult
)
results
.
getResults
().
get
(
0
);
## System
.
out
.
println
(
""Sample id: ""
+
result
.
getObjectIds
());
}
}
V3ExecuteOperationsSynchronous.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/sample/create/CreateSamplesOperation""
,
""as/dto/operation/SynchronousOperationExecutionOptions""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
CreateSamplesOperation
,
SynchronousOperationExecutionOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
var
operation
=
new
CreateSamplesOperation
([
sample
]);
v3
.
executeOperations
([
operation
],
new
SynchronousOperationExecutionOptions
()).
done
(
function
(
results
)
{
var
result
=
results
.
getResults
()[
0
];
console
.
log
(
""Sample id: ""
+
result
.
getObjectIds
());
});
});
</
script
>
## Notifications

The executeOperations method can notify about finished or failed
operation executions. At the moment the only supported notification
method is email (OperationExecutionEmailNotification).
For successfully finished executions an email contains:
execution id
execution description
list of operation summaries and operation results
For failed executions an email contains:
execution id
execution description
list of operation summaries
error
V3ExecuteOperationsEmailNotification.java
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecutionEmailNotification
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3ExecuteOperationsEmailNotification
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
CreateSamplesOperation
operation
=
new
CreateSamplesOperation
(
sample
);
AsynchronousOperationExecutionOptions
options
=
new
AsynchronousOperationExecutionOptions
();
options
.
setNotification
(
new
OperationExecutionEmailNotification
(
""my@email1.com""
,
""my@email2.com""
));
AsynchronousOperationExecutionResults
results
=
(
AsynchronousOperationExecutionResults
)
v3
.
executeOperations
(
sessionToken
,
## Arrays
.
asList
(
operation
),
options
);
## System
.
out
.
println
(
""Execution id: ""
+
results
.
getExecutionId
());
}
}
V3ExecuteOperationsEmailNotification.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/sample/create/CreateSamplesOperation""
,
""as/dto/operation/AsynchronousOperationExecutionOptions""
,
""as/dto/operation/OperationExecutionEmailNotification""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
CreateSamplesOperation
,
AsynchronousOperationExecutionOptions
,
OperationExecutionEmailNotification
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
var
operation
=
new
CreateSamplesOperation
([
sample
]);
var
options
=
new
AsynchronousOperationExecutionOptions
();
options
.
setNotification
(
new
OperationExecutionEmailNotification
([
""my@email1.com""
,
""my@email2.com""
]));
v3
.
executeOperations
([
operation
],
options
).
done
(
function
(
results
)
{
console
.
log
(
""Execution id: ""
+
results
.
getExecutionId
());
});
});
</
script
>
Method getOperationExecutions / searchOperationExecutions

Operation execution information can be fetched by an owner of an
execution (i.e. a person that called executeOperations method) or an
admin. Both getOperationExecutions and searchOperationExecutions methods
work similar to the other get/search methods in the V3 API.
The operation execution information that both methods return can be
divided into 3 categories:
basic information (code, state, owner, description, creationDate,
startDate, finishDate etc.)
summary information (summary of operations, progress, error,
results)
detailed information (details of operations, progress, error,
results)
Each category can have a different availability time (i.e. time for how
long a given information is stored in the system). The availability
times can be set via the executeOperations method options (both
SynchronousOperationExecutionOptions and
## AsynchronousOperationExecutionOptions):
basic information (setAvailabilityTime)
summary information (setSummaryAvailabilityTime)
detailed information (setDetailsAvailabilityTime)
If the times are not explicitly set, then the following defaults are
## used:
basic information (1 year)
summary information (1 month)
detailed information (1 day)
The current availability of each category can be checked with
getAvailability, getSummaryAvailability, getDetailsAvailability methods
of OperationExecution class. The availability can have one of the
## following values:
AVAILABLE - an information is available and can be fetched
DELETE_PENDING - an explicit request to delete the information has
been made with updateOperationExecutions or
deleteOperationExecutions method
DELETED - an explicit request to delete the information has been
processed and the information has been deleted
TIME_OUT_PENDING - an availability time has expired, the
information has been scheduled to be removed
TIMED_OUT - an availability time has expired, the information has
been removed
Update of availability values and deletion of operation execution
related information are done with two separate V3 maintenance tasks
(please check service.properties for their configuration).
V3GetOperationExecutionsAsynchronous.java
import
java.util.Arrays
;
import
java.util.Map
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3GetOperationExecutionsAsynchronous
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
CreateSamplesOperation
operation
=
new
CreateSamplesOperation
(
sample
);
// Asynchronous execution: information about an asynchronous operation execution is always gathered, the executionId
// is also always automatically generated and returned with AsynchronousOperationExecutionResults.
AsynchronousOperationExecutionOptions
options
=
new
AsynchronousOperationExecutionOptions
();
// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.
// Availability times should be specified in seconds.
options
.
setAvailabilityTime
(
30
*
24
*
60
*
60
);
// one month
options
.
setSummaryAvailabilityTime
(
24
*
60
*
60
);
// one day
options
.
setDetailsAvailabilityTime
(
60
*
60
);
// one hour
// Execute operation
AsynchronousOperationExecutionResults
results
=
(
AsynchronousOperationExecutionResults
)
v3
.
executeOperations
(
sessionToken
,
## Arrays
.
asList
(
operation
),
options
);
// It is an asynchronous execution. It might be still waiting for a free thread,
// it may be already executing or it may have already finished. It does not matter.
// We can already fetch the information about it.
// Specify what information to fetch about the execution
OperationExecutionFetchOptions
fo
=
new
OperationExecutionFetchOptions
();
fo
.
withSummary
();
fo
.
withSummary
().
withOperations
();
fo
.
withSummary
().
withProgress
();
fo
.
withSummary
().
withResults
();
fo
.
withSummary
().
withError
();
fo
.
withDetails
();
fo
.
withDetails
().
withOperations
();
fo
.
withDetails
().
withProgress
();
fo
.
withDetails
().
withResults
();
fo
.
withDetails
().
withError
();
// Get information about the execution
## Map
<
IOperationExecutionId
,
OperationExecution
>
executions
=
v3
.
getOperationExecutions
(
sessionToken
,
## Arrays
.
asList
(
results
.
getExecutionId
()),
fo
);
OperationExecution
execution
=
executions
.
get
(
results
.
getExecutionId
());
// Summary contains String representation of operations, progress, results and error
## String
summaryOperation
=
execution
.
getSummary
().
getOperations
().
get
(
0
);
## System
.
out
.
println
(
""Summary.operation: ""
+
summaryOperation
);
## System
.
out
.
println
(
""Summary.progress: ""
+
execution
.
getSummary
().
getProgress
());
## System
.
out
.
println
(
""Summary.results: ""
+
execution
.
getSummary
().
getResults
());
## System
.
out
.
println
(
""Summary.error: ""
+
execution
.
getSummary
().
getError
());
// Details contain object representation of operations, progress, results and error
CreateSamplesOperation
detailsOperation
=
(
CreateSamplesOperation
)
execution
.
getDetails
().
getOperations
().
get
(
0
);
## System
.
out
.
println
(
""Details.operation: ""
+
detailsOperation
);
## System
.
out
.
println
(
""Details.progress: ""
+
execution
.
getSummary
().
getProgress
());
## System
.
out
.
println
(
""Details.results: ""
+
execution
.
getSummary
().
getResults
());
## System
.
out
.
println
(
""Details.error: ""
+
execution
.
getSummary
().
getError
());
}
}
V3GetOperationExecutionsAsynchronous.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/sample/create/CreateSamplesOperation""
,
""as/dto/operation/AsynchronousOperationExecutionOptions""
,
""as/dto/operation/fetchoptions/OperationExecutionFetchOptions""
,
""as/dto/operation/id/OperationExecutionPermId""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
CreateSamplesOperation
,
AsynchronousOperationExecutionOptions
,
OperationExecutionFetchOptions
,
OperationExecutionPermId
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
var
operation
=
new
CreateSamplesOperation
([
sample
]);
// Asynchronous execution: information about an asynchronous operation execution is always gathered, the executionId
// is also always automatically generated and returned with AsynchronousOperationExecutionResults.
var
options
=
new
AsynchronousOperationExecutionOptions
();
// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.
// Availability times should be specified in seconds.
options
.
setAvailabilityTime
(
30
*
24
*
60
*
60
);
// one month
options
.
setSummaryAvailabilityTime
(
24
*
60
*
60
);
// one day
options
.
setDetailsAvailabilityTime
(
60
*
60
);
// one hour
// Execute operation
v3
.
executeOperations
([
operation
],
options
).
done
(
function
(
results
)
{
// It is an asynchronous execution. It might be still waiting for a free thread,
// it may be already executing or it may have already finished. It does not matter.
// We can already fetch the information about it.
// Specify what information to fetch about the execution
var
fo
=
new
OperationExecutionFetchOptions
();
fo
.
withSummary
();
fo
.
withSummary
().
withOperations
();
fo
.
withSummary
().
withProgress
();
fo
.
withSummary
().
withResults
();
fo
.
withSummary
().
withError
();
fo
.
withDetails
();
fo
.
withDetails
().
withOperations
();
fo
.
withDetails
().
withProgress
();
fo
.
withDetails
().
withResults
();
fo
.
withDetails
().
withError
();
// Get information about the execution
v3
.
getOperationExecutions
([
results
.
getExecutionId
()
],
fo
).
done
(
function
(
executions
)
{
var
execution
=
executions
[
results
.
getExecutionId
()];
// Summary contains String representation of operations, progress, results and error
var
summaryOperation
=
execution
.
getSummary
().
getOperations
()[
0
];
console
.
log
(
""Summary.operation: ""
+
summaryOperation
);
console
.
log
(
""Summary.progress: ""
+
execution
.
getSummary
().
getProgress
());
console
.
log
(
""Summary.results: ""
+
execution
.
getSummary
().
getResults
());
console
.
log
(
""Summary.error: ""
+
execution
.
getSummary
().
getError
());
// Details contain object representation of operations, progress, results and error
var
detailsOperation
=
execution
.
getDetails
().
getOperations
()[
0
];
console
.
log
(
""Details.operation: ""
+
detailsOperation
);
console
.
log
(
""Details.progress: ""
+
execution
.
getSummary
().
getProgress
());
console
.
log
(
""Details.results: ""
+
execution
.
getSummary
().
getResults
());
console
.
log
(
""Details.error: ""
+
execution
.
getSummary
().
getError
());
});
});
});
</
script
>
V3GetOperationExecutionsSynchronous.java
import
java.util.Arrays
;
import
java.util.Map
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.SynchronousOperationExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.OperationExecutionPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3GetOperationExecutionsSynchronous
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE_7""
);
CreateSamplesOperation
operation
=
new
CreateSamplesOperation
(
sample
);
// Synchronous execution: to gather information about a synchronous operation execution, the executionId has to
// be explicitly set in the options object. OperationExecutionPermId created with no-argument constructor automatically
// generates a random permId value.
SynchronousOperationExecutionOptions
options
=
new
SynchronousOperationExecutionOptions
();
options
.
setExecutionId
(
new
OperationExecutionPermId
());
// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.
// Availability times should be specified in seconds.
options
.
setAvailabilityTime
(
30
*
24
*
60
*
60
);
// one month
options
.
setSummaryAvailabilityTime
(
24
*
60
*
60
);
// one day
options
.
setDetailsAvailabilityTime
(
60
*
60
);
// one hour
// Execute operation
v3
.
executeOperations
(
sessionToken
,
## Arrays
.
asList
(
operation
),
options
);
// Specify what information to fetch about the execution
OperationExecutionFetchOptions
fo
=
new
OperationExecutionFetchOptions
();
fo
.
withSummary
();
fo
.
withSummary
().
withOperations
();
fo
.
withSummary
().
withProgress
();
fo
.
withSummary
().
withResults
();
fo
.
withSummary
().
withError
();
fo
.
withDetails
();
fo
.
withDetails
().
withOperations
();
fo
.
withDetails
().
withProgress
();
fo
.
withDetails
().
withResults
();
fo
.
withDetails
().
withError
();
// Get information about the execution
## Map
<
IOperationExecutionId
,
OperationExecution
>
executions
=
v3
.
getOperationExecutions
(
sessionToken
,
## Arrays
.
asList
(
options
.
getExecutionId
()),
fo
);
OperationExecution
execution
=
executions
.
get
(
options
.
getExecutionId
());
// Summary contains String representation of operations, progress, results and error
## String
summaryOperation
=
execution
.
getSummary
().
getOperations
().
get
(
0
);
## System
.
out
.
println
(
""Summary.operation: ""
+
summaryOperation
);
## System
.
out
.
println
(
""Summary.progress: ""
+
execution
.
getSummary
().
getProgress
());
## System
.
out
.
println
(
""Summary.results: ""
+
execution
.
getSummary
().
getResults
());
## System
.
out
.
println
(
""Summary.error: ""
+
execution
.
getSummary
().
getError
());
// Details contain object representation of operations, progress, results and error
CreateSamplesOperation
detailsOperation
=
(
CreateSamplesOperation
)
execution
.
getDetails
().
getOperations
().
get
(
0
);
## System
.
out
.
println
(
""Details.operation: ""
+
detailsOperation
);
## System
.
out
.
println
(
""Details.progress: ""
+
execution
.
getSummary
().
getProgress
());
## System
.
out
.
println
(
""Details.results: ""
+
execution
.
getSummary
().
getResults
());
## System
.
out
.
println
(
""Details.error: ""
+
execution
.
getSummary
().
getError
());
}
}
V3GetOperationExecutionsSynchronous.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/sample/create/CreateSamplesOperation""
,
""as/dto/operation/SynchronousOperationExecutionOptions""
,
""as/dto/operation/fetchoptions/OperationExecutionFetchOptions""
,
""as/dto/operation/id/OperationExecutionPermId""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
CreateSamplesOperation
,
SynchronousOperationExecutionOptions
,
OperationExecutionFetchOptions
,
OperationExecutionPermId
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
var
operation
=
new
CreateSamplesOperation
([
sample
]);
// Synchronous execution: to gather information about a synchronous operation execution, the executionId has to
// be explicitly set in the options object. OperationExecutionPermId created with no-argument constructor automatically
// generates a random permId value.
var
options
=
new
SynchronousOperationExecutionOptions
();
options
.
setExecutionId
(
new
OperationExecutionPermId
());
// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.
// Availability times should be specified in seconds.
options
.
setAvailabilityTime
(
30
*
24
*
60
*
60
);
// one month
options
.
setSummaryAvailabilityTime
(
24
*
60
*
60
);
// one day
options
.
setDetailsAvailabilityTime
(
60
*
60
);
// one hour
// Execute operation
v3
.
executeOperations
([
operation
],
options
).
done
(
function
()
{
// Specify what information to fetch about the execution
var
fo
=
new
OperationExecutionFetchOptions
();
fo
.
withSummary
();
fo
.
withSummary
().
withOperations
();
fo
.
withSummary
().
withProgress
();
fo
.
withSummary
().
withResults
();
fo
.
withSummary
().
withError
();
fo
.
withDetails
();
fo
.
withDetails
().
withOperations
();
fo
.
withDetails
().
withProgress
();
fo
.
withDetails
().
withResults
();
fo
.
withDetails
().
withError
();
// Get information about the execution
v3
.
getOperationExecutions
([
options
.
getExecutionId
()
],
fo
).
done
(
function
(
executions
)
{
var
execution
=
executions
[
options
.
getExecutionId
()];
// Summary contains String representation of operations, progress, results and error
var
summaryOperation
=
execution
.
getSummary
().
getOperations
()[
0
];
console
.
log
(
""Summary.operation: ""
+
summaryOperation
);
console
.
log
(
""Summary.progress: ""
+
execution
.
getSummary
().
getProgress
());
console
.
log
(
""Summary.results: ""
+
execution
.
getSummary
().
getResults
());
console
.
log
(
""Summary.error: ""
+
execution
.
getSummary
().
getError
());
// Details contain object representation of operations, progress, results and error
var
detailsOperation
=
execution
.
getDetails
().
getOperations
()[
0
];
console
.
log
(
""Details.operation: ""
+
detailsOperation
);
console
.
log
(
""Details.progress: ""
+
execution
.
getSummary
().
getProgress
());
console
.
log
(
""Details.results: ""
+
execution
.
getSummary
().
getResults
());
console
.
log
(
""Details.error: ""
+
execution
.
getSummary
().
getError
());
});
});
});
</
script
>
Method updateOperationExecutions / deleteOperationExecutions

The updateOperationExecutions and deleteOperationExecutions methods can
be used to explicitly delete some part of information or delete all the
information about a given operation execution before a corresponding
availability time expires.
V3UpdateOperationExecutions.java
import
java.util.Arrays
;
import
java.util.Map
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.update.OperationExecutionUpdate
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3UpdateOperationExecutions
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
CreateSamplesOperation
operation
=
new
CreateSamplesOperation
(
sample
);
AsynchronousOperationExecutionOptions
options
=
new
AsynchronousOperationExecutionOptions
();
// Execute operation
AsynchronousOperationExecutionResults
results
=
(
AsynchronousOperationExecutionResults
)
v3
.
executeOperations
(
sessionToken
,
## Arrays
.
asList
(
operation
),
options
);
// You can explicitly request a deletion of summary or details. Here we want to delete details.
OperationExecutionUpdate
update
=
new
OperationExecutionUpdate
();
update
.
setExecutionId
(
results
.
getExecutionId
());
update
.
deleteDetails
();
v3
.
updateOperationExecutions
(
sessionToken
,
## Arrays
.
asList
(
update
));
// Let's check the execution information
OperationExecutionFetchOptions
fo
=
new
OperationExecutionFetchOptions
();
fo
.
withSummary
();
fo
.
withDetails
();
## Map
<
IOperationExecutionId
,
OperationExecution
>
executions
=
v3
.
getOperationExecutions
(
sessionToken
,
## Arrays
.
asList
(
results
.
getExecutionId
()),
fo
);
OperationExecution
execution
=
executions
.
get
(
results
.
getExecutionId
());
// Summary availability is AVAILABLE. Details availability is either DELETE_PENDING or DELETED
// depending on whether a maintenance task has already processed the deletion request.
## System
.
out
.
println
(
## ""Summary: ""
+
execution
.
getSummary
());
## System
.
out
.
println
(
""Summary.availability: ""
+
execution
.
getSummaryAvailability
());
## System
.
out
.
println
(
## ""Details: ""
+
execution
.
getDetails
());
## System
.
out
.
println
(
""Details.availability: ""
+
execution
.
getDetailsAvailability
());
}
}
V3UpdateOperationExecutions.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/sample/create/CreateSamplesOperation""
,
""as/dto/operation/AsynchronousOperationExecutionOptions""
,
""as/dto/operation/update/OperationExecutionUpdate""
,
""as/dto/operation/fetchoptions/OperationExecutionFetchOptions""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
CreateSamplesOperation
,
AsynchronousOperationExecutionOptions
,
OperationExecutionUpdate
,
OperationExecutionFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
var
operation
=
new
CreateSamplesOperation
([
sample
]);
var
options
=
new
AsynchronousOperationExecutionOptions
();
// Execute operation
v3
.
executeOperations
([
operation
],
options
).
done
(
function
(
results
)
{
// You can explicitly request a deletion of summary or details. Here we want to delete details.
var
update
=
new
OperationExecutionUpdate
();
update
.
setExecutionId
(
results
.
getExecutionId
());
update
.
deleteDetails
();
v3
.
updateOperationExecutions
([
update
]).
done
(
function
()
{
// Let's check the execution information
var
fo
=
new
OperationExecutionFetchOptions
();
fo
.
withSummary
();
fo
.
withDetails
();
v3
.
getOperationExecutions
([
results
.
getExecutionId
()
],
fo
).
done
(
function
(
executions
)
{
var
execution
=
executions
[
results
.
getExecutionId
()];
// Summary availability is AVAILABLE. Details availability is either DELETE_PENDING or DELETED
// depending on whether a maintenance task has already processed the deletion request.
console
.
log
(
## ""Summary: ""
+
execution
.
getSummary
());
console
.
log
(
""Summary.availability: ""
+
execution
.
getSummaryAvailability
());
console
.
log
(
## ""Details: ""
+
execution
.
getDetails
());
console
.
log
(
""Details.availability: ""
+
execution
.
getDetailsAvailability
());
});
});
});
});
</
script
>
V3DeleteOperationExecutions.java
import
java.util.Arrays
;
import
java.util.Map
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.delete.OperationExecutionDeletionOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId
;
public
class
V3DeleteOperationExecutions
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
SampleCreation
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
CreateSamplesOperation
operation
=
new
CreateSamplesOperation
(
sample
);
AsynchronousOperationExecutionOptions
options
=
new
AsynchronousOperationExecutionOptions
();
// Execute operation
AsynchronousOperationExecutionResults
results
=
(
AsynchronousOperationExecutionResults
)
v3
.
executeOperations
(
sessionToken
,
## Arrays
.
asList
(
operation
),
options
);
// Explicitly request a deletion of all the information about the execution
OperationExecutionDeletionOptions
deletionOptions
=
new
OperationExecutionDeletionOptions
();
deletionOptions
.
setReason
(
""test reason""
);
v3
.
deleteOperationExecutions
(
sessionToken
,
## Arrays
.
asList
(
results
.
getExecutionId
()),
deletionOptions
);
// Let's check whether the execution information is still available
## Map
<
IOperationExecutionId
,
OperationExecution
>
executions
=
v3
.
getOperationExecutions
(
sessionToken
,
## Arrays
.
asList
(
results
.
getExecutionId
()),
new
OperationExecutionFetchOptions
());
OperationExecution
execution
=
executions
.
get
(
results
.
getExecutionId
());
// Depending on whether a maintenance task has already processed the deletion request
// the execution will be either null or the returned execution availability will be DELETE_PENDING.
## System
.
out
.
println
(
## ""Availability: ""
+
(
execution
!=
null
?
execution
.
getAvailability
()
## :
null
));
}
}
V3DeleteOperationExecutions.html
<
script
>
require
([
""openbis""
,
""as/dto/sample/create/SampleCreation""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/space/id/SpacePermId""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/sample/create/CreateSamplesOperation""
,
""as/dto/operation/AsynchronousOperationExecutionOptions""
,
""as/dto/operation/delete/OperationExecutionDeletionOptions""
,
""as/dto/operation/fetchoptions/OperationExecutionFetchOptions""
],
function
(
openbis
,
SampleCreation
,
EntityTypePermId
,
SpacePermId
,
ExperimentIdentifier
,
CreateSamplesOperation
,
AsynchronousOperationExecutionOptions
,
OperationExecutionDeletionOptions
,
OperationExecutionFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
sample
=
new
SampleCreation
();
sample
.
setTypeId
(
new
EntityTypePermId
(
## ""MY_SAMPLE_TYPE_CODE""
));
sample
.
setSpaceId
(
new
SpacePermId
(
## ""MY_SPACE_CODE""
));
sample
.
setExperimentId
(
new
ExperimentIdentifier
(
""/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE""
));
sample
.
setCode
(
## ""MY_SAMPLE_CODE""
);
var
operation
=
new
CreateSamplesOperation
([
sample
]);
var
options
=
new
AsynchronousOperationExecutionOptions
();
// Execute operation
v3
.
executeOperations
([
operation
],
options
).
done
(
function
(
results
)
{
// Explicitly request a deletion of all the information about the execution
var
deletionOptions
=
new
OperationExecutionDeletionOptions
();
deletionOptions
.
setReason
(
""test reason""
);
v3
.
deleteOperationExecutions
([
results
.
getExecutionId
()
],
deletionOptions
).
done
(
function
()
{
// Let's check whether the execution information is still available
v3
.
getOperationExecutions
([
results
.
getExecutionId
()
],
new
OperationExecutionFetchOptions
()).
done
(
function
(
executions
)
{
var
execution
=
executions
[
results
.
getExecutionId
()];
// Depending on whether a maintenance task has already processed the deletion request
// the execution will be either null or the returned execution availability will be DELETE_PENDING.
console
.
log
(
## ""Availability: ""
+
(
execution
!=
null
?
execution
.
getAvailability
()
## :
null
));
});
});
});
});
</
script
>
### Configuration

Many aspects of the operation execution behavior can be configured via
service.properties file.
More details on what exactly can be configured can be found in the file
itself.
## Semantic Annotations

If terms like: semantic web, RDF, OWL are new to you, then it is highly
recommended to read the following tutorial first:
http://www.linkeddatatools.com/semantic-web-basics
.
In short: semantic annotations allow you to define a meaning for openBIS
sample types, property types and sample property assignments by the
means of ontology terms. This, together with standards like “Dublin
## Core” (
http://dublincore.org/
) can help you integrate openBIS with
other systems and exchange data between them with a well defined meaning
easily.
To describe a meaning of a single sample type, property type or sample
property assignment a collection of semantic annotations can be used.
Therefore, for instance, you can use one annotation to describe a
general meaning of a property and another one to describe a unit that is
used for its values.
In order to make the openBIS configuration easier to maintain sample
property assignments inherit semantic annotations from a corresponding
property type. This inheritance works only for sample property
assignments without any semantic annotations, i.e. if there is at least
one semantic annotation defined at a sample property assignment level
then nothing gets inherited from the property type level anymore. The
inheritance makes it possible to define a meaning of a property once, at
the property type level, and override it, only if needed, at sample
property assignment level.
V3 API provides the following methods to manipulate the semantic
## annotations:
createSemanticAnnotations
updateSemanticAnnotations
deleteSemanticAnnotations
getSemanticAnnotations
searchSemanticAnnotations
These methods work similar to the other create/update/delete/get/search
V3 API counterparts.
Moreover, once semantic annotations are defined, it is possible to
search for samples and sample types that have a given semantic
annotation. To do it, one has to use searchSamples and searchSampleTypes
methods and specify appropriate withType().withSemanticAnnotations()
condition in SampleSearchCriteria or withSemanticAnnotations() condition
in SampleTypeSearchCriteria.
## Web App Settings

The web app settings functionality is a user specific key-value map
where a user specific configuration can be stored. The settings are
persistent, i.e. they can live longer than a user session that created
them. Web app settings of a given user can be read/updated only by that
user or by an instance admin.
WebAppSettingsExample.java
import
java.util.Arrays
;
import
java.util.Map
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.person.Person
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.person.fetchoptions.PersonFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.person.id.IPersonId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.person.id.Me
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.person.update.PersonUpdate
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.WebAppSetting
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.create.WebAppSettingCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.fetchoptions.WebAppSettingsFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.update.WebAppSettingsUpdateValue
;
public
class
WebAppSettingsExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
PersonUpdate
update
=
new
PersonUpdate
();
// update the currently logged in user
update
.
setUserId
(
new
## Me
());
// add ""setting1a"" and ""setting1b"" to ""app1"" (other settings for ""app1"" will remain unchanged)
WebAppSettingsUpdateValue
app1
=
update
.
getWebAppSettings
(
""app1""
);
app1
.
add
(
new
WebAppSettingCreation
(
""setting1a""
,
""value1a""
));
app1
.
add
(
new
WebAppSettingCreation
(
""setting1b""
,
""value1b""
));
// set ""setting2a"", ""setting2b"" and ""setting2c"" for ""app2"" (other settings for ""app2"" will be removed)
WebAppSettingsUpdateValue
app2
=
update
.
getWebAppSettings
(
""app2""
);
app2
.
set
(
new
WebAppSettingCreation
(
""setting2a""
,
""value2a""
),
new
WebAppSettingCreation
(
""setting2b""
,
""value2b""
),
new
WebAppSettingCreation
(
""setting2c""
,
""value2c""
));
// remove ""setting3a"" from ""app3"" (other settings for ""app3"" will remain unchanged)
WebAppSettingsUpdateValue
app3
=
update
.
getWebAppSettings
(
""app3""
);
app3
.
remove
(
""setting3a""
);
v3
.
updatePersons
(
sessionToken
,
## Arrays
.
asList
(
update
));
// option 1 : fetch a person with all settings of all web apps
PersonFetchOptions
personFo1
=
new
PersonFetchOptions
();
personFo1
.
withAllWebAppSettings
();
// option 2 : fetch a person with either all or chosen settings of chosen web apps
PersonFetchOptions
personFo2
=
new
PersonFetchOptions
();
// option 2a : fetch ""app1"" with all settings
WebAppSettingsFetchOptions
app1Fo
=
personFo2
.
withWebAppSettings
(
""app1""
);
app1Fo
.
withAllSettings
();
// option 2b : fetch ""app2"" with chosen settings
WebAppSettingsFetchOptions
app2Fo
=
personFo2
.
withWebAppSettings
(
""app2""
);
app2Fo
.
withSetting
(
""setting2a""
);
app2Fo
.
withSetting
(
""setting2b""
);
## Map
<
IPersonId
,
## Person
>
persons
=
v3
.
getPersons
(
sessionToken
,
## Arrays
.
asList
(
new
## Me
()),
personFo2
);
## Person
person
=
persons
.
values
().
iterator
().
next
();
// get ""setting1a"" for ""app1""
WebAppSetting
setting1a
=
person
.
getWebAppSettings
(
""app1""
).
getSetting
(
""setting1a""
);
## System
.
out
.
println
(
setting1a
.
getValue
());
// get all fetched settings for ""app2""
## Map
<
## String
,
WebAppSetting
>
settings2
=
person
.
getWebAppSettings
(
""app2""
).
getSettings
();
## System
.
out
.
println
(
settings2
);
}
}
WebAppSettingsExample.html
<
script
>
require
([
""jquery""
,
""openbis""
,
""as/dto/person/update/PersonUpdate""
,
""as/dto/person/id/Me""
,
""as/dto/webapp/create/WebAppSettingCreation""
,
""as/dto/person/fetchoptions/PersonFetchOptions""
],
function
(
$
,
openbis
,
PersonUpdate
,
## Me
,
WebAppSettingCreation
,
PersonFetchOptions
)
{
$
(
document
).
ready
(
function
()
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
update
=
new
PersonUpdate
();
// update the currently logged in user
update
.
setUserId
(
new
## Me
());
// add ""setting1a"" and ""setting1b"" to ""app1"" (other settings for ""app1"" will remain unchanged)
var
app1
=
update
.
getWebAppSettings
(
""app1""
);
app1
.
add
(
new
WebAppSettingCreation
(
""setting1a""
,
""value1a""
));
app1
.
add
(
new
WebAppSettingCreation
(
""setting1b""
,
""value1b""
));
// set ""setting2a"", ""setting2b"" and ""setting2c"" for ""app2"" (other settings for ""app2"" will be removed)
var
app2
=
update
.
getWebAppSettings
(
""app2""
);
app2
.
set
([
new
WebAppSettingCreation
(
""setting2a""
,
""value2a""
),
new
WebAppSettingCreation
(
""setting2b""
,
""value2b""
),
new
WebAppSettingCreation
(
""setting2c""
,
""value2c""
)
]);
// remove ""setting3a"" from ""app3"" (other settings for ""app3"" will remain unchanged)
var
app3
=
update
.
getWebAppSettings
(
""app3""
);
app3
.
remove
(
""setting3a""
);
v3
.
updatePersons
([
update
]).
done
(
function
()
{
// option 1 : fetch a person with all settings of all web apps
var
personFo1
=
new
PersonFetchOptions
();
personFo1
.
withAllWebAppSettings
();
// option 2 : fetch a person with either all or chosen settings of chosen web apps
var
personFo2
=
new
PersonFetchOptions
();
// option 2a : fetch ""app1"" with all settings
var
app1Fo
=
personFo2
.
withWebAppSettings
(
""app1""
);
app1Fo
.
withAllSettings
();
// option 2b : fetch ""app2"" with chosen settings
var
app2Fo
=
personFo2
.
withWebAppSettings
(
""app2""
);
app2Fo
.
withSetting
(
""setting2a""
);
app2Fo
.
withSetting
(
""setting2b""
);
v3
.
getPersons
([
new
## Me
()
],
personFo2
).
done
(
function
(
persons
)
{
var
person
=
persons
[
new
## Me
()];
// get ""setting1a"" for ""app1""
var
setting1a
=
person
.
getWebAppSettings
(
""app1""
).
getSetting
(
""setting1a""
);
console
.
log
(
setting1a
.
getValue
());
// get all fetched settings for ""app2""
var
settings2
=
person
.
getWebAppSettings
(
""app2""
).
getSettings
();
console
.
log
(
settings2
);
});
});
});
});
</
script
>
## Imports

The imports that are normally accesible via “Import” menu in the generic
openBIS UI can be also used programatically from within a V3 custom AS
service. Such an import process consists of two steps:
uploading a file to /openbis/upload servlet to be temporarily stored
under a specific user session key (more information on the upload
servlet can be found
here
)
importing the uploaded file using one
of ch.ethz.sis.openbis.generic.asapi.v3.plugin.service.IImportService
methods accessible from within a V3 custom AS service
## Currently available import methods:
String createExperiments(String sessionToken, String uploadKey,
String experimentTypeCode, boolean async, String userEmail)
String updateExperiments(String sessionToken, String uploadKey,
String experimentTypeCode, boolean async, String userEmail)
String createSamples(String sessionToken, String uploadKey, String
sampleTypeCode, String defaultSpaceIdentifier, String
spaceIdentifierOverride, String experimentIdentifierOverride,
boolean updateExisting, boolean async, String userEmail)
String updateSamples(String sessionToken, String uploadKey, String
sampleTypeCode, String defaultSpaceIdentifier, String
spaceIdentifierOverride, String experimentIdentifierOverride,
boolean async, String userEmail)
String updateDataSets(String sessionToken, String uploadKey, String
dataSetTypeCode, boolean async, String userEmail)
String createMaterials(String sessionToken, String uploadKey, String
materialTypeCode, boolean updateExisting, boolean async, String
userEmail)
String updateMaterials(String sessionToken, String uploadKey, String
materialTypeCode, boolean ignoreUnregistered, boolean async, String
userEmail)
String generalImport(String sessionToken, String uploadKey, String
defaultSpaceIdentifier, boolean updateExisting,
boolean async, String userEmail) - import of samples and materials
from an Excel file
String customImport(String sessionToken, String uploadKey, String
customImportCode, boolean async, String userEmail) - import
delegated to a dropbox
## Parameters:
## Parameter
## Type
## Methods
## Description
sessionToken
## String
## ALL
openBIS session token; to get a session token of a currently logged in user inside a custom AS service context.getSessionToken() method shall be used.
uploadKey
## String
## ALL
A key the file to be imported has been uploaded to (see the 1st step of the import process described above).
async
boolean
## ALL
A flag that controls whether the import should be performed synchronously (i.e. in the current thread) or asynchronously (i.e. in a separate thread). For asynchronous imports an email with either an execution result or error is sent to the specified email address (see userEmail parameter).
userEmail
## String
## ALL
An email address where an execution result or error should be sent to (only for asynchronous imports - see async parameter).
experimentTypeCode
## String
createExperiments, updateExperiments
A type of experiments to be created/updated.
sampleTypeCode
## String
createSamples, updateSamples
A type of samples to be created/updated.
dataSetTypeCode
## String
updateDataSets
A type of data sets to be updated.
materialTypeCode
## String
createMaterials, updateMaterials
A type of materials to be created/updated.
customImportCode
## String
customImport
A code of a custom import the import process should be delegated to. A custom import sends the uploaded file to a dropbox. Inside a dropbox the uploaded file can be accessed via transaction.getIncoming() method.
defaultSpaceIdentifier
## String
createSamples, updateSamples, generalImport
A default space identifier. If null then identifiers of samples to be created/updated are expected to be specified in the uploaded file. If not null then:
codes of samples to be created are automatically generated and the samples are created in the requested default space
identifiers of samples to be updated can omit the space part (the requested default space will be automatically added)
spaceIdentifierOverride
## String
createSamples, updateSamples
A space identifier to be used instead of the ones defined in the uploaded file.
experimentIdentifierOverride
## String
createSamples, updateSamples
An experiment identifier to be used instead of the ones defined in the uploaded file.
updateExisting
boolean
createSamples, createMaterials, generalImport
A flag that controlls whether in case of an attempt to create an already existing entity an update should be performed or such a creation should fail.
ignoreUnregistered
boolean
updateMaterials
A flag that controlls whether in case of an attempt to update a nonexistent entity such update should be silently ignored or it should fail.
## File formats:
The TSV examples below assume experiment/sample/dataset/material type
used contains exactly one property called “DESCRIPTION”.
## Method
## Template
createExperiments
create-experiments-import-template.tsv
updateExperiments
update-experiments-import-template.tsv
createSamples
create-samples-import-template.tsv
updateSamples
update-samples-import-template.tsv
updateDataSets
update-data-sets-import-template.tsv
createMaterials
create-materials-import-template.tsv
updateMaterials
update-materials-import-template.tsv
generalImport
customImport
any kind of file
## Return values:
All methods return a message with a short summary of the performed
operation, e.g. a synchronous createSamples method call could return a
message like “Registration of 1 sample(s) is complete.” while the
asynchronous version could return a message like “When the import is
complete the confirmation or failure report will be sent by email.”.
An example webapp to upload a file with samples and a custom AS service
to import that file is presented below.
ImportSamplesWebAppExample.html
<!DOCTYPE html>
<
html
>
<
head
>
<
meta
charset
=
""utf-8""
>
<
title
>
Samples import
</
title
>
<
script
type
=
""text/javascript""
src
=
""/openbis-test/resources/api/v3/config.js""
></
script
>
<
script
type
=
""text/javascript""
src
=
""/openbis-test/resources/api/v3/require.js""
></
script
>
</
head
>
<
body
>
<
script
>
require
([
""jquery""
,
""openbis""
,
""as/dto/service/id/CustomASServiceCode""
,
""as/dto/service/CustomASServiceExecutionOptions""
],
function
(
$
,
openbis
,
CustomASServiceCode
,
CustomASServiceExecutionOptions
)
{
$
(
document
).
ready
(
function
()
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
uploadFrame
=
$
(
""#uploadFrame""
);
uploadFrame
.
load
(
function
()
{
alert
(
""Upload finished""
)
});
var
uploadForm
=
$
(
""#uploadForm""
);
uploadForm
.
find
(
""input[name=sessionID]""
).
val
(
sessionToken
);
var
importForm
=
$
(
""#importForm""
);
importForm
.
submit
(
function
(
e
)
{
e
.
preventDefault
();
var
sampleType
=
importForm
.
find
(
""input[name=sampleType]""
).
val
();
var
serviceId
=
new
CustomASServiceCode
(
""import-service""
);
var
serviceOptions
=
new
CustomASServiceExecutionOptions
();
serviceOptions
.
withParameter
(
""sampleType""
,
sampleType
);
facade
.
executeCustomASService
(
serviceId
,
serviceOptions
).
done
(
function
(
result
)
{
alert
(
""Import successful: ""
+
result
);
}).
fail
(
function
(
error
)
{
alert
(
""Import failed: ""
+
error
.
message
);
});
return
false
;
});
});
});
</
script
>
<
iframe
id
=
""uploadFrame""
name
=
""uploadFrame""
style
=
""display: none""
></
iframe
>
<
h1
>
Step 1 : upload samples file
</
h1
>
<
form
id
=
""uploadForm""
method
=
""post""
action
=
""/openbis/upload""
enctype
=
""multipart/form-data""
target
=
""uploadFrame""
>
<
input
type
=
""file""
name
=
""importWebappUploadKey""
multiple
=
""multiple""
>
<
input
type
=
""hidden""
name
=
""sessionID""
>
<
input
type
=
""hidden""
name
=
""sessionKeysNumber""
value
=
""1""
>
<
input
type
=
""hidden""
name
=
""sessionKey_0""
value
=
""importWebappUploadKey""
>
<
input
type
=
""submit""
>
</
form
>
<
h1
>
Step 2 : import samples file
</
h1
>
<
form
id
=
""importForm""
>
<
label
>
## Sample Type
</
label
>
<
input
type
=
""text""
name
=
""sampleType""
>
<
input
type
=
""submit""
>
</
form
>
</
body
>
</
html
>
ImportSamplesServiceExample.py
def
process
(
context
,
parameters
## ):
sampleType
=
parameters
.
get
(
""sampleType""
)
return
context
.
getImportService
()
.
createSamples
(
context
.
getSessionToken
(),
""importWebappUploadKey""
,
sampleType
,
## None
,
## None
,
## None
,
## False
,
## False
,
## None
);
### Generate identifiers
V3 API provides 2 methods for generating unique identifiers:
createPermIdStrings - generates globally unique identifiers that
consist of a timestamp and a sequence generated number (e.g.
“20180531170854641-944”); this method uses one global sequence.
createCodes - generates identifiers that are unique for a given
entity kind and consist of a prefix and a sequence generated number
(e.g. “MY-PREFIX-147”); this method uses a dedicated sequence for
each entity kind.
GenerateIdentifiersExample.java
import
java.util.List
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.EntityKind
;
public
class
GenerateIdentifiersExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
## List
<
## String
>
permIds
=
v3
.
createPermIdStrings
(
sessionToken
,
2
);
## List
<
## String
>
codes
=
v3
.
createCodes
(
sessionToken
,
## ""MY-PREFIX-""
,
EntityKind
.
## SAMPLE
,
3
);
## System
.
out
.
println
(
permIds
);
// example output: [20180531170854641-944, 20180531170854641-945]
## System
.
out
.
println
(
codes
);
// example output: [MY-PREFIX-782, MY-PREFIX-783, MY-PREFIX-784]
}
}
GenerateIdentifiersExample.html
<
script
>
require
([
""jquery""
,
""openbis""
,
""as/dto/entitytype/EntityKind""
],
function
(
$
,
openbis
,
EntityKind
)
{
$
(
document
).
ready
(
function
()
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
v3
.
createPermIdStrings
(
2
).
then
(
function
(
permIds
)
{
console
.
log
(
permIds
);
// example output: [20180531170854641-944, 20180531170854641-945]
});
v3
.
createCodes
(
## ""MY-PREFIX-""
,
EntityKind
.
## SAMPLE
,
3
).
then
(
function
(
codes
)
{
console
.
log
(
codes
);
// example output: [MY-PREFIX-782, MY-PREFIX-783, MY-PREFIX-784]
});
});
});
</
script
>
V. DSS Methods

Search files

The searchFiles method can be used to search for data set files at a
single data store (Java version) or at multiple data stores at the same
time (Javascript version).
Similar to the other V3 search methods it takes as parameters a
sessionToken, search criteria and fetch options and returns a search
result object.
When searching across multiple data stores the results from each data
store are combined together and returned back as a single regular search
result object as if it was returned by only one data store.
## Example

V3SearchDataSetFilesExample.java
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.search.DataSetSearchCriteria
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.DataSetFile
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fetchoptions.DataSetFileFetchOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.search.DataSetFileSearchCriteria
;
public
class
V3SearchDataSetFilesExample
{
public
static
void
main
(
## String
[]
args
)
{
// we assume here that v3 objects for both AS and DSS have been already created and we have already called login on AS to get the sessionToken (please check ""Accessing the API"" section for more details)
DataSetFileSearchCriteria
criteria
=
new
DataSetFileSearchCriteria
();
DataSetSearchCriteria
dataSetCriteria
=
criteria
.
withDataSet
().
withOrOperator
();
dataSetCriteria
.
withCode
().
thatEquals
(
## ""MY_DATA_SET_CODE_1""
);
dataSetCriteria
.
withCode
().
thatEquals
(
## ""MY_DATA_SET_CODE_2""
);
// Searches for files at at a single data store
SearchResult
<
DataSetFile
>
result
=
dssV3
.
searchFiles
(
sessionToken
,
criteria
,
new
DataSetFileFetchOptions
());
for
(
DataSetFile
file
## :
result
.
getObjects
())
{
## System
.
out
.
println
(
""DataSet: ""
+
file
.
getDataSetPermId
()
+
"" has file: ""
+
file
.
getPath
());
}
}
}
V3SearchDataSetFilesAtAllDataStoresExample.html
<
script
>
require
([
""openbis""
,
""dss/dto/datasetfile/search/DataSetFileSearchCriteria""
,
""dss/dto/datasetfile/fetchoptions/DataSetFileFetchOptions""
],
function
(
DataSetFileSearchCriteria
,
DataSetFileFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
criteria
=
new
DataSetFileSearchCriteria
();
var
dataSetCriteria
=
criteria
.
withDataSet
().
withOrOperator
();
dataSetCriteria
.
withCode
().
thatEquals
(
## ""MY_DATA_SET_CODE_1""
);
dataSetCriteria
.
withCode
().
thatEquals
(
## ""MY_DATA_SET_CODE_2""
);
var
fetchOptions
=
new
DataSetFileFetchOptions
();
// getDataStoreFacade() call (without any parameters) returns a facade object that uses all available data stores,
// e.g. calling searchFiles on such a facade searches for files at all available data stores
v3
.
getDataStoreFacade
().
searchFiles
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
file
)
{
console
.
log
(
""DataSet: ""
+
file
.
getDataSetPermId
()
+
"" has file: ""
+
file
.
getPath
());
});
});
});
</
script
>
V3SearchDataSetFilesAtChosenDataStoresExample.html
<
script
>
require
([
""openbis""
,
""dss/dto/datasetfile/search/DataSetFileSearchCriteria""
,
""dss/dto/datasetfile/fetchoptions/DataSetFileFetchOptions""
],
function
(
DataSetFileSearchCriteria
,
DataSetFileFetchOptions
)
{
// we assume here that v3 object has been already created and we have already called login (please check ""Accessing the API"" section for more details)
var
criteria
=
new
DataSetFileSearchCriteria
();
var
dataSetCriteria
=
criteria
.
withDataSet
().
withOrOperator
();
dataSetCriteria
.
withCode
().
thatEquals
(
## ""MY_DATA_SET_CODE_1""
);
dataSetCriteria
.
withCode
().
thatEquals
(
## ""MY_DATA_SET_CODE_2""
);
var
fetchOptions
=
new
DataSetFileFetchOptions
();
// getDataStoreFacade(""DSS1"",""DSS2"") returns a facade object that uses only ""DSS1"" and ""DSS2"" data stores,
// e.g. calling searchFiles on such a facade searches for files only at these two data stores even if there
// are more datastores available
v3
.
getDataStoreFacade
(
## ""DSS1""
,
## ""DSS2""
).
searchFiles
(
criteria
,
fetchOptions
).
done
(
function
(
result
)
{
result
.
getObjects
().
forEach
(
function
(
file
)
{
console
.
log
(
""DataSet: ""
+
file
.
getDataSetPermId
()
+
"" has file: ""
+
file
.
getPath
());
});
});
});
</
script
>
Downloading files, folders, and datasets

Datasets that are created in Open BIS can be accessed by V3 API in a
number of different ways. It’s possible to download individual files,
folders, and entire datasets as illustrated in the following examples.
To get started, it is necessary to reference both the AS API
(IApplicationServerApi) and the DSS API (IDataStoreServerAPI), and login
and get a session token object.
The API provides two methods for downloading:
Simple downloading: A single InputStream is returned which contains
all files and file meta data.
Fast downloading: A FastDownloadSession object is returned which is
used by a helper class to download files in parallel streams in
chunks. It is based on the
SIS File Transfer Protocol
.
## Simple Downloading

By setting the DataSetFileDownloadOptions it’s possible to change how
data is downloaded - data can be downloaded file by file, or by folder,
by an entire dataset in a recursive manner. It is also possible to
search for datasets by defining the appropriate search criteria
(DataSetFileSearchCriteria).
In order to download content via the V3 DSS API, the dataset needs to
already be inside Open BIS. It is necessary to know the dataset code at
the very minimum. It is helpful to also know the file path to the file
desired to download.
Download a single file located inside a dataset

Here is how to download a single file and print out the contents, when
the dataset code and the file path are known. Here a search is not
necessary since the file path and dataset code are known.
A note about recursion

Note that when only downloading one file, it is better to set the
recursive flag to false in DataSetFileDownloadOptions, although it makes
no difference in the results returned. The recursive flag really only
matters when downloading entire datasets or directories - if it is true,
then the entire tree of contents will be downloaded, if false, then the
single path requested will be downloaded. If that path is just a
directory then the returned result will consist of just meta data about
the directory.
Download a single file
import
java.io.InputStream
;
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3DSSExample1
{
## // DATASET EXAMPLE STRUCTURE
// The dataset consists of a root folder with 2 files and a subfolder with 1 file
## // root:
//   - file1.txt
//   - file2.txt
## //   - subfolder:
//      - file3.txt
public
static
void
main
(
## String
[]
args
)
{
## String
## AS_URL
=
""https://localhost:8443/openbis/openbis""
;
## String
## DSS_URL
=
""https://localhost:8444/datastore_server""
;
// Reference the DSS
IDataStoreServerApi
dss
=
HttpInvokerUtils
.
createStreamSupportingServiceStub
(
IDataStoreServerApi
.
class
,
## DSS_URL
+
IDataStoreServerApi
.
## SERVICE_URL
,
10000
);
// Reference the AS and login & get a session token
IApplicationServerApi
as
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
## AS_URL
+
IApplicationServerApi
.
## SERVICE_URL
,
10000
);
## String
sessionToken
=
as
.
login
(
""admin""
,
""password""
);
// Download a single file with a path and a dataset code
DataSetFileDownloadOptions
options
=
new
DataSetFileDownloadOptions
();
options
.
setRecursive
(
false
);
IDataSetFileId
fileToDownload
=
new
DataSetFilePermId
(
new
DataSetPermId
(
""20161205154857065-25""
),
""root/subfolder/file3.txt""
);
// Download the files into a stream and read them with the file reader
// Here there is only one file, but we need to put it in an array anyway
InputStream
stream
=
dss
.
downloadFiles
(
sessionToken
,
## Arrays
.
asList
(
fileToDownload
),
options
);
DataSetFileDownloadReader
reader
=
new
DataSetFileDownloadReader
(
stream
);
DataSetFileDownload
file
=
null
;
// Print out the contents
while
((
file
=
reader
.
read
())
!=
null
)
{
## System
.
out
.
println
(
## ""Downloaded ""
+
file
.
getDataSetFile
().
getPath
()
+
"" ""
+
file
.
getDataSetFile
().
getFileLength
());
## System
.
out
.
println
(
## ""-----FILE CONTENTS-----""
);
## System
.
out
.
println
(
file
.
getInputStream
());
}
}
}
Download a folder located inside a dataset

The example below demonstrates how to download a folder and all its
contents, when the dataset code and the folder path are known. The goal
here is to download the directory called “subfolder” and the file
“file3.txt” which will return two objects, one representing the metadata
of the directory, and the other representing both the meta data of
file3.txt and the file contents. Note that setting recursive flag to
true will return both the subfolder directory object AND file3.txt,
while setting recursive flag to false will return just the meta data of
the directory object.
Download a folder
import
java.io.InputStream
;
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3DSSExample2
{
## // DATASET EXAMPLE STRUCTURE
// The dataset consists of a root folder with 2 files and a subfolder with 1 file
## // root:
//   - file1.txt
//   - file2.txt
## //   - subfolder:
//      - file3.txt
public
static
void
main
(
## String
[]
args
)
{
## String
## AS_URL
=
""https://localhost:8443/openbis/openbis""
;
## String
## DSS_URL
=
""https://localhost:8444/datastore_server""
;
// Reference the DSS
IDataStoreServerApi
dss
=
HttpInvokerUtils
.
createStreamSupportingServiceStub
(
IDataStoreServerApi
.
class
,
## DSS_URL
+
IDataStoreServerApi
.
## SERVICE_URL
,
10000
);
// Reference the AS and login & get a session token
IApplicationServerApi
as
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
## AS_URL
+
IApplicationServerApi
.
## SERVICE_URL
,
10000
);
## String
sessionToken
=
as
.
login
(
""admin""
,
""password""
);
// Download a single folder (containing a file inside) with a path and a data set code
DataSetFileDownloadOptions
options
=
new
DataSetFileDownloadOptions
();
IDataSetFileId
fileToDownload
=
new
DataSetFilePermId
(
new
DataSetPermId
(
""20161205154857065-25""
),
""root/subfolder""
);
// Setting recursive flag to true will return both the subfolder directory object AND file3.txt
options
.
setRecursive
(
true
);
// Setting recursive flag to false will return just the meta data of the directory object
//options.setRecursive(false);
// Read the contents and print them out
InputStream
stream
=
dss
.
downloadFiles
(
sessionToken
,
## Arrays
.
asList
(
fileToDownload
),
options
);
DataSetFileDownloadReader
reader
=
new
DataSetFileDownloadReader
(
stream
);
DataSetFileDownload
file
=
null
;
while
((
file
=
reader
.
read
())
!=
null
)
{
## System
.
out
.
println
(
## ""Downloaded ""
+
file
.
getDataSetFile
().
getPath
()
+
"" ""
+
file
.
getDataSetFile
().
getFileLength
());
## System
.
out
.
println
(
## ""-----FILE CONTENTS-----""
);
## System
.
out
.
println
(
file
.
getInputStream
());
}
}
}
Search for a dataset and download all its contents, file by file

Here is an example that demonstrates how to search for datasets and
download the contents file by file. Here recursion is not used - see
example 4 for a recursive example. To search for datasets, it is
necessary to assign the appropriate criteria in the
DataSetFileSearchCriteria object. It is also possible to search for
datasets that contain certain files, as demonstrated below. Searching
for files via the searchFiles method returns a list of DataSetFile
objects that contain meta data about the files and also the file
contents. The meta data includes the file perm ids, the dataset perm ids
(the perm ids are objects, not simple codes!), the file path, the file
length, and whether or not the file is a directory. With this list of
files, it is possible to iterate and access the contents as shown in
this example.
Search & download a whole dataset, file by file
import
java.io.InputStream
;
import
java.util.LinkedList
;
import
java.util.List
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.DataSetFile
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fetchoptions.DataSetFileFetchOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.search.DataSetFileSearchCriteria
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3DSSExample3
{
## // DATASET EXAMPLE STRUCTURE
// The dataset consists of a root folder with 2 files and a subfolder with 1 file
## // root:
//   - file1.txt
//   - file2.txt
## //   - subfolder:
//      - file3.txt
public
static
void
main
(
## String
[]
args
)
{
## String
## AS_URL
=
""https://localhost:8443/openbis/openbis""
;
## String
## DSS_URL
=
""https://localhost:8444/datastore_server""
;
// Reference the DSS
IDataStoreServerApi
dss
=
HttpInvokerUtils
.
createStreamSupportingServiceStub
(
IDataStoreServerApi
.
class
,
## DSS_URL
+
IDataStoreServerApi
.
## SERVICE_URL
,
10000
);
// Reference the AS and login & get a session token
IApplicationServerApi
as
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
## AS_URL
+
IApplicationServerApi
.
## SERVICE_URL
,
10000
);
## String
sessionToken
=
as
.
login
(
""admin""
,
""password""
);
// Create search criteria
DataSetFileSearchCriteria
criteria
=
new
DataSetFileSearchCriteria
();
criteria
.
withDataSet
().
withCode
().
thatEquals
(
""20161205154857065-25""
);
// Search for a dataset with a certain file inside like this:
//criteria.withDataSet().withChildren().withPermId(mypermid);
// Search for the files & put the file perm ids in a list for easy access
// (file perm ids are objects containing meta data describing the file)
SearchResult
<
DataSetFile
>
result
=
dss
.
searchFiles
(
sessionToken
,
criteria
,
new
DataSetFileFetchOptions
());
## List
<
DataSetFile
>
files
=
result
.
getObjects
();
// This returns the following list of objects:
// DataSetFile(""root"", isDirectory = true)
// DataSetFile(""root/file1.txt"", isDirectory = false)
// DataSetFile(""root/file2.txt"", isDirectory = false)
// DataSetFile(""root/subfolder"", isDirectory = true)
// DataSetFile(""root/subfolder/file3.txt"", isDirectory = false)
## List
<
IDataSetFileId
>
fileIds
=
new
LinkedList
<
IDataSetFileId
>
();
for
(
DataSetFile
file
## :
files
)
{
## System
.
out
.
println
(
file
.
getPath
()
+
"" ""
+
file
.
getFileLength
());
fileIds
.
add
(
file
.
getPermId
());
}
// Download the files & print the contents
DataSetFileDownloadOptions
options
=
new
DataSetFileDownloadOptions
();
options
.
setRecursive
(
false
);
InputStream
stream
=
dss
.
downloadFiles
(
sessionToken
,
fileIds
,
options
);
DataSetFileDownloadReader
reader
=
new
DataSetFileDownloadReader
(
stream
);
DataSetFileDownload
file
=
null
;
while
((
file
=
reader
.
read
())
!=
null
)
{
## System
.
out
.
println
(
## ""Downloaded ""
+
file
.
getDataSetFile
().
getPath
()
+
"" ""
+
file
.
getDataSetFile
().
getFileLength
());
## System
.
out
.
println
(
file
.
getInputStream
());
}
}
}
Download a whole dataset recursively

Here is a simplified way to download a dataset. Instead of downloading
files one by one, it is possible to download the entire dataset
recursively by simply setting the recursive file to true in the
DataSetFileDownloadOptions object.
Download a whole dataset recursively
import
java.io.InputStream
;
import
java.util.Arrays
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3DSSExample4
{
## // DATASET EXAMPLE STRUCTURE
// The dataset consists of a root folder with 2 files and a subfolder with 1 file
## // root:
//   - file1.txt
//   - file2.txt
## //   - subfolder:
//      - file3.txt
public
static
void
main
(
## String
[]
args
)
{
## String
## AS_URL
=
""https://localhost:8443/openbis/openbis""
;
## String
## DSS_URL
=
""https://localhost:8444/datastore_server""
;
// Reference the DSS
IDataStoreServerApi
dss
=
HttpInvokerUtils
.
createStreamSupportingServiceStub
(
IDataStoreServerApi
.
class
,
## DSS_URL
+
IDataStoreServerApi
.
## SERVICE_URL
,
10000
);
// Reference the AS and login & get a session token
IApplicationServerApi
as
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
## AS_URL
+
IApplicationServerApi
.
## SERVICE_URL
,
10000
);
## String
sessionToken
=
as
.
login
(
""admin""
,
""password""
);
// Download the files and print the contents
DataSetFileDownloadOptions
options
=
new
DataSetFileDownloadOptions
();
IDataSetFileId
fileId
=
new
DataSetFilePermId
(
new
DataSetPermId
(
""20161205154857065-25""
));
options
.
setRecursive
(
true
);
InputStream
stream
=
dss
.
downloadFiles
(
sessionToken
,
## Arrays
.
asList
(
fileId
),
options
);
DataSetFileDownloadReader
reader
=
new
DataSetFileDownloadReader
(
stream
);
DataSetFileDownload
file
=
null
;
while
((
file
=
reader
.
read
())
!=
null
)
{
file
.
getInputStream
();
## System
.
out
.
println
(
## ""Downloaded ""
+
file
.
getDataSetFile
().
getPath
()
+
"" ""
+
file
.
getDataSetFile
().
getFileLength
());
}
}
}
Search and list all the files inside a data store

Here is an example that demonstrates how to list all the files in a data
store. By simply leaving the following line as is:
DataSetFileSearchCriteria criteria = new DataSetFileSearchCriteria();
it will automatically return every object in the data store. This is
useful when it is desired to list an entire directory or iterate over
the whole data store.
Search and list all files inside a data store
import
java.io.InputStream
;
import
java.util.LinkedList
;
import
java.util.List
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.DataSetFile
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fetchoptions.DataSetFileFetchOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.search.DataSetFileSearchCriteria
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
V3DSSExample5
{
## // DATASET EXAMPLE STRUCTURE
// The dataset consists of a root folder with 2 files and a subfolder with 1 file
## // root:
//   - file1.txt
//   - file2.txt
## //   - subfolder:
//      - file3.txt
public
static
void
main
(
## String
[]
args
)
{
## String
## AS_URL
=
""https://localhost:8443/openbis/openbis""
;
## String
## DSS_URL
=
""https://localhost:8444/datastore_server""
;
// Reference the DSS
IDataStoreServerApi
dss
=
HttpInvokerUtils
.
createStreamSupportingServiceStub
(
IDataStoreServerApi
.
class
,
## DSS_URL
+
IDataStoreServerApi
.
## SERVICE_URL
,
10000
);
// Reference the AS and login & get a session token
IApplicationServerApi
as
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
## AS_URL
+
IApplicationServerApi
.
## SERVICE_URL
,
10000
);
## String
sessionToken
=
as
.
login
(
""admin""
,
""password""
);
// Create search criteria
DataSetFileSearchCriteria
criteria
=
new
DataSetFileSearchCriteria
();
criteria
.
withDataSet
();
//comment out this line below, and just leave the criteria empty - and it will return everything.
//criteria.withDataSet().withCode().thatEquals(""20151201115639682-98322"");
// Search for the files & put the file perm ids (objects containing meta data) in a list for easy access
SearchResult
<
DataSetFile
>
result
=
dss
.
searchFiles
(
sessionToken
,
criteria
,
new
DataSetFileFetchOptions
());
## List
<
DataSetFile
>
files
=
result
.
getObjects
();
## List
<
IDataSetFileId
>
fileIds
=
new
LinkedList
<
IDataSetFileId
>
();
for
(
DataSetFile
file
## :
files
)
{
## System
.
out
.
println
(
file
.
getPath
()
+
"" ""
+
file
.
getFileLength
());
fileIds
.
add
(
file
.
getPermId
());
}
// Download the files and print the contents
DataSetFileDownloadOptions
options
=
new
DataSetFileDownloadOptions
();
options
.
setRecursive
(
false
);
InputStream
stream
=
dss
.
downloadFiles
(
sessionToken
,
fileIds
,
options
);
DataSetFileDownloadReader
reader
=
new
DataSetFileDownloadReader
(
stream
);
DataSetFileDownload
file
=
null
;
while
((
file
=
reader
.
read
())
!=
null
)
{
## System
.
out
.
println
(
## ""Downloaded ""
+
file
.
getDataSetFile
().
getPath
()
+
"" ""
+
file
.
getDataSetFile
().
getFileLength
());
## System
.
out
.
println
(
## ""-----FILE CONTENTS-----""
);
## System
.
out
.
println
(
file
.
getInputStream
());
}
}
}
## Fast Downloading

Fast downloading is based on the
SIS File Transfer Protocol
and
## library. Downloading is done in two steps:
Create a fast download session with the
method
createFastDownloadSession()
on  V3 DSS API. One parameter
is a list of data set file ids. Such an id contains the data set
code and the path to the file inside the data set. If a file id
points to a folder the whole folder will be downloaded. The last
parameter specifies download preferences. Currently only the wished
number of parallel download streams can be specified. The API call
returns a
FastDownloadSession
object.
Download the files with the helper class
FastDownloader
## . The
simplest usage is just do ``
Search and list all files inside a data store
new FastDownloader(downloadSession).downloadTo(destinationFolder);
The files are stored in the destination folder in
/
.
Here is a complete example:
Search and list all files inside a data store
import
java.io.File
;
import
java.nio.file.Path
;
import
java.util.ArrayList
;
import
java.util.Collection
;
import
java.util.List
;
import
java.util.Map
;
import
java.util.Map.Entry
;
import
org.apache.commons.lang3.time.StopWatch
;
import
ch.ethz.sis.filetransfer.DownloadListenerAdapter
;
import
ch.ethz.sis.filetransfer.IDownloadItemId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.DataSet
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.fetchoptions.DataSetFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.search.DataSetSearchCriteria
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fastdownload.FastDownloadSession
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fastdownload.FastDownloadSessionOptions
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.fastdownload.FastDownloadResult
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.fastdownload.FastDownloader
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
import
ch.systemsx.cisd.openbis.common.api.client.ServiceFinder
;
public
class
V3FastDownloadExample
{
public
static
void
main
(
## String
[]
args
)
{
IApplicationServerApi
v3
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
""http://localhost:8888/openbis/openbis""
+
IApplicationServerApi
.
## SERVICE_URL
,
10000
);
## String
sessionToken
=
v3
.
login
(
""test""
,
""password""
);
// Search for some data sets
DataSetSearchCriteria
searchCriteria
=
new
DataSetSearchCriteria
();
searchCriteria
.
withCode
().
thatStartsWith
(
""201902""
);
DataSetFetchOptions
fetchOptions
=
new
DataSetFetchOptions
();
fetchOptions
.
withDataStore
();
fetchOptions
.
withPhysicalData
();
## List
<
DataSet
>
dataSets
=
v3
.
searchDataSets
(
sessionToken
,
searchCriteria
,
fetchOptions
).
getObjects
();
// Get the DSS URL from the first data set assuming that all data sets from the same data store
## String
dssUrl
=
dataSets
.
get
(
0
).
getDataStore
().
getDownloadUrl
();
## System
.
out
.
println
(
""url:""
+
dssUrl
);
// Create DSS server
IDataStoreServerApi
dssServer
=
new
ServiceFinder
(
""datastore_server""
,
IDataStoreServerApi
.
## SERVICE_URL
)
.
createService
(
IDataStoreServerApi
.
class
,
dssUrl
);
// We download all files of the all found data sets.
## List
<
DataSetFilePermId
>
fileIds
=
new
ArrayList
<>
();
for
(
DataSet
dataSet
## :
dataSets
)
{
fileIds
.
add
(
new
DataSetFilePermId
(
new
DataSetPermId
(
dataSet
.
getCode
())));
}
// Create the download session for 2 streams in parallel (if possible)
FastDownloadSession
downloadSession
=
dssServer
.
createFastDownloadSession
(
sessionToken
,
fileIds
,
new
FastDownloadSessionOptions
().
withWishedNumberOfStreams
(
2
));
// Do the actual download into 'targets/fast-download' and print the time needed by using a download listener
FastDownloadResult
result
=
new
FastDownloader
(
downloadSession
).
withListener
(
new
DownloadListenerAdapter
()
{
private
StopWatch
stopWatch
=
new
StopWatch
();
## @Override
public
void
onDownloadStarted
()
{
stopWatch
.
start
();
}
## @Override
public
void
onDownloadFinished
(
## Map
<
IDownloadItemId
,
## Path
>
itemPaths
)
{
## System
.
out
.
println
(
""Successfully finished after ""
+
stopWatch
);
}
## @Override
public
void
onDownloadFailed
(
## Collection
<
## Exception
>
e
)
{
## System
.
out
.
println
(
""Downloading failed after ""
+
stopWatch
);
}
})
.
downloadTo
(
new
## File
(
""targets/fast-download""
));
// Print the mapping of data set file id to the actual path
for
(
## Entry
<
IDataSetFileId
,
## Path
>
entry
## :
result
.
getPathsById
().
entrySet
())
{
## System
.
out
.
println
(
entry
);
}
v3
.
logout
(
sessionToken
);
}
}
What happens under the hood?

The files to be downloaded are chunked into chunks of maximum size 1 MB.
On the DSS a special web service (
FileTransferServerServlet
) provides
these chunks. On the client side these chunks are requested and stored
in the file system. This is done in parallel if possible and requested
(withWishedNumberOfStreams). The server tells the client the actual
number of streams available for parallel downloading without slowing
down DSS. The actual number of streams depends on
the wished number of streams
the number of streams currently used by other download sessions
the maximum number of allowed streams as specified by the
property
api.v3.fast-download.maximum-number-of-allowed-streams
in
## DSS
service.properties
. Default value is 10.
The actual number of streams is half of the number of free streams or
the wished number of streams, if it is less. The number of free streams
is given by the difference between the maximum number of allowed streams
and the total number of used streams.
It is possible that the actual number of streams is zero if the server
is currently too busy with downloading (that is, there is no free
dowload stream available). The FastDownloader will retry it later.
## Customizing Fast Dowloading

There are three ways to customizing the FastDownloader:
withListener(): Adds a listener which will be notified when
the download session has been started/finished/failed,
the download of a file/folder has been started/finished and
a chunk has been downloaded.
There can be several listeners. By default there are no
listeners. Note, that listeners are notified in a separated
thread associated with the download session.
withLogger(): Sets a logger. By default nothing is logged.
withRetryProviderFactory(): Sets the factory which creates a retry
provider. A retry provider knows when and how often a failed action
(e.g. sever request) should be retried. By default it is retried
three times. The first retry is a second later. For each following
retry the waiting time is increases by the factor two.
Register Data Sets

To register datasets using the Java or JavaScript API use one of the
following examples as a template.
## Example (Java)
Register Data Set
import
java.util.UUID
;
import
org.eclipse.jetty.client.HttpClient
;
import
org.eclipse.jetty.client.api.Request
;
import
org.eclipse.jetty.client.util.MultiPartContentProvider
;
import
org.eclipse.jetty.client.util.StringContentProvider
;
import
org.eclipse.jetty.http.HttpMethod
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi
;
import
ch.ethz.sis.openbis.generic.dssapi.v3.dto.dataset.create.UploadedDataSetCreation
;
import
ch.systemsx.cisd.common.http.JettyHttpClientFactory
;
import
ch.systemsx.cisd.common.spring.HttpInvokerUtils
;
public
class
RegisterDataSet
{
public
static
void
main
(
## String
[]
args
)
throws
## Exception
{
final
## String
## AS_URL
=
""http://localhost:8888/openbis/openbis""
;
final
## String
## DSS_URL
=
""http://localhost:8889/datastore_server""
;
final
OpenBIS
openbisV3
=
new
OpenBIS
(
## AS_URL
,
## DSS_URL
);
openbisV3
.
login
(
""admin""
,
""password""
);
final
## Path
path
=
## Path
.
of
(
""/uploadPath""
);
final
## String
uploadId
=
openbisV3
.
uploadFileWorkspaceDSS
(
path
);
final
UploadedDataSetCreation
creation
=
new
UploadedDataSetCreation
();
creation
.
setUploadId
(
uploadId
);
creation
.
setExperimentId
(
new
ExperimentIdentifier
(
## ""/DEFAULT/DEFAULT/DEFAULT""
));
creation
.
setTypeId
(
new
EntityTypePermId
(
## ""ATTACHMENT""
,
EntityKind
.
## DATA_SET
));
try
{
final
DataSetPermId
dataSetPermId
=
openbisV3
.
createUploadedDataSet
(
creation
);
// A data set assigned to the experiment ""/DEFAULT/DEFAULT/DEFAULT"" with the folder ""uploadPath"" is created
## System
.
out
.
println
(
""dataSetPermId=""
+
dataSetPermId
);
}
catch
(
final
## Exception
e
)
{
e
.
printStackTrace
();
}
openbisV3
.
logout
();
}
}
## Example (Javascript)
Register Data Set
<!DOCTYPE html>
<
html
>
<
head
>
<
meta
charset
=
""utf-8""
>
<
title
>
Dataset upload
</
title
>
<
script
type
=
""text/javascript""
src
=
""/openbis-test/resources/api/v3/config.js""
></
script
>
<
script
type
=
""text/javascript""
src
=
""/openbis-test/resources/api/v3/require.js""
></
script
>
</
head
>
<
body
>
<
label
for
=
""myfile""
>
Select a file:
</
label
>
<
input
type
=
""file""
id
=
""myFile""
/>
<
script
>
require
([
""openbis""
,
""dss/dto/dataset/create/UploadedDataSetCreation""
,
""as/dto/experiment/id/ExperimentIdentifier""
,
""as/dto/entitytype/id/EntityTypePermId""
,
""as/dto/entitytype/EntityKind""
],
function
(
openbis
,
UploadedDataSetCreation
,
ExperimentIdentifier
,
EntityTypePermId
,
EntityKind
)
{
var
testProtocol
=
window
.
location
.
protocol
;
var
testHost
=
window
.
location
.
hostname
;
var
testPort
=
window
.
location
.
port
;
var
testUrl
=
testProtocol
+
""//""
+
testHost
+
"":""
+
testPort
;
var
testApiUrl
=
testUrl
+
""/openbis/openbis/rmi-application-server-v3.json""
;
var
openbisV3
=
new
openbis
(
testApiUrl
);
var
fileInput
=
document
.
getElementById
(
""myFile""
);
fileInput
.
onchange
=
(
e
)
=>
{
var
files
=
e
.
target
.
files
;
openbisV3
.
login
(
""admin""
,
""password""
).
done
(
sessionToken
=>
{
var
dataStoreFacade
=
openbisV3
.
getDataStoreFacade
();
dataStoreFacade
.
uploadFilesWorkspaceDSS
(
files
).
done
(
uploadId
=>
{
var
creation
=
new
UploadedDataSetCreation
();
creation
.
setUploadId
(
uploadId
);
creation
.
setExperimentId
(
new
ExperimentIdentifier
(
## ""/DEFAULT/DEFAULT/DEFAULT""
));
creation
.
setTypeId
(
new
EntityTypePermId
(
## ""ATTACHMENT""
,
EntityKind
.
## DATA_SET
));
dataStoreFacade
.
createUploadedDataSet
(
creation
).
done
(
dataSetPermId
=>
{
// A data set assigned to the experiment ""/DEFAULT/DEFAULT/DEFAULT"" with the folder ""uploadPath"" is created
console
.
log
(
""dataSetPermId=""
+
dataSetPermId
);
openbisV3
.
logout
();
}).
fail
(
error
=>
{
console
.
error
(
error
);
openbisV3
.
logout
();
});
});
});
}
});
</
script
>
</
body
>
</
html
>
VI. Web application context

When making web applications and embedding them into an openBIS tab on
the core UI is often required to have information about the context
those applications are being loaded for two particular purposes:
Making the application context sensitive and show
information/functionality related to the current context. The
context object provided by
getWebAppContext()
contains all
information required for this purpose.
Login into the facade without presenting the user with another login
screen since they have already login into openBIS. For
that
loginFromContext()
can be used.
This methods only exist on the Javascript facade with the purpose of
being used on embedded web applications, calling them from an external
web application will do nothing.
WebAppContextExample.html
<
script
>
require
([
'openbis'
],
function
(
openbis
)
{
var
openbisV3
=
new
openbis
();
var
webappcontext
=
openbisV3
.
getWebAppContext
();
console
.
log
(
webappcontext
.
getWebappCode
());
console
.
log
(
webappcontext
.
getSessionId
());
console
.
log
(
webappcontext
.
getEntityKind
());
console
.
log
(
webappcontext
.
getEntityType
());
console
.
log
(
webappcontext
.
getEntityIdentifier
());
console
.
log
(
webappcontext
.
getEntityPermId
());
openbisV3
.
loginFromContext
();
openbisV3
.
getSessionInformation
().
done
(
function
(
sessionInfo
)
{
console
.
log
(
sessionInfo
.
getUserName
());
});
});
</
script
>",I. Architecture,0,en_20.10.0-11_software-developer-documentation_apis_java-javascript-v3-api_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_java-javascript-v3-api.txt,2025-09-30T12:09:01.039250Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_matlab-v3-api:0,Matlab (V3 API) - How to access openBIS from MATLAB,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/matlab-v3-api.html,openbis,"Matlab (V3 API) - How to access openBIS from MATLAB

## Preamble

openBIS
is a research data management system developed by
## ETH SIS
. Data stored in openBIS can be accessed directly via the web UI or programmatically using APIs. For example,
pyBIS
is a project that provides a Python 3 module for interacting with openBIS.
## MATLAB
is a high-level numerical computing environment that is popular in many areas of science. This repository provides a toolbox to access data in openBIS directly from MATLAB.
## Setup

The toolbox interacts with openBIS by calling pyBIS functions directly from MATLAB. Therefore, both Python and MATLAB have to be installed and configured properly. Please consult the
MATLAB - Python compatibility table
to choose the correct versions. Also note that Python 2.7 is no longer supported!
macOS

On macOS, the setup has been tested with a Miniconda Python distribution.
Download and install
## Miniconda3
(use a Python version according to the
MATLAB - Python compatibility table
)
Open the terminal and install pyBIS with pip:
pip
install
pybis
Find the path to your Python executable:
which
python
Open MATLAB and set the Python executable. On Matlab R2019b or later, use the command:
pyenv('Version',
'Path/to/python')
. Replace with the path found in previous step. On earlier versions of Matlab, the
pyenv
command is called
pyversion
.
## Windows 10

On Windows using the Anaconda or Miniconda approach did not work (for some reason, MATLAB could not find the Python modules). On the other hand, using the standard Python installation seems to work.
Download and install Python
here
(use a Python version according to the
MATLAB - Python compatibility table
). Make sure to choose the
64-bit version
.
During the installation, make sure Python is added to the Path and registered as default Python interpreter. To do this, select the little tick box
## Add
## Python
3.x
to
## PATH
in the installation window:
Open Windows PowerShell and install pyBIS with pip:
pip
install
pybis
Find the path to your Python executable by typing:
## Get-Command
python
. The path is listed in the Source column, i.e.
C:\Users\user\AppData\Local\Programs\Python\Python38\python.exe
. Copy the path by selecting it and pressing
## Ctrl-C
Open MATLAB and set the Python executable. On Matlab R2019b or later, use the command:
pyenv('Version',
'C:\Path\to\Programs\python.exe')
. Replace with the path found in step 4. On earlier versions of Matlab, the
pyenv
command is called
pyversion
.
### Usage

## Download
this repository
and add it to your Matlab Path. If you are running the toolbox for the first time, make sure to carry out the steps described under
## Setup
above. An
example script
demonstrating some common usage patterns is provided in the repository. The script can be run interactively in the MATLAB Live Editor. Type
doc
OpenBis
in the Matlab Command Window to access the built-in documentation.
## Notes

I do not have time to test these instructions and the toolbox with all combinations of Python & Matlab versions on different operating systems. In general, a combination of recent Python and Matlab versions should work on macOS and Windows. If you run into any issues, please feel free to contact the
SIS Helpdesk
.",Preamble,0,en_20.10.0-11_software-developer-documentation_apis_matlab-v3-api_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_matlab-v3-api.txt,2025-09-30T12:09:01.142556Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:0,Personal Access Tokens,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html,openbis,"## Personal Access Tokens

## Background

“Personal access token” (in short: PAT) is an openBIS feature that was
introduced to simplify integration of openBIS with other systems. Such
integrations are usually done using openBIS V3 API and therefore require
an external application to authenticate in openBIS to fetch or create
some data. Without “Personal access tokens” the only way of
authenticating in openBIS V3 API was the V3 API login method. Given a
user name and a password the login method would return back an openBIS
session token, which could be later used in other V3 API calls as a
secret and a proof of who we are.
Unfortunately, even though this approach worked well it had some
limitations. These were mainly caused by the nature of session tokens in
## openBIS:
session tokens are short lived
session tokens do not survive openBIS restarts
obtaining a new session token requires a user name and a password
Because of these limitations external applications had to be prepared
for a situation where an openBIS session token stops working. They had
to know how to recover. When one session token expired or was
invalidated they had to obtain a new one by calling the login method
again and providing a user name and a password. But even then the whole
state of the previous session (e.g. files stored in the session
workspace) would be gone and not available in the new session.
Depending on a use case and a type of the integration that could cause
smaller or bigger headaches for the developers of the external system.
Fortunately, “Personal access tokens” come to a rescue.
What are “Personal access tokens” ?

A personal access token (in short: PAT) is very similar to a session
token but there are also some important differences.
## Similarities:
a PAT is bound to a specific user and represents that user’s
session. Two users can’t share a session using PAT. Internal PAT
sessions identifier is the combination of both the userId and the
session name.
a PAT is a secret that must not be publicly shared (having a user’s
PAT one can perform any actions in openBIS that this user could
normally perform, except for user and PAT management)
a user can have multiple PATs active at the same time
a PAT can be used in places where a regular session token could be
normally used, e.g. to call V3 API methods (a full list of endpoints
that support PATs is presented below)
## Differences:
a PAT is created using a dedicated “createPersonalAccessTokens” V3
API method (not using “login” method as a regular session token)
a PAT can be long lived (its validFrom and validTo dates are defined
at the moment of creation), still it should be replaced periodically
for security reasons
a PAT session survives openBIS restarts, i.e. the same PAT can be
used before and after a restart (session workspace folder state is
also kept)
multiple PATs may represent a single PAT session (both PATs must
have the same “session name”) - this becomes useful for handling a
transition period from one soon to be expired PAT to a new PAT that
replaces it without losing the session’s state
Who can create a “Personal access token” ?

Any openBIS user can manage its own PATs. Instance admin users can
manage all PATs in the system.
Where can I use “Personal access tokens” ?

Endpoints that support PATs:
## AS:
## V3 API
File Upload Servlet (class: UploadServiceServlet, path: /upload)
## File Download Servlet (class: DownloadServiceServlet, path:
/download)
## Session Workspace Provider
## DSS:
## V3 API
## File Upload Servlet (class: StoreShareFileUploadServlet, path:
/store_share_file_upload)
File Download Servlet (class: DatasetDownloadServlet, path: /*)
## Session Workspace Upload Servlet (class:
## SessionWorkspaceFileUploadServlet, path:
/session_workspace_file_upload)
## Session Workspace Download Servlet (class:
## SessionWorkspaceFileDownloadServlet, path:
/session_workspace_file_download)
## Session Workspace Provider
## SFTP
Where “Personal access tokens” are stored ?

PATs are stored in “personal-access-tokens.json” JSON file. By default
the file is located in the main openBIS folder where it survives openBIS
restarts and upgrades.
The location can be changed using “personal-access-tokens-file-path”
property in AS service.properties. The JSON file is read at the openBIS
start up.
How long should my “Personal Access Tokens” be valid ?

Because of security reasons PATs should not be valid indefinitely.
Instead, each PAT should have a well defined validity period after which
it should be replaced with a new PAT with a different hash. To make this
transition as smooth as possible please use the following guide:
create PAT_1 with sessionName = <MY_SESSION> and use it in
your integration
when PAT_1 is soon to be expired, create PAT_2 with the same
sessionName = <MY_SESSION> (both PAT_1 and PAT_2 will work
at this point and will refer to the same openBIS session)
replace PAT_1 with PAT_2 in your integration
PATs created by the same user and with the same “session name” refer
under the hood to the same openBIS session. Therefore, even if one of
such PATs expires the session is kept active and its state is
maintained.
### Configuration

“Personal access tokens” functionality is enabled by default. To
## configure it please use AS service.properties:
# personal access tokens feature
personal-access-tokens-enabled = true",Personal Access Tokens,0,en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens.txt,2025-09-30T12:09:01.219381Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:1,Personal Access Tokens,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html,openbis,"# change the default location of the JSON file that stores personal access tokens (default: personal-access-tokens.json file in the main openBIS folder)
personal-access-tokens-file-path = MY_FOLDER/personal-access-tokens.json",change the default location of the JSON file that stores personal access tokens (default: personal-access-tokens.json file in the main openBIS folder),1,en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens_1,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens.txt,2025-09-30T12:09:01.219381Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:2,Personal Access Tokens,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html,openbis,"# set maximum allowed validity period (in seconds) - personal access token with a longer validity period cannot be created (default: 30 days)
personal-access-tokens-max-validity-period = 2592000",set maximum allowed validity period (in seconds) - personal access token with a longer validity period cannot be created (default: 30 days),2,en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens_2,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens.txt,2025-09-30T12:09:01.219381Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:3,Personal Access Tokens,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html,openbis,"# set validity warning period (in seconds) - owners of personal access tokens that are going to expire within this warning period are going to receive email notifications (default: 5 days)
personal-access-tokens-validity-warning-period = 259200
## Typical Application Workflow

Most typical use case for Personal Access Tokens is to run code on a
third party service against openBIS.
On such services we want to have:
A long lasting session with openBIS for several days that survives
restarts.
We don’t want to keep the user and password stored.
For such services we recommend to create a PAT on log in and store the
PAT instead. We provide the example Gradle project with the java class
PersonalAccessTokensApplicationWorkflows (
source downloadable
here
)
as the recommend way to manage getting the most up to date personal
access token for an application and user. Including creation and renewal
management.
private
static
final
## String
## URL
=
""https://openbis-sis-ci-sprint.ethz.ch/openbis/openbis""
+
IApplicationServerApi
.
## SERVICE_URL
;
private
static
final
int
## TIMEOUT
=
10000
;
private
static
final
## String
## USER
=
""admin""
;
private
static
final
## String
## PASSWORD
=
""changeit""
;
public
static
void
main
(
## String
[]
args
)
{
IApplicationServerApi
v3
=
HttpInvokerUtils
.
createServiceStub
(
IApplicationServerApi
.
class
,
## URL
,
## TIMEOUT
);
## String
sessionToken
=
v3
.
login
(
## USER
,
## PASSWORD
);
## System
.
out
.
println
(
""sessionToken: ""
+
sessionToken
);
PersonalAccessTokenPermId
pat
=
PersonalAccessTokensApplicationWorkflows
.
getApplicationPersonalAccessTokenOnLogin
(
v3
,
sessionToken
,
## ""MY_APPLICATION""
);
## System
.
out
.
println
(
""pat: ""
+
pat
);
v3
.
logout
(
sessionToken
);
}
package
ch.ethz.sis.pat
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.PersonalAccessToken
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.create.PersonalAccessTokenCreation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.fetchoptions.PersonalAccessTokenFetchOptions
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.id.PersonalAccessTokenPermId
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.pat.search.PersonalAccessTokenSearchCriteria
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.session.SessionInformation
;
import
org.apache.commons.lang3.time.DateUtils
;
import
java.util.Calendar
;
import
java.util.Date
;
import
java.util.List
;
import
java.util.Map
;
public
class
PersonalAccessTokensApplicationWorkflows
{
private
static
final
int
## DAY_IN_SECONDS
=
24
*
60
*
60
;
private
static
final
## String
## PERSONAL_ACCESS_TOKENS_MAX_VALIDITY_PERIOD
=
""personal-access-tokens-max-validity-period""
;
private
static
final
## String
## PERSONAL_ACCESS_TOKENS_VALIDITY_WARNING_PERIOD
=
""personal-access-tokens-validity-warning-period""
;
private
PersonalAccessTokensApplicationWorkflows
()
{
}
/*
* This utility method returns the current application token, creates one if no one is found and renews it if is close to expiration.
* Requires are real session token hence requires a form where the user can input its user and password on an application.
*/
public
static
PersonalAccessTokenPermId
getApplicationPersonalAccessTokenOnLogin
(
IApplicationServerApi
v3
,
## String
sessionToken
,
## String
applicationName
)
{
// Obtain servers renewal information
## Map
<
## String
,
## String
>
information
=
v3
.
getServerInformation
(
sessionToken
);
int
personalAccessTokensRenewalPeriodInSeconds
=
## Integer
.
parseInt
(
information
.
get
(
PersonalAccessTokensApplicationWorkflows
.
## PERSONAL_ACCESS_TOKENS_VALIDITY_WARNING_PERIOD
));
int
personalAccessTokensRenewalPeriodInDays
=
personalAccessTokensRenewalPeriodInSeconds
/
## DAY_IN_SECONDS
;
int
personalAccessTokensMaxValidityPeriodInSeconds
=
## Integer
.
parseInt
(
information
.
get
(
PersonalAccessTokensApplicationWorkflows
.
## PERSONAL_ACCESS_TOKENS_MAX_VALIDITY_PERIOD
));
int
personalAccessTokensMaxValidityPeriodInDays
=
personalAccessTokensMaxValidityPeriodInSeconds
/
## DAY_IN_SECONDS
;
// Obtain user id
SessionInformation
sessionInformation
=
v3
.
getSessionInformation
(
sessionToken
);
// Search for PAT for this user and application
// NOTE: Standard users only get their PAT but admins get all, filtering with the user solves this corner case
PersonalAccessTokenSearchCriteria
personalAccessTokenSearchCriteria
=
new
PersonalAccessTokenSearchCriteria
();
personalAccessTokenSearchCriteria
.
withSessionName
().
thatEquals
(
applicationName
);
personalAccessTokenSearchCriteria
.
withOwner
().
withUserId
().
thatEquals
(
sessionInformation
.
getPerson
().
getUserId
());
SearchResult
<
PersonalAccessToken
>
personalAccessTokenSearchResult
=
v3
.
searchPersonalAccessTokens
(
sessionToken
,
personalAccessTokenSearchCriteria
,
new
PersonalAccessTokenFetchOptions
());
PersonalAccessToken
bestTokenFound
=
null
;
PersonalAccessTokenPermId
bestTokenFoundPermId
=
null
;
// Obtain longer lasting application token
for
(
PersonalAccessToken
personalAccessToken
## :
personalAccessTokenSearchResult
.
getObjects
())
{
if
(
personalAccessToken
.
getValidToDate
().
after
(
new
## Date
()))
{
if
(
bestTokenFound
==
null
)
{
bestTokenFound
=
personalAccessToken
;
}
else
if
(
personalAccessToken
.
getValidToDate
().
after
(
bestTokenFound
.
getValidToDate
()))
{
bestTokenFound
=
personalAccessToken
;
}
}
}
// If best token doesn't exist, create
if
(
bestTokenFound
==
null
)
{
bestTokenFoundPermId
=
createApplicationPersonalAccessToken
(
v3
,
sessionToken
,
applicationName
,
personalAccessTokensMaxValidityPeriodInDays
);
}
// If best token is going to expire in less than the warning period, renew
## Calendar
renewalDate
=
## Calendar
.
getInstance
();
renewalDate
.
add
(
## Calendar
.
## DAY_OF_MONTH
,
personalAccessTokensRenewalPeriodInDays
);
if
(
bestTokenFound
!=
null
&&
bestTokenFound
.
getValidToDate
().
before
(
renewalDate
.
getTime
()))
{
bestTokenFoundPermId
=
createApplicationPersonalAccessToken
(
v3
,
sessionToken
,
applicationName
,
personalAccessTokensMaxValidityPeriodInDays
);
}
// If we have not created or renewed, return current
if
(
bestTokenFoundPermId
==
null
)
{
bestTokenFoundPermId
=
bestTokenFound
.
getPermId
();
}
return
bestTokenFoundPermId
;
}
private
static
PersonalAccessTokenPermId
createApplicationPersonalAccessToken
(
IApplicationServerApi
v3
,
## String
sessionToken
,
## String
applicationName
,
int
personalAccessTokensMaxValidityPeriodInDays
)
{
PersonalAccessTokenCreation
creation
=
new
PersonalAccessTokenCreation
();
creation
.
setSessionName
(
applicationName
);
creation
.
setValidFromDate
(
new
## Date
(
## System
.
currentTimeMillis
()
-
DateUtils
.
## MILLIS_PER_DAY
));
creation
.
setValidToDate
(
new
## Date
(
## System
.
currentTimeMillis
()
+
DateUtils
.
## MILLIS_PER_DAY
*
personalAccessTokensMaxValidityPeriodInDays
));
## List
<
PersonalAccessTokenPermId
>
personalAccessTokens
=
v3
.
createPersonalAccessTokens
(
sessionToken
,
## List
.
of
(
creation
));
return
personalAccessTokens
.
get
(
0
);
}
}
## V3 API

Code examples for personal access tokens can be found in the main V3 API documentation:
openBIS V3 API#PersonalAccessTokens",set validity warning period (in seconds) - owners of personal access tokens that are going to expire within this warning period are going to receive email notifications (default: 5 days),3,en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens_3,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens.txt,2025-09-30T12:09:01.219381Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_python-v3-api:0,Python (V3 API) - pyBIS!,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/python-v3-api.html,openbis,"Python (V3 API) - pyBIS!

pyBIS is a Python module for interacting with openBIS. pyBIS is designed to be most useful in a
## Jupyter Notebook
or IPython environment, especially if you are developing Python scripts for automatisation. Jupyter Notebooks offer some sort of IDE for openBIS, supporting TAB completition and immediate data checks, making the life of a researcher hopefully easier.
Dependencies and Requirements

pyBIS relies the openBIS API v3
openBIS version 16.05.2 or newer is required
19.06.5 or later is recommended
pyBIS uses Python 3.6 or newer and the Pandas module
### Installation

pip
install
--
upgrade
pybis
That command will download install pyBIS and all its dependencies. If pyBIS is already installed, it will be upgraded to the latest version.
If you haven’t done yet, install Jupyter and/or Jupyter Lab (the next Generation of Jupyter):
pip
install
jupyter
pip
install
jupyterlab
### General Usage

TAB completition and other hints in Jupyter / IPython

in a Jupyter Notebook or IPython environment, pybis helps you to enter the commands
After every dot
.
you might hit the
## TAB
key in order to look at the available commands.
if you are unsure what parameters to add to a , add a question mark right after the method and hit
## SHIFT+ENTER
Jupyter will then look up the signature of the method and show some helpful docstring
Checking input

When working with properties of entities, they might use a
controlled vocabulary
or are of a specific
property type
.
Add an underscore
_
character right after the property and hit
## SHIFT+ENTER
to show the valid values
When a property only acceps a controlled vocabulary, you will be shown the valid terms in a nicely formatted table
if you try to assign an
invalid value
to a property, you’ll receive an error immediately
## Glossary

## spaces:
used for authorisation eg. to separate two working groups. If you have permissions in a space, you can see everything which in that space, but not necessarily in another space (unless you have the permission).
## projects:
a space consists of many projects.
## experiments / collections:
a projects contain many experiments. Experiments can have
properties
## samples / objects:
an experiment contains many samples. Samples can have
properties
## dataSet:
a dataSet which contains the actual
data files
, either pyhiscal (stored in openBIS dataStore) or linked
## attributes:
every entity above contains a number of attributes. They are the same accross all instances of openBIS and independent of their type.
## properties:
Additional specific key-value pairs, available for these entities:
experiments
samples
dataSets
every single instance of an entity must be of a specific
entity type
(see below). The type defines the set of properties.
## experiment type / collection type:
a type for experiments which specifies its properties
## sample type / object type:
a type for samples / objects which specifies its properties
## dataSet type:
a type for dataSets which specifies its properties
## property type:
a single property, as defined in the entity types above. It can be of a classic data type (e.g. INTEGER, VARCHAR, BOOLEAN) or its values can be controlled (CONTROLLEDVOCABULARY).
## plugin:
a script written in
## Jython
which allows to check property values in a even more detailed fashion
connect to OpenBIS

login

In an
interactive session
e.g. inside a Jupyter notebook, you can use
getpass
to enter your password safely:
from
pybis
import
## Openbis
o
=
## Openbis
(
'https://example.com'
)
o
=
## Openbis
(
'example.com'
)
# https:// is assumed
import
getpass
password
=
getpass
.
getpass
()
o
.
login
(
'username'
,
password
,
save_token
=
## True
)
# save the session token in ~/.pybis/example.com.token
In a
script
you would rather use two
environment variables
to provide username and password:
from
pybis
import
## Openbis
o
=
## Openbis
(
os
.
environ
[
## 'OPENBIS_HOST'
])
o
.
login
(
os
.
environ
[
## 'OPENBIS_USERNAME'
],
os
.
environ
[
## 'OPENBIS_PASSWORD'
])
As an even better alternative, you should use personal access tokens (PAT) to avoid username/password altogether. See below.
Verify certificate

By default, your SSL-Certification is being verified. If you have a test-instance with a self-signed certificate, you’ll need to turn off this verification explicitly:
from
pybis
import
## Openbis
o
=
## Openbis
(
'https://test-openbis-instance.com'
,
verify_certificates
=
## False
)
Check session token, logout()

Check whether your session, i.e. the
session token
is still valid and log out:
print
(
f
## ""Session is active:
{
o
.
is_session_active
()
}
and token is
{
o
.
token
}
""
)
o
.
logout
()
print
(
f
## ""Session is active:
{
o
.
is_session_active
()
""}
Authentication without user/password

In some configurations Openbis can be accessible via Single Sign On technology (SSO), in that case users may not have their own user/password.
Upon login, Openbis generates a unique access token that can be used to allow pybis log into the active user session. You may find this token in cookies of the ELN UI.
To log in with a session token, you need to use
set_token
## method:
from
pybis
import
## Openbis
o
=
## Openbis
(
'https://test-openbis-instance.com'
)
o
.
set_token
(
""some_user-220808165456793xA3D0357C5DE66A5BAD647E502355FE2C""
)
# logged into 'some_user' session!
## Note
Keep you access tokens safe and don’t share it with others! They are invalidated when one of the following situations happen:
Explicit logout() call.
Number of sessions per user has reached beyond configured limit.
Session timeout is reached.
Openbis instance is restarted.
Personal access token (PAT)

As an (new) alternative to login every time you run a script, you can create tokens which
once issued, do
not need username or password
are
much longer valid
than session tokens (default is one year)
survive restarts
of an openBIS instance
To create a token, you first need a valid session – either through classic login or by assigning an existing valid session token:
from
pybis
import
## Openbis
o
=
## Openbis
(
'https://test-openbis-instance.com'
)
o
.
login
(
""username""
,
""password""
)
# or
o
.
set_token
(
""your_username-220808165456793xA3D0357C5DE66A5BAD647E502355FE2C""
)
Then you can create a new personal access token (PAT) and use it for all further pyBIS queries:
pat
=
o
.
get_or_create_personal_access_token
(
sessionName
=
## ""Project A""
)
o
.
set_token
(
pat
,
save_token
=
## True
)
## You may also use permId directly:
pat
=
o
.
get_or_create_personal_access_token
(
sessionName
=
## ""Project A""
)
o
.
set_token
(
pat
.
permId
,
save_token
=
## True
)
## Note
If there is an existing PAT with the same
sessionName
which is still valid and the validity is within the warning period (defined by the server), then this existing PAT is returned instead. However, you can enforce creating a new PAT by passing the argument
force=True
.
## Note
Most operations are permitted using the PAT,
except
## :
all operations on personal access tokens itself
i.e. create, list, delete operations on tokens
For these operations, you need to use a session token instead.
To get a list of all currently available tokens:
o
.
get_personal_access_tokens
()
o
.
get_personal_access_tokens
(
sessionName
=
## ""APPLICATION_1""
)
To delete the first token shown in the list:
o
.
get_personal_access_tokens
()[
0
]
.
delete
(
'some reason'
)
## Caching

## With
pyBIS
1.17.0
, a lot of caching has been introduced to improve the speed of object lookups that do not change often. If you encounter any problems, you can turn it off like this:
o
=
## Openbis
(
'https://example.com'
,
use_cache
=
## False
)
# or later in the script
o
.
use_cache
=
## False
o
.
clear_cache
()
o
.
clear_cache
(
'sampleType'
)
Mount openBIS dataStore server

Prerequisites: FUSE / SSHFS

Mounting an openBIS dataStore server requires FUSE / SSHFS to be installed (requires root privileges). The mounting itself requires no root privileges.
Mac OS X
Follow the installation instructions on
https://osxfuse.github.io
Unix Cent OS 7
$
sudo
yum
install
epel-release
$
sudo
yum
--enablerepo
=
epel
-y
install
fuse-sshfs
$
user
=
""
$(
whoami
)
""
$
usermod
-a
## -G
fuse
""
$user
""
After the installation, an
sshfs
command should be available.
Mount dataStore server with pyBIS

Because the mount/unmount procedure differs from platform to platform, pyBIS offers two simple methods:
o
.
mount
()
o
.
mount
(
username
,
password
,
hostname
,
mountpoint
,
volname
)
o
.
is_mounted
()
o
.
unmount
()
o
.
get_mountpoint
()
Currently, mounting is supported for Linux and Mac OS X only.
All attributes, if not provided, are re-used by a previous login() command, including personal access tokens. If no mountpoint is provided, the default mounpoint will be
~/hostname
. If this directory does not exist, it will be created. The directory must be empty before mounting.
## Masterdata

OpenBIS stores quite a lot of meta-data along with your dataSets. The collection of data that describes this meta-data (i.e. meta-meta-data) is called masterdata. It consists of:
sample types
dataSet types
material types
experiment types
property types
vocabularies
vocabulary terms
plugins (jython scripts that allow complex data checks)
tags
semantic annotations
browse masterdata

sample_types
=
o
.
get_sample_types
()
# get a list of sample types
sample_types
.
df
# DataFrame object
st
=
o
.
get_sample_types
()[
3
]
# get 4th element of that list
st
=
o
.
get_sample_type
(
## 'YEAST'
)
st
.
code
st
.
generatedCodePrefix
st
.
attrs
.
all
()
# get all attributes as a dict
st
.
get_validationPlugin
()
# returns a plugin object
st
.
get_property_assignments
()
# show the list of properties
# for that sample type
o
.
get_material_types
()
o
.
get_dataset_types
()
o
.
get_experiment_types
()
o
.
get_collection_types
()
o
.
get_property_types
()
pt
=
o
.
get_property_type
(
## 'BARCODE_COMPLEXITY_CHECKER'
)
pt
.
attrs
.
all
()
o
.
get_plugins
()
pl
=
o
.
get_plugin
(
'Diff_time'
)
pl
.
script
# the Jython script that processes this property
o
.
get_vocabularies
()
o
.
get_vocabulary
(
## 'BACTERIAL_ANTIBIOTIC_RESISTANCE'
)
o
.
get_terms
(
vocabulary
=
## 'STORAGE'
)
o
.
get_tags
()
create property types

## Samples
(objects),
experiments
(collections) and
dataSets
contain type-specific
properties
. When you create a new sample, experiment or datasSet of a given type, the set of properties is well defined. Also, the values of these properties are being type-checked.
The first step in creating a new entity type is to create a so called
property type
## :
pt_text
=
o
.
new_property_type
(
code
=
## 'MY_NEW_PROPERTY_TYPE'
,
label
=
'yet another property type'
,
description
=
'my first property'
,
dataType
=
## 'VARCHAR'
,
)
pt_text
.
save
()
pt_int
=
o
.
new_property_type
(
code
=
## 'MY_NUMBER'
,
label
=
'property contains a number'
,
dataType
=
## 'INTEGER'
,
)
pt_int
.
save
()
pt_voc
=
o
.
new_property_type
(
code
=
## 'MY_CONTROLLED_VOCABULARY'
,
label
=
'label me'
,
description
=
'give me a description'
,
dataType
=
## 'CONTROLLEDVOCABULARY'
,
vocabulary
=
## 'STORAGE'
,
)
pt_voc
.
save
()
pt_richtext
=
o
.
new_property_type
(
code
=
## 'MY_RICHTEXT_PROPERTY'
,
label
=
'richtext data'
,
description
=
'property contains rich text'
,
dataType
=
## 'MULTILINE_VARCHAR'
,
metaData
=
{
'custom_widget'
## :
## 'Word Processor'
}
)
pt_richtext
.
save
()
pt_spread
=
o
.
new_property_type
(
code
=
## 'MY_TABULAR_DATA'
,
label
=
'data in a table'
,
description
=
'property contains a spreadsheet'
,
dataType
=
## 'XML'
,
metaData
=
{
'custom_widget'
## :
## 'Spreadsheet'
}
)
pt_spread
.
save
()
## The
dataType
attribute can contain any of these values:
## INTEGER
## VARCHAR
## MULTILINE_VARCHAR
## REAL
## TIMESTAMP
## DATE
## BOOLEAN
## HYPERLINK
## XML
## CONTROLLEDVOCABULARY
## MATERIAL
## SAMPLE
When choosing
## CONTROLLEDVOCABULARY
, you must specify a
vocabulary
attribute (see example). Likewise, when choosing
## MATERIAL
, a
materialType
attribute must be provided.
To create a
richtext property
, use
## MULTILINE_VARCHAR
as
dataType
and set
metaData
to
{'custom_widget'
## :
## 'Word
## Processor'}
as shown in the example above.
To create a
tabular, spreadsheet-like property
, use
## XML
as
dataType
and set
metaData
to
{'custom_widget'
## :
## 'Spreadhseet'}
as shown in the example above.
## Note
: PropertyTypes that start with a $ are by definition
managedInternally
and therefore this attribute must be set to True.
create sample types / object types

The second step (after creating a property type, see above) is to create the
sample type
. The new name for
sample
is
object
. You can use both methods interchangeably:
new_sample_type()
==
new_object_type()
sample_type
=
o
.
new_sample_type
(
code
=
'my_own_sample_type'
,
# mandatory
generatedCodePrefix
=
## 'S'
,
# mandatory
description
=
''
,
autoGeneratedCode
=
## True
,
subcodeUnique
=
## False
,
listable
=
## True
,
showContainer
=
## False
,
showParents
=
## True
,
showParentMetadata
=
## False
,
validationPlugin
=
## 'Has_Parents'
# see plugins below
)
sample_type
.
save
()
## When
autoGeneratedCode
attribute is set to
## True
, then you don’t need to provide a value for
code
when you create a new sample. You can get the next autoGeneratedCode like this:
sample_type
.
get_next_sequence
()
# eg. 67
sample_type
.
get_next_code
()
# e.g. FLY77
From pyBIS 1.31.0 onwards, you can provide a
code
even for samples where its sample type has
autoGeneratedCode=True
to offer the same functionality as ELN-LIMS. In earlier versions of pyBIS, providing a code in this situation caused an error.
assign and revoke properties to sample type / object type

The third step, after saving the sample type, is to
assign or revoke properties
to the newly created sample type. This assignment procedure applies to all entity types (dataset type, experiment type).
sample_type
.
assign_property
(
prop
=
'diff_time'
,
# mandatory
section
=
''
,
ordinal
=
5
,
mandatory
=
## True
,
initialValueForExistingEntities
=
'initial value'
showInEditView
=
## True
,
showRawValueInForms
=
## True
)
sample_type
.
revoke_property
(
'diff_time'
)
sample_type
.
get_property_assignments
()
⚠️ Note: ordinal position
If a new property is assigned in a place of an existing property, the old property assignment ordinal value will be increased by 1
create a dataset type

The second step (after creating a
property type
, see above) is to create the
dataset type
. The third step is to
assign or revoke the properties
to the newly created dataset type.
dataset_type
=
o
.
new_dataset_type
(
code
=
'my_dataset_type'
,
# mandatory
description
=
## None
,
mainDataSetPattern
=
## None
,
mainDataSetPath
=
## None
,
disallowDeletion
=
## False
,
validationPlugin
=
## None
,
)
dataset_type
.
save
()
dataset_type
.
assign_property
(
'property_name'
)
dataset_type
.
revoke_property
(
'property_name'
)
dataset_type
.
get_property_assignments
()
create an experiment type / collection type

The second step (after creating a
property type
, see above) is to create the
experiment type
.
The new name for
experiment
is
collection
. You can use both methods interchangeably:
new_experiment_type()
==
new_collection_type()
experiment_type
=
o
.
new_experiment_type
(
code
,
description
=
## None
,
validationPlugin
=
## None
,
)
experiment_type
.
save
()
experiment_type
.
assign_property
(
'property_name'
)
experiment_type
.
revoke_property
(
'property_name'
)
experiment_type
.
get_property_assignments
()
create material types

Materials and material types are deprecated in newer versions of openBIS.
material_type
=
o
.
new_material_type
(
code
,
description
=
## None
,
validationPlugin
=
## None
,
)
material_type
.
save
()
material_type
.
assign_property
(
'property_name'
)
material_type
.
revoke_property
(
'property_name'
)
material_type
.
get_property_assignments
()
create plugins

Plugins are Jython scripts that can accomplish more complex data-checks than ordinary types and vocabularies can achieve. They are assigned to entity types (dataset type, sample type etc).
Documentation and examples can be found here
pl
=
o
.
new_plugin
(
name
=
'my_new_entry_validation_plugin'
,
pluginType
=
## 'ENTITY_VALIDATION'
,
# or 'DYNAMIC_PROPERTY' or 'MANAGED_PROPERTY',
entityKind
=
## None
,
# or 'SAMPLE', 'MATERIAL', 'EXPERIMENT', 'DATA_SET'
script
=
'def calculate(): pass'
# a JYTHON script
)
pl
.
save
()
Users, Groups and RoleAssignments

Users can only login into the openBIS system when:
they are present in the authentication system (e.g. LDAP)
the username/password is correct
the user’s mail address needs is present
the user is already added to the openBIS user list (see below)
the user is assigned a role which allows a login, either directly assigned or indirectly assigned via a group membership
o
.
get_groups
()
group
=
o
.
new_group
(
code
=
'group_name'
,
description
=
'...'
)
group
=
o
.
get_group
(
'group_name'
)
group
.
save
()
group
.
assign_role
(
role
=
## 'ADMIN'
,
space
=
## 'DEFAULT'
)
group
.
get_roles
()
group
.
revoke_role
(
role
=
## 'ADMIN'
,
space
=
## 'DEFAULT'
)
group
.
add_members
([
'admin'
])
group
.
get_members
()
group
.
del_members
([
'admin'
])
group
.
delete
()
o
.
get_persons
()
person
=
o
.
new_person
(
userId
=
'username'
)
person
.
space
=
## 'USER_SPACE'
person
.
save
()
# person.delete() is currently not possible.
person
.
assign_role
(
role
=
## 'ADMIN'
,
space
=
## 'MY_SPACE'
)
person
.
assign_role
(
role
=
## 'OBSERVER'
)
person
.
get_roles
()
person
.
revoke_role
(
role
=
## 'ADMIN'
,
space
=
## 'MY_SPACE'
)
person
.
revoke_role
(
role
=
## 'OBSERVER'
)
o
.
get_role_assignments
()
o
.
get_role_assignments
(
space
=
## 'MY_SPACE'
)
o
.
get_role_assignments
(
group
=
## 'MY_GROUP'
)
ra
=
o
.
get_role_assignment
(
techId
)
ra
.
delete
()
## Spaces

Spaces are fundamental way in openBIS to divide access between groups. Within a space, data can be easily shared. Between spaces, people need to be given specific access rights (see section above). The structure in openBIS is as follows:
space
project
experiment / collection
sample / object
dataset
space
=
o
.
new_space
(
code
=
'space_name'
,
description
=
''
)
space
.
save
()
o
.
get_spaces
(
start_with
=
0
,
# start_with and count
count
=
10
,
# enable paging
)
space
=
o
.
get_space
(
## 'MY_SPACE'
)
# get individual attributes
space
.
code
space
.
description
space
.
registrator
space
.
registrationDate
space
.
modifier
space
.
modificationDate
# set individual attribute
# most of the attributes above are set automatically and cannot be modified.
space
.
description
=
'...'
# get all attributes as a dictionary
space
.
attrs
.
all
()
space
.
delete
(
'reason for deletion'
)
## Projects

Projects live within spaces and usually contain experiments (aka collections):
space
project
experiment / collection
sample / object
dataset
project
=
o
.
new_project
(
space
=
space
,
code
=
'project_name'
,
description
=
'some project description'
)
project
=
space
.
new_project
(
code
=
'project_code'
,
description
=
'project description'
)
project
.
save
()
o
.
get_projects
(
space
=
## 'MY_SPACE'
,
# show only projects in MY_SPACE
start_with
=
0
,
# start_with and count
count
=
10
,
# enable paging
)
o
.
get_projects
(
space
=
## 'MY_SPACE'
)
space
.
get_projects
()
project
.
get_experiments
()
# see details and limitations in Section 'search for experiments'
project
.
get_attachments
()
# deprecated, as attachments are not compatible with ELN-LIMS.
# Attachments are an old concept and should not be used anymore.
p
.
add_attachment
(
# deprecated, see above
fileName
=
'testfile'
,
description
=
'another file'
,
title
=
'one more attachment'
)
project
.
download_attachments
(
<
path
or
cwd
>
)
# deprecated, see above
# get individual attributes
project
.
code
project
.
description
# set individual attribute
project
.
description
=
'...'
# get all attributes as a dictionary
project
.
attrs
.
all
()
project
.
freeze
=
## True
project
.
freezeForExperiments
=
## True
project
.
freezeForSamples
=
## True
## Experiments / Collections

## Experiments live within projects:
space
project
experiment / collection
sample / object
dataset
The new name for
experiment
is
collection
. You can use boths names interchangeably:
get_experiment()
=
get_collection()
new_experiment()
=
new_collection()
get_experiments()
=
get_collections()
create a new experiment

exp
=
o
.
new_experiment
code
=
## 'MY_NEW_EXPERIMENT'
,
type
=
## 'DEFAULT_EXPERIMENT'
,
project
=
## '/MY_SPACE/YEASTS'
)
exp
.
save
()
search for experiments

experiments
=
o
.
get_experiments
(
project
=
## 'YEASTS'
,
space
=
## 'MY_SPACE'
,
type
=
## 'DEFAULT_EXPERIMENT'
,
tags
=
'*'
,
finished_flag
=
## False
,
props
=
[
'name'
,
'finished_flag'
]
)
experiments
=
project
.
get_experiments
()
experiment
=
experiments
[
0
]
# get first experiment of result list
experiment
=
experiment
for
experiment
in
experiments
## :
# iterate over search results
print
(
experiment
.
props
.
all
())
dataframe
=
experiments
.
df
# get Pandas DataFrame of result list
exp
=
o
.
get_experiment
(
## '/MY_SPACE/MY_PROJECT/MY_EXPERIMENT'
)
Note: Attributes download
## The
get_experiments()
method, by default, returns fewer details to make the download process faster.
However, if you want to include specific attributes in the results, you can do so by using the
attrs
parameter.
## The
get_experiments()
method results include only
identifier
,
permId
,
type
,
registrator
,
registrationDate
,
modifier
,
modificationDate
experiments = o.get_experiments(
project       = 'YEASTS',
space         = 'MY_SPACE',
type          = 'DEFAULT_EXPERIMENT',
attrs          = [""parents"", ""children""]
)",Jupyter Notebook,0,en_20.10.0-11_software-developer-documentation_apis_python-v3-api_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_python-v3-api.txt,2025-09-30T12:09:01.367390Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_python-v3-api:1,Python (V3 API) - pyBIS!,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/python-v3-api.html,openbis,"identifier             permId                type               registrator    registrationDate     modifier    modificationDate     parents                    children
--  ---------------------  --------------------  -----------------  -------------  -------------------  ----------  -------------------  -------------------------  ----------
0  /MY_SPACE/YEASTS/EXP1  20230407070122991-46  DEFAULT_EXPERIMENT  admin          2023-04-07 09:01:23  admin       2023-04-07 09:02:22  ['/MY_SPACE/YEASTS/EXP2']  []
## ⚠️ Clarification
get_datasets()
method is always downloading object properties
Not downloaded attributes (e.g
parents
,
children
) will not be removed upon
save()
unless explicitly done by the user.
## None
values of list
attributes
are ignored during saving process
Experiment attributes

exp
.
attrs
.
all
()
# returns all attributes as a dict
exp
.
attrs
.
tags
=
[
'some'
,
'tags'
]
exp
.
tags
=
[
'some'
,
'tags'
]
# same thing
exp
.
save
()
exp
.
code
exp
.
description
exp
.
registrator
...
exp
.
project
=
'my_project'
exp
.
space
=
'my_space'
exp
.
freeze
=
## True
exp
.
freezeForDataSets
=
## True
exp
.
freezeForSamples
=
## True
exp
.
save
()
# needed to save/update the changed attributes and properties
Experiment properties

Getting properties
experiment
.
props
==
ds
.
p
# you can use either .props or .p to access the properties
experiment
.
p
# in Jupyter: show all properties in a nice table
experiment
.
p
()
# get all properties as a dict
experiment
.
props
.
all
()
# get all properties as a dict
experiment
.
p
(
'prop1'
,
'prop2'
)
# get some properties as a dict
experiment
.
p
.
get
(
'$name'
)
# get the value of a property
experiment
.
p
[
'property'
]
# get the value of a property
Setting properties
experiment
.
experiment
=
'first_exp'
# assign sample to an experiment
experiment
.
project
=
'my_project'
# assign sample to a project
experiment
.
p
.
+
## TAB
# in Jupyter/IPython: show list of available properties
experiment
.
p
.
my_property_
+
## TAB
# in Jupyter/IPython: show datatype or controlled vocabulary
experiment
.
p
[
'my_property'
]
=
""value""
# set the value of a property
experiment
.
p
.
set
(
'my_property, '
value
')   # set the value of a property
experiment
.
p
.
my_property
=
""some value""
# set the value of a property
experiment
.
p
.
set
({
'my_property'
## :
'value'
})
# set the values of some properties
experiment
.
set_props
({
key
## :
value
})
# set the values of some properties
experiment
.
save
()
# needed to save/update the changed attributes and properties
## Samples / Objects

## Samples usually live within experiments/collections:
space
project
experiment / collection
sample / object
dataset
The new name for
sample
is
object
. You can use boths names interchangeably:
get_sample()
=
get_object()
new_sample()
=
new_object()
get_samples()
=
get_objects()
etc.
sample
=
o
.
new_sample
(
type
=
## 'YEAST'
,
space
=
## 'MY_SPACE'
,
experiment
=
## '/MY_SPACE/MY_PROJECT/EXPERIMENT_1'
,
parents
=
[
parent_sample
,
## '/MY_SPACE/YEA66'
],
# you can use either permId, identifier
children
=
[
child_sample
],
# or sample object
props
=
{
""name""
## :
""some name""
,
""description""
## :
""something interesting""
}
)
sample
=
space
.
new_sample
(
type
=
## 'YEAST'
)
sample
.
save
()
sample
=
o
.
get_sample
(
## '/MY_SPACE/MY_SAMPLE_CODE'
)
sample
=
o
.
get_sample
(
'20170518112808649-52'
)
samples
=
o
.
get_samples
(
type
=
## 'UNKNOWN'
)
# see details and limitations in Section 'search for samples / objects'
# get individual attributes
sample
.
space
sample
.
code
sample
.
permId
sample
.
identifier
sample
.
type
# once the sample type is defined, you cannot modify it
# set attribute
sample
.
space
=
## 'MY_OTHER_SPACE'
sample
.
experiment
# a sample can belong to one experiment only
sample
.
experiment
=
## '/MY_SPACE/MY_PROJECT/MY_EXPERIMENT'
sample
.
project
sample
.
project
=
## '/MY_SPACE/MY_PROJECT'
# only works if project samples are
enabled
sample
.
tags
sample
.
tags
=
[
'guten_tag'
,
'zahl_tag'
]
sample
.
attrs
.
all
()
# returns all attributes as a dict
sample
.
props
.
all
()
# returns all properties as a dict
sample
.
get_attachments
()
# deprecated, as attachments are not compatible with ELN-LIMS.
# Attachments are an old concept and should not be used anymore.
sample
.
download_attachments
(
<
path
or
cwd
>
)
# deprecated, see above
sample
.
add_attachment
(
'testfile.xls'
)
# deprecated, see above
sample
.
delete
(
'deleted for some reason'
)
create/update/delete many samples in a transaction

Creating a single sample takes some time. If you need to create many samples, you might want to create them in one transaction. This will transfer all your sample data at once. The Upside of this is the
gain in speed
. The downside: this is a
all-or-nothing
operation, which means, either all samples will be registered or none (if any error occurs).
create many samples in one transaction
trans
=
o
.
new_transaction
()
for
i
in
range
(
0
,
100
## ):
sample
=
o
.
new_sample
(
...
)
trans
.
add
(
sample
)
trans
.
commit
()
update many samples in one transaction
trans
=
o
.
new_transaction
()
for
sample
in
o
.
get_samples
(
count
=
100
## ):
sample
.
prop
.
some_property
=
'different value'
trans
.
add
(
sample
)
trans
.
commit
()
delete many samples in one transaction
trans
=
o
.
new_transaction
()
for
sample
in
o
.
get_samples
(
count
=
100
## ):
sample
.
mark_to_be_deleted
()
trans
.
add
(
sample
)
trans
.
reason
(
'go what has to go'
)
trans
.
commit
()
## Note:
You can use the
mark_to_be_deleted()
,
unmark_to_be_deleted()
and
is_marked_to_be_deleted()
methods to set and read the internal flag.
parents, children, components and container

sample
.
get_parents
()
sample
.
set_parents
([
## '/MY_SPACE/PARENT_SAMPLE_NAME'
)
sample
.
add_parents
(
## '/MY_SPACE/PARENT_SAMPLE_NAME'
)
sample
.
del_parents
(
## '/MY_SPACE/PARENT_SAMPLE_NAME'
)
sample
.
get_children
()
sample
.
set_children
(
## '/MY_SPACE/CHILD_SAMPLE_NAME'
)
sample
.
add_children
(
## '/MY_SPACE/CHILD_SAMPLE_NAME'
)
sample
.
del_children
(
## '/MY_SPACE/CHILD_SAMPLE_NAME'
)
# A Sample may belong to another Sample, which acts as a container.
# As opposed to DataSets, a Sample may only belong to one container.
sample
.
container
# returns a sample object
sample
.
container
=
## '/MY_SPACE/CONTAINER_SAMPLE_NAME'
# watch out, this will change the identifier of the sample to:
# /MY_SPACE/CONTAINER_SAMPLE_NAME:SAMPLE_NAME
sample
.
container
=
''
# this will remove the container.
# A Sample may contain other Samples, in order to act like a container (see above)
# caveat: containers are NOT compatible with ELN-LIMS
# The Sample-objects inside that Sample are called «components» or «contained Samples»
# You may also use the xxx_contained() functions, which are just aliases.
sample
.
get_components
()
sample
.
set_components
(
## '/MY_SPACE/COMPONENT_NAME'
)
sample
.
add_components
(
## '/MY_SPACE/COMPONENT_NAME'
)
sample
.
del_components
(
## '/MY_SPACE/COMPONENT_NAME'
)
sample tags

sample
.
get_tags
()
sample
.
set_tags
(
'tag1'
)
sample
.
add_tags
([
'tag2'
,
'tag3'
])
sample
.
del_tags
(
'tag1'
)
Sample attributes and properties

Getting properties
sample
.
attrs
.
all
()
# returns all attributes as a dict
sample
.
attribute_name
# return the attribute value
sample
.
props
==
ds
.
p
# you can use either .props or .p to access the properties
sample
.
p
# in Jupyter: show all properties in a nice table
sample
.
p
()
# get all properties as a dict
sample
.
props
.
all
()
# get all properties as a dict
sample
.
p
(
'prop1'
,
'prop2'
)
# get some properties as a dict
sample
.
p
.
get
(
'$name'
)
# get the value of a property
sample
.
p
[
'property'
]
# get the value of a property
Setting properties
sample
.
experiment
=
'first_exp'
# assign sample to an experiment
sample
.
project
=
'my_project'
# assign sample to a project
sample
.
p
.
+
## TAB
# in Jupyter/IPython: show list of available properties
sample
.
p
.
my_property_
+
## TAB
# in Jupyter/IPython: show datatype or controlled vocabulary
sample
.
p
[
'my_property'
]
=
""value""
# set the value of a property
sample
.
p
.
set
(
'my_property, '
value
')   # set the value of a property
sample
.
p
.
my_property
=
""some value""
# set the value of a property
sample
.
p
.
set
({
'my_property'
## :
'value'
})
# set the values of some properties
sample
.
set_props
({
key
## :
value
})
# set the values of some properties
sample
.
save
()
# needed to save/update the attributes and properties
search for samples / objects

The result of a search is always list, even when no items are found. The
.df
attribute returns
the Pandas dataFrame of the results.
samples
=
o
.
get_samples
(
space
=
## 'MY_SPACE'
,
type
=
## 'YEAST'
,
tags
=
[
'*'
],
# only sample with existing tags
start_with
=
0
,
# start_with and count
count
=
10
,
# enable paging
where
=
{
## ""$SOME.WEIRD-PROP""
## :
""hello""
# only receive samples where properties match
}
registrationDate
=
""2020-01-01""
,
# date format: YYYY-MM-DD
modificationDate
=
""<2020-12-31""
,
# use > or < to search for specified date and later / earlier
attrs
=
[
# show these attributes in the dataFrame
'sample.code'
,
'registrator.email'
,
'type.generatedCodePrefix'
],
parent_property
=
'value'
,
# search in a parent's property
child_property
=
'value'
,
# search in a child's property
container_property
=
'value'
# search in a container's property
parent
=
## '/MY_SPACE/PARENT_SAMPLE'
,
# sample has this as its parent
parent
=
'*'
,
# sample has at least one parent
child
=
## '/MY_SPACE/CHILD_SAMPLE'
,
child
=
'*'
,
# sample has at least one child
container
=
## 'MY_SPACE/CONTAINER'
,
container
=
'*'
# sample lives in a container
props
=
[
## '$NAME'
,
## 'MATING_TYPE'
]
# show these properties in the result
)
sample
=
samples
[
9
]
# get the 10th sample
# of the search results
sample
=
samples
[
## '/SPACE/AABC'
]
# same, fetched by identifier
for
sample
in
samples
## :
# iterate over the
print
(
sample
.
code
)
# search results
samples
.
df
# returns a Pandas DataFrame object
samples
=
o
.
get_samples
(
props
=
""*""
)
# retrieve all properties of all samples
Note: Attributes download
## The
get_samples()
method, by default, returns fewer details to make the download process faster.
However, if you want to include specific attributes in the results, you can do so by using the
attrs
parameter.
## The
get_samples()
method results include only
identifier
,
permId
,
type
,
registrator
,
registrationDate
,
modifier
,
modificationDate
samples
=
o
.
get_samples
(
space
=
## 'MY_SPACE'
,
type
=
## 'YEAST'
,
attrs
=
[
""parents""
,
""children""
]
)
identifier
permId
type
registrator
registrationDate
modifier
modificationDate
parents
children
--
---------------------
--------------------
-----------------
-------------
-------------------
----------
-------------------
-------------------------
----------
0
/
## MY_SPACE
/
## YEASTS
/
## SAMPLE1
20230407070121337
-
47
## YEAST
admin
2023
-
04
-
07
09
## :
06
## :
23
admin
2023
-
04
-
07
09
## :
06
## :
22
[
## '/MY_SPACE/YEASTS/EXP2'
]
[]
## ⚠️ Clarification
get_samples()
method is always downloading object properties
Not downloaded attributes (e.g
parents
,
children
) will not be removed upon
save()
unless explicitly done by the user.
## None
values of list
attributes
are ignored during saving process
## Example:
# get sample with get_sample() method
sample
=
o
.
get_sample
(
## '/DEFAULT/DEFAULT/EXP2'
)
sample
## Out
[
1
## ]:
attribute
value
-------------------
------------------------------
code
## EXP2
permId
20230823205338303
-
49
identifier
/
## DEFAULT
/
## DEFAULT
/
## EXP2
type
## EXPERIMENTAL_STEP
project
/
## DEFAULT
/
## DEFAULT
parents
[]
# empty list
children
[
## '/DEFAULT/DEFAULT/EXP3'
]
components
[]
# get sample with get_samples() method
samples
=
o
.
get_samples
(
identifier
=
## '/DEFAULT/DEFAULT/EXP2'
)
samples
[
0
]
## Out
[
1
## ]:
attribute
value
-------------------
------------------------------
code
## EXP2
permId
20230823205338303
-
49
identifier
/
## DEFAULT
/
## DEFAULT
/
## EXP2
type
## EXPERIMENTAL_STEP
project
/
## DEFAULT
/
## DEFAULT
parents
# None value
children
# None value
components
[]
freezing samples

sample
.
freeze
=
## True
sample
.
freezeForComponents
=
## True
sample
.
freezeForChildren
=
## True
sample
.
freezeForParents
=
## True
sample
.
freezeForDataSets
=
## True
## Datasets

Datasets are by all means the most important openBIS entity. The actual files are stored as datasets; all other openBIS entities mainly are necessary to annotate and to structure the data:
space
project
experiment / collection
sample / object
dataset
working with existing dataSets

search for datasets
This example does the following
search for all datasets of type
## SCANS
, retrieve the first 10 entries
print out all properties
print the list of all files in this dataset
download the dataset
datasets
=
sample
.
get_datasets
(
type
=
## 'SCANS'
,
start_with
=
0
,
count
=
10
)
for
dataset
in
datasets
## :
print
(
dataset
.
props
())
print
(
dataset
.
file_list
)
dataset
.
download
()
dataset
=
datasets
[
0
]
Note: Attributes download
## The
get_datasets()
method, by default, returns fewer details to make the download process faster.
However, if you want to include specific attributes in the results, you can do so by using the
attrs
parameter.
## The
get_datasets()
method results include only
permId
,
type
,
experiment
,
sample
,
registrationDate
,
modificationDate
,
location
,
status
,
presentInArchive
,
size
datasets
=
o
.
get_datasets
(
space
=
## 'MY_SPACE'
,
attrs
=
[
""parents""
,
""children""
]
)
permId
type
experiment
sample
registrationDate
modificationDate
location
status
presentInArchive
size
parents
children
--
--------------------
--------
------------------------
---------------------
-------------------
-------------------
---------------------------------------
---------
------------------
------
------------------------
------------------------
0
20230526101657295
-
48
## RAW_DATA
/
## MY_SPACE
/
## DEFAULT
/
## DEFAULT
/
## MY_SPACE
/
## DEFAULT
/
## EXP1
2023
-
05
-
26
12
## :
16
## :
58
2023
-
05
-
26
12
## :
17
## :
37
1
## F60C7DC
-
63
## D8
-
4
## C07
/
20230526101657295
-
48
## AVAILABLE
## False
469
[]
[
'20230526101737019-49'
]
1
20230526101737019
-
49
## RAW_DATA
/
## MY_SPACE
/
## DEFAULT
/
## DEFAULT
/
## MY_SPACE
/
## DEFAULT
/
## EXP1
2023
-
05
-
26
12
## :
17
## :
37
2023
-
05
-
26
12
## :
17
## :
37
1
## F60C7DC
-
63
## D8
-
4
## C07
/
20230526101737019
-
49
## AVAILABLE
## False
127
[
'20230526101657295-48'
]
[]
## ⚠️ Clarification
get_datasets()
method is always downloading object properties
Not downloaded attributes (e.g
parents
,
children
) will not be removed upon
save()
unless explicitly done by the user.
## None
values of list
attributes
are ignored during saving process
## More dataset functions:
ds
=
o
.
get_dataset
(
'20160719143426517-259'
)
ds
.
get_parents
()
ds
.
get_children
()
ds
.
sample
ds
.
experiment
ds
.
physicalData
ds
.
status
# AVAILABLE   LOCKED   ARCHIVED
# ARCHIVE_PENDING   UNARCHIVE_PENDING
# BACKUP_PENDING
ds
.
archive
()
# archives a dataset, i.e. moves it to a slower but cheaper diskspace (tape).
# archived datasets cannot be downloaded, they need to be unarchived first.
# This is an asynchronous process,
# check ds.status regularly until the dataset becomes 'ARCHIVED'
ds
.
unarchive
()
# this starts an asynchronous process which gets the dataset from the tape.
# Check ds.status regularly until it becomes 'AVAILABLE'
ds
.
attrs
.
all
()
# returns all attributes as a dict
ds
.
props
.
all
()
# returns all properties as a dict
ds
.
add_attachment
()
# Deprecated. Attachments usually contain meta-data
ds
.
get_attachments
()
# about the dataSet, not the data itself.
ds
.
download_attachments
(
<
path
or
cwd
>
)
# Deprecated, as attachments are not compatible with ELN-LIMS.
# Attachments are an old concept and should not be used anymore.
download dataSets

o
.
download_prefix
# used for download() and symlink() method.
# Is set to data/hostname by default, but can be changed.
ds
.
get_files
(
start_folder
=
""/""
)
# get file list as Pandas dataFrame
ds
.
file_list
# get file list as array
ds
.
file_links
# file list as a dict containing direct https links
ds
.
download
()
# simply download all files to data/hostname/permId/
ds
.
download
(
destination
=
'my_data'
,
# download files to folder my_data/
create_default_folders
=
## False
,
# ignore the /original/DEFAULT folders made by openBIS
wait_until_finished
=
## False
,
# download in background, continue immediately
workers
=
10
# 10 downloads parallel (default)
)
ds
.
download_path
# returns the relative path (destination) of the files after a ds.download()
ds
.
is_physical
()
# TRUE if dataset is physically
link dataSets

Instead of downloading a dataSet, you can create a symbolic link to a dataSet in the openBIS dataStore. To do that, the openBIS dataStore needs to be mounted first (see mount method above).
## Note:
Symbolic links and the mount() feature currently do not work with Windows.
o
.
download_prefix
# used for download() and symlink() method.
# Is set to data/hostname by default, but can be changed.
ds
.
symlink
()
# creates a symlink for this dataset: data/hostname/permId
# tries to mount openBIS instance
# in case it is not mounted yet
ds
.
symlink
(
target_dir
=
'data/dataset_1/'
,
# default target_dir is: data/hostname/permId
replace_if_symlink_exists
=
## True
)
ds
.
is_symlink
()
dataSet attributes and properties

Getting properties
ds
.
attrs
.
all
()
# returns all attributes as a dict
ds
.
attribute_name
# return the attribute value
ds
.
props
==
ds
.
p
# you can use either .props or .p to access the properties
ds
.
p
# in Jupyter: show all properties in a nice table
ds
.
p
()
# get all properties as a dict
ds
.
props
.
all
()
# get all properties as a dict
ds
.
p
(
'prop1'
,
'prop2'
)
# get some properties as a dict
ds
.
p
.
get
(
'$name'
)
# get the value of a property
ds
.
p
[
'property'
]
# get the value of a property
Setting properties
ds
.
experiment
=
'first_exp'
# assign dataset to an experiment
ds
.
sample
=
'my_sample'
# assign dataset to a sample
ds
.
p
.
+
## TAB
# in Jupyter/IPython: show list of available properties
ds
.
p
.
my_property_
+
## TAB
# in Jupyter/IPython: show datatype or controlled vocabulary
ds
.
p
[
'my_property'
]
=
""value""
# set the value of a property
ds
.
p
.
set
(
'my_property, '
value
')   # set the value of a property
ds
.
p
.
my_property
=
""some value""
# set the value of a property
ds
.
p
.
set
({
'my_property'
## :
'value'
})
# set the values of some properties
ds
.
set_props
({
key
## :
value
})
# set the values of some properties
search for dataSets

The result of a search is always list, even when no items are found
## The
.df
attribute returns the Pandas dataFrame of the results
datasets
=
o
.
get_datasets
(
type
=
## 'MY_DATASET_TYPE'
,
**
{
## ""SOME.WEIRD:PROP""
## :
""value""
},
# property name contains a dot or a
# colon: cannot be passed as an argument
start_with
=
0
,
# start_with and count
count
=
10
,
# enable paging
registrationDate
=
""2020-01-01""
,
# date format: YYYY-MM-DD
modificationDate
=
""<2020-12-31""
,
# use > or < to search for specified date and later / earlier
parent_property
=
'value'
,
# search in a parent's property
child_property
=
'value'
,
# search in a child's property
container_property
=
'value'
# search in a container's property
parent
=
## '/MY_SPACE/PARENT_DS'
,
# has this dataset as its parent
parent
=
'*'
,
# has at least one parent dataset
child
=
## '/MY_SPACE/CHILD_DS'
,
child
=
'*'
,
# has at least one child dataset
container
=
## 'MY_SPACE/CONTAINER_DS'
,
container
=
'*'
,
# belongs to a container dataset
attrs
=
[
# show these attributes in the dataFrame
'sample.code'
,
'registrator.email'
,
'type.generatedCodePrefix'
],
props
=
[
## '$NAME'
,
## 'MATING_TYPE'
]
# show these properties in the result
)
datasets
=
o
.
get_datasets
(
props
=
""*""
)
# retrieve all properties of all dataSets
dataset
=
datasets
[
0
]
# get the first dataset in the search result
for
dataset
in
datasets
## :
# iterate over the datasets
...
df
=
datasets
.
df
# returns a Pandas dataFrame object of the search results
In some cases, you might want to retrieve precisely certain datasets. This can be achieved by
methods chaining (but be aware, it might not be very performant):
datasets
=
o
.
get_experiments
(
project
=
## 'YEASTS'
)
\
.
get_samples
(
type
=
## 'FLY'
)
\
.
get_datasets
(
type
=
## 'ANALYZED_DATA'
,
props
=
[
## 'MY_PROPERTY'
],
## MY_PROPERTY
=
'some analyzed data'
)
## another example:
datasets
=
o
.
get_experiment
(
## '/MY_NEW_SPACE/MY_PROJECT/MY_EXPERIMENT4'
)
\
.
get_samples
(
type
=
## 'UNKNOWN'
)
\
.
get_parents
()
\
.
get_datasets
(
type
=
## 'RAW_DATA'
)
freeze dataSets

once a dataSet has been frozen, it cannot be changed by anyone anymore
so be careful!
ds
.
freeze
=
## True
ds
.
freezeForChildren
=
## True
ds
.
freezeForParents
=
## True
ds
.
freezeForComponents
=
## True
ds
.
freezeForContainers
=
## True
ds
.
save
()
create a new dataSet

ds_new
=
o
.
new_dataset
(
type
=
## 'ANALYZED_DATA'
,
experiment
=
## '/SPACE/PROJECT/EXP1'
,
sample
=
## '/SPACE/SAMP1'
,
files
=
[
'my_analyzed_data.dat'
],
props
=
{
'name'
## :
'some good name'
,
'description'
## :
'...'
}
)
ds_new
.
save
()
create dataSet with zipfile

## DataSet containing one zipfile which will be unzipped in openBIS:
ds_new
=
o
.
new_dataset
(
type
=
## 'RAW_DATA'
,
sample
=
## '/SPACE/SAMP1'
,
zipfile
=
'my_zipped_folder.zip'
,
)
ds_new
.
save
()
create dataSet with mixed content

mixed content means: folders and files are provided
a relative specified folder (and all its content) will end up in the root, while keeping its structure
../measurements/
–>
/measurements/
some/folder/somewhere/
–>
/somewhere/
relative files will also end up in the root
my_file.txt
–>
/my_file.txt
../somwhere/else/my_other_file.txt
–>
/my_other_file.txt
some/folder/file.txt
–>
/file.txt
useful if DataSet contains files and folders
the content of the folder will be zipped (on-the-fly) and uploaded to openBIS
openBIS will keep the folder structure intact
relative path will be shortened to its basename. For example:
local
openBIS
../../myData/
myData/
some/experiment/results/
results/
ds_new
=
o
.
new_dataset
(
type
=
## 'RAW_DATA'
,
sample
=
## '/SPACE/SAMP1'
,
files
=
[
'../measurements/'
,
'my_analyis.ipynb'
,
'results/'
]
)
ds_new
.
save
()
create dataSet container

A DataSet of kind=CONTAINER contains other DataSets, but no files:
ds_new
=
o
.
new_dataset
(
type
=
## 'ANALYZED_DATA'
,
experiment
=
## '/SPACE/PROJECT/EXP1'
,
sample
=
## '/SPACE/SAMP1'
,
kind
=
## 'CONTAINER'
,
props
=
{
'name'
## :
'some good name'
,
'description'
## :
'...'
}
)
ds_new
.
save
()
get, set, add and remove parent datasets

dataset
.
get_parents
()
dataset
.
set_parents
([
'20170115220259155-412'
])
dataset
.
add_parents
([
'20170115220259155-412'
])
dataset
.
del_parents
([
'20170115220259155-412'
])
get, set, add and remove child datasets

dataset
.
get_children
()
dataset
.
set_children
([
'20170115220259155-412'
])
dataset
.
add_children
([
'20170115220259155-412'
])
dataset
.
del_children
([
'20170115220259155-412'
])
dataSet containers

A DataSet may belong to other DataSets, which must be of kind=CONTAINER
As opposed to Samples, DataSets may belong (contained) to more than one DataSet-container
caveat: containers are NOT compatible with ELN-LIMS
dataset
.
get_containers
()
dataset
.
set_containers
([
'20170115220259155-412'
])
dataset
.
add_containers
([
'20170115220259155-412'
])
dataset
.
del_containers
([
'20170115220259155-412'
])
a DataSet of kind=CONTAINER may contain other DataSets, to act like a folder (see above)
the DataSet-objects inside that DataSet are called components or contained DataSets
you may also use the xxx_contained() functions, which are just aliases.
caveat: components are NOT compatible with ELN-LIMS
dataset
.
get_components
()
dataset
.
set_components
([
'20170115220259155-412'
])
dataset
.
add_components
([
'20170115220259155-412'
])
dataset
.
del_components
([
'20170115220259155-412'
])
## Semantic Annotations

create semantic annotation for sample type ‘UNKNOWN’:
sa
=
o
.
new_semantic_annotation
(
entityType
=
## 'UNKNOWN'
,
predicateOntologyId
=
'po_id'
,
predicateOntologyVersion
=
'po_version'
,
predicateAccessionId
=
'pa_id'
,
descriptorOntologyId
=
'do_id'
,
descriptorOntologyVersion
=
'do_version'
,
descriptorAccessionId
=
'da_id'
)
sa
.
save
()
Create semantic annotation for property type (predicate and descriptor values omitted for brevity)
sa
=
o
.
new_semantic_annotation
(
propertyType
=
## 'DESCRIPTION'
,
...
)
sa
.
save
()
## Create
semantic annotation for sample property assignment (predicate and descriptor values omitted for brevity)
sa
=
o
.
new_semantic_annotation
(
entityType
=
## 'UNKNOWN'
,
propertyType
=
## 'DESCRIPTION'
,
...
)
sa
.
save
()
## Create
a semantic annotation directly from a sample type. Will also create sample property assignment annotations when propertyType is given:
st
=
o
.
get_sample_type
(
## ""ORDER""
)
st
.
new_semantic_annotation
(
...
)
Get all
semantic annotations
o
.
get_semantic_annotations
()
## Get
semantic annotation by perm id
sa
=
o
.
get_semantic_annotation
(
""20171015135637955-30""
)
## Update
semantic annotation
sa
.
predicateOntologyId
=
'new_po_id'
sa
.
descriptorOntologyId
=
'new_do_id'
sa
.
save
()
## Delete
semantic annotation
sa
.
delete
(
'reason'
)
## Tags

new_tag
=
o
.
new_tag
(
code
=
'my_tag'
,
description
=
'some descriptive text'
)
new_tag
.
description
=
'some new description'
new_tag
.
save
()
o
.
get_tags
()
o
.
get_tag
(
'/username/TAG_Name'
)
o
.
get_tag
(
'TAG_Name'
)
tag
.
get_experiments
()
tag
.
get_samples
()
tag
.
get_owner
()
# returns a person object
tag
.
delete
(
'why?'
)
Vocabulary and VocabularyTerms

An entity such as Sample (Object), Experiment (Collection), Material or DataSet can be of a specific
entity type
## :
## Sample Type (Object Type)
## Experiment Type (Collection Type)
DataSet Type
## Material Type
Every type defines which
## Properties
may be defined. Properties act like
## Attributes
, but they are type-specific. Properties can contain all sorts of information, such as free text, XML, Hyperlink, Boolean and also
## Controlled Vocabulary
. Such a Controlled Vocabulary consists of many
VocabularyTerms
. These terms are used to only allow certain values entered in a Property field.
So for example, you want to add a property called
## Animal
to a Sample and you want to control which terms are entered in this Property field. For this you need to do a couple of steps:
create a new vocabulary
AnimalVocabulary
add terms to that vocabulary:
## Cat, Dog, Mouse
create a new PropertyType (e.g.
## Animal
) of DataType
## CONTROLLEDVOCABULARY
and assign the
AnimalVocabulary
to it
create a new SampleType (e.g.
## Pet
) and
assign
the created PropertyType to that Sample type.
If you now create a new Sample of type
## Pet
you will be able to add a property
## Animal
to it which only accepts the terms
## Cat, Dog
or
## Mouse
.
create new Vocabulary with three VocabularyTerms
voc
=
o
.
new_vocabulary
(
code
=
## 'BBB'
,
description
=
'description of vocabulary aaa'
,
urlTemplate
=
'https://ethz.ch'
,
terms
=
[
{
""code""
## :
'term_code1'
,
""label""
## :
""term_label1""
,
""description""
## :
""term_description1""
},
{
""code""
## :
'term_code2'
,
""label""
## :
""term_label2""
,
""description""
## :
""term_description2""
},
{
""code""
## :
'term_code3'
,
""label""
## :
""term_label3""
,
""description""
## :
""term_description3""
}
]
)
voc
.
save
()
voc
.
vocabulary
=
'description of vocabulary BBB'
voc
.
chosenFromList
=
## True
voc
.
save
()
# update
create additional VocabularyTerms
term
=
o
.
new_term
(
code
=
## 'TERM_CODE_XXX'
,
vocabularyCode
=
## 'BBB'
,
label
=
'here comes a label'
,
description
=
'here might appear a meaningful description'
)
term
.
save
()
update VocabularyTerms
To change the ordinal of a term, it has to be moved either to the top with the
.move_to_top()
method or after another term using the
.move_after_term('TERM_BEFORE')
method.
voc
=
o
.
get_vocabulary
(
## 'STORAGE'
)
term
=
voc
.
get_terms
()[
## 'RT'
]
term
.
label
=
## ""Room Temperature""
term
.
official
=
## True
term
.
move_to_top
()
term
.
move_after_term
(
'-40'
)
term
.
save
()
term
.
delete
()
Change ELN Settings via pyBIS

## Main Menu

The ELN settings are stored as a
JSON string
in the
$eln_settings
property of the
## GENERAL_ELN_SETTINGS
sample. You can show the
Main Menu settings
## like this:
import
json
settings_sample
=
o
.
get_sample
(
## ""/ELN_SETTINGS/GENERAL_ELN_SETTINGS""
)
settings
=
json
.
loads
(
settings_sample
.
props
[
""$eln_settings""
])
print
(
settings
[
""mainMenu""
])
{
'showLabNotebook'
## :
## True
,
'showInventory'
## :
## True
,
'showStock'
## :
## True
,
'showObjectBrowser'
## :
## True
,
'showExports'
## :
## True
,
'showStorageManager'
## :
## True
,
'showAdvancedSearch'
## :
## True
,
'showUnarchivingHelper'
## :
## True
,
'showTrashcan'
## :
## False
,
'showVocabularyViewer'
## :
## True
,
'showUserManager'
## :
## True
,
'showUserProfile'
## :
## True
,
'showZenodoExportBuilder'
## :
## False
,
'showBarcodes'
## :
## False
,
'showDatasets'
## :
## True
}
To modify the
Main Menu settings
, you have to change the settings dictionary, convert it back to json and save the sample:
settings
[
'mainMenu'
][
'showTrashcan'
]
=
## False
settings_sample
.
props
[
'$eln_settings'
]
=
json
.
dumps
(
settings
)
settings_sample
.
save
()
## Storages

## The
ELN storages settings
can be found in the samples of project
## /ELN_SETTINGS/STORAGES
o
.
get_samples
(
project
=
## '/ELN_SETTINGS/STORAGES'
)
To change the settings, just change the sample’s properties and save the sample:
sto
=
o
.
get_sample
(
## '/ELN_SETTINGS/STORAGES/BENCH'
)
sto
.
props
()
{
'$name'
## :
## 'Bench'
,
'$storage.row_num'
## :
'1'
,
'$storage.column_num'
## :
'1'
,
'$storage.box_num'
## :
'9999'
,
'$storage.storage_space_warning'
## :
'80'
,
'$storage.box_space_warning'
## :
'80'
,
'$storage.storage_validation_level'
## :
## 'BOX_POSITION'
,
'$xmlcomments'
## :
## None
,
'$annotations_state'
## :
## None
}
sto
.
props
[
'$storage.box_space_warning'
]
=
'80'
sto
.
save
()
## Templates

## The
ELN templates settings
can be found in the samples of project
## /ELN_SETTINGS/TEMPLATES
o
.
get_samples
(
project
=
## '/ELN_SETTINGS/TEMPLATES'
)
To change the settings, use the same technique as shown above with the storages settings.
## Custom Widgets

To change the
Custom Widgets settings
, get the
property_type
and set the
metaData
## attribute:
pt
=
o
.
get_property_type
(
## 'YEAST.SOURCE'
)
pt
.
metaData
=
{
'custom_widget'
## :
## 'Spreadsheet'
}
pt
.
save
()
Currently, the value of the
custom_widget
key can be set to either
## Spreadsheet
(for tabular, Excel-like data)
## Word
## Processor
(for rich text data)
Things object

General flow of data processing in PyBIS consists of:
preparing a JSON request to OpenBIS
receiving a JSON response and validating it
packing it in user-friendly
class
containing some helper methods.
There are multiple classes implemented, depending on the initial PyBIS call it may change (e.g. pybis.sample.Sample for
get_sample()
or pybis.experiment.Experiment for
get_experiment()
).
## In
[
1
## ]:
experiment
=
o
.
get_experiment
(
## '/DEFAULT/DEFAULT/DEFAULT'
)
## In
[
2
## ]:
type
(
experiment
)
## Out
[
3
## ]:
pybis
.
experiment
.
## Experiment
For methods returning multiple results (e.g.
get_samples()
,
get_experiments()
,
get_groups()
), a special class has been designed to hold the response. This class is pybis.things.Things.
## In
[
1
## ]:
experiments
=
o
.
get_experiments
()
## In
[
2
## ]:
type
(
experiments
)
## Out
[
3
## ]:
pybis
.
things
.
## Things
## Things
class offers three main ways to access the received data:
Json response
## Objects
DataFrame
Accessing the Json response (
things.response['objects']
) directly bypasses the need to build additional Python objects; its main use case is for integrations where there are numerous results returned.
On the other hand, Objects (
things.objects
) and DataFrame (
things.df
) will build the needed Python objects the first time they are used; they offer a more pretty output, and their main use case is to be used in
Interactive applications like Jupyter Notebooks.
JSON response

## All
## Things
objects contain parsed JSON response from the OpenBIS, it may help with advanced searches and validation schemes.
It is accessible via
response
attribute.
## Example
experiments
=
o
.
get_experiments
()
for
experiment
in
experiments
.
response
[
'objects'
## ]:
print
(
experiment
[
'properties'
])
## Would produce following output:
{}
{
## '$NAME'
## :
## 'Storages Collection'
}
{
## '$NAME'
## :
## 'Template Collection'
}
{
## '$NAME'
## :
## 'Storage Positions Collection'
}
{
## '$NAME'
## :
## 'General Protocols'
,
## '$DEFAULT_OBJECT_TYPE'
## :
## 'GENERAL_PROTOCOL'
}
{
## '$NAME'
## :
## 'Product Collection'
,
## '$DEFAULT_OBJECT_TYPE'
## :
## 'PRODUCT'
}
DataFrame

df
attribute returns
pandas.core.frame.DataFrame
object with columns defined adequate to the response it is containing.
## Note
DataFrame building can be time-consuming depending on the size of data. Therefore its loading has been deferred to the first access to
df
attribute (i.e. DataFrame is being lazy-loaded)
## Example
experiments
=
o
.
get_experiments
()
experiments
.
df
## Would produce following output:
permId
identifier
registrationDate
modificationDate
type
registrator
0
20240209011800684
-
1
/
## DEFAULT
/
## DEFAULT
/
## DEFAULT
2024
-
02
-
09
02
## :
18
## :
01
2024
-
02
-
09
02
## :
18
## :
01
## UNKNOWN
system
1
20240209011808121
-
4
/
## ELN_SETTINGS
/
## STORAGES
/
## STORAGES_COLLECTION
2024
-
02
-
09
02
## :
18
## :
08
2024
-
02
-
09
02
## :
18
## :
08
## COLLECTION
system
2
20240209011808121
-
5
/
## ELN_SETTINGS
/
## TEMPLATES
/
## TEMPLATES_COLLECTION
2024
-
02
-
09
02
## :
18
## :
08
2024
-
02
-
09
02
## :
18
## :
08
## COLLECTION
system
3
20240209011808121
-
6
/
## STORAGE
/
## STORAGE_POSITIONS
/
## STORAGE_POSITIONS_C
...
2024
-
02
-
09
02
## :
18
## :
08
2024
-
02
-
09
02
## :
18
## :
08
## COLLECTION
system
4
20240209011808121
-
17
/
## METHODS
/
## PROTOCOLS
/
## GENERAL_PROTOCOLS
2024
-
02
-
09
02
## :
18
## :
08
2024
-
02
-
09
02
## :
18
## :
08
## COLLECTION
system
5
20240209011808121
-
18
/
## STOCK_CATALOG
/
## PRODUCTS
/
## PRODUCT_COLLECTION
2024
-
02
-
09
02
## :
18
## :
08
2024
-
02
-
09
02
## :
18
## :
08
## COLLECTION
system
6
20240209011822486
-
24
/
## DEFAULT_LAB_NOTEBOOK
/
## DEFAULT_PROJECT
/
## DEFAULT_
...
2024
-
02
-
09
02
## :
18
## :
22
2024
-
02
-
09
02
## :
18
## :
22
## DEFAULT_EXPERIMENT
system
## Objects

objects
attribute, similarly to
df
builds a list of objects in a lazy way to easily access underlying data.
## Note
Not all PyBIS methods implements objects creation.
## Example
st
=
o
.
get_sample_type
(
## 'EXPERIMENTAL_STEP'
)
type
(
st
.
get_property_assignments
()
.
objects
[
0
])
st
.
get_property_assignments
()
.
objects
[
0
]
## Would produce following output:
pybis.entity_type.PropertyAssignment",⚠️ Clarification,1,en_20.10.0-11_software-developer-documentation_apis_python-v3-api_1,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_python-v3-api.txt,2025-09-30T12:09:01.367390Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_apis_python-v3-api:2,Python (V3 API) - pyBIS!,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/python-v3-api.html,openbis,"attribute                        value
-------------------------------  -------------------
propertyType                     $NAME
dataType                         VARCHAR
section                          General info
ordinal                          1
mandatory                        False
initialValueForExistingEntities
showInEditView                   True
showRawValueInForms              False
registrator
registrationDate                 2024-02-09 02:18:24
plugin
unique                           False
Best practices

## Logout

Every PyBIS
login()
call makes OpenBIS create a special session object and allocate resources to keep it alive. These sessions are terminated only when:
## Explicit
logout()
call is performed.
Number of sessions per user has reached beyond configured limit.
Session timeout is reached.
Keeping large number of idle concurrent sessions may influence your OpenBIS instance. Please use
logout()
in your scripts whenever you feel like OpenBIS connection is no longer required.
Iteration over tree structure

OpenBIS data model is constructed in a tree structure, iterating over it ban be done in at least 2 ways:
By method chaining (i.e. using the result of the previous call):
for
space
in
o
.
get_spaces
## ():
print
(
space
.
code
)
for
project
in
space
.
get_projects
## ():
print
(
f
'
\t
{
project
.
code
}
'
)
for
experiment
in
project
.
get_experiments
## ():
print
(
f
'
\t\t
{
experiment
.
code
}
'
)
for
sample
in
experiment
.
get_samples
## ():
print
(
f
'
\t\t\t
{
sample
.
code
}
'
)
for
dataset
in
sample
.
get_datasets
## ():
print
(
f
'
\t\t\t\t
{
dataset
.
code
}
'
)
## By individual call of Openbis object:
for
space
in
o
.
get_spaces
## ():
print
(
space
.
code
)
for
project
in
o
.
get_projects
(
space
=
space
.
code
## ):
print
(
f
'
\t
{
project
.
code
}
'
)
for
experiment
in
o
.
get_experiments
(
space
=
space
.
code
,
project
=
project
.
code
## ):
print
(
f
'
\t\t
{
experiment
.
code
}
'
)
for
sample
in
o
.
get_samples
(
space
=
space
.
code
,
project
=
project
.
code
,
experiment
=
experiment
.
code
## ):
print
(
f
'
\t\t\t
{
sample
.
code
}
'
)
for
dataset
in
o
.
get_datasets
(
sample
=
sample
.
code
## ):
print
(
f
'
\t\t\t\t
{
dataset
.
code
}
'
)
First solution is faster and cleaner to use, it is a recommended way to iterate over the data structure.
Iteration over raw data

If performance is of the higher priority, iterating over the raw data would be recommended solution:
for
space
in
o
.
get_spaces
()
.
response
[
'objects'
## ]:
print
(
space
[
'code'
])
for
project
in
o
.
get_projects
(
space
=
space
[
'code'
])
.
response
[
'objects'
## ]:
print
(
f
'
\t
{
project
[
""code""
]
}
'
)
for
experiment
in
o
.
get_experiments
(
space
=
space
[
'code'
],
project
=
project
[
'code'
])
.
response
[
'objects'
## ]:
print
(
f
'
\t\t
{
experiment
[
""code""
]
}
'
)
for
sample
in
o
.
get_samples
(
space
=
space
[
'code'
],
project
=
project
[
'code'
],
experiment
=
experiment
[
'code'
])
.
response
## :
print
(
f
'
\t\t\t
{
sample
[
""code""
]
}
'
)
for
dataset
in
o
.
get_datasets
(
sample
=
sample
[
'code'
])
.
response
## :
print
(
f
'
\t\t\t\t
{
dataset
[
""code""
]
}
'
)",Logout,2,en_20.10.0-11_software-developer-documentation_apis_python-v3-api_2,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_apis_python-v3-api.txt,2025-09-30T12:09:01.367390Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_client-side-extensions_eln-lims-web-ui-extensions:0,ELN-LIMS WEB UI extensions,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/client-side-extensions/eln-lims-web-ui-extensions.html,openbis,"ELN-LIMS WEB UI extensions

## Introduction

The current aim of this extensions is to accommodate two groups of
## modifications:
Pure Configuration, enabling/disabling some features, to clean the
interface and make it less confusing for non expert users. Very often
also to add type extensions for types specified with another master data
extension.
extending the interface to accommodate additional functionality without
needing to deal with the internals.
Plugin structure

plugins folder

Each folder on this folder is a ELN UI extension.
Each extension currently contains a single file with name “plugin.js”.
config.js file

Contains a section called  PLUGINS_CONFIGURATION indicating the plugins
to be loaded from the plugins folder.
var
## PLUGINS_CONFIGURATION
=
{
extraPlugins
## :
[
""life-sciences""
,
""flow""
,
""microscopy""
]
}
plugin.js file

Contains the actual source of the plugin, we can distinguish three clear
sections/patterns on the skeleton of the interface:
## Interface:
https://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/js/config/ELNLIMSPlugin.js
1. Configuring views through the use of a JSON structure. Part of this
## structure are:
forcedDisableRTF (Deprecated in favour of Custom Widgets
configurable from the Instance Settings on the UI)
forceMonospaceFont (Deprecated in favour of Custom Widgets
configurable from the Instance Settings on the UI)
experimentTypeDefinitionsExtension
sampleTypeDefinitionsExtension
dataSetTypeDefinitionsExtension
These are used extensively since they come at a very low development
effort. Best examples of how to use these definition extensions can be
found in technologies that ship with the ELN:
## Generic Technology:
https://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/generic/plugin.js
## Life Sciences Technology:
https://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/life-sciences/plugin.js
2. Extending views through the use of the
## Interceptor
## Pattern
## Template Methods:
## ONLY
allow to add content in certain portions
of the Interface.
## ONLY
available for Experiment, Sample and
DataSet form views. These template methods are easy to use, they
allow to add custom components isolating the programmer from the
rest of the form.
experimentFormTop
experimentFormBottom
sampleFormTop
dataSetFormBottom
dataSetFormTop
dataSetFormBottom
Event Listeners: Allow to listen the before/after paint events for
## ALL
form views and list views. Allow the programmer to change
the model before is displayed and any part of the view after.
Provide versatility but with added complexity of dealing with the
complete form.
beforeViewPaint
afterViewPaint
Template methods are only needed to add custom components to from
views. Best examples of how to use these can be found in
technologies that ship with the ELN:
## Microscopy Technology:
https://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/microscopy/plugin.js
## 3. Other Extensions:
onSampleSave: Reserved for internal use and discouraged to use. It
is tricky to use properly.
getExtraUtilities: Allows to extend the utilities menu. A great
example is this template:
https://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/template-extra-utilities/plugin.js
Source Code Examples (plugin.js)

### Configuration Only Extensions

An example with only type configurations extensions is show below.
function
MyTechnology
()
{
this
.
init
();
}
$
.
extend
(
MyTechnology
.
prototype
,
ELNLIMSPlugin
.
prototype
,
{
init
## :
function
()
{
},
experimentTypeDefinitionsExtension
## :
{
## ""FOLDER""
## :
{
## ""TOOLBAR""
## :
{
## CREATE
## :
false
,
## FREEZE
## :
false
,
## EDIT
## :
false
,
## MOVE
## :
false
,
## DELETE
## :
false
,
## UPLOAD_DATASET
## :
false
,
## UPLOAD_DATASET_HELPER
## :
false
,
## EXPORT_ALL
## :
false
,
## EXPORT_METADATA
## :
true
}
}
},
sampleTypeDefinitionsExtension
## :
{
## ""SAMPLE_TYPE""
## :
{
## ""TOOLBAR""
## :
{
## CREATE
## :
true
,
## EDIT
## :
true
,
## FREEZE
## :
true
,
## MOVE
## :
true
,
## COPY
## :
true
,
## DELETE
## :
true
,
## PRINT
## :
true
,
## HIERARCHY_GRAPH
## :
true
,
## HIERARCHY_TABLE
## :
true
,
## UPLOAD_DATASET
## :
true
,
## UPLOAD_DATASET_HELPER
## :
true
,
## EXPORT_ALL
## :
true
,
## EXPORT_METADATA
## :
true
,
## TEMPLATES
## :
true
,
## BARCODE
## :
true
},
## ""SHOW""
## :
false
,
## ""SAMPLE_CHILDREN_DISABLED""
## :
false
,
## ""SAMPLE_CHILDREN_ANY_TYPE_DISABLED""
## :
false
,
## ""SAMPLE_PARENTS_DISABLED""
## :
false
,
## ""SAMPLE_PARENTS_ANY_TYPE_DISABLED""
## :
true
,
## ""SAMPLE_PARENTS_HINT""
## :
[{
## ""LABEL""
## :
## ""Parent Label""
,
## ""TYPE""
## :
## ""PARENT_TYPE""
,
## ""ANNOTATION_PROPERTIES""
## :
[]
}],
## ""SAMPLE_CHILDREN_HINT""
## :
[{
## ""LABEL""
## :
## ""Children Label""
,
## ""TYPE""
## :
## ""CHILDREN_TYPE""
,
## ""MIN_COUNT""
## :
0
,
## ""ANNOTATION_PROPERTIES""
## :
[{
## ""TYPE""
## :
## ""ANNOTATION.SYSTEM.COMMENTS""
,
## ""MANDATORY""
## :
false
}]
}],
## ""ENABLE_STORAGE""
## :
false
,
## ""SHOW_ON_NAV""
## :
false
,
## ""SHOW_ON_NAV_FOR_PARENT_TYPES""
## :
undefined
,
extraToolbar
## :
undefined
},
},
dataSetTypeDefinitionsExtension
## :
{
## ""DATASET_TYPE""
## :
{
## ""TOOLBAR""
## :
{
## EDIT
## :
true
,
## FREEZE
## :
true
,
## MOVE
## :
true
,
## ARCHIVE
## :
true
,
## DELETE
## :
true
,
## HIERARCHY_TABLE
## :
true
,
## EXPORT_ALL
## :
true
,
## EXPORT_METADATA
## :
true
},
## ""DATASET_PARENTS_DISABLED""
## :
false
,
extraToolbar
## :
undefined
},
}
});
profile
.
plugins
.
push
(
new
MyTechnology
());
## Toolbar Extensions

An example with only toolbar extensions is shown below, variables with a
## dollar sign ‘$’ indicate they are jquery components:
function
MyTechnology
()
{
this
.
init
();
}
$
.
extend
(
MyTechnology
.
prototype
,
ELNLIMSPlugin
.
prototype
,
{
init
## :
function
()
{
},
sampleTypeDefinitionsExtension
## :
{
## ""SAMPLE_TYPE""
## :
{
extraToolbar
## :
function
(
mode
,
sample
)
{
var
toolbarModel
=
[];
if
(
mode
===
FormMode
.
## VIEW
)
{
var
$demoButton
=
FormUtil
.
getButtonWithIcon
(
""glyphicon-heart""
,
function
()
{
//This empty function could be a call to do something in particular
});
toolbarModel
.
push
({
component
## :
$demoButton
,
tooltip
## :
## ""Demo""
});
}
return
toolbarModel
;
}
},
},
dataSetTypeDefinitionsExtension
## :
{
## ""DATASET_TYPE""
## :
{
extraToolbar
## :
function
(
mode
,
dataset
)
{
var
toolbarModel
=
[];
if
(
mode
===
FormMode
.
## VIEW
)
{
var
$demoButton
=
FormUtil
.
getButtonWithIcon
(
""glyphicon-heart""
,
function
()
{
//This empty function could be a call to do something in particular
});
toolbarModel
.
push
({
component
## :
$demoButton
,
tooltip
## :
## ""Demo""
});
}
return
toolbarModel
;
}
},
}
});
profile
.
plugins
.
push
(
new
MyTechnology
());
Extra Views as Utilities

Please check the provided example:
https://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/template-extra-utilities/plugin.js",Introduction,0,en_20.10.0-11_software-developer-documentation_client-side-extensions_eln-lims-web-ui-extensions_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_client-side-extensions_eln-lims-web-ui-extensions.txt,2025-09-30T12:09:01.512757Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_client-side-extensions_index:0,Client-Side Extensions,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/client-side-extensions/index.html,openbis,"## Client-Side Extensions

ELN-LIMS WEB UI extensions
## Introduction
Plugin structure
plugins folder
config.js file
plugin.js file
Source Code Examples (plugin.js)
### Configuration Only Extensions
## Toolbar Extensions
Extra Views as Utilities
openBIS webapps
## Introduction
## Example
## Directory Structure
plugin.properties
## URL
### Server Configuration
### Jetty Configuration
Embedding webapps in the OpenBIS UI
## Introduction
Configuring embedded webapps
Creating embedded webapps
Linking to subtabs of other entity detail views
Cross communication openBIS > DSS
## Background
### Default Configuration
### Basic Configuration
### Advanced Configuration
Embedding openBIS Grids in Web Apps
### Requirements
## Use
Image Viewer component",Client-Side Extensions,0,en_20.10.0-11_software-developer-documentation_client-side-extensions_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_client-side-extensions_index.txt,2025-09-30T12:09:01.585218Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_client-side-extensions_openbis-webapps:0,openBIS webapps,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/client-side-extensions/openbis-webapps.html,openbis,"openBIS webapps

## Introduction

Webapps are HTML5 apps that interact with openBIS. Webapps can be
distributed as core-plugins. To supply a webapp plugin, create a folder
called
webapps
in the
as
. Each subfolder of the
webapps
folder is
treated as a webapp plugin. A webapp plugin requires two things, a
plugin.properties
file, as with all plugins, and a folder containing
the content of the webapp. This folder can have any name and needs to be
referenced in the
plugin.properties
file with the key
webapp-folder
.
It is recommended to name the webapp folder
html
as done in the
examples below. This has the advantage that an existing subfolder named
etc
will not be changed after an upgrade of the plugin. That is, the
content of the folder
html/etc
will be completely untouched by
upgrades. This feature allows to provide an initial configuration (say
in
html/etc/config.js
) with some default settings which can be
overridden by the customer.
The webapp is then served by the same web server (jetty) that serves
openBIS. The name of the webapp defines the URL used to access it. See
the example below. The file index.html is used as a welcome page if the
user does not specifically request a particular page.
## Warning
An openBIS webapp is
not
a J2EE webapp. It has more in common with an app for mobile devices.
## Example

This is an example of a webapp. In a real webapp, the name of the webapp
can be any valid folder name. The same goes for the folder in the webapp
containing the the code. The name of the webapp folder is what is used
to define the URL. The name of the folder containing the code is neither
shown nor available to the user.
## Directory Structure

[module]
[version]
as
webapps
example-webapp
plugin.properties
html
index.html
fun-viewer
plugin.properties
html
code
index.html
plugin.properties

# The properties file for an example webapps plugin
# This file has no properties defined because none need to be defined.
webapp-folder = html
## URL

If openBIS is served at the URL
https://my.domain.com:8443/openbis
,
the above webapps will be available under the following URLs:
https://my.domain.com:8443/openbis/webapp/example-webapp
https://my.domain.com:8443/openbis/webapp/fun-viewer
### Server Configuration

There are two things to consider in the server configuration. The
injection of webapps is done through Jetty, which is the web server we
use for openBIS. If you use the default provided jetty.xml
configuration, then you do not need to do anything extra; if, on the
other hand, you have a custom jetty.xml configuration, then you will
need to update your jetty.xml file to support webapps.
### Jetty Configuration

If your openBIS server has a custom jetty.xml file, you will need to
modify the file to include support for injecting web apps. To do this,
you will need to replace
org.eclipse.jetty.deploy.providers.WebAppProvider by
ch.systemsx.cisd.openbis.generic.server.util.OpenbisWebAppProvider in
addAppProvider
call to your jetty.xml.
jetty.xml
## <Call
name=
""addBean""
>
## <Arg>
## <New
id=
""DeploymentManager""
class=
""org.eclipse.jetty.deploy.DeploymentManager""
>
## <Set
name=
""contexts""
>
## <Ref
id=
## ""Contexts""
/>
## </Set>
## <Call
name=
""addAppProvider""
>
## <Arg>
## <New
class=
""ch.systemsx.cisd.openbis.generic.server.util.OpenbisWebAppProvider""
>
## <Set
name=
""monitoredDir""
## ><Property
name=
""jetty.home""
default=
"".""
/>
/webapps
## </Set>
## <Set
name=
""scanInterval""
>
0
## </Set>
## <Set
name=
""extractWars""
>
true
## </Set>
## </New>
## </Arg>
## </Call>
## </New>
## </Arg>
## </Call>
Embedding webapps in the OpenBIS UI

## Introduction

Webapps can be used as both standalone applications as well as can be
embedded in the OpenBIS web UI. Standalone webapps are built to
completely replace the original OpenBIS web interface with customer
adjusted layout and functionality. Users of the standalone webapps are
usually completely unaware of the default OpenBIS look and feel. The
webapp itself provides them with all the functionality they need: login
pages, web forms, searches, images, charts etc. The standalone webapp is
a right choice when you want to build a very specific and fully featured
web interface from scratch. If you want to use the default OpenBIS UI
but extend it with some custom functionality then embedding a webapp in
the OpenBIS UI is probably a way to go. To make a webapp visible as a
part of the default OpenBIS UI you have to define where the webapp
should be shown using “openbisui-contexts” property. Moreover some of
the contexts also require additional information describing when the
webapp should be shown. For instance, to embed a webapp in the
experiment details view that will be displayed for experiments with type
## “MY_EXPERIMENT_TYPE” your plugin.properties file should look like:
plugin.propeties
webapp-folder = html
openbisui-contexts = experiment-details-view
experiment-entity-types = MY_EXPERIMENT_TYPE
Configuring embedded webapps

A full list of supported properties is presented below.
## Property Key
## Description
Allowed values
openbisui-contexts
Place where the webapp is shown in the OpenBIS UI.
modules-menu
webapp is an item in the modules top menu
experiment-details-view
webapp is a tab in the experiment details view
requires experiment-entity-types to be defined
sample-details-view
webapp is a tab in the sample details view
requires sample-entity-types to be defined
data-set-details-view
webapp is a tab in the data set details view
requires data-set-entity-types to be defined
material-details-view
webapp is a tab in the material details view
requires material-entity-types to be defined
Accepts a comma separated list of values with regular expressions, e.g. “modules-menu, .*-details-view”
label
The label. It will be shown in the GUI.
## String
sorting
Sorting of the webapp. Webapps are sorted by “sorting” and “folder name” ascending with nulls last (webapps without sorting are presented last).
## Integer
experiment-entity-types
Types of experiments the webapp should be displayed for.
Accepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”
sample-entity-types
Types of samples the webapp should be displayed for.
Accepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”
data-set-entity-types
Types of data sets the webapp should be displayed for.
Accepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”
material-entity-types
Types of materials the webapp should be displayed for.
Accepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”
Creating embedded webapps

Embedded webapps similar to the standalone counterparts are HTML5
applications that interact with OpenBIS. Because embedded webapps are
shown inside the OpenBIS UI they have access to additional information
about the context they are displayed in. For instance, a webapp that is
displayed in the experiment-details-view context knows that it is
displayed for an experiment entity, with a given type, identifier and
permid. Having this information the webapp can adjust itself and display
only data related to the currently chosen entity. Apart from the entity
details, a webapp also receives a current sessionId that can be used for
calling OpenBIS JSON RPC services. This way embedded webapps can reuse a
current session that was created when a user logged in to the OpenBIS
rather than provide their own login pages for authentication. A sample
webapp that makes use of this context information is presented below:
webapp.html
<
html
>
<
head
>
<!-- include jquery library required by the openbis.js -->
<
script
src
=
""/openbis/resources/js/jquery.js""
></
script
>
<!-- include openbis library to gain access to the openbisWebAppContext and openbis objects -->
<
script
src
=
""/openbis/resources/js/openbis.js""
></
script
>
</
head
>
<
body
>
<
div
id
=
""log""
></
div
>
<
script
>
$
(
document
).
ready
(
function
(){
// create a context object to access the context information
var
c
=
new
openbisWebAppContext
();
$
(
""#log""
).
append
(
""SessionId: ""
+
c
.
getSessionId
()
+
""<br/>""
);
$
(
""#log""
).
append
(
""EntityKind: ""
+
c
.
getEntityKind
()
+
""<br/>""
);
$
(
""#log""
).
append
(
""EntityType: ""
+
c
.
getEntityType
()
+
""<br/>""
);
$
(
""#log""
).
append
(
""EntityIdentifier: ""
+
c
.
getEntityIdentifier
()
+
""<br/>""
);
$
(
""#log""
).
append
(
""EntityPermId: ""
+
c
.
getEntityPermId
()
+
""<br/>""
);
// create an OpenBIS facade to call JSON RPC services
var
o
=
new
openbis
();
// reuse the current sessionId that we received in the context for all the facade calls
o
.
useSession
(
c
.
getSessionId
());
// call one of the OpenBIS facade methods
o
.
listProjects
(
function
(
response
){
$
(
""#log""
).
append
(
""<br/>Projects:<br/>""
);
$
.
each
(
response
.
result
,
function
(
index
,
value
){
$
(
""#log""
).
append
(
value
.
code
+
""<br/>""
);
});
});
});
</
script
>
</
body
>
</
html
>
Linking to subtabs of other entity detail views

A link from a webapp to an entity subtab looks like this:
<a href=""#"" onclick=""window.top.location.hash='#entity=[ENTITY_KIND]&permId=[PERM_ID]&ui-subtab=[SECTION];return false;"">Link Text</a>
, for example
<a href=""#"" onclick=""window.top.location.hash='#entity=EXPERIMENT&permId=20140716095938913-1&ui-subtab=webapp-section_test-webapp;return false;"">Experiment webapp</a>
ENTITY_KIND = ‘EXPERIMENT’ / ‘SAMPLE’ / ‘DATA_SET’ / ‘MATERIAL’
PERM_ID = Entity permid
SECTION = Subtab identifier.
## Notes about subtab identifiers:
The valid subtab identifiers can be found from
ch.systemsx.cisd.openbis.generic.client.web.client.application.framework.DisplayTypeIDGenerator.java
Managed property subtab identifiers are of format
‘managed_property_section_[MANAGED_PROPERTY_TYPE_CODE]’
Webapp subtab identifiers are of format
‘webapp-section_[WEBAPP_CODE]’ (webapp code is a name of the
webapp core-plugin folder, i.e.
[technology]/[version]/as/webapps/[WEBAPP_CODE])
Cross communication openBIS > DSS

## Background

Sometimes is required for a web app started in openBIS to make a call to
the DSS. This happens often to upload files or navigate datasets between
others.
Making calls to different domains is forbidden by the web security
sandbox and a common client side issue.
To make the clients accept the responses without additional
configuration by the users, the server should set a special header
“Access-Control-Allow-Origin” on the response when accessing from a
different domain or port.
### Default Configuration

This is done automatically by the DSS for any requests coming from well
known openBIS web apps.
A well known openBIS web app is a web app running using the same URL
configured for openbis on the DSS service.properties.
DSS service.properties
# The URL of the openBIS server
server-url = https://sprint-openbis.ethz.ch:8446
Even if the web app is accessible from other URLs, not using the URL
configured on the DSS service.properties will lead to the DSS not
recognizing the app.
## Warning
As a consequence the DSS will not set the necessary header and the client will reject the responses.
### Basic Configuration

This is required very often in enterprise environments where the
reachable openBIS URL is not necessarily the one configured on the DSS
service.properties.
Is possible to add additional URLS configuring the AS
service.properties.
AS service.properties
trusted-cross-origin-domains= https://195.176.122.56:8446
The first time the DSS will need to check the valid URLs after a start
up will contact the AS to retrieve the additional trusted domain list.
### Advanced Configuration

A very typical approach is to run both the AS and DSS on the same port
using a reverse proxy like Apache or NGNIX. This way the web security
sandbox is respected. On this case the “Access-Control-Allow-Origin”
header is unnecessary and will also work out of the box.
## Warning
Even with this configuration, sometimes happens that a web app call the DSS using an auto detected URL given by openBIS. This auto detected URL not necessarily respects your proxy configuration, giving a different port or hostname to the DSS.
On this case you will need to solve the problems with one of the methods
explained above or modify your web app.
Embedding openBIS Grids in Web Apps

Users of openBIS will have encountered the advanced and powerful table
views used in the application. These views allow for sorting and
filtering. It is possible to take advantage of these views in custom web
UIs.
### Requirements

It is possible to use openBIS table views in a web UI when the data for
the table comes from an aggregation service. The parameters to the
aggregation service are passed as URL query parameters, thus an
additional requirement is that all the aggregation service parameters
can be passed this way. A final requirement is that the web UI be
exposed as an embedded webapp (this is necessary because of the way
openBIS keeps track of the user of the system). If these requirements
are met, then it will be possible to embed an openBIS table view display
the aggregation service data in a web UI.
## Use

To embed a table view, add an iframe to the web UI. The URL of the
iframe should have the following form:
{openbis url}?viewMode=GRID#action=AGGREGATION_SERVICE&serviceKey={aggregation service key}&dss={data store server code}[& gridSettingsId][& gridHeaderText][& service parameters]
## Parameters:
## Parameter
## Description
## Required
serviceKey
An aggregation service that will be used for generating the data for the grid.
true
dss
A code of a data store that will be used for generating the data for the grid.
true
gridSettingsId
An identifier of the grid that will be used for storing the grid settings (visibility of columns, sorting, filters etc.). If not specified then the serviceKey parameter is used.
false
gridHeaderText
A header of the grid. If not specified then the header is not shown.
false
## Example:
http://localhost:8888/openbis-test/index.html?viewMode=GRID#action=AGGREGATION_SERVICE&serviceKey=sp-233&dss=standard&gridSettingsId=myTestGridSettingsId&gridHeaderText=myTestGridHeaderText&name=hello
## Full Example
<!DOCTYPE html>
<
html
xmlns
=
""http://www.w3.org/1999/xhtml""
xml:lang
=
""en-US""
lang
=
""en-US""
>
<
head
>
<
meta
http-equiv
=
""content-type""
content
=
""text/html; charset=utf-8""
/>
<
title
>
## Embedded Grid Example
</
title
>
</
head
>
<
body
>
<
iframe
src
=
""http://localhost:8888/openbis-test/index.html?viewMode=GRID#action=AGGREGATION_SERVICE&serviceKey=sp-233&dss=standard&gridSettingsId=myTestGridSettingsId&gridHeaderText=myTestGridHeaderText&name=hello""
width
=
""100%""
height
=
""95%""
style
=
""border: none""
>
</
body
>
</
html
>
Image Viewer component

## Image viewer screenshot:
Example usage of the image viewer component:
<!DOCTYPE html>
<
html
>
<
head
>
<
meta
charset
=
""utf-8""
>
<
title
>
## Image Viewer Example
</
title
>
<
link
rel
=
""stylesheet""
href
=
""/openbis/resources/lib/bootstrap/css/bootstrap.min.css""
>
<
link
rel
=
""stylesheet""
href
=
""/openbis/resources/lib/bootstrap-slider/css/bootstrap-slider.min.css""
>
<
link
rel
=
""stylesheet""
href
=
""/openbis/resources/components/imageviewer/css/image-viewer.css""
>
<
script
type
=
""text/javascript""
src
=
""/openbis/resources/config.js""
></
script
>
<
script
type
=
""text/javascript""
src
=
""/openbis/resources/require.js""
></
script
>
</
head
>
<
body
>
<
script
>
// ask for jquery library, openbis-screening facade and the image viewer component
require
([
""jquery""
,
""openbis-screening""
,
""components/imageviewer/ImageViewerWidget""
],
function
(
$
,
openbis
,
ImageViewerWidget
)
{
$
(
document
).
ready
(
function
()
{
var
facade
=
new
openbis
();
facade
.
login
(
""admin""
,
""password""
,
function
(
response
)
{
// create the image viewer component for the specific data sets
var
widget
=
new
ImageViewerWidget
(
facade
,
[
""20140513145946659-3284""
,
""20140415140347875-53""
,
""20140429125231346-56""
,
""20140429125614418-59""
,
""20140506132344798-146""
]);
// do the customization once the component is loaded
widget
.
addLoadListener
(
function
()
{
var
view
=
widget
.
getDataSetChooserWidget
().
getView
();
// example of how to customize a widget
view
.
getDataSetText
=
function
(
dataSetCode
)
{
return
""My data set: ""
+
dataSetCode
;
};
// example of how to add a change listener to a widget
widget
.
getDataSetChooserWidget
().
addChangeListener
(
function
(
event
)
{
console
.
log
(
""data set changed from: ""
+
event
.
getOldValue
()
+
"" to: ""
+
event
.
getNewValue
());
});
});
// render the component and add it to the page
$
(
""#container""
).
append
(
widget
.
render
());
});
});
});
</
script
>
<
div
id
=
""container""
style
=
""padding: 20px""
></
div
>
</
body
>
</
html
>",Introduction,0,en_20.10.0-11_software-developer-documentation_client-side-extensions_openbis-webapps_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_client-side-extensions_openbis-webapps.txt,2025-09-30T12:09:01.650998Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_development-environment_architectural-overview:0,Architectural Overview,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/development-environment/architectural-overview.html,openbis,"## Architectural Overview

Repository organization

The repository contains these kind of modules used to build the openBIS distributable:
api-*: API Facades
app-*: Applications
build: Build scripts
core-plugins-*: Core plugins distributed with openBIS
lib-*: Internally maintained libraries used to build openBIS
server-*: Server components
test-*: Integration tests
ui-*: User interfaces",Architectural Overview,0,en_20.10.0-11_software-developer-documentation_development-environment_architectural-overview_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_development-environment_architectural-overview.txt,2025-09-30T12:09:01.720095Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_development-environment_index:0,Development Environment,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/development-environment/index.html,openbis,"## Development Environment

### System Requirements
## Architectural Overview
Repository organization
Installation And Configuration Guide
Building openBIS
Where the build is found?
Why we disable tests to make the build?
Why the core UI made using GWT is not build anymore?
How to compile the V3 JS bundle used by the new Admin UI in production?
Development of openBIS
### Requirements
## Step By Step
Source Code Auto Formatting
## Commit Messages Formatting
## Source Code Copyright Header
## Typical Errors
IntelliJ can’t find package com.sun.*, but I can compile the project using the command line!
IntelliJ can’t find a particular method
Test seem to run through Gradle and fail
Test seem to run through intelliJ but throw a package not open error
Development of NG UI
Setting up IntelliJ Idea",Development Environment,0,en_20.10.0-11_software-developer-documentation_development-environment_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_development-environment_index.txt,2025-09-30T12:09:01.784837Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_development-environment_installation-and-configuration-guide:0,Installation And Configuration Guide,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/development-environment/installation-and-configuration-guide.html,openbis,"Installation And Configuration Guide

Building openBIS

git
clone
https
## :
//
sissource
.
ethz
.
ch
/
sispub
/
openbis
.
git
cd
app
-
openbis
-
installer
/
./
gradlew
clean
./
gradlew
build
-
x
test
""-Dorg.gradle.jvmargs=--add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.desktop/java.awt.font=ALL-UNNAMED""
Where the build is found?

./
app
-
openbis
-
installer
/
targets
/
gradle
/
distributions
/
openBIS
-
installation
-
standard
-
technologies
-
## SNAPSHOT
-
rXXXXXXXXXX
.
tar
.
gz
Why we disable tests to make the build?

They increase the time to obtain a build plus some tests could have additional environment
requirements.
Why the core UI made using GWT is not build anymore?

The core UI is deprecated for removal on next mayor release and requires JDK8.
For now it can be build following the next commands and only with JDK8:
git
clone
https
## :
//
sissource
.
ethz
.
ch
/
sispub
/
openbis
.
git
cd
core
-
plugin
-
openbis
/
./
gradlew
clean
./
gradlew
buildCoreUIPackageUsingJDK8
-
x
test
How to compile the V3 JS bundle used by the new Admin UI in production?

git
clone
https
## :
//
sissource
.
ethz
.
ch
/
sispub
/
openbis
.
git
cd
api
-
openbis
-
javascript
/
./
gradlew
clean
./
gradlew
bundleOpenbisStaticResources
-
x
test
The output can be found at:
server-application-server/source/java/ch/systemsx/cisd/openbis/public/resources/api/v3
config.bundle.js
config.bundle.min.js
openbis.bundle.js
openbis.bundle.min.js
Development of openBIS

### Requirements

### Software Requirements
IntelliJ IDEA CE
## Step By Step

## File
->
## New
->
## Project
## From
## Existing
## Sources
## Select
the
build
folder
to
load
the
gradle
model
## After
the
model
is
loaded
execute
the
tasks
## :
openBISDevelopementEnvironmentASPrepare
openBISDevelopementEnvironmentASStart
openBISDevelopementEnvironmentDSSStart
Source Code Auto Formatting

OpenBIS source code uses a particular style preset that guarantees all code is formatted uniformly.
To make use of the preset go to File/Settings or IntelliJIDEA/Preferences depending on your OS.
Then import the XML file under ‘docs/codestyle/SIS_Conventions_IntelliJ_V3.xml’. See images below:
## Commit Messages Formatting

OpenBIS source code commit messages use a particular formatting.
This formatting guarantees that there is a User Story behind it.
To ensure commits follow the formatting ‘Git Hooks’ are provided.
Just copy them from the root folder of this repo run the next command:
%/>
cp
./
docs
/
hooks
/*
./.
git
/
hooks
/
%/>
git
add
## README
.
md
%/>
git
commit
-
m
""Test incorrectly formatted message""
## Aborting
commit
.
## Your
commit
message
is
missing
an
issue
number
(
## 'SSDM-XXXXX:'
)
## Source Code Copyright Header

OpenBIS source code is licensed under SIS copyright and licensed under ‘Apache 2 License’:
http
## :
//
www
.
apache
.
org
/
licenses
/
## LICENSE
-
2.0
To guarantee all new source files contain the appropriate license a preset is provided.
To make use of the preset go to File/Settings or IntelliJIDEA/Preferences depending on your OS.
Then import the XML file under ‘docs/copyright/Copyright_IntelliJ.xml’ under the copyright section as the image below indicate.
Last, set the Copyright Profile under the Copyright section as the image below indicate:
## Typical Errors

IntelliJ can’t find package com.sun.*, but I can compile the project using the command line!

Turn off “File | Settings | Build, Execution, Deployment | Compiler | Java Compiler | Use –release
option for cross-compilation”.
IntelliJ can’t find a particular method

Code compatiblity 1.8 is set by default to work well with our javadoc tools but it can be set to 17 on IntelliJ. See image below.
Test seem to run through Gradle and fail

They need to be set to run using IntelliJ.
Test seem to run through intelliJ but throw a package not open error

The project does not uses modules yet. Add ‘–add-opens’ statements manually when launching the tests as shown below.
Development of NG UI

Generate openBIS JS bundle by running in command line
cd /<OPENBIS_PROJECT_ROOT>/api-openbis-javascript
./gradlew :bundleOpenbisStaticResources
Start openBIS in your chosen IDE (NG UI assumes it will run
## at: http://localhost:8888/openbis-test/):
run openBISDevelopementEnvironmentASPrepare gradle task
run openBISDevelopementEnvironmentASStart gradle task
In command line do:
cd /<OPENBIS_PROJECT_ROOT>/ui-admin
npm install
npm run dev
Open in your chosen browser a url, by default: http://localhost:9999/admin
Setting up IntelliJ Idea

Under “IntelliJ IDEA” -> “Preferences” -> “Languages and Frameworks” -> Javascript, set the
language version to ECMAScript 6.",:,0,en_20.10.0-11_software-developer-documentation_development-environment_installation-and-configuration-guide_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_development-environment_installation-and-configuration-guide.txt,2025-09-30T12:09:01.850370Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_custom-import:0,Custom Import,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/custom-import.html,openbis,"## Custom Import

## Introduction

## Custom
## Import
is a feature designed to give web users a chance to
import a file via
## Jython
## Dropboxes
.
### Usage

To upload a file via
## Custom
## Import
, the user should
choose
## Import
->
## Custom
## Import
in openBIS top menu. The
## Custom
## Import
tab will be opened, and the user will get the combo box
filled with the list of configured imports. After selecting the desired
## Custom
## Import,
the
user will be asked to select a file. After
selecting a file and clicking
the
## Save
button, the import will start.
The user should be aware, that the import is done in a synchronous way,
sometimes it might take a while to import data (it depends on the
dropbox code).
If a template file has been configured a download link will appear. The
downloaded template file can be used to create the file to be imported.
### Configuration

To have the possibility to use a
## Custom
## Import
functionality, this
needs an AS
core plugin
of type
custom-imports. The
plugin.properties
of each plugin has several
## parameters:
parameter name
description
name
The value of this parameter will be used as a name of Custom Import in web UI.
dss-code
This parameter needs to specify the code of the datastore server running the dropbox which should be used by the Custom Import.
dropbox-name
The value is the name of the dropbox that is used by the Custom Import.
description
Specifies a description of the Custom Import. The description is shown as a tooltip in the web UI.
template-entity-kind
Custom import templates are represented in OpenBIS as entity attachments. To make a given file available as a custom import template create an attachment with this file and refer to this attachment with template-entity-kind, template-entity-permid, template-attachment-name parameters, where: template-entity-kind is the kind of the entity the attachment has been added to (allowed values: PROJECT, EXPERIMENT, SAMPLE), template-entity-permid is the perm id of that entity and template-attachment-name is the file name of the attachment.
template-entity-permid
template-attachment-name
Example configuration

plugin.properties
name = Example custom import
dss-code = DSS1
dropbox-name = jython-dropbox-1
description = This is an example custom import
template-entity-kind = PROJECT
template-entity-permid = 20120814111307034-82319
template-attachment-name = project_custom_import_template.txt
The dropbox needs to be defined on
the
## DSS
side as a
## RPC
dropbox
## :
service.properties
dss-rpc.put.<DATA_SET_TYPE> = jython-dropbox-1",Custom Import,0,en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_custom-import_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_custom-import.txt,2025-09-30T12:09:01.924801Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_index:0,Legacy Server-Side Extensions,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/index.html,openbis,"## Legacy Server-Side Extensions

## Custom Import
## Introduction
### Usage
### Configuration
Example configuration
## Processing Plugins
## Introduction
## Multiple Processing Queues
## Archiving
## Generic Processing Plugins
RevokeLDAPUserAccessMaintenanceTask
DataSetCopierForUsers
DataSetCopier
DataSetCopierForUsers
JythonBasedProcessingPlugin
ReportingBasedProcessingPlugin
DataSetAndPathInfoDBConsistencyCheckProcessingPlugin
ScreeningReportingBasedProcessingPlugin
## Reporting Plugins
## Introduction
## Generic Reporting Plugins
DecoratingTableModelReportingPlugin
## Transformations
GenericDssLinkReportingPlugin
AggregationService
JythonAggregationService
IngestionService
JythonIngestionService
JythonBasedReportingPlugin
TSVViewReportingPlugin
## Screening Reporting Plugins
ScreeningJythonBasedAggregationServiceReportingPlugin
ScreeningJythonBasedDbModifyingAggregationServiceReportingPlugin
ScreeningJythonBasedReportingPlugin
## Search Domain Services
Configuring a Service
Querying a Service
## Service Implementations
BlastDatabase
## Optional Query Parameters
## Search Results",Legacy Server-Side Extensions,0,en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_index_0,concept,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_index.txt,2025-09-30T12:09:01.987797Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_processing-plugins:0,Processing Plugins,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/processing-plugins.html,openbis,"## Processing Plugins

## Introduction

A processing plugin runs on the DSS. It processes a specified set of data sets. The user can trigger a processing plugin in the openBIS Web application. After processing an e-mail is sent to the user.
A processing plugin is configured on the DSS best by introducing a
core plugin
of type
processing-plugins
. All processing plugins have the following properties in common:
## Property Key
## Description
class
The fully-qualified Java class name of the reporting plugin. The class has to implement IProcessingPluginTask.
label
The label. It will be shown in the GUI.
dataset-types
Comma-separated list of regular expressions. The plugin can process only data sets of types matching one of the regular expressions.  If new data set types are registered with openBIS, the DSS will need to be restarted before the new data set types are known to the processing plugins.
properties-file
Path to an optional file with additional properties.
allowed-api-parameter-classes
A comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is allowed as a class of a Java parameter object of a remote API call. For more details see API Security.
disallowed-api-parameter-classes
A comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is not allowed as a class of a Java parameter object of a remote API call. For more details see API Security.
## Multiple Processing Queues

By default only one processing plugin task is processed. All other
scheduled tasks have to wait in a queue. This can be inconvenient if
there is a mixture of long tasks (taking hours or even days) and short
tasks (taking only seconds or minutes).
DSS can be configured two run more than one processing queue. Each queue
(except the default one) has a name (which also appears in the log
file). Also a regular expression is associated with the queue. When a
processing plugin task is scheduled the appropriate queue is selected by
the ID of the processing plugin (this is either a name in the
property
processing-plugins
of
service.properties
of DSS or the name
of the core-plugin folder). If the ID matches the regular expression the
task is added to the corresponding queue. If non of the regular
expression matches the default queue is used.
The queues have to be specified by the
property
data-set-command-queue-mapping
. It contains a comma-separated
list of queue definitions. Each definition has the form
<queue
name>:<regular
expression>
## Archiving

If archiving is enable (i.e.
archiver.class
in
service.properties
of
DSS is defined or a core-plugin of type
miscellaneous
with
## ID
archiver
is defined) there will be three processing plugins with
the following IDs:
## Archiving
,
## Copying
data
sets
to
archive
, and
## Unarchiving
## Generic Processing Plugins

RevokeLDAPUserAccessMaintenanceTask

## Note
This Maintenance Task should only be used if the server uses
LDAP only, it will take users from other authentication services as
missing.
## Description
: Renames, deactivates and delete all roles from users
that are no longer available on LDAP following the next algorithm.
Grabs all active users.
The users that follow all the points of the next criteria are
renamed to userId-YYYY.MM.DD and deactivated:
Are not a system user.
Don’t have the ETL_SERVER role.
Don’t have a LDAP principal.
### Configuration
## :
## Property Key
## Description
server-url
LDAP server URL.
security-principal-distinguished-name
LDAP principal distinguished name.
security-principal-password
LDAP principal password.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.RevokeLDAPUserAccessMaintenanceTask
interval = 60 s
server-url = ldap://d.ethz.ch/DC=d,DC=ethz,DC=ch
security-principal-distinguished-name = CN=cisd-helpdesk,OU=EthUsers,DC=d,DC=ethz,DC=ch
security-principal-password = ******
DataSetCopierForUsers

DataSetCopier

## Description
: Copies all files of the specified data sets to another (remote) folder. The actual copying is done by the rsync command.
### Configuration
## :
## Property Key
## Description
destination
Path to the destination folder. This can be a path to a local/mounted folder or to a remote folder accessible via SSH. In this case the name of the host has to appear as a prefix. General syntax: [
:][
:]
hard-link-copy
If true hard links are created for each file of the data sets. This works only if the share which stores the data set is in the same local file system as the destination folder. Default: false.
rename-to-dataset-code
If true the copied data set will be renamed to the data set code. Default: false.
rsync-executable
Optional path to the executable command rsync.
rsync-password-file
Path to the rsync password file. It is only needed if an rsync module is used.
ssh-executable
Optional path to the executable command ssh. SSH is only needed for not-mounted folders which are accessible via SSH.
ln-executable
Optional path to the executable command ln. The ln command is only needed when hard-link-copy = true.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DataSetCopier
label = Copy to analysis incoming folder
dataset-types = MS_DATA, UNKNOWN
destination = analysis-server:analysis-incoming-data
rename-to-dataset-code = true
DataSetCopierForUsers

## Description
: Copies all files of the specified data sets to a
(remote) user folder. The actual copying is done by the rsync command.
### Configuration
## :
## Property Key
## Description
destination
Path template to the destination folder. It should contain ${user} as a placeholder for the user ID.
The path can point to a local/mounted folder or to a remote folder accessible via SSH. In this case the name of the host has to appear as a prefix. General syntax: [
:][
:]
hard-link-copy
If true hard links are created for each file of the data sets. This works only if the share which stores the data set is in the same local file system as the destination folder. Default: false.
rename-to-dataset-code
If true the copied data set will be renamed to the data set code. Default: false.
rsync-executable
Optional path to the executable command rsync.
rsync-password-file
Path to the rsync password file. It is only needed if an rsync module is used.
ssh-executable
Optional path to the executable command ssh. SSH is only needed for not-mounted folders which are accessible via SSH.
ln-executable
Optional path to the executable command ln. The ln command is only needed when hard-link-copy = true.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DataSetCopierForUsers
label = Copy to user playground
dataset-types = MS_DATA, UNKNOWN
destination = tmp/playground/${user}/data-sets
hard-link-copy = true
rename-to-dataset-code = true
JythonBasedProcessingPlugin

## Description
: Invokes a Jython script to do the processing. For more details see
Jython-based Reporting and Processing Plugins
.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonBasedProcessingPlugin
label = Calculate some numbers
dataset-types = MS_DATA, UNKNOWN
script-path = script.py
ReportingBasedProcessingPlugin

## Description
: Runs a Jython-based reporting plugin of type
TABLE_MODEL and sends the result table as a TSV file to the user.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
single-report
If true only one report will be sent. Otherwise a report for each data set will be sent. Default: false
email-subject
Subject of the e-mail to be sent. Default: None
email-body
Body of the e-mail to be sent. Default: None
attachment-name
Name of the attached TSV file. Default: report.txt
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.ReportingBasedProcessingPlugin
label = Create monthly report
dataset-types = MS_DATA, UNKNOWN
script-path = script.py
email-subject = DSS Monthly Report
DataSetAndPathInfoDBConsistencyCheckProcessingPlugin

## Description
: The processing task checks the consistency between the
data store and the meta information stored in the
PathInfoDB
. It will
## check for:
existence (i.e. exists in PathInfoDB but not on file system or
exists on file system but not in PathInfoDB)
file size
CRC32 checksum
If it finds any deviations, it will send out an email which contains all differences found.
### Configuration
: Properties common for all processing plugins (see Introduction)
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DataSetAndPathInfoDBConsistencyCheckProcessingPlugin
label = Check consistency between data store and path info database
dataset-types = .*
creening Processing Plugins
ScreeningReportingBasedProcessingPlugin

## Description
: Runs a Jython-based reporting plugin of type
TABLE_MODEL and sends the result table as a TSV file to the user. There
is some extra support for screening.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
single-report
If true only one report will be sent. Otherwise a report for each data set will be sent. Default: false
email-subject
Subject of the e-mail to be sent. Default: None
email-body
Body of the e-mail to be sent. Default: None
attachment-name
Name of the attached TSV file. Default: report.txt
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningReportingBasedProcessingPlugin
label = Create monthly report
dataset-types = HCS_IMAGE
script-path = script.py
email-subject = DSS Monthly Report",Processing Plugins,0,en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_processing-plugins_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_processing-plugins.txt,2025-09-30T12:09:02.055018Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_reporting-plugins:0,Reporting Plugins,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/reporting-plugins.html,openbis,"## Reporting Plugins

## Introduction

A reporting plugin runs on the DSS. It creates a report as a table or an
URL for a specified set of data sets or key-value pairs. The user can
invoke a reporting plugin in the openBIS Web application. The result
will be shown as a table or a link.
A reporting plugin is one of the three following types. The differences
are the type of input and output:
## TABLE_MODEL:
## Input
: A set of data sets.
## Output
: A table
## DSS_LINK:
## Input
: One data set.
## Output
: An URL
## AGGREGATION_TABLE_MODEL:
## Input
: A set of key-value pairs.
## Output
: A table
A reporting plugin is configured on the DSS best by introducing a
core
plugin
of type
reporting-plugins
. All reporting plugins have the following properties
## in common:
## Property Key
## Description
class
The fully-qualified Java class name of the reporting plugin. The class has to implement IReportingPluginTask.
label
The label. It will be shown in the GUI.
dataset-types
Comma-separated list of regular expressions. The plugin can create a report only for the data sets of types matching one of the regular expressions. If new data set types are registered with openBIS, the DSS will need to be restarted before the new data set types are known to the processing plugins. This is a mandatory property for reporting plugins of type TABLE_MODEL and DSS_LINK. It will be ignored if the type is AGGREGATION_TABLE_MODEL.
properties-file
Path to an optional file with additional properties.
servlet.
Properties for an optional servlet. It provides resources referred by URLs in the output of the reporting plugin.
This should be used if the servlet is only needed by this reporting plugin. If other plugins also need this servlet it should be configured as a core plugin of type services.
allowed-api-parameter-classes
A comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is allowed as a class of a Java parameter object of a remote API call. For more details see API Security.
disallowed-api-parameter-classes
A comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is not allowed as a class of a Java parameter object of a remote API call. For more details see API Security.
## Generic Reporting Plugins

DecoratingTableModelReportingPlugin

## Type
## : TABLE_MODEL
## Description
: Modifies the output of a reporting plugin of type
## TABLE_MODEL
### Configuration
## :
## Property Key
## Description
reporting-plugin.class
The fully-qualified Java class name of the wrapped reporting plugin of type TABLE_MODEL
reporting-plugin.
Property of the wrapped reporting plugin.
transformation.class
The fully-qualified Java class name of the transformation. It has to implement ITableModelTransformation.
transformation.
Property of the transformation to be applied.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DecoratingTableModelReportingPlugin
label = Analysis Summary
dataset-types = HCS_IMAGE_ANALYSIS_DATA
reporting-plugin.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin
reporting-plugin.separator = ,
transformation.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.EntityLinksDecorator
transformation.link-columns = BARCODE, GENE
transformation.BARCODE.entity-kind = SAMPLE
transformation.BARCODE.default-space = DEMO
transformation.GENE.entity-kind = MATERIAL
transformation.GENE.material-type = GENE
## Transformations

EntityLinksDecorator

## Description
: Changes plain columns into entity links.
### Configuration
## :
## Property Key
## Description
link-columns
Comma-separated list of column keys.
.entity-kind
Entity kind of column
. Possible values are MATERIAL and SAMPLE.
.default-space
Optional space code for SAMPLE columns. It will be used if the column value contains only the sample code.
.material-type
Mandatory type code for MATERIAL columns.
GenericDssLinkReportingPlugin

## Type
## : DSS_LINK
## Description
: Creates an URL for a file inside the data set.
### Configuration
## :
## Property Key
## Description
download-url
Base URL. Contains protocol, domain, and port.
data-set-regex
Optional regular expression which specifies the file.
data-set-path
Optional relative path in the data set to narrow down the search.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.GenericDssLinkReportingPlugin
label = Summary
dataset-types = MS_DATA
download-url = https://my.domain.org:8443
data-set-regex = summary.*
data-set-path = report
AggregationService

## Warning
Import Note on Authorization
In AggregationServices and IngestionServices, the service programmer needs to ensure proper authorization by himself. He can do so by using the methods from
IAuthorizationService
. The user id, which is needed when calling these methods, can be obtained from
DataSetProcessingContext
(when using Java), or the variable
userId
(when using Jython).
## Type:
## AGGREGATION_TABLE_MODEL
## Description
: An abstract superclass for aggregation service
reporting plugins. An aggregation service reporting plugin takes a hash
map containing user parameters as an argument and returns tabular data
(in the form of a TableModel). The
JythonBasedAggregationServiceReportingPlugin below is a subclass that
allows for implementation of the logic in Jython.
### Configuration
: Dependent on the subclass.
To implement an aggregation service in Java, define a subclass
of
ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.AggregationService
.
This subclass must implement the method
TableModel createReport(Map<String, Object>, DataSetProcessingContext).
## Example
## :
ExampleAggregationServicePlugin
package
ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard
;
import
java.io.File
;
import
java.util.Map
;
import
java.util.Properties
;
import
ch.systemsx.cisd.openbis.dss.generic.shared.DataSetProcessingContext
;
import
ch.systemsx.cisd.openbis.generic.shared.basic.dto.TableModel
;
import
ch.systemsx.cisd.openbis.generic.shared.util.IRowBuilder
;
import
ch.systemsx.cisd.openbis.generic.shared.util.SimpleTableModelBuilder
;
/**
* @author Chandrasekhar Ramakrishnan
*/
public
class
ExampleAggregationServicePlugin
extends
AggregationService
{
private
static
final
long
serialVersionUID
=
## 1L
;
/**
* Create a new plugin.
*
* @param properties
* @param storeRoot
*/
public
ExampleAggregationServicePlugin
(
## Properties
properties
,
## File
storeRoot
)
{
super
(
properties
,
storeRoot
);
}
## @Override
public
TableModel
createReport
(
## Map
<
## String
,
## Object
>
parameters
,
DataSetProcessingContext
context
)
{
SimpleTableModelBuilder
builder
=
new
SimpleTableModelBuilder
(
true
);
builder
.
addHeader
(
## ""String""
);
builder
.
addHeader
(
## ""Integer""
);
IRowBuilder
row
=
builder
.
addRow
();
row
.
setCell
(
## ""String""
,
## ""Hello""
);
row
.
setCell
(
## ""Integer""
,
20
);
row
=
builder
.
addRow
();
row
.
setCell
(
## ""String""
,
parameters
.
get
(
""name""
).
toString
());
row
.
setCell
(
## ""Integer""
,
30
);
return
builder
.
getTableModel
();
}
}
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.ExampleAggregationServicePlugin
label = My Report
JythonAggregationService

## Type:
## AGGREGATION_TABLE_MODEL
## Description
: Invokes a Jython script to create an aggregation
service report. For more details see
Jython-based Reporting and Processing Plugins
.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonAggregationService
label = My Report
script-path = script.py
IngestionService

## Type:
## AGGREGATION_TABLE_MODEL
## Description
: An abstract superclass for aggregation service
reporting plugins that modify entities in the database. A db-modifying
aggregation service reporting plugin takes a hash map containing user
parameters and a transaction as arguments and returns tabular data (in
the form of a TableModel). The transaction is an
IDataSetRegistrationTransactionV2
,
the same interface that is used by
dropboxes
to register and modify entities. The JythonBasedDbModifyingAggregationServiceReportingPlugin below is a subclass that allows for implementation of the logic in Jython.
### Configuration
: Dependent on the subclass.
To implement an aggregation service in Java, define a subclass
of
ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.IngestionService
.
This subclass must implement the method
TableModel process(IDataSetRegistrationTransactionV2 transaction, Map<String, Object> parameters, DataSetProcessingContext context)
## Example
## :
ExampleDbModifyingAggregationService.java
package
ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard
;
import
java.io.File
;
import
java.util.Map
;
import
java.util.Properties
;
import
ch.systemsx.cisd.etlserver.registrator.api.v2.IDataSetRegistrationTransactionV2
;
import
ch.systemsx.cisd.openbis.dss.generic.shared.DataSetProcessingContext
;
import
ch.systemsx.cisd.openbis.dss.generic.shared.dto.DataSetInformation
;
import
ch.systemsx.cisd.openbis.generic.shared.basic.dto.TableModel
;
import
ch.systemsx.cisd.openbis.generic.shared.util.IRowBuilder
;
import
ch.systemsx.cisd.openbis.generic.shared.util.SimpleTableModelBuilder
;
/**
* An example aggregation service
*
* @author Chandrasekhar Ramakrishnan
*/
public
class
ExampleDbModifyingAggregationService
extends
IngestionService
<
DataSetInformation
>
{
private
static
final
long
serialVersionUID
=
## 1L
;
/**
* @param properties
* @param storeRoot
*/
public
ExampleDbModifyingAggregationService
(
## Properties
properties
,
## File
storeRoot
)
{
super
(
properties
,
storeRoot
);
}
## @Override
public
TableModel
process
(
IDataSetRegistrationTransactionV2
transaction
,
## Map
<
## String
,
## Object
>
parameters
,
DataSetProcessingContext
context
)
{
transaction
.
createNewSpace
(
""NewDummySpace""
,
null
);
SimpleTableModelBuilder
builder
=
new
SimpleTableModelBuilder
(
true
);
builder
.
addHeader
(
## ""String""
);
builder
.
addHeader
(
## ""Integer""
);
IRowBuilder
row
=
builder
.
addRow
();
row
.
setCell
(
## ""String""
,
## ""Hello""
);
row
.
setCell
(
## ""Integer""
,
20
);
row
=
builder
.
addRow
();
row
.
setCell
(
## ""String""
,
parameters
.
get
(
""name""
).
toString
());
row
.
setCell
(
## ""Integer""
,
30
);
return
builder
.
getTableModel
();
}
}
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.ExampleDbModifyingAggregationService
label = My Report
JythonIngestionService

## Type:
## AGGREGATION_TABLE_MODEL
## Description
: Invokes a Jython script to register and modify entitiesand create an aggregation service report. The script receives a transaction as an argument. For more details see
Jython-based Reporting and Processing Plugins
.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
share-id
Optional, defaults to 1 when not stated otherwise.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonIngestionService
label = My Report
script-path = script.py
JythonBasedReportingPlugin

## Type:
## TABLE_MODEL
## Description
: Invokes a Jython script to create the report. For more
details see
Jython-based Reporting and Processing
## Plugins
.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonBasedReportingPlugin
label = My Report
dataset-types = MS_DATA, UNKNOWN
script-path = script.py
TSVViewReportingPlugin

## Type:
## TABLE_MODEL
## Description
: Presents the main data set file as a table. The main
file is specified by the Main Data Set Pattern and the Main Data Set
Path of the data set type. The file can be a CSV/TSV file or an Excel
file. This reporting plugin works only for one data set.
### Configuration
## :
## Property Key
## Description
separator
Separator character. This property will be ignored if the file is an Excel file. Default: TAB character
ignore-comments
If true all rows starting with ‘#’ will be ignored. Default: true
ignore-trailing-empty-cells
If true trailing empty cells will be ignored. Default: false
excel-sheet
Name or index of the Excel sheet used. This property will only be used if the file is an Excel file. Default: 0
transpose
If true transpose the original table, that is exchange rows with columns. Default: false
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin
label = My Report
dataset-types = MS_DATA, UNKNOWN
separator = ;
## Screening Reporting Plugins

ScreeningJythonBasedAggregationServiceReportingPlugin

## Type:
## AGGREGATION_TABLE_MODEL
## Description
: Invokes a Jython script to create an aggregation
service report. For more details see
Jython-based Reporting and
## Processing
## Plugins
. There is some extra support for screening.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningJythonBasedReportingPlugin
label = My Report
dataset-types = HCS_IMAGE
script-path = script.py
ScreeningJythonBasedDbModifyingAggregationServiceReportingPlugin

## Type:
## AGGREGATION_TABLE_MODEL
## Description
: Invokes a Jython script to register and modify entities
and create an aggregation service report. The screening-specific version
has access to the screening facade for queries to the imaging database
and is given a screening transaction that supports registering plate
images and feature vectors. For more details see
Jython-based Reporting
and Processing
## Plugins
.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningJythonBasedReportingPlugin
label = My Report
dataset-types = HCS_IMAGE
script-path = script.py
ScreeningJythonBasedReportingPlugin

## Type:
## TABLE_MODEL
## Description
: Invokes a Jython script to create the report. For more details see
Jython-based Reporting and Processing Plugins
.
There is some extra support for screening.
### Configuration
## :
## Property Key
## Description
script-path
Path to the jython script.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningJythonBasedAggregationServiceReportingPlugin
label = My Report
script-path = script.py",Reporting Plugins,0,en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_reporting-plugins_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_reporting-plugins.txt,2025-09-30T12:09:02.135046Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_as-api-listener:0,API Listener Core Plugin (V3 API),https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/as-api-listener.html,openbis,"API Listener Core Plugin (V3 API)

## Introduction

The V3 API listener core plugin is an implementation of the interceptor pattern:
<https://en.wikipedia.org/wiki/Interceptor_pattern>
It actually intercepts twice, right before an operation is executed, and right after.
Its main focus is to help integrations. It gives an opportunity to integrators to execute additional functionality before or after an api call with the next purposes:
Modify the API call inputs/outputs immediately before/after they reach its executor.
Trigger additional internal logic.
Notify third party systems.
## Core Plugin

To archive these goals is necessary to provide a core plugin of the type ‘api-listener’ to the AS:
Plugin.properties

It is required to provide an ‘operation-listener.class’ indicating the class name of the listener that will be loaded.
Additionally any number of properties following the pattern
operation-listener.<your-custom-name>
can be provided. Custom properties are provided to help maintainability, they give an opportunity to the integrator to only need to compile the listener once and configure it differently for different instances.
plugin.properties
operation-listener.class = ch.ethz.sis.openbis.generic.server.asapi.v3.executor.operation.OperationListenerExample
operation-listener.your-config-property = Your Config Message
lib

The core plugin should contain a lib folder with a jar containing a class that implements the interface IOperationListener, this interface is provided with the V3 API jar and provides 3 methods:
setup: Runs on startup. Gives one opportunity to read the configuration provided to the core plugin
beforeOperation: Runs before each operation occurs. In addition to the operation intercepted it also provides access to the api and the session token used for the operation.
afterOperation: Intercepts after the operation occurs. In addition to the operation intercepted it also provides access to the api, the session token used for the operation, the operation result and any exception that happened during the operation.
## Warning
### Implicit Requirements
Requirement 1:  The Listener should be Thread Safe Code
A single instance of the Listener is created during the server startup. Since a single instance is used to serve all requests thread safe code is a requirement. We strongly suggest to not to keep any state.
Requirement 2: The Listener should not throw Exceptions
If the listener throw an exception it will make the API call fail.
Requirement 3: The Listener should use IOperation and IOperationResult as indicated below
All API Operations go through every listener so the method signatures should use IOperation and IOperationResult.
Please use instanceof for safe casting.
IOperationListener
package
ch.ethz.sis.openbis.generic.asapi.v3.plugin.listener
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperationResult
;
import
java.util.Properties
;
public
interface
IOperationListener
<
## OPERATION
extends
IOperation
,
## RESULT
extends
IOperationResult
>
{
public
static
final
## String
## LISTENER_PROPERTY_KEY
=
""operation-listener""
;
public
static
final
## String
## LISTENER_CLASS_KEY
=
## LISTENER_PROPERTY_KEY
+
"".class""
;
public
abstract
void
setup
(
## Properties
properties
);
public
abstract
void
beforeOperation
(
IApplicationServerApi
api
,
## String
sessionToken
,
## OPERATION
operation
);
public
abstract
void
afterOperation
(
IApplicationServerApi
api
,
## String
sessionToken
,
## OPERATION
operation
,
## RESULT
result
,
RuntimeException
runtimeException
);
}
## Example - Logging

The next implementation example captures the calls and logs on the standard openbis log the operation name:
OperationListenerExample
package
ch.ethz.sis.openbis.generic.server.asapi.v3.executor.operation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperation
;
import
ch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperationResult
;
import
ch.ethz.sis.openbis.generic.asapi.v3.plugin.listener.IOperationListener
;
import
ch.systemsx.cisd.common.logging.LogCategory
;
import
ch.systemsx.cisd.common.logging.LogFactory
;
import
org.apache.log4j.Logger
;
import
java.util.Properties
;
public
class
OperationListenerExample
implements
IOperationListener
<
IOperation
,
IOperationResult
>
{
private
static
final
## Logger
operationLog
=
LogFactory
.
getLogger
(
LogCategory
.
## OPERATION
,
OperationListenerExample
.
class
);
private
## String
yourConfigProperty
=
null
;
## @Override
public
void
setup
(
## Properties
properties
)
{
yourConfigProperty
=
properties
.
getProperty
(
""operation-listener.your-config-property""
);
operationLog
.
info
(
""setup: ""
+
yourConfigProperty
);
}
## @Override
public
void
beforeOperation
(
IApplicationServerApi
api
,
## String
sessionToken
,
IOperation
operation
)
{
operationLog
.
info
(
""beforeOperation: ""
+
operation
.
getClass
().
getSimpleName
());
}
## @Override
public
void
afterOperation
(
IApplicationServerApi
api
,
## String
sessionToken
,
IOperation
operation
,
IOperationResult
result
,
RuntimeException
runtimeException
)
{
operationLog
.
info
(
""afterOperation: ""
+
operation
.
getClass
().
getSimpleName
());
}
}
## Example - Loggin Sources

You can download a complete example with sources
here
to use as a template to make your own.",Introduction,0,en_20.10.0-11_software-developer-documentation_server-side-extensions_as-api-listener_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_as-api-listener.txt,2025-09-30T12:09:02.207746Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_as-services:0,Custom Application Server Services,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/as-services.html,openbis,"## Custom Application Server Services

## Introduction

On Data Store Server (DSS) aggregation/ingestion services based on Jython scripts can be used to extend openBIS by custom services. These services have full access on data store and Application Server (AS).
Often only access on AS is needed. Going over DSS is a detour. For such cases it is better to write an AS core plugin of type
services
.
How to write a custom AS service core plugin

Here is the recipe to create an AS core plugin of type
services
## :
The folder
<core
plugin
folder>/<module>/<version>/as/services/<core
plugin
name>
has to be created.
In this folder two files have to be created:
plugin.properties
and
script.py
. The properties file should contain:
plugin.properties
class = ch.ethz.sis.openbis.generic.server.asapi.v3.helper.service.JythonBasedCustomASServiceExecutor
script-path = script.py
The script file should have the function
process
with two arguments. The first argument is the context. It contains the methods
getSessionToken()
and
getApplicationService()
which returns an instance of
ch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi
. The second argument is a map of key-value pairs. The key is a string and the values is an arbitrary object. Anything returned by the script will be returned to the caller of the service. Here is an example of a script which creates a space:
script.py
from
ch.ethz.sis.openbis.generic.asapi.v3.dto.space.create
import
SpaceCreation
def
process
(
context
,
parameters
## ):
space_creation
=
SpaceCreation
()
space_creation
.
code
=
parameters
.
get
(
'space_code'
);
result
=
context
.
applicationService
.
createSpaces
(
context
.
sessionToken
,
[
space_creation
]);
return
## ""Space created:
%s
""
%
result
Note, that all changes on the AS database will be done in one transaction.
How to use a custom AS service

The application API version 3 offers the following method to search for existing services:
SearchResult
<
CustomASService
>
searchCustomASServices
(
## String
sessionToken
,
CustomASServiceSearchCriteria
searchCriteria
,
CustomASServiceFetchOptions
fetchOptions
)
The following Java code example returns all available services:
SearchResult
<
CustomASService
>
services
=
service
.
searchCustomASServices
(
sessionToken
,
new
CustomASServiceSearchCriteria
(),
new
CustomASServiceFetchOptions
());
With the following method of the API version 3 a specified service can
## be executed:
public
## Object
executeCustomASService
(
## String
sessionToken
,
ICustomASServiceId
serviceId
,
CustomASServiceExecutionOptions
options
);
## The
serviceId
can be obtained from a
CustomASService
object (as returned by the
searchCustomASServices
method) by the getter method
getCode()
. It can also be created as an instance of
CustomASServiceCode
. Note, that the service code is just the core plugin name.
Parameter bindings (i.e. key-value pairs) are specified in the
CustomASServiceExecutionOptions
object by invoking for each binding the method
withParameter()
.
Here is a code example:
CustomASServiceExecutionOptions
options
=
new
CustomASServiceExecutionOptions
().
withParameter
(
""space_code""
,
""my-space""
);
## Object
result
=
service
.
executeCustomASService
(
sessionToken
,
new
CustomASServiceCode
(
""space-creator""
),
options
);
## System
.
out
.
println
(
result
);",Custom Application Server Services,0,en_20.10.0-11_software-developer-documentation_server-side-extensions_as-services_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_as-services.txt,2025-09-30T12:09:02.272348Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_core-plugins:0,Core Plugins,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/core-plugins.html,openbis,"## Core Plugins

## Motivation

## The
service.properties
file of openBIS Application Server (AS) and Data Store Server (DSS) can be quite big because of all the configuration data for maintenance tasks, drop-boxes, reporting and processing plugins, etc. Making this configuration more modular will improve the structure. It would also allow to have core plugins shipped with distribution and customized plugins separately. This makes maintenance of these plugins more independent. For example, a new maintenance task plugin can be added in an update without any need for an admin to put the configuration data manually into the
service.properties
file.
## Core Plugins Folder Structure

All plugins whether they are a part of the distribution or added and maintained are stored in the folder usually called
core-plugins
. Standard (i.e. core) plugins are part of the distribution. During installation the folder
core-plugins
is unpacked as a sibling folder of
openBIS-server
and
datastore_server
.
The folder structure is organized as follows:
The file
core-plugins.properties
containing the following properties:
enabled-modules
: comma-separated list of regular expressions for all enabled modules.
disabled-core-plugins
: comma-separated list of disabled plugins. All plugins are disabled for which the beginning of full plugin ID matches one of the terms of this list. To disable initialization of master data of a module - disable it’s core plugin “initialize-master-data”
The children of
core-plugins
are folders denoting modules like the standard technologies,
proteomics
and
screening
. For customization, any module can be added.
Each module folder has children which are numbered folders. The number denotes the version of the plugins of that module. The version with the largest number will be used. Different modules can have different largest version numbers.
Every version folder has the subfolder
as
and/or
dss
which have subfolders for the various types of plugins. The types are different for AS and DSS:
## AS:
maintenance-tasks
: Maintenance tasks triggered by some time schedule. Property
class
denotes fully-qualified class name of a class implementing
ch.systemsx.cisd.common.maintenance.IMaintenanceTask
. For more details see
## Maintenance Tasks
.
dss-data-sources
: Definition of data sources with corresponding data source definitions for DSS. For more details see
Installation and Administrator Guide of the openBIS Server
.
query-databases
: Databases for SQL queries. For more details see
## Custom Database Queries
.
custom-imports
: Custom file imports to DSS via Web interface. For more details see
## Custom Import
.
services
: Custom services. For more details see
## Custom Application Server Services
.
webapps
: HTML5 applications that use the openBIS API. For more details see
openBIS webapps
.
miscellaneous
: Any additional properties.
## DSS:
drop-boxes
: ETL server threads for registration of data sets.                            `
reporting-plugins
: Reports visible in openBIS. Property
class
denotes fully-qualified class name of a class implementing
ch.systemsx.cisd.openbis.dss.generic.server.plugins.tasks.IReportingPluginTask
. For more details see
## Reporting Plugins
.
processing-plugins
: Processing tasks triggered by users. Property
class
denotes fully-qualified class name of a class implementing
ch.systemsx.cisd.openbis.dss.generic.server.plugins.tasks.IProcessingPluginTask
. For more details see
## Processing Plugins
.
maintenance-tasks
: Maintenance tasks triggered by some time schedule. Property
class
denotes fully-qualified class name of a class implementing
ch.systemsx.cisd.common.maintenance.IMaintenanceTask
. For more details see
## Maintenance Tasks
.
search-domain-services
: Services for variaous search domains (e.g. search on sequence databases using BLAST). Property
class
denotes fully-qualified class name of a class implementing
ch.systemsx.cisd.openbis.dss.generic.shared.api.internal.v2.ISearchDomainService
.
data-sources
: Internal or external database sources.
services
: Services based on servlets. Property
class
denotes fully-qualified class name of a class implementing
javax.servlet.Servlet
.
imaging-overview-plugins
: Data set type specific provider of the overview image of a data set. Property
class
denotes fully-qualified class name of a class implementing
## ch.systemsx.cisd.openbis.dss.generic.server.IDatasetImageOverviewPlugin
.
file-system-plugins
: Provider of a custom DSS file system (FTP/SFTP) view hierarchy. Property
class
denotes fully-qualified class name of a class
implementing
ch.systemsx.cisd.openbis.dss.generic.server.fs.IResolverPlugin
Property code denotes the name of the top-level directory
under which the custom hierarchy will be visible
miscellaneous
: Any additional properties.
Folders of each of these types can have an arbitrary number of subfolders. But if the type folder is present it should have at least one subfolder. Each defining one plugin. The name of these subfolders define the plugin ID. It has to be unique over all plugins independent of module and plugin type. It should not contain the characters space ‘ ‘, comma ‘
,
’, and equal sign ‘
=
’.
Each plugin folder should contain at least the file
plugin.properties
. There could be additional files (referred in
plugin.properties
) but no subfolders.
Here is an example of a typical structure of a core plugins folder:
core-plugins
core-plugins.properties
proteomics
1
as
initialize-master-data.py
dss
drop-boxes
ms-injection
plugin.properties
maintenance-tasks
data-set-clean-up
plugin.properties
screening
1
core-plugin.properties
as
initialize-master-data.py
maintenance-tasks
material-reporting
mapping.txt
plugin.properties
custom-imports
myCustomImport
plugin.properties
dss
drop-boxes
hcs-dropbox
lib
custom-lib.jar
hcs-dropbox.py
plugin.properties
You might noticed the file
initialize-master-data.py
in AS core plugins sections  in this example. It is a script to register master data in the openBIS core database. For more details see
Installation and Administrator Guide of the openBIS Server
.
Each plugin can refer to any number of files. These files are part of
the plugin folder. In
plugin.properties
they are referred relative to
the plugin folder, that is by file name. Example:
plugin.properties
incoming-dir = ${incoming-root-dir}/incoming-hcs
incoming-data-completeness-condition = auto-detection
top-level-data-set-handler = ch.systemsx.cisd.openbis.dss.etl.jython.JythonPlateDataSetHandler
script-path = hcs-dropbox.py
storage-processor = ch.systemsx.cisd.openbis.dss.etl.PlateStorageProcessor
storage-processor.data-source = imaging-db
storage-processor.define-channels-per-experiment = false
### Merging Configuration Data

At start up of AS and DSS merges the content of
service.properties
with the content of all
plugin.properties
of the latest version per enabled module. Plugin properties can be deleted by adding
<plugin
ID>.<plugin
property
key>
=
## __DELETED__
to service.properties. Example:
simple-dropbox.incoming-data-completeness-condition
=
## __DELETED__
This leads to a deletion of the property
incoming-data-completeness-condition
specified in
plugins.properties
of the plugin
simple-dropbox
.
Merging is done by injection the properties of
plugin.properties
into
service.properties
by adding the plugin ID as a prefix to the property key (not for
miscellaneous).
For example, the property
script-path
of plugin
hcs-dropbox
becomes
hcs-dropbox.script-path
. References to files inside the plugin are replaced by a path relative to the working directory. For the various plugin types (except
miscellaneous
) the plugin ID is appended to the related property in
service.properties
for this plugin type. For example, plugins of type
drop-boxes
are added to the property
inputs
.
Enabling Modules and Disabling Plugins

There are three methods to control which plugins are available and witch not:
enabling by property
enabled-modules
in
core-plugins.properties
: This enables all plugins of certain modules.
disabling by property
disabled-core-plugins
in
core-plugins.properties
: This allows to disable on a fine grade level specific plugins.
disabling by marker file: Plugin developers should use this method when developing new plugins.
## Enabling Modules

The property
enabled-modules
in
core-plugins.properties
is a comma-separated list of regular expressions denoting modules. All plugins in a module folder of
core-plugins
folder are enabled if the module name matches one of these regular expressions. If this list is empty or the property hasn’t been specified no core-plugin will be used. Note, that this property is manipulated by openBIS Installer for Standard Technologies. Example:
service.properties
enabled-modules
=
screening,
proteomics,
dev-module-.*
Disabling Core Plugins by Property

The property
disabled-core-plugins
in
core-plugins.properties
allows to disable plugins selectively either by module name, module combined with plugin type or full plugin ID. Example:
service.properties
disabled-core-plugins
=
screening,
proteomics:reporting-plugins,
proteomics:maintenance-tasks:data-set-clean-up
Disabling Core Plugins by Marker File

The empty marker file
disabled
in a certain plugin folder disables the particular plugin.
## Core Plugin Dependency

A core plugin can depend on another core plugin. The dependency is specified in
<module>/<version>/core-plugin.properties
. It has a property named
required-plugins
. Its value is a comma-separated list of core-plugins on which it depends. The dependency can be pecified selectively either by module name, module combined with plugin type or full plugin ID. Example:
core-plugin.properties
required-plugins
=
module-a,
module-b:initialize-master-data,
module-b:reporting-plugins,
module-a:drop-boxes:generic
Rules for Plugin Writers

As a consequence of the way plugins are merged with
service.properties
writers of plugins have to obey the following rules:
Plugin IDs have to be unique among all plugins whether they are defined in
service.properties
or as core plugins. The only exceptions are plugins of type
miscellaneous
.
## In
plugin.properties
other properties can be referred by the usual
${<property
key>
} notation. The referred property can be in
service.properties
or in any
plugin.properties
.
As convention use
${incoming-root-dir
} when defining the incoming folder for a drop box.
Refer files in
plugin.properties
only by names and add them as siblings of
plugin.properties
to the plugin folder. Note, that different plugins can refer files with the same name. There will be no ambiguity which file is meant.
In order to be completely independent from updates of the core plugins which are part of the distribution create your own module, like
my-plugins
, and put all your plugins there. Do not forget to add your module to the property
enabled-modules
in
core-plugins.properties
.
Using Java libraries in Core Plugins

OpenBIS allows you to include Java libraries in core plugin folders. The *.jar files have to be stored in
<code
plugin
folder>/lib
folder. For instance, in order to use “my-lib.jar” in “my-dropbox” a following file structure is needed:
service.properties
my
-
technology
1
dss
drop
-
boxes
my
-
dropbox
lib
my
-
lib
.
jar
dropbox
.
py
plugin
.
properties
Having this structure, Java classes from “my-lib.jar” can be imported and used in “dropbox.py” script.
## Note
Currently this feature is only supported for DSS core plugins. Under the hood, a symbolic link to a jar file is created in “datastore_server/lib” folder during DSS startup.",Core Plugins,0,en_20.10.0-11_software-developer-documentation_server-side-extensions_core-plugins_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_core-plugins.txt,2025-09-30T12:09:02.341859Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:0,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"## Dropboxes

## Jython Dropboxes

## Introduction

The jython dropbox feature makes it possible for a script written in the
Python language to control the data set registration process of the
openBIS Data Store Server. A script can modify the files in the dropbox
and register data sets, samples, and experiments as part of its
processing. The framework provides tools to track file operations and,
if necessary, revert them, ensuring that the incoming file or directory
is returned to its original state in the event of an error.
By deafult python 2.5 is used, but it’s possible to use python version
2.7.
## Dropboxes are dss core plugins:
## Core Plugins
## Simple Example

Here is an example that registers files that arrive in the drop box as
data sets. They are explicitly attached to the experiment “JYTHON” in
the project “TESTPROJ” and space “TESTGROUP”.
data-set-handler-basic.py
def
process
(
transaction
## ):
# Create a data set
dataSet
=
transaction
.
createNewDataSet
()
# Reference the incoming file that was placed in the dropbox
incoming
=
transaction
.
getIncoming
()
# Add the incoming file into the data set
transaction
.
moveFile
(
incoming
.
getAbsolutePath
(),
dataSet
)
# Get an experiment for the data set
exp
=
transaction
.
getExperiment
(
## ""/TESTGROUP/TESTPROJ/JYTHON""
)
# Set the owner of the data set -- the specified experiment
dataSet
.
setExperiment
(
exp
)
This example is is unrealistically simple, but contains all the elements
necessary to implement a jython drop box. The main idea is to perform
several operations within the bounds of a transaction on the data and
metadata. The transaction is used to track the changes made so they can
be executed together or all reverted if a problem occurs.
## More Realistic Example

The above example demonstrates the concept, but it is unrealistically
simple. In general, we want to be able to determine and specify the
experiment/sample for a data set and explicitly set the data set type as
well.
In this example, we handle a usage scenario where there is one
experiment done every day. All data produced on a single day is
associated with the experiment for that date. If the experiment for a
given day does not exist, it is created.
data-set-handler-experiment-reg.py
from
datetime
import
datetime
def
process
(
transaction
## ):
# Try to get the experiment for today
now_str
=
datetime
.
today
()
.
strftime
(
'%Y%m
%d
'
)
expid
=
## ""/TESTGROUP/TESTPROJ/""
+
now_str
exp
=
transaction
.
getExperiment
(
expid
)
# Create an experiment if necessary
if
## None
==
exp
## :
exp
=
transaction
.
createNewExperiment
(
expid
,
## ""COMPOUND_HCS""
)
exp
.
setPropertyValue
(
## ""DESCRIPTION""
,
""An experiment created on ""
+
datetime
.
today
()
.
strftime
(
'%Y-%m-
%d
'
))
exp
.
setPropertyValue
(
## ""COMMENT""
,
now_str
)
dataSet
=
transaction
.
createNewDataSet
()
incoming
=
transaction
.
getIncoming
()
transaction
.
moveFile
(
incoming
.
getAbsolutePath
(),
dataSet
)
dataSet
.
setDataSetType
(
## ""HCS_IMAGE""
)
dataSet
.
setExperiment
(
exp
)
More complex processing is also possible. In the following sections, we
explain how to configure a jython dropbox and describe the API in
greater detail.
## Model

The model underlying dropbox registration is the following: when a new
file or folder is found in the dropbox folder, the process function of
the script file is invoked with a
data set registration transaction
as an argument.
The process function has the responsibility of looking at the incoming
file or folder and determining what needs to be registered or modified
in the metadata database and what data needs to be stored on the file
system. The
IDataSetRegistrationTransaction
interface
defines the API for specifying entities to register and update.
Committing a transaction is actually a two-part process. The metadata is stored in the openBIS application server’s database; the data is kept on the file system in a sharded directory structure beneath the data store server’s
store
directory. All modifications requested as part of a transaction are committed atomically — they either all succeed or all fail.
## Several
## Events
occur in the process of committing a transaction. By defining jython functions, it is possible to be notified and intervene when an event occurs. Because the infrastructure reserves the right to delay or retry actions if resources become unavailable, the process function and event functions cannot use global variables to communicate with each other. Instead, they should use the registration context object to communicate. Anything stored in the registration context must, however, be serializable by Java serialization.
## Details

### Dropbox Configuration

A jython dropbox is typically distributed as a
core plugin
and configured in its plugin.properties file. A dropbox configured to run a jython script, which is kept in the same directory as plugin.properties. The configuration requires a storage processor and the name of the script (a full path is not necessary if the script is in the same directory as the plugin.properties). Here is an example configuration for a dropbox that uses the jython handler.
plugin.properties
#
# REQUIRED PARAMETERS
#
# The directory to watch for new data sets
incoming-dir = ${root-dir}/incoming-jython",Dropboxes,0,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:1,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JythonTopLevelDataSetHandlerV2 or a subclass thereof
top-level-data-set-handler = ch.systemsx.cisd.etlserver.registrator.api.v2.JythonTopLevelDataSetHandlerV2",The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JythonTopLevelDataSetHandlerV2 or a subclass thereof,1,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_1,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:2,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# The script to execute, reloaded and recompiled each time a file/folder is placed in the dropbox
script-path = ${root-dir}/data-set-handler.py","The script to execute, reloaded and recompiled each time a file/folder is placed in the dropbox",2,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_2,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:3,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# The appropriate storage processor
storage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor",The appropriate storage processor,3,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_3,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:4,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# Specify jython version. Default is whatever is specified in datastore server service.properties under property ""jython-version""
plugin-jython-version=2.5
#
# OPTIONAL PARAMETERS
#","Specify jython version. Default is whatever is specified in datastore server service.properties under property ""jython-version""",4,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_4,reference,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:5,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# False if incoming directory is assumed to exist.
# Default - true: Incoming directory will be created on start up if it doesn't exist.
incoming-dir-create = true",False if incoming directory is assumed to exist.,5,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_5,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:6,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# Defines how the drop box decides if a folder is ready to process: either by a 'marker-file' or a time out which is called 'auto-detection'
# The time out is set globally in the service.properties and is called 'quiet-period'. This means when the number of seconds is over and no changes have
# been made to the incoming folder the drop will start to register. The marker file must have the following naming schema: '.MARKER_is_finished_<incoming_folder_name>'
incoming-data-completeness-condition = marker-file",Defines how the drop box decides if a folder is ready to process: either by a 'marker-file' or a time out which is called 'auto-detection',6,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_6,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:7,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# Defines whether the dropbox should handle .h5 archives as folders (true) or as files (false). Default is true.
h5-folders = true",Defines whether the dropbox should handle .h5 archives as folders (true) or as files (false). Default is true.,7,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_7,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:8,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# Defines whether the dropbox should handle .h5ar archives as folders (true) or as files (false). Default is true.
h5ar-folders = true
Development mode

Set property
development-mode
=
true
in your dropbox to enable a quick
feedback loop when developing your dropbox. By default dropboxes have
complex auto-recovery mechanism working, which on errors waits and
retries the registration several times. It can be useful in case of
short network problems or other unexpected turbulences. In this case it
can take a long time between the dropbox tries to register something,
and actual error report. During development it is essential to have a
quick feedback if your dropbox does what it should or not. Thus - set
the development mode if you are modifying your script and remember to
set it back when you are done.
Jython version

Set property
plugin-jython-version=2.7
in your dropbox
plugin.properties to change default jython version for the single
dropbox. Available are versions 2.5 and 2.7
Jython API

When a new file is placed in the dropbox, the framework compiles and
executes the script, checks that the signatures of the
process
function and any defined event-handling functions are correct, and then
invokes its
process
function.
IDataSetRegistrationTransaction

Have a look
at
IDataSetRegistrationTransactionV2
for the calls available in a transaction. Note that you need to use the
file methods in the transaction, like e.g.
moveFile()
,  rather than
manipulating the file system directly to get fully transactional
behavior.
TransDatabase queries

The query object returned
by
getDatabaseQuery(String
dataSourceName)
allows to perform any query
and executing any statement on the given query database in the context
of a database transaction. Here are the methods available from the query
## interface:
public
interface
DynamicQuery
{
/**
* Performs a SQL query. The returned List is connected to the database and
* updateable.
*
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template.
*
* @return The result set as List; each row is represented as one Map<String,Object>.
*/
## List
<
## Map
<
## String
,
## Object
>>
select
(
final
## String
query
,
final
## Object
...
parameters
);
/**
* Performs a SQL query. The returned List is connected and
* updateable.
*
* @param type  The Java type to return one rows in the returned
*            result set.
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template.
*
* @return The result set as List; each row is represented as one Map<String,Object>.
*/
<
## T
>
## List
<
## T
>
select
(
final
## Class
<
## T
>
type
,
final
## String
query
,
final
## Object
...
parameters
);
/**
* Executes a SQL statement.
*
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template.
*
* @return The number of rows updated by the SQL statement, or -1 if not
*         applicable. <b>Note:</b> Not all JDBC drivers support this
*         cleanly.
*/
int
update
(
final
## String
query
,
final
## Object
...
parameters
);
/**
* Executes a SQL statement as a batch for all parameter values provided.
*
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template. At least
*            one of the parameters needs to be an array or
*            <code>Collection</code>. If multiple parameters are arrays or
*            <code>Collection</code>, all of them need to have the same
*            size.
*
* @return The number of rows updated by the SQL statement, or -1 if not
*         applicable. <b>Note:</b> Not all JDBC drivers support this
*         cleanly.
*/
int
batchUpdate
(
final
## String
query
,
final
## Object
...
parameters
);
/**
* Executes a SQL statement. Supposed to be used for INSERT statements with
* an automatically generated integer key.
*
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template.
*
* @return The automatically generated key. <b>Note:</b> Not all JDBC
*         drivers support this cleanly.
*/
long
insert
(
final
## String
query
,
final
## Object
...
parameters
);
/**
* Executes a SQL statement. Supposed to be used for INSERT statements with
* one or more automatically generated keys.
*
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template.
*
* @return The automatically generated keys. <b>Note:</b> Not all JDBC
*         drivers support this cleanly and it is in general driver-dependent
*         what keys are present in the returned map.
*/
## Map
<
## String
,
## Object
>
insertMultiKeys
(
final
## String
query
,
final
## Object
...
parameters
);
/**
* Executes a SQL statement as a batch for all parameter values provided.
* Supposed to be used for INSERT statements with an automatically generated
* integer key.
*
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template. At least
*            one of the parameters needs to be an array or
*            <code>Collection</code>. If multiple parameters are arrays or
*            <code>Collection</code>, all of them need to have the same
*            size.
*
* @return The automatically generated key for each element of the batch.
*         <b>Note:</b> Not all JDBC drivers support this cleanly.
*/
long
[]
batchInsert
(
final
## String
query
,
final
## Object
...
parameters
);
/**
* Executes a SQL statement as a batch for all parameter values provided.
* Supposed to be used for INSERT statements with one or more automatically
* generated keys.
*
* @param query  The SQL query template.
* @param parameters  The parameters to fill into the SQL query template. At least
*            one of the parameters needs to be an array or
*            <code>Collection</code>. If multiple parameters are arrays or
*            <code>Collection</code>, all of them need to have the same
*            size.
*
* @return The automatically generated keys for each element of the batch.
*         <b>Note:</b> Not all JDBC drivers support this cleanly and it is
*         in general driver-dependent what keys are present in the returned map.
*/
## Map
<
## String
,
## Object
>[]
batchInsertMultiKeys
(
final
## String
query
,
final
## Object
...
parameters
);
}
## Events / Registration Process Hooks

The script can be informed of events that occur during the registration
process. To be informed of an event, define a function in the script
file with the name specified in the table. The script can do anything it
wants within an event function. Typical things to do in event functions
include sending emails or registering data in secondary databases. Some
of the event functions can be used to control the behavior of the
registration.
This table summarizes the supported events.
## Events Table

## Function Name
## Return Value
## Description
pre_metadata_registration(DataSetRegistrationContext context)
void
Called before the openBIS AS is informed of the metadata modifications. Throwing an exception in this method aborts the transaction.
post_metadata_registration(DataSetRegistrationContext context)
void
The metadata has been successfully stored in the openBIS AS. This can also be a place to register data in a secondary transaction, with the semantics that any errors are ignored.
rollback_pre_registration(DataSetRegistrationContext context, Exception exception)
void
Called if the metadata was not successfully storedin the openBIS AS.
post_storage(DataSetRegistrationContext context)
void
Called once the data has been placed in the appropriate sharded directory of the store. This can only happen if the metadata was successfully registered with the AS.
should_retry_processing(DataSetRegistrationContext context, Exception problem)
boolean
A problem occurred with the process function, should the operation be retried? A retry happens only if this method returns true.
Note: the
rollback_pre_registration
function is intended to handle
cases when the dropbox code finished properly, but the registration of
data in openbis failed. These kinds of problems are impossible to handle
from inside of the
process
function. The exceptions raised during the
call to the
process
function should be handled by the function itself
by catching exceptions.
### Typical Usage Table

## Function Name
### Usage
pre_metadata_registration(DataSetRegistrationContext context)
This event can be used as a place to register information in a secondary database. If the transaction in the secondary database does not commit, false can be returned to prevent the data from entering openBIS.
post_metadata_registration(DataSetRegistrationContext context)
This event can be used as a place to register information in a secondary database. Errors encountered are ignored.
rollback_pre_registration(DataSetRegistrationContext context, Exception exception)
Undoing a commit to a secondary transaction. Sending an email to the admin that the data set could not be stored.
post_storage(DataSetRegistrationContext context)
Sending an email to tell the user that the data has been successfully registered. Notifying an external system that a data set has been registered.
should_retry_processing(DataSetRegistrationContext context, Exception problem)
Informing openBIS if it should retry processing a data set.
## Example Scripts

A simple script that registers the incoming file as a data set
associated with a particular experiment.
data-set-handler-basic.py
def
process
(
transaction
## ):
dataSet
=
transaction
.
createNewDataSet
()
incoming
=
transaction
.
getIncoming
()
transaction
.
moveFile
(
incoming
.
getAbsolutePath
(),
dataSet
)
dataSet
.
setExperiment
(
transaction
.
getExperiment
(
## ""/TESTGROUP/TESTPROJ/JYTHON""
))
A script that registers the incoming file and associates it to a daily
experiment, which is created if necessary.
data-set-handler-experiment-reg.py
from
datetime
import
datetime
def
process
(
transaction
)
# Try to get the experiment for today
now_str
=
datetime
.
today
()
.
strftime
(
'%Y%m
%d
'
)
expid
=
## ""/TESTGROUP/TESTPROJ/""
+
now_str
exp
=
transaction
.
getExperiment
(
expid
)
# Create an experiment
if
## None
==
exp
## :
exp
=
transaction
.
createNewExperiment
(
expid
,
## ""COMPOUND_HCS""
)
exp
.
setPropertyValue
(
## ""DESCRIPTION""
,
""An experiment created on ""
+
datetime
.
today
()
.
strftime
(
'%Y-%m-
%d
'
))
exp
.
setPropertyValue
(
## ""COMMENT""
,
now_str
)
dataSet
=
transaction
.
createNewDataSet
()
incoming
=
transaction
.
getIncoming
()
transaction
.
moveFile
(
incoming
.
getAbsolutePath
(),
dataSet
)
dataSet
.
setDataSetType
(
## ""HCS_IMAGE""
)
dataSet
.
setExperiment
(
exp
)
Delete, Move, or Leave Alone on Error

When a problem occurs processing a file in the dropbox, the processing
is retried. This behavior can be controlled (see
#Errors
). If openBIS determines that it should not
retry after an error or that it cannot successfully register the
entities requested, the registration fails. It it possible to configure
what happens to a file in the dropbox if a registration fails. The
configuration can specify a behavior – delete the file, move it to an
error folder, or leave it untouched – for each of several possible
sources of errors.
By default, the file is left untouched in every case. To change this
behavior, specify an on-error-decision property on the drop box. This
has one required sub-key, “class”; other sub-keys are determined by the
class.
## Summary

## Main Key:
on-error-decision
## Required Sub Keys:
class : The class the implements the decision
## There is currently one class available :
ch.systemsx.cisd.etlserver.registrator.ConfiguredOnErrorActionDecision
This class has the following sub keys:
invalid-data-set (a data set that fails validation)
validation-script-error (the validation script did not execute
correctly)
registration-error (openBIS failed to register the data set)
registration-script-error (the registration script did not
execute correctly)
storage-processor-error (the storage processor reports an error)
post-registration-error (an error happened after the data set
had been registered and stored)
## Example

plugin.properties
#
# On Error Decision
#
# The class that implements the decision
on-error-decision.class = ch.systemsx.cisd.etlserver.registrator.ConfiguredOnErrorActionDecision",Defines whether the dropbox should handle .h5ar archives as folders (true) or as files (false). Default is true.,8,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_8,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:9,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# What to do if the validation script has problems
on-error-decision.validation-script-error = MOVE_TO_ERROR",What to do if the validation script has problems,9,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_9,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:10,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# What to do if the openBIS does not accept the entities
on-error-decision.registration-error = MOVE_TO_ERROR",What to do if the openBIS does not accept the entities,10,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_10,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:11,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# What to do if the registration script has problems
on-error-decision.registration-script-error = MOVE_TO_ERROR",What to do if the registration script has problems,11,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_11,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:12,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# What to do if the storage processor does not run correctly
on-error-decision.storage-processor-error = MOVE_TO_ERROR",What to do if the storage processor does not run correctly,12,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_12,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:13,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# What to do if an error occurs after the entities have been registered in openBIS
on-error-decision.post-registration-error = MOVE_TO_ERROR
## Search

The transaction provides an interface for listing and searching for core
entities, experiment, sample, and data set.
## API

To use the search capability, one must first retrieve the search service
from the transaction. By default the search service returns the entities
filtered to only those accessible by the user on behalf of wich, the
script is running. It is still possible to search all existing entities
by using unfiltered search service accessible from the transaction via
method getSearchServiceUnfiltered().
## Experiment

For experiment, there is a facility for listing all experiments that
belong to a specified project.
Sample and Data Set

For sample and data set, a more powerful search capability is available.
This requires a bit more knowledge of the java classes, but is very
flexible. For each entity, there is a simplified method that performs a
search for samples or data sets, respectively, with a specified value
for a particular property, optionally restricted by entity type (sample
type or data set type). This provides an easy-to-use interface for a
common case. More complex searches, however, need to use the more
powerful API.
## Authorization Service

The transaction provides an interface for querying the access privileges
of a user and for filtering collections of entities down to those
visible to a user.
## API

To use the authorization service, one must first retrieve the it from
the transaction.
## Example

## Combined Example

In this example, we create a data set, list experiments belonging to a
project, search for samples, search for data sets, and assign the
experiment, sample, and parent data sets based on the results of the
searches.
data-set-handler-with-search.py
def
process
(
tr
## ):
data_set
=
tr
.
createNewDataSet
()
incoming
=
tr
.
getIncoming
()
tr
.
moveFile
(
incoming
.
getAbsolutePath
(),
data_set
)
# Get the search service
search_service
=
tr
.
getSearchService
()
# List all experiments in a project
experiments
=
search_service
.
listExperiments
(
""/cisd/noe""
)
# Search for all samples with a property value determined by the file name; we don't care about the type
samplePropValue
=
incoming
.
getName
()
samples
=
search_service
.
searchForSamples
(
## ""ORGANISM""
,
samplePropValue
,
## None
)
# If possible, set the owner to the first sample, otherwise the first experiment
if
samples
.
size
()
>
0
## :
data_set
.
setSample
(
samples
[
0
])
else
## :
data_set
.
setExperiment
(
experiments
[
0
])
# Search for any potential parent data sets and use them as parents
parent_data_sets
=
search_service
.
searchForDataSets
(
## ""COMMENT""
,
""no comment""
,
## ""HCS_IMAGE""
)
parent_data_set_codes
=
map
(
lambda
each
## :
each
.
getDataSetCode
(),
parent_data_sets
)
data_set
.
setParentDatasets
(
parent_data_set_codes
)
An example from the Deep Sequencing environment handling BAM files:
data-set-handler-alignment.py
'''
This is handling bowtie-BAM files and extracts some properties from the BAM header and
the samtools flagstat command. The results are formatted and attached  as a property
to the openBIS DataSet.
Prerequisites are the DataSetType: ALIGNMENT and
the following properties assigned to the DataSetType mentioned above:
ALIGNMENT_SOFTWARE, ISSUED_COMMAND, SAMTOOLS_FLAGSTAT,
## TOTAL_READS, MAPPED_READS
Obviously you need a working samtools binary
## Note:
print statements go to: ~openbis/sprint/datastore_server/log/startup_log.txt
'''
import
os
from
ch.systemsx.cisd.openbis.generic.shared.api.v1.dto
import
SearchCriteria
## FOLDER
=
'/net/bs-dsu-data/array0/dsu/dss/incoming-jython-alignment/'
## SAMTOOLS
=
'/usr/local/dsu/samtools/samtools'
def
process
(
transaction
## ):
incoming
=
transaction
.
getIncoming
()
# Create a data set and set type
dataSet
=
transaction
.
createNewDataSet
(
## ""ALIGNMENT""
)
dataSet
.
setMeasuredData
(
## False
)
incomingPath
=
incoming
.
getAbsolutePath
()
# Get the incoming name
name
=
incoming
.
getName
()
# expected incoming Name, e.g.:ETHZ_BSSE_110429_63558AAXX_1_sorted.bam
split
=
name
.
split
(
""_""
)
sample
=
split
[
2
]
+
'_'
+
split
[
3
]
+
':'
+
split
[
4
]
# Extract values from a samtools view and set the results as DataSet properties
# Command: samtools view -H ETHZ_BSSE_110429_63558AAXX_1_sorted.bam
arguments
=
## SAMTOOLS
+
' view -H '
+
## FOLDER
+
name
#print('Arguments: '+ arguments)
cmdResult
=
os
.
popen
(
arguments
)
.
read
()
properties
=
cmdResult
.
split
(
""
\n
""
)[
-
2
]
.
split
(
'
\t
'
)
aligner
=
(
properties
[
1
]
.
split
(
':'
)[
1
]
.
upper
()
+
'_'
+
properties
[
2
]
.
split
(
':'
)[
1
])
command
=
properties
[
3
]
arguments
=
## SAMTOOLS
+
' flagstat '
+
## FOLDER
+
name
cmdResult
=
os
.
popen
(
arguments
)
.
read
()
totalReads
=
cmdResult
.
split
(
'
\n
'
)[
0
]
.
split
(
' '
)[
0
]
mappedReads
=
cmdResult
.
split
(
'
\n
'
)[
2
]
.
split
(
' '
)[
0
]
dataSet
.
setPropertyValue
(
## ""ALIGNMENT_SOFTWARE""
,
aligner
)
dataSet
.
setPropertyValue
(
## ""ISSUED_COMMAND""
,
command
)
dataSet
.
setPropertyValue
(
## ""SAMTOOLS_FLAGSTAT""
,
cmdResult
)
dataSet
.
setPropertyValue
(
## ""TOTAL_READS""
,
totalReads
)
dataSet
.
setPropertyValue
(
## ""MAPPED_READS""
,
mappedReads
)
# Add the incoming file into the data set
transaction
.
moveFile
(
incomingPath
,
dataSet
)
# Get the search service
search_service
=
transaction
.
getSearchService
()
# Search for the sample
sc
=
SearchCriteria
()
sc
.
addMatchClause
(
SearchCriteria
.
MatchClause
.
createAttributeMatch
(
SearchCriteria
.
MatchClauseAttribute
.
## CODE
,
sample
));
foundSamples
=
search_service
.
searchForSamples
(
sc
)
if
foundSamples
.
size
()
>
0
## :
dataSet
.
setSample
(
foundSamples
[
0
])
Error Handling

Automatic Retry (auto recovery)

OpenBIS has a complex mechanism to ensure that the data registration via
dropboxes is atomic. When error occurs during data registration, the
dropbox will try several times before it gives up on the process. The
retries can happen to the initial processing of the data, as well as to
the registration in application server. Even if these fail there is
still a chance to finish the registration. If the registration reaches
the certain level it stores the checkpoint on the disk. If at any point
the process fails, or the dss goes down it tries to recover from the
checkpoint.
There are two types of checkpoint files: State files and marker files.
There are stored in two different directories. The default location for
the state files is
datastore_sever/recovery-state
. This can be changed
by the property
dss-recovery-state-dir
in DSS
service.properties
.
The default location for the marker files was
<store
location>/<share
id>/recovery-marker
. This may lead to problems
if this local is remote. Since version 20.10.6 the default location is
datastore_sever/recovery-marker-dir
. This can be changed by the
property
dss-recovery-marker-dir
in DSS
service.properties
.
## The
process
function will be retried if a
should_retry_processing
function is defined in the dropbox script and
it returns true. There are two configuration settings that affect this
behavior. The setting
process-max-retry-count
limits the number of
times the process function can be retried. The number of times to retry
before giving up and the waiting periods are defined using properties
shown in the table below.
IMPORTANT NOTE: Please note, that the registration is considered as
failed only after, the whole retrying / recovery process will fail. It
means that it can take a long time before the .faulty_paths file is
created, even when there is a simple dropbox error.
Therefor during development of a dropbox we recommend using
development mode
, wich basically sets all retry values to 0, thus disabling the auto-recovery feature.
## Key
## Default Value
## Meaning
process-max-retry-count
6
The maximum number of times the process function can be retried.
process-retry-pause-in-sec
300
The amount of time to wait between retries of the process function.
metadata-registration-max-retry-count
6
The number of times registering metadata with the server can be retried.
metadata-registration-retry-pause-in-sec
300
The number of times registering metadata with the server can be retried.
recovery-max-retry-count
50
The number of times the recovery from checkpoint can be retries.
recovery-min-retry-period
60
The amount of time to wait between recovery from checkpoint retries.
## Manual Recovery

The registration of data sets with Jython dropboxes has been designed to
be quite robust. Nonetheless, there are situations in which problems may
arise. This can especially be a problem during the development of
dropboxes. Here are the locations and semantics of several important
files and folders that can be useful for debugging a dropbox.
File or Folder
## Meaning
datastore_server/log-registrations
Keeps logs of registrations. See the registration log documentation for more information.
[store]/[share]/pre-staging
Contains hard-link copies of the original data. Dropbox process operate on these hardlink copies.
[store]/[share]/staging
The location used to prepare data sets for registration.
[store]/[share]/pre-commit
Where data from data sets are kept while register the metadata with the AS. Once metadata registration succeeds, files are moved from this folder into the final store directory.
[store]/[share]/recovery-marker (before version 20.10.6)
datastore_sever/recovery-marker-dir (since version 20.10.6)
Directories, one per dropbox, where marker files are kept that indicate that a recovery should happen on an incoming file if it is reprocessed. Deleting a marker file will force the incoming file to be processed as a new file, not a recovery.
### Classpath / Configuration

If you want other jython modules to be available to the code that
implements the drop box, you will need to modify the
datastore_server.conf file and add something like
-Dpython.path=data/dropboxes/scripts:lib/jython-lib
To the JAVA_OPTS environment variable. The line should now look
## something like this:
JAVA_OPTS=${JAVA_OPTS:=-server
-d64
-Dpython.path=data/dropboxes/scripts:lib/jython-lib}
If the Jython dropbox need third-party JAR files they have to be added
to the core plugin in a sub-folder
lib/
.
Validation scripts

## See
Jython DataSetValidator
.
## Global Thread Parameters

If you want to write a drop box which uses some parameters defined in
the service.properties you can access those properties via
the
getGlobalState
. Here we show an example how to use:
Global tread properties
def
getThreadProperties
(
transaction
## ):
threadPropertyDict
=
{}
threadProperties
=
transaction
.
getGlobalState
()
.
getThreadParameters
()
.
getThreadProperties
()
for
key
in
threadProperties
## :
try
## :
threadPropertyDict
[
key
]
=
threadProperties
.
getProperty
(
key
)
except
## :
pass
return
threadPropertyDict
# You can later access the thread properties like this:
threadPropertyDict
=
getThreadProperties
(
transaction
)
incomingRootDir
=
threadPropertyDict
[
u
'incoming-root-dir'
]
Sending Emails from a Drop box

def
post_storage
(
context
## ):
mailClient
=
context
.
getGlobalState
()
.
getMailClient
()
results
=
context
.
getPersistentMap
()
.
get
(
## PERSISTANT_KEY_MAP
)
sendEmail
(
mailClient
,
results
[
0
])
def
process
(
transaction
## ):
transaction
.
getRegistrationContext
()
.
getPersistentMap
()
.
put
(
## PERSISTANT_KEY_MAP
,
[
fcId
])
Java Dropboxes

The above examples show how to implement dropboxes in Python. Python,
however, is not the only language option: it is also possible to write
dropboxes in Java. Whereas Python has the advantage of short turnaround
and less verbose syntax, Java is a good choice in the dropbox employs
complex logic and/or does not need to be modified frequently. A natural
progression is to use Python at the beginning, when creating a new
dropbox, to take advantage of the short turnaround cycle and then move
to Java once the dropbox implementation becomes more stable. Since the
API is the same, this language transition process is quite painless.
### Configuration

As with other dropboxes, a Java dropbox should be deployed as a core-plugin.
plugin.properties
#
# REQUIRED PARAMETERS
#
# The directory to watch for new data sets
incoming-dir = ${root-dir}/incoming-java-dropbox",What to do if an error occurs after the entities have been registered in openBIS,13,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_13,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:14,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JavaTopLevelDataSetHandlerV2 or a subclass thereof
top-level-data-set-handler = ch.systemsx.cisd.etlserver.registrator.api.v2.JavaTopLevelDataSetHandlerV2",The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JavaTopLevelDataSetHandlerV2 or a subclass thereof,14,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_14,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:15,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# The class that implements the dropbox (must implement ch.systemsx.cisd.etlserver.registrator.api.v2.IJavaDataSetRegistrationDropboxV2)
program-class = ch.systemsx.cisd.etlserver.registrator.api.v2.ExampleJavaDataSetRegistrationDropboxV2",The class that implements the dropbox (must implement ch.systemsx.cisd.etlserver.registrator.api.v2.IJavaDataSetRegistrationDropboxV2),15,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_15,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:16,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# The appropriate storage processor
storage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor

#
# OPTIONAL PARAMETERS
#",The appropriate storage processor,16,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_16,reference,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:17,Dropboxes,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html,openbis,"# False if incoming directory is assumed to exist.
# Default - true: Incoming directory will be created on start up if it doesn't exist.
incoming-dir-create = true
The program-class parameter specifies the class that implements the
logic of the dropbox. This class must implement the
IJavaDataSetRegistrationDropboxV2 interface. This class, and any other
code it uses, should be packaged in a jar file that is provided with the
core-plugin. The name of the jar file can be freely chosen.
## Implementation

To implement a dropbox in Java, implement
the IJavaDataSetRegistrationDropboxV2 interface, which codifies the
interaction between the datastore server and the dropbox. We recommend
subclassing AbstractJavaDataSetRegistrationDropboxV2 to bootstrap the
implementation of this interface.
IJavaDataSetRegistrationDropboxV2
/**
* The interface that V2 dropboxes must implement. Defines the process method, which is called to
* handle new data in the dropbox's incoming folder, and various event methods called as the
* registration process progresses.
*
* @author Pawel Glyzewski
*/
public
interface
IJavaDataSetRegistrationDropboxV2
{
/**
* Invoked when new data is found in the incoming folder. Implements the logic of registering
* and modifying entities.
*
* @param transaction The transaction that offers methods for registering and modifying entities
*            and performing operations on the file system.
*/
public
void
process
(
IDataSetRegistrationTransactionV2
transaction
);
/**
* Invoked just before the metadata is registered with the openBIS AS. Gives dropbox
* implementations an opportunity to perform additional operations. If an exception is thrown in
* this method, the transaction is rolledback.
*
* @param context Context of the registration. Offers access to the global state and persistent
*            map.
*/
public
void
preMetadataRegistration
(
DataSetRegistrationContext
context
);
/**
* Invoked if the transaction is rolledback before the metadata is registered with the openBIS
## * AS.
*
* @param context Context of the registration. Offers access to the global state and persistent
*            map.
* @param throwable The throwable that triggered rollback.
*/
public
void
rollbackPreRegistration
(
DataSetRegistrationContext
context
,
## Throwable
throwable
);
/**
* Invoked just after the metadata is registered with the openBIS AS. Gives dropbox
* implementations an opportunity to perform additional operations. If an exception is thrown in
* this method, it is logged but otherwise ignored.
*
* @param context Context of the registration. Offers access to the global state and persistent
*            map.
*/
public
void
postMetadataRegistration
(
DataSetRegistrationContext
context
);
/**
* Invoked after the data has been stored in its final location on the file system and the
* storage has been confirmed with the AS.
*
* @param context Context of the registration. Offers access to the global state and persistent
*            map.
*/
public
void
postStorage
(
DataSetRegistrationContext
context
);
/**
* Is a function defined that can be used to check if a failed registration should be retried?
* Primarily for use implementations of this interface that dispatch to dynamic languages.
*
* @return true shouldRetryProcessing is defined, false otherwise.
*/
public
boolean
isRetryFunctionDefined
();
/**
* Given the problem with registration, should it be retried?
*
* @param context Context of the registration. Offers access to the global state and persistent
*            map.
* @param problem The exception that caused the registration to fail.
* @return true if the registration should be retried.
*/
public
boolean
shouldRetryProcessing
(
DataSetRegistrationContext
context
,
## Exception
problem
)
throws
NotImplementedException
;
}
Sending Emails in a drop box (simple)

from
ch.systemsx.cisd.common.mail
import
EMailAddress
def
process
(
transaction
## ):
replyTo
=
EMailAddress
(
""manuel.kohler@id.ethz.ch""
)
fromAddress
=
replyTo
recipient1
=
EMailAddress
(
""recipient1@ethz.ch""
)
recipient2
=
EMailAddress
(
""recipient2@ethz.ch""
)
transaction
.
getGlobalState
()
.
getMailClient
()
.
sendEmailMessage
(
""This is the subject""
,
\
""This is the body""
,
replyTo
,
fromAddress
,
recipient1
,
recipient2
);
Java Dropbox Example

This is a simple example of a pure-java dropbox that creates a sample
and registers the incoming file as a data set of this sample.
ExampleJavaDataSetRegistrationDropboxV2.java
package
ch.systemsx.cisd.etlserver.registrator.api.v2
;
import
ch.systemsx.cisd.etlserver.registrator.api.v1.IDataSet
;
import
ch.systemsx.cisd.etlserver.registrator.api.v1.ISample
;
import
ch.systemsx.cisd.openbis.dss.generic.shared.api.internal.v1.IExperimentImmutable
;
/**
* An example dropbox implemented in Java.
*
* @author Chandrasekhar Ramakrishnan
*/
public
class
ExampleJavaDataSetRegistrationDropboxV2
extends
AbstractJavaDataSetRegistrationDropboxV2
{
## @Override
public
void
process
(
IDataSetRegistrationTransactionV2
transaction
)
{
## String
sampleId
=
## ""/CISD/JAVA-TEST""
;
ISample
sample
=
transaction
.
createNewSample
(
sampleId
,
## ""DYNAMIC_PLATE""
);
IExperimentImmutable
exp
=
transaction
.
getExperiment
(
## ""/CISD/NEMO/EXP-TEST-1""
);
sample
.
setExperiment
(
exp
);
IDataSet
dataSet
=
transaction
.
createNewDataSet
();
dataSet
.
setSample
(
sample
);
transaction
.
moveFile
(
transaction
.
getIncoming
().
getAbsolutePath
(),
dataSet
);
}
}
Java Code location
The Java file should go into a
lib
folder and should be wrapped as a
jar
. The name does not matter.
While building a jar, the project should have the following
## dependencies:
openBIS-API-dropbox-<version>.jar
,
lib-commonbase-<version>.jar
and
cisd-hotdeploy-13.01.0.jar
## . The
first two are available in the distribution in the archives
openBIS-API-commonbase-<version>.zip
and
openBIS-API-dropbox-<version>.zip
, the third one is available in
the Ivy repo
.
Example path where the created
jar
## should reside:
servers/core-plugins/illumina-ngs/2/dss/drop-boxes/register-cluster-alignment-java/lib
Create a
jar
from your java dropbox file:
jar
cvf
foo.jar
foo.java
Restart the DSS
Calling an Aggregation Service from a drop box

drop box code
'''
## @author:
## Manuel Kohler
'''
from
ch.systemsx.cisd.openbis.dss.generic.server.EncapsulatedOpenBISService
import
createQueryApiServer
def
process
(
transaction
## ):
# use the etl server session token
session_token
=
transaction
.
getOpenBisServiceSessionToken
()
# To find out do SQL on the openBIS DB: select code from data_stores;
dss
=
## ""STANDARD""
# folder name under the reporting_plugins
service_key
=
""reporting_experimental""
# some parameters which are handed over
d
=
{
""param1""
## :
""hello""
,
""param2""
## :
""from a drop box""
}
# connection to the openbis server returns IQueryApiServer
s
=
createQueryApiServer
(
""http://127.0.0.1:8888/openbis/openbis/""
,
""600""
)
# Actual call
# Parameters: String sessionToken, String dataStoreCode,String serviceKey, Map<String, Object> parameters)
s
.
createReportFromAggregationService
(
session_token
,
dss
,
service_key
,
d
)
Known limitations

## Blocking

Registering/updating a large number of entities can cause other
concurrent operations that try to modify the same or related entities to
be blocked. This limitation applies to both dropboxes and batch
operations triggered from the web UI. Lists of operations that are
blocked are presented below. Each list contains operations that cannot
be performed when a specific kind of entity is being registered/updated.
## Experiment:
creating/updating an experiment in the same project
updating the same space
updating the same project
updating the same experiment
## Sample:
creating/updating an experiment in the same project
creating/updating a sample in the same experiment
updating the same space
updating the same project
updating the same experiment
updating the same sample
Data set:
creating/updating an experiment in the same project
creating/updating a sample in the same experiment
creating a dataset in the same experiment
updating the same space
updating the same project
updating the same experiment
updating the same sample
## Material:
updating the same material",False if incoming directory is assumed to exist.,17,en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes_17,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes.txt,2025-09-30T12:09:02.425703Z,2
docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_index:0,Server-Side Extensions,https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/index.html,openbis,"## Server-Side Extensions

## Core Plugins
## Motivation
## Core Plugins Folder Structure
### Merging Configuration Data
Enabling Modules and Disabling Plugins
## Enabling Modules
Disabling Core Plugins by Property
Disabling Core Plugins by Marker File
## Core Plugin Dependency
Rules for Plugin Writers
Using Java libraries in Core Plugins
## Custom Application Server Services
## Introduction
How to write a custom AS service core plugin
How to use a custom AS service
API Listener Core Plugin (V3 API)
## Introduction
## Core Plugin
Plugin.properties
lib
## Example - Logging
## Example - Loggin Sources
## Dropboxes
## Jython Dropboxes
## Introduction
## Simple Example
## More Realistic Example
## Model
## Details
### Dropbox Configuration
Development mode
Jython version
Jython API
IDataSetRegistrationTransaction
TransDatabase queries
## Events / Registration Process Hooks
## Events Table
### Typical Usage Table
## Example Scripts
Delete, Move, or Leave Alone on Error
## Summary
## Example
## Search
## API
## Experiment
Sample and Data Set
## Authorization Service
## API
## Example
## Combined Example
Error Handling
Automatic Retry (auto recovery)
## Manual Recovery
### Classpath / Configuration
Validation scripts
## Global Thread Parameters
Sending Emails from a Drop box
Java Dropboxes
### Configuration
## Implementation
Sending Emails in a drop box (simple)
Java Dropbox Example
Calling an Aggregation Service from a drop box
Known limitations
## Blocking",Server-Side Extensions,0,en_20.10.0-11_software-developer-documentation_server-side-extensions_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_software-developer-documentation_server-side-extensions_index.txt,2025-09-30T12:09:03.667848Z,2
docs:openbis:en_20.10.0-11_system-documentation_changelog_CHANGELOG:0,OpenBIS Change Log,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/changelog/CHANGELOG.html,openbis,"OpenBIS Change Log

## Version 20.10.11 (02 Dec 2024)

## Core

Bugfix: Fix Property Types DB, remove vocabulary type from converted Varchar types (BIS-1069)
Bugfix: SFTP now reports the right timestamps (BIS-1432)
Bugfix: Name collision of testing PNG with the production one, image export does not work for uncompressed data (BIS-1439)
Bugfix: Search fails if integer dynamic property is evaluated and used for sorting (BIS-1448)
Bugfix: SearchFiles ignores fetch options (BIS-1487)
Bugfix: Number of DSS Java threads gradually increasing to critically high values (BIS-1640)
## ELN

Improvement: Move addition of new widget for properties in ELN Settings to top of list (BIS-918)
Improvement: Login with SSO/SwitchAAI was not showing the loading spinning clock (BIS-1488)
Improvement: “+New” dropdown list in forms replaced, this improves usability in small screens/tables (BIS-1512)
Bugfix: Now experiments from Experiments/Collections table in Project page can be deleted (BIS-983)
Bugfix: Uniqueness of QRcodes was only checked against objects a user has access to, now to all (BIS-1038)
Bugfix: URL display in text fields changed in 20.10.9 (BIS-1461)
Bugfix: Error after login in 20.10.9 reported by external users (BIS-1485)
Bugfix: Failing to download files with comma in name (BIS-1515)
Bugfix: XLSX Import failure when using “Ignore if Exists” on certain scenarios (BIS-1528)
Bugfix: XLSX Import failure if images are embedded in text fields (BIS-1534)
Bugfix: XLSX Import failure if code fields are not uppercase, now they uppercase automatically (BIS-1545)
Bugfix: XLSX Import failure of ELN Standard Technologies master data (BIS-1582)
Bugfix: Barcode/QR scanner view broken in 20.10.9.1 (BIS-1533)
Bugfix: Samples without experiments get blank edit form (BIS-1537)
Bugfix: ELN lims dropbox ignores plugin.properties config (BIS-1593)
Bugfix: Advanced search date widget not configured with openBIS date and timestamp format (BIS-1616)
Bugfix: Search for products does not work when creating a Template for a Request (BIS-1617)
## Version 20.10.10 (10 Oct 2024)

## Core

New Feature: Forward all properties from plugins we ship to the as and dss service.properties (BIS-1399)
New Feature: Forward all properties from plugins we ship to OS environment variables (BIS-1587)- New Feature: Make SFTP Certificate configurable on the dss service.properties (BIS-1401)
New Feature: Make SFTP Certificate configurable on the dss service.properties (BIS-1401)
Improvement: Remove rotateLogFiles from datastore_server.sh and datamover.sh (BIS-1396)
## ELN

New Feature: Move InstanceProfile.js parameters to the ELN Settings UI (BIS-1400)
## Version 20.10.9.1 (16 Aug 2024)

## ELN

Bugfix: ELN UI - Blanc Sample fields (BIS-1460)
## Version 20.10.9 (31 Jul 2024)

## Core

Improvement: V3 API - Allow to use existing sessionToken with Java and JS facade (BIS-1336)
Improvement: PDF Export - Formulas within spreadsheets are not converted to calculated values (BIS-1076)
Improvement: PDF Export - Added section names in exported pdfs (BIS-1335)
Improvement: Excel Import - Import the exported image data (BIS-1050)
Improvement: Excel Import - Masterdata import, existing plugins support (BIS-1096)
Improvement: Excel Import - Large imports support (BIS-1059)
Improvement: Excel Export - “pdf” folder from exports renamed to “hierarchy” (BIS-1338)
Improvement: Excel Export - Empty Experiment folders from export removed (BIS-1337)
Improvement: Excel Export - Use identifiers instead of PermID in object properties for exported pdf (BIS-1055)
Bugfix: PDF Export - Failed PDF export when there are several colored spans (BIS-1371)
Bugfix: Excel Import - Exception during variable substitution (BIS-1058, BIS-1373)
Bugfix: Excel Import - Issue with import of exported object with data in spreadsheet (BIS-1049)
Bugfix: Excel Export - Object Properties References missing on xlsx (BIS-1051)
## ELN

Improvement: ELN UI - Spreadsheet Improvements (BIS-1025)
Improvement: ELN UI - Object/Experiment Links in word processor (BIS-1029)
Improvement: ELN UI - ELN UI: GFB - Notes taking Widget for Experiments (BIS-1003)
Improvement: ELN UI - GFB - Object/Experiment Links in spreadsheet (BIS-1004)
Improvement: ELN UI - Customise ELN Welcome Page (BIS-1087)
Improvement: ELN UI - Change behavior of eln-lims-dropbox script with hidden files (BIS-1093)
Bugfix: ELN UI - Blanc Sample Form (BIS-1369)
Bugfix: ELN UI - ELN-LIMS Dropbox dataset with metadata.json containing UTF-8 Characters fails (BIS-1052)
### Bugfix: ELN UI - Large Configurations Support ~ large vocabularies fail (BIS-1389)
Bugfix: ELN UI - ELN-LIMS URL Encoding Issues (BIS-1057)
Bugfix: ELN UI - ELN-LIMS loading without DSS (BIS-1028)
## Admin

Improvement: Admin UI - Show identifier (name) for object property in Tables (BIS-1261)
Improvement: Admin UI - Show user account in admin UI (BIS-776)
## Version 20.10.8 (29 May 2024)

## Core

New Feature: Additional configuration parameters for MultiDataSetArchiveSanityCheckMaintenanceTask (SSDM-14098)
New Feature: Registration of shared/space objects can be prevented at the system level (SSDM-14067)
New Feature: Extend User and Email field length in PersonPE and Database (BIS-799)
New Feature: Timezone of timestamp changed to database timezone (SSDM-14228)
New Feature: EXCEL Master Data Importer API (preview, non final) (BIS-772, BIS-773, BIS-994, BIS-999, BIS-1010, BIS-1011, BIS-1025, BIS-1040)
New Feature: EXCEL Master Data Exporter API (preview, to be changed) (BIS-772, BIS-773, BIS-994, BIS-999, BIS-1010, BIS-1011, BIS-1025, BIS-1040)
Improvement: Excel Export - Add support for cells bigger than 32k (BIS-789, BIS-790)
Improvement: Excel Export - internal name space types are skipped from updates done by non system users (BIS-793)
Bugfix: Excel Import - Keep Dynamic Properties Dynamic (SSDM-14224)
Bugfix: UserManagementMaintenanceTask: role assignment error (SSDM-14261)
Bugfix: Deadlock on display settings (SSDM-14263)
Bugfix: DataSetAndPathInfoDBConsistencyCheckTask causing endless repetitions of tryGetDataSetLocation, causing AS logs to dramatically blow up (SSDM-14237)
Bugfix: DSS Becomes Zombie when Dropbox folder is unreachable (SSDM-14074)
Bugfix: MultiDataSetArchiveSanityCheckMaintenanceTask fails for h5ar files (SSDM-14124)
Bugfix: MultiDataSetArchiver sanity check skips checksum verification if pathinfo db entries are missing (SSDM-14125)
Bugfix: Enabling extra logging switches causes problems on AS start up (SSDM-14207)
Bugfix: FastDownloadServlet and V3.searchFiles do not release data set locks (SSDM-14203)
Bugfix: DSS threads do not release data set locks before dying (SSDM-14204)
## ELN

Bugfix: Problem with selecting checkboxes at Object Browser > Register Objects popup in Firefox (SSDM-14195)
Bugfix: ELN: not clickable checkboxes in multi-select dropdowns in popups (SSDM-14226)
Bugfix: Typo in create inventory space form (SSDM-14133)
Bugfix: Fix Toolbar Plugin Not Loading (BIS-991)
Bugfix: Login screen disabled on small width screens / Android (BIS-984)
Bugfix: Remove Life Sciences and Basic ELN Types from Installer, Link on Community Github (BIS-800)
## Admin

Improvement: Improve the error message shown when a user with an already existing userid is attempted to be created (SSDM-14194)
## Version 20.10.7.3 (23 November 2023)

## ELN

Bugfix: Circular Annotations deletion (SSDM-14135)
## Admin

Improvement: XLS Master Data Importer: Make Version Optional, vocabularies bug fix (SSDM-14129)
## Version 20.10.7.2 (13 October 2023)

## ELN

Improvement: Make object property a link (SSDM-13901)
Improvement: Support spaces for identifiers separation in the PASTE ANY search (SSDM-13829)
Improvement: Support PermIDs in the PASTE ANY Parent/Children Search (SSDM-13830)
Improvement: Rename “Scan barcode” to “Scan QR codes/ barcodes” everywhere in ELN UI (SSDM-13842)
Bugfix: Collection /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION not created in new multi-group instances (SSDM-14068)
Bugfix: XLS Imports and ELN UI don’t take URLs with certain special characters (SSDM-13905)
Bugfix: Issues with calculations in ELN spreadsheets (SSDM-13736)
Bugfix: Multi-position box deletion bug (SSDM-13834)
Bugfix: General Settings of the ELN overwrite the general settings of the Group in the multiple group instance (SSDM-13934)
Bugfix: Remove system pop up from the back button (support issues on mobile/tablets) (SSDM-13894)
Bugfix: Sample form can’t update Objects when project-samples is disabled (SSDM-13962)
Bugfix: Explicit “false” is not saved for “boolean” properties (SSDM-14093)
## Admin

Improvement: XLS Master Data Importer: Make Version Optional (SSDM-13961)
## Core

New Feature: V3 API : Import (BIS-771)
New Feature: V3 API : Provide bundles with all V3 API JS files (BIS-761)
Bugfix: Usage Reporting Task : Task fails with NPE if samples without space were created (SSDM-14065)
Bugfix: User Management Task : Task removes roles that could have been configured on the json config (SSDM-13940)
Bugfix: User Management Task : Task should not create exiting user space if a user is de-activated and re-activated (SSDM-13421)
Bugfix: Properties : Fix properties that contain both a value and a link to a controlled vocabulary term (SSDM-13843)
Bugfix: PAT : Remove hash from validity PAT warning emails (SSDM-14099)
## Version 20.10.7.1 (25 July 2023)

## ELN

Improvement: ELN Dropbox provides now a report with all error messages when registration fails. (SSDM-13794)
Improvement: Navigation further avoids to show empty folders in some situations. (SSDM-13827)
Bugfix: Back button behaviour was in some situations incorrect. Example: pressing back after navigating to an experiment from an identifier in a table. (SSDM-13811)
Bugfix: Zenodo Export form did break in two situations, now fixed. (SSDM-13813, SSDM-13824)
Bugfix: Dataset Viewer was not showing the list of files on the DataSet form, only on the Collection/Sample forms, is now fixed. (SSDM-13827)
## Admin

Improvement: Vocabulary term template now provides an explanation on how to use it. (SSDM-13817)
## Version 20.10.7 (5 July 2023)

## Core

Improvement: Improved SFTP Folder listing performance (SSDM-13489)
Improvement: Improved SFTP download performance (SSDM-13490)
Improvement: V3 autogenerated code behaviour can be overridden by providing a code as it works on V1 (SSDM-12646)
Bugfix: V3 rights for creation/update are now consistent with what is intended. Before in some cases POWER USER was necessary for things USER should be enough (SSDM-13718)
Bugfix: V3 Removal of property type that already has some values now works correctly (SSDM-13784)
Bugfix: User Management maintenance task now reuses the same space for a user if it gets deactivated/activated again (SSDM-13421)
Bugfix: User Management maintenance task now assign rights correctly when moving a user from one group to another instead of rights getting lost (SSDM-13716)
Bugfix: DataSetArchiverOrphanFinderTask fix erroneous reporting of missing tar files when using archivers on multi-group instances with sub-folders (SSDM-13725)
## ELN

Removal: Plasmapper 2.0 integration since the external service was been decommissioned (SSDM-13664)
New Feature: New Barcode / QR Code widget supporting scanner and camera in all places where barcodes could be used on the UI (SSDM-12100)
New Feature: Mobile Support, navigation component can be collapsed . (SSDM-12100)
New Feature: Dataset table in Object and Collection Forms (SSDM-13683)
New Feature: NOT operator on Advance Search (SSDM-13427)
Improvement: XLS Templates no longer contain names of types, to avoid long verbose names (SSDM-12531)
Improvement: Number is now formatted with separators following the US locale (SSDM-13640)
Improvement: Project View now separates the overview  from the list of collections/experiments (SSDM-13643)
Improvement: Navigation menu refreshes when moving objects (SSDM-13720)
Improvement: Navigation menu refreshes when copying objects (SSDM-13786)
Improvement: Navigation menu nodes are not cached, this helps use cases when nodes are updated out of user control (SSDM-13785)
Bugfix: Problem deleting spaces(SSDM-13676)
Bugfix: Some label rendering glitches fixed (SSDM-13692)
Bugfix: Label incorrectly named renamed from Name to Code on Objected move menu (SSDM-13728)
Bugfix: Some glitches with repeated columns tables fixed (SSDM-13712)
Bugfix: XLS Dataset exports (SSDM-13693)
Bugfix: Windows Postgres version detection (SSDM-13709)
Admin UI

Improvement: XLS Imports now skips DB sanity check used by large migration greatly lowering import times (SSDM-13788)
Bugfix: Now unofficial terms stay unofficial if the official checkbox is not checked. (SSDM-13730)
ELN/Admin UI

Improvement: High resolution logos and icons (SSDM-13504)
Improvement: Navigation, removal of empty nodes (SSDM-13671)
## Version 20.10.6 (26 April 2023)

## Core

New Feature: OpenBIS class to interact with AS and DSS with methods to handle uploads and semantic annotations. (SSDM-13017)
New Feature: Dataset Creation from V3 API: Java, javascript and python3 facades support. (SSDM-13253) right arrow openBISV3API-RegisterDataSets
New Feature: V3 API search criteria methods withSampleProperty, withVocabularyProperty added (SSDM-12986)
New Feature: Maintenance Task that un-archives all data from a certain point in time and verifies checksums (SSDM-12971) right arrow Maintenance Tasks - MultiDataSetArchiveSanityCheckMaintenanceTask
New Feature: Maintenance Task : Delete temporary stage directories from failed registrations from Dropboxes and sharding directories (SSDM-11605) right arrow Enabled by default, no action required
Improvement: Performance improvements when adding children and creating new entities on the persistence layer. As used by XLS Imports. (SSDM-13033, SSDM-13195, SSDM-13207)
Improvement: Performance improvements when using general search from the V1 API as used by the Core UI (SSDM-13449)
Improvement: Make dropbox recovery marker directory configurable (SSDM-12056)
Improvement: Make dropbox .faulty-paths file creation more consistent (SSDM-13501)
Improvement: User management task make instance admins configurable (SSDM-13080)
Bugfix: ArchivingByRequestTask : NPE when a data set does not have an experiment (SSDM-13172)
Bugfix: MultiDataSetArchiver : make unarchiving more robust with StrongLink (SSDM-13084)
Bugfix: DSS registration on AS -  Wait for safe registration (SSDM-13327)
Bugfix: PAT support for single sign on setups (SSDM-13362)
Admin UI / ELN

New Feature: New navigation component. (SSDM-12451, SSDM-11608, SSDM-12480, SSDM-13274, SSDM-13118, SSDM-12479, SSDM-12098)right arrow User Documentation - Navigation Menu
New Feature: Table XLS Exports of metadata and master data. (SSDM-13206, SSDM-13163, SSDM-13256, SSDM-13450, SSDM-13463, SSDM-13414, SSDM-13420) right arrow User Documentation - Tables
New Feature: XLS Imports, zip support (SSDM-13422)
Bugfix: XLS Imports, Allow “”-”” in codes.  Allow updating properties if the file contains a $property. All deletions to require delete tag and other bug fixes  (SSDM-13320, SSDM-13343, SSDM-13447, SSDM-13446)
Admin UI

New Feature: Info view. (SSDM-13133)
## ELN

New Feature: Dropbox monitor (SSDM-13135)right arrow User Documentation - Dropbox Monitor
New Feature: Barcode Scanner using camera from the main menu right arrow User Documentation - Barcodes
New Feature: New Plugin Interface methods and improve the interface documentation (SSDM-13203, SSDM-13265) right arrow ELN-LIMS WEB UI Extensions
Improvement: Removal of REQUEST.ORDER_NUMBER (SSDM-13199)
Improvement: XLS Menu items renames (SSDM-13202)
Improvement: Better error messages and email notifications on the ELN-LIMS dropbox. (SSDM-11306)
Improvement: Publication API Support for Multi Group setups (Zenodo and research collection exports) (SSDM-11744)
Improvement: Performance improvement when building a dropdown with thousands of items on Chrome browsers (SSDM-13549)
Improvement: ELN Dropbox helper tool updated (SSDM-13360)
Improvement: Use the Vocabulary Term URL on the VIEW mode on forms (SSDM-13436)
Improvement: +new button in ELN is only be added if object type is specified (SSDM-13268)
Improvement: Show permId and Identifiers more prominently (SSDM-13197)
Improvement: remove +Add button when adding parents in ELN (SSDM-13169)
Improvement: Inconsistency in Collection forms between Inventory and Lab notebook (SSDM-12991)
Improvement: Add column with default barcode in Collection tables (SSDM-12889)
Improvement: Search by size for datasets (SSDM-12472)
Improvement: Changes on General Settings in multi-group instances (SSDM-13262)
Improvement: Match view of folders on the SFTP to the ELN Navigation menu (SSDM-13261, SSDM-12929)
Improvement: Use default ELN-LIMS settings on new instances instead of forcing manual configuration (SSDM-12892)
Improvement: Preserve Plugin configurations (SSDM-13413)
Bugfix: Date filter on tables (SSDM-13315, SSDM-13156)
Bugfix: Boolean value to be now treated as Try-Valued, True, False or Null (SSDM-13481, SSDM-13085)
Bugfix: after saving the ELN Settings, the page stays in edit mode, not in view mode (SSDM-13196)
Bugfix: UnicodeEncodeError in OPERATION.generalExports.py (SSDM-13367)
Bugfix: file-authentication: having special characters in name prevents editing in User profile (SSDM-13171)
## Version 20.10.5 (29 November 2022)

## Core

(SSDM-11550) New Feature : Personal Access Tokens (PAT)
(SSDM-12514) New Feature : API Event listener for integrations
(SSDM-12625) New Feature : Active Users Email triggered from Admin UI
(SSDM-13173) New Feature: Workspace API Extension
(SSDM-12900) Bugfix : Sessions not properly closed
(SSDM-12496) Bugfix  : XLS Import Issue with Project property field in XLS import conflict with openBIS Project
(SSDM-12933) Bugfix  : XLS Import problem with upgrades on some instances - error about object types that already exist
(SSDM-12988) Improvement : XLS Import Improved Error Message - for object references
(SSDM-13031) Improvement  : XLS Import Improved Error Message - Missing header
(SSDM-12996) Improvement : XLS Import Admin UI Integration
(SSDM-13015) New Feature  : XLS Import Semantic Annotations
(SSDM-11744) Improvement : Publications API to support multi-group instance (RC and Zenodo)
(SSDM-12286, SSDM-13163) New Feature : Exports for Master data and Metadata - Core
(SSDM-12905) Bugfix : MultiDataSetArchiver problems with LTS
(SSDM-13117) Bugfix :  MultiDataSetArchiver wrongly detected inconsistency between pathinfo DB and filesystem for H5 files
(SSDM-13084) MultiDataSetArchiver : retry mechanism for unarchiving
(SSDM-13052) MultiDataSetArchiving : resource does not exist error
(SSDM-13018) MultiDataSetArchiver : retry sanity check on any exception
(SSDM-12980) Bugfix: UserManagementTask - Role Assignments - Corner Case Null Pointer
(SSDM-12976) Improvements: Additional archiver options for unreliable File API backends
(SSDM-12855) Improvement: Avoid TS Vector too big when migrating to 20.10 + Format improvement
(SSDM-12936)  Improvement: Disable converting types on XLS Master Data
## Jupyter Integration:

(SSDM-12909) Bugfix : Session token not automatically saved in Jupyter notebook
Admin UI

(SSDM-11675) Improvement : Get rid of Redux and Redux-Saga
(SSDM-12451) New Feature : New database navigation component in Admin UI (tech preview)
## ELN-LIMS:

(SSDM-11539) New Feature: Space management in ELN or new admin UI
(SSDM-11968) Bugfix : Update CKEditor to fix underscore problem
(SSDM-12220) Bugfix : Show description of boolean fields
(SSDM-12327) Bugfix : keep parents and children in object templates
(SSDM-12740) Bugfix : Cannot unselect an object type in Default Object type field in Collection
(SSDM-12938) Bugfix : Search in property of type “Object” is not restricted to the specified object type
(SSDM-12946) Bugfix : Dataset metadata not cleared on type change
(SSDM-12981) Bugfix : Storage Widget Multi position delete corner case
(SSDM-12997) Bugfix : DataSet container widget is not cleared when changing the sample
(SSDM-12987) Bugfix : When I flag “Delete also all descendant objects” openbis goes in loop
(SSDM-12582) Improvement : ELN Inconsistencies
(SSDM-12584) Improvement : Add missing attributes to dataset search, archiving, unarchiving
(SSDM-12634) Improvement : Support more Barcodes scanners
(SSDM-12654) Improvement : Change dropdown for deletion of dependencies in trashcan
(SSDM-12729) Improvement : Make property “order status” mandatory in requests and orders
(SSDM-12730) Improvement : Show Description in Project form by default
(SSDM-12766) Improvement : Export should have names instead of codes
(SSDM-12809) Improvement : Show warning when leaving an object without saving after adding parents/children
(SSDM-12930) Improvement : Update Settings texts
(SSDM-12937) Improvement : Disallow config changes to system spaces,Resolved
(SSDM-12990) Improvement : ELN Storage : if a storage is deleted, the positions assigned to it are not and it is not possible to delete them afterwards
(SSDM-13105) Improvement : Superscript and subscript text in CKE editor
(SSDM-13159) Improvement : Load improvements for Parents Table and Storage Views
(SSDM-13152) New Feature : Exports for Master data and Metadata - UI
## Version 20.10.4 (3 August 2022)

## Core

(SSDM-9831)  New Feature: Sample FK Properties : Materials Migration
(SSDM-10984) New Feature: Excel masterdata spreadsheet rewrite in Java with parser giving error messages by line
(SSDM-12561) Improvement: Search Engine: Improve search using existing joins
(SSDM-12527) Improvement: Search Engine: Remove joins and enforce early filtering with subqueries for PropertySearchCriteria
(SSDM-12661) Improvement: Refactor AbstractMaintenanceTask to AbstractGroupMaintenanceTask
(SSDM-12554) Improvement: users removed from user management config file are not always disabled
(SSDM-12574) Improvement: Source of root certificates and checks for certificate chains used in openBIS AS+DSS cert stores
(SSDM-12656) Improvement: Archiver: Test consistency between data store and pathinfo database BEFORE writing tarball
(SSDM-12500) Improvement: Archiving: Calculate data set size if not found in database
(SSDM-12464) Improvement: Multi Data Set Archiving: Check after successful finalization multi_dataset_archive database
(SSDM-12565) Improvement: Maintenance task to delete unused datasets on scratch share.
(SSDM-12707) Bugfix: NPE in DataSetArchiverOrphanFinderTask
(SSDM-12393) Bugfix: DSS startup check for AS MaintenanceTasks
(SSDM-12703) Bugfix: SFTP shows non-existing files as empty files/folders
(SSDM-12655) Bugfix: Search complete openBIS repo for places where we open a v3 api session internally and close them
(SSDM-12782) Bugfix: Fix Vocabulary from Property Type Conversion
## ELN

(SSDM-12621) New Feature : Creation of spaces
(SSDM-12622) New Feature : New Processing Plugin Tool View
(SSDM-12623) New Feature : Custom Imports -> New Upload to Dropbox Tool View
(SSDM-12551) New Feature : Add filter for parents and children in ELN tables
(SSDM-12815) New Feature : Barcode Characters Configurable
(SSDM-12650) New Feature : Virtual FTP to follow current ELN-LIMS settings to categorise spaces and types visibility
(SSDM-12520) Improvement : Experiment type to Experiment table in project overview and remove selection of Experiment type in project overview
(SSDM-12765) Bugfix: Freezing failing with SwitchAAI IDs
(SSDM-12518) Bugfix: Issue with boolean values in tables
(SSDM-12694) Bugfix: Storage deletion in ELN
(SSDM-12735) Bugfix: Navigation Tree doesn’t show data sets for the children of a sample
(SSDM-12771) Bugfix: cannot create request with new product added to request on multi group instances
(SSDM-12767) Bugfix: On first creation the JS settings from plugins where not being respected
(SSDM-12651) Bugfix: Batch upload of storage positions avoids repeating box names
(SSDM-12732) Bugfix: SFTP shows non-existing files as empty files/folders
## Version 20.10.3.1 (13 June 2022)

## Core

(SSDM-12045) Improvement : UserManagementMaintenanceTask - Improved template
(SSDM-12485) Improvement : UserManagementMaintenanceTask - Create empty mapping file
(SSDM-12081) Bugfix : freezing affects trashcan
(SSDM-12530) Bugfix : poor performance of events search maintenance task - memory leak
(SSDM-12556) Bugfix : poor performance of events search maintenance task - fetching too many events
## ELN

(SSDM-11623) Improvement : Multi Group Support - Group configuration is only applied to its spaces
(SSDM-12370) Improvement : Truncate long lists of parents/children displayed in tables
(SSDM-12468) Improvement : Improved performance for tables
(SSDM-12519) Improvement : add property “$show_in_project_overview” to ENTRY object type
(SSDM-12309) Bugfix : object type Chemical shows storage widget even if this is disabled in the Settings
(SSDM-12400) Bugfix : missing scroll down in new XLS batch upload template
(SSDM-12412) Bugfix : when a new type is created in a multi-group instance, show in main menu is automatically enabled in all settings
(SSDM-12417) Bugfix : error without text given if I do not add a Label for parents in ELN Settings
(SSDM-12459) Bugfix : CKEditor missing some features and displaying wrong layout
(SSDM-12477) Bugfix : description is not shown in MULTILINEVARCHAR fields
(SSDM-12522) Bugfix : selection of experiments to show in project overview does not work properly
(SSDM-12526) Bugfix : delete message for object type ENTRY shows html tags
## Version 20.10.3 (7 March 2022)

## Core

(SSDM-11728) Bugfix : Dynamic Properties evaluation fails if sample components are accessed
(SSDM-12059) Bugfix : SFTP : datasets connected only to samples are not shown
(SSDM-12033) Bugfix : SFTP: delay for a user to access the recently added group
(SSDM-11556) Improvement : export-master-data.py should export fields descriptions
(SSDM-12293) Improvement : UserManagementMaintenanceTask: Allow to assign roles to all user spaces
(SSDM-12051) Improvement : Extend DeletedObject by identifier and entity type
(SSDM-11784) Improvement : Upgrade apache sshd library (for our sftp service needed)
(SSDM-11953) Improvement : Update R version on JupyterHub image to 4.1x
(SSDM-11978) Improvement : Upgrade Jackson to 2.9.10.8
(SSDM-12031) Improvement : Upgrade to latest jetty 9.4 version
(SSDM-11579) New Feature : AS Maintenance Task - Lib folder not being loaded
(SSDM-11354) New Feature : Query Engine : Caching Implementation
(SSDM-11954) New Feature : Maintenance task which removed deleted data sets from the archive
(SSDM-12110) Remove CIFEX from openBIS
## ELN

(SSDM-9305) Bugfix : Hints for children set in Settings are not shown when editing objects
(SSDM-12184) Bugfix : Plain Text Widget and Monospace font saves HTML tags
(SSDM-10064) Bugfix : Typo correction in Request form
(SSDM-11256) Bugfix : Error when two storages with same code are created in 2 different ELN_SETTINGS
(SSDM-11339) Bugfix : Move folders of disabled users to “Others disabled” in multi-group instances
(SSDM-11819) Bugfix : REAL data type : Missing comparator for advanced search and wrong sorting for properties
(SSDM-11977) Bugfix : Using bold text in ENTRY title breaks alphabetical sorting in main menu
(SSDM-11980) Bugfix : Data Set Upload form does not work after a failed login attempt (ID#18459457)
(SSDM-12055) Bugfix : Parents and children sections are not always displayed in the same place in object forms
(SSDM-12072) Bugfix : Rescaling images embedded in text fields in Collection table overview
(SSDM-12227) Bugfix : Move objects does not always work correctly
(SSDM-12229) Bugfix : ELN Filters all text received from server instead only needed causing Glitch with plain txt
(SSDM-12310) Bugfix : Rich Text: Images with non-ASCII characters in the file name are not shown
(SSDM-12323) Bugfix : Sample deletion confirmation popup unexpected HTML tags - (SSDM-10066) Improvement : Delete Experiment/Project should delete Object/Datasets/Experiment on the Experiment/Project
(SSDM-12049) Bugfix : Export metadata and data is not working
(SSDM-12073) Bugfix : Could not add new user
(SSDM-12361) Bugfix : Object templates cannot be created in group_settings of multi-group instance
(SSDM-12057) Bugfix : User name stored in a session token has different letter case than the actual user name (ID#18459293)
(SSDM-10474) Improvement : Collection Forms Views Configured by Individual Collection
(SSDM-12103) Improvement : Change “deleted” message to “moved to trashcan” for entries that are moved to the trashcan
(SSDM-11098) Improvement : Keep order of properties for types as specified in admin UI
(SSDM-11533) Improvement : Show parents/children name in addition to identifiers in ELN tables
(SSDM-12194) Improvement : Barcodes: Making check on minimum length more robust and validate custom barcodes
(SSDM-12146) Improvement : Modify ELN masterdata plugin to avoid creation of default spaces in multigroup instances
(SSDM-12218) Improvement : Tables Improvements for Rich Text Content
(SSDM-8701) New Feature : XLS Import
(SSDM-12312) New Feature : Allow extra tool actions
(SSDM-10071) New Feature : Entity history
(SSDM-9646) New Feature : Add select all/deselect all to all tables in ELN
(SSDM-10541) New Feature : Add move all to ELN tables
(SSDM-11664) Table Widget : Common table widget for ELN and NG UI
(SSDM-11951) Table Widget : Sorting by multiple columns
(SSDM-12023) Table Widget : Filtering by date ranges
(SSDM-12025) Table Widget : Dropdown filter for Boolean properties
(SSDM-12149) Table Widget : Show by default more columns in ELN tables
(SSDM-12250) Table Widget : Sticky first column
pyBIS

(SSDM-11738) : get_samples() with children
New Admin UI

(SSDM-12150) : New Feature : XLS Import
(SSDM-11169) : New Feature : Property types overview
(SSDM-11727) : Remove the concept of local property types
## Version 20.10.2.3 (15 November 2021)

## ELN

Fix security vulnerability.
## Version 20.10.2.2 (30 November 2021)

## Core

(SSDM-11586) Bugfix: Pybis - uses session from last login when used in JupyterHub
(SSDM-11792) Bugfix: Pybis - remove the usage of environment variables in Jupyter Authenticator and Pybis
(SSDM-11879) Bugfix: Can’t edit samples in Core UI when project samples disabled
(SSDM-11462) Bugfix: V3 API - Nested fetched sort options don’t work as expected
(SSDM-11917) Bugfix: V3 API - Don’t break when assigning project to sample on project samples disabled.
(SSDM-11859) Bugfix: V1 API - reverting a deletion via coreUI or ELN is very inefficient
(SSDM-11863) Bugfix: Python Master data export doesn’t escape special characters on description
(SSDM-11948) Bugfix: Multi Data Set Unarchiving making more robust in case of deleted data sets
(SSDM-11964) Bugfix: user management task constantly recreates user space
(SSDM-11602) V3 API - getRights: Adding DELETE and updating EDIT
(SSDM-11884) Permanent deletion should show dependent deletion sets
(SSDM-11885) Improve postregistration in case of error
## ELN

(SSDM-10078) Bugfix: Non deletable datasets can’t be moved to trashcan
(SSDM-10301) Bugfix: 2nd level of Parents/Children now shows on Parents/Children table
(SSDM-11425) Bugfix: Enter dates manually not always work
(SSDM-11622) Bugfix: Multi Group Storage Support: Storage and Templates are now created for the selected group settings instead randomly
(SSDM-11876) Bugfix: Multi Group Ordering Support: create request in a multigroup instance fails because the wrong space is used
(SSDM-11734) Bugfix: Advanced Search - Selecting Type OR BUG
(SSDM-11786) Bugfix: installer fail when folder exists but is empty
(SSDM-11949) Bugfix: Archiving helper
(SSDM-11972) Bugfix: ELN Data Set View doesn’t show metadata and files of Data Sets of type UNKNOWN
(SSDM-11844) Multi Group Storage Support: Editing Users/Groups from the ELN/LIMS
(SSDM-11670) Side Menu Links Plugin Template
(SSDM-9867) Advance search shows dropdown for well known values.
(SSDM-10681) Rename “use as template” to “copy to Experiment” for protocols addition
(SSDM-11455) Storage Tool that shows storage left for all users
(SSDM-11557) Remove UNKOWN type from More dropdown in ELN Project
(SSDM-11958) Show UNKNOWN dataset type on navigation menus by default
(SSDM-11733) eln-lims dropbox metadata registration support
(SSDM-11732) eln-lims dropbox metadata.json template export in ELN UI
(SSDM-11855) Multi Group Storage Support: When creating storage positions only available groups show, when updating only the ones belonging to the same group as position
## Version 20.10.2.1 (6 October 2021)

## Core

(SSDM-11740) Fix SFTP to use session token
## ELN

(SSDM-11799) Can not create copy of an object with children
Version 20.10.2 GA (General Availability) (22 September 2021)

## Core

(SSDM-10942) V3 API search : Improve partial match search
(SSDM-10941) V3 API search : Searching for several words does not scale efficiently
(SSDM-10971) MaintenanceTaskPlugin allows to run MaintenanceTask at specified days/times
(SSDM-10831) Entity Deletion History - maintenance task
(SSDM-11000) V3 API search : Implement prefix matching efficient search
(SSDM-11080) Bugfix : Relationships history : Sample-project relations are not stored/returned
(SSDM-11124) Bugfix : Issue with deletion of MICROSCOPY_EXPERIMENT and objects
(SSDM-11227) Bugfix : openbis_statistics_server build not working out of the box
(SSDM-11165) Cloning a dropbox
(SSDM-10832) V3 API : Entity Deletion History
(SSDM-11166) Bad performance of MicroscopyThumbnailCreationTask
(SSDM-11252) openBIS capabilities config for adding a parent for which the user has only observer rights
(SSDM-10988) Bugfix : V3 API search : Add missing fields in partial match search
(SSDM-11323) Bugfix : Null pointer is thrown in 20.10 using getSpace() from SampleImmutable in V2
(SSDM-11158) Bugfix : V3 API search : Number of results after translation has changed error
(SSDM-10497) V3 API search : NOT implementation
(SSDM-10231) V3 API search : Full Text Search on Standard Engine
(SSDM-11271) Plugin that generates thumbnails should allow to set the number of concurrent thumbs to do
(SSDM-11268) SFTP Hierarchy resolver that shows the tree the same way as on the ELN-LIMS with Microscopy/Flow enabled
(SSDM-11267) SFTP Hierarchy resolver that shows the tree the same way as on the ELN-LIMS UI
(SSDM-11237) Make locale settings in all databases consistent
(SSDM-11388) Bugfix : V3 API search : Search not working properly for some empty search criteria
(SSDM-11223) API : Enable Compression in Jetty for the API
(SSDM-10994) V3 API search: Sorting by multiple properties
(SSDM-10962) Disable unnecessary plugings in the default Docker installation
(SSDM-11420) Bugfix : Widget for addition of datasets when creating new jupyter notebook does not have scrollbar
(SSDM-11577) Bugfix : Moving objects to space called SHARED_MATERIALS does not work
(SSDM-11559) Bugfix : Cannot remove widget assignment in ELN settings
## ELN

(SSDM-10940) Warning when searching for more than 3 words in global search
(SSDM-10986) Anonymous login in ELN
(SSDM-11086) Matching mode : Remember the last selected option in user settings
(SSDM-11091) Bugfix : Editing a storage position does not work
(SSDM-11049) Bugfix : Data set registration with file with leading space
(SSDM-11118) Bugfix : Error Messages and wrong order in ELN
(SSDM-11133) Bugfix : Incorrect automatic experiment code generation
(SSDM-11174) Bugfix : Opening microscopy image viewer breaks “New…” and “More…” dropdowns
(SSDM-11151) Bugfix : ELN hangs when copying a sample with STORAGE_POSITION child
(SSDM-11213) Bugfix : ‘visible’ flag at the property level is not respected
(SSDM-11132) Bugfix : Deletion of rows in spreadsheet component does not work
(SSDM-11238) Bugfix : Error asigning storage
(SSDM-11013) The limit of 50 samples on ELN Tree counts the ones you want to hide for the total
(SSDM-11269) The limit of 50 samples on ELN Tree doesn’t allow to expand after certain level
(SSDM-11305) Move Object should allow to choose all child objects and datasets on the same space and project to be included
(SSDM-11270) Extend plugin system to allow hiding datasets by type
(SSDM-10944) Prototype NG_UI table integration in ELN
(SSDM-11215) Bugfix : Flag ‘create-continuous-sample-codes’ is not respected in some places
(SSDM-10968) Add space management to ELN
(SSDM-11347) Bugfix : Overlapping messages during user registration in ELN
(SSDM-11005) Bugfix : Vulneraibility check of Util.showError()
(SSDM-11341) Bugfix: Hide Nagios dataset type from ELN UI
(SSDM-11555) Bugfix : Export ignores first 3 digits of MULTILINE_VARCHAR fields
(SSDM-11540) Bugfix : Export does not always work if objects contain a spreadheet
New Admin UI

(SSDM-10936) Bugfix : Bug with Order of Requests with missing quantity
(SSDM-11346) Bugfix : Remove user in new admin UI does not work
(SSDM-10833) Entity deletion history
(SSDM-10939) Add SWITCH aai login to NG UI
(SSDM-11178) Make new admin UI and ELN consistent
Version 20.10.1 EA (Early Access) (12 March 2021)

## Core

(SSDM-10320) Bugfix : Installer fails to Upgrade
(SSDM-10316) Bugfix : SWITCH AAI user management tasks adds user folders each time it runs
(SSDM-10385) Bugfix : SSLError when trying to connect to OpenBIS from JupyterHub
(SSDM-10317) Bugfix : Missing material properties in full text search
(SSDM-10306) Dataset Uploader : Accepting modern certificate authorities
(SSDM-10366) Unique property values support (database changes only)
(SSDM-10382) Bugfix : sample_identifier column doesn’t update on some row updates on the database
(SSDM-10332) New Search Engine : Full text search aggregation running on the database
(SSDM-10304) openBIS sync : do not synchronize internally managed master data
(SSDM-10140) Don’t start if incorrect Postgres version is found
(SSDM-10416) Disable Lucene character escape function in openBIS 20.10
(SSDM-10473) Bugfix : tsvector_document of experiments_all not updated when project is moved
(SSDM-10493) Bugfix : tsvector_document, sample_identifier and space_id not corretly updated when project moved to another space
(SSDM-10469) V3 API - add “openbis-version” on the getServerInformation
(SSDM-10413) V3 API - a method that would return available query databases is missing
(SSDM-10395) V3 API - add “deletePersons” method
(SSDM-10429) Bugfix : fix login issue in JupyterHub
(SSDM-10405) New Search Engine : Nested AND/OR (Implementation)
(SSDM-10304) openBIS sync : do not synchronize internally managed master data
(SSDM-10574) Bugfix : Sorting by a non existing property on sample/experiment/dataset search lead to elements not containing it to not to appear on the results
(SSDM-10566) Bugfix : V3 API search : Search by code issues
(SSDM-10538) Bugfix : Project samples - inconsistent sample space and project after sample space change
(SSDM-10471) Bugfix : Fix issue with DSS check script
(SSDM-9413) Statistics collection for openBIS
(SSDM-10702) Bugfix : V3 API search : Search by code issues
(SSDM-10797) Bugfix : Search Engine Bug : String Equals ending in space not matching
(SSDM-10894) Bugfix : UserManagementMaintenanceTask fails with stacked file authentication services
(SSDM-10679) Bugfix : Possible Authorization Bug on DSS Search
(SSDM-10611) Bugfix : obis doesn’t work with git-annex version 8
(SSDM-10707) Bugfix : Wildcard behavior in coreUI advance search
(SSDM-10696) Change entity-history.enabled to true by default
(SSDM-10539) Work on integration between LDAP and SWITCH AAI
(SSDM-10782) Modify user management task for multi-group to support shared inventory spaces
(SSDM-10677) V3 API : Make wildcard search configurable
(SSDM-10911) Bugfix : V3 API: Global search compatibility fixes
(SSDM-10830) Entity Deletion History - new database table
(SSDM-10411) V3 API - a method for plugins evaluation is missing
(SSDM-10390) Search Engine : minor performance issues found during the new UI performance testing
(SSDM-10196) New Search engine : Missing criteria methods
## ELN

(SSDM-10000) ELN - Barcodes Follow Up
(SSDM-10149) Plugin Toolbar Extension
(SSDM-10309) User Manager Improvements
(SSDM-10387) Bugfix : ELN success message of batch uploads says ‘samples’ instead of ‘objects’
(SSDM-10931) Bugfix : ELN Navigation tree doesn’t show data sets
(SSDM-10913) Bugfix : ### ELN Global search compatibility fixes (UI)
(SSDM-10904) Bugfix : Fix bug in DSS eln-lims-api reporting-plugin: openBIS java.lang.IllegalStateException: zip file closed
(SSDM-10940) Warning when searching for more than 3 words in global search
(SSDM-10936) Bugfix : Bug with Order of Requests with missing quantity
(SSDM-10519) ELN UI : Fix Full Text Search sorting to score and show rank
New Admin UI

(SSDM-10186) NEW openBIS UI - Group Management page
(SSDM-10401) NEW openBIS UI - Plugins management page
(SSDM-10432) NEW openBIS UI - queries execution
(SSDM-10402) NEW openBIS UI - queries management
(SSDM-10431) NEW openBIS UI - plugins evaluation
(SSDM-10663) NEW openBIS UI - “Initial value” field is not shown when needed
(SSDM-10646) NEW openBIS UI - revise internal property type check
(SSDM-10433) NEW openBIS UI - table overviews
(SSDM-10420) NEW openBIS UI - use the new naming (e.g ‘Object’ instead of ‘Sample’)
(SSDM-10912) NEW openBIS UI : enable the UI core plugin in the installer by default
(SSDM-10428) Bugfix : NEW openBIS UI - maintain the application path in URL
Version 20.10.0 RC (Release Candidate) (27 October 2020)

New features and Major Changes compared to release 19.06
Extensions to the data model:
Date Data Type: Intended to be use when timestamps are not needed.
Sample Property Type: Allows to link Samples without using the Parent/Children relationships.
Sample Relationship Properties: Allows to add information to relationship connections.
Changes to the data model:
The Internally Managed and Internal Namespace concepts for properties have been merged. Now there is only Internally managed. Only the SYSTEM user can modify these.
Search engine: Completely rewritten to be faster, scale better and lower memory consumption. Queries will now behave like classic database queries instead of fuzzy full text search queries.
Admin UI Currently a preview, will replace the Core UI on the future.

## ELN-LIMS UI

Improved plugin system for the UI.
Microscopy and Flow Cytometry UI are now ELN plugins.
Mayor Technology Upgrades, now using:
Java 11
## Postgres 11
## Bioformats 6.5.1
## Jetty 9.4.30
and other many upgrades.
and lots, lots, lots of bug fixes.
For more details see sprint change log between S301 and S334.
## Deprecated

As a rule of a thumb, deprecated features should stop being used since they can be removed in future releases.
Material entity type is deprecated: Sample Property Types can be used instead for the same use cases.
File Type is deprecated.
Managed Properties are deprecated.
Core UI is deprecated. For admin tasks use the new Admin UI.
V1 API: A reminder that all new developments should be done using the V3 API, V1 even if still kept for backwards compatibility with old plugins is not developed anymore.
GeneralInformationService
GeneralInformationChangingService
## V3 API

isInternalNamesSpace & setInternalNameSpace : Now manage the same flag “Internally Managed”
FetchOptions.cacheMode : The new search engine ignores this, always getting the results from the database.
EntityWithPropertiesSortOptions.fetchedFieldsScore : The new search engine ignores this, only full text search has a weights system to sort results, non usable on standard queries.
AbstractEntitySearchCriteria.withProperty : deprecated in favour of using withXXXProperty, XXX being String, Date, Boolean or Number, check Javadoc for more details.
AbstractEntitySearchCriteria.withAnyProperty : deprecated in favour of using withAnyXXXProperty, XXX being String, Date, Boolean or Number, check Javadoc for more details.
Hot-deployed (aka “predeployed”) Java plugins (i.e. entity validation plugin, dynamic property plugin, managed property plugin) are deprecated. WARNING: using them may lead to a deadlock during the Application Server startup.
## Removed

The technologies for proteomics and screening where deprecated on 19.06 not been provided by the installer. Now they are finally removed and openBIS instances with these technologies CAN’T be upgraded. The upgrade procedure will detect their presence and prevent the upgrade.",Version 20.10.11 (02 Dec 2024),0,en_20.10.0-11_system-documentation_changelog_CHANGELOG_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_changelog_CHANGELOG.txt,2025-09-30T12:09:03.738123Z,2
docs:openbis:en_20.10.0-11_system-documentation_changelog_index:0,Changelog,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/changelog/index.html,openbis,"## Changelog

OpenBIS Change Log
## Version 20.10.11 (02 Dec 2024)
## Core
## ELN
## Version 20.10.10 (10 Oct 2024)
## Core
## ELN
## Version 20.10.9.1 (16 Aug 2024)
## ELN
## Version 20.10.9 (31 Jul 2024)
## Core
## ELN
## Admin
## Version 20.10.8 (29 May 2024)
## Core
## ELN
## Admin
## Version 20.10.7.3 (23 November 2023)
## ELN
## Admin
## Version 20.10.7.2 (13 October 2023)
## ELN
## Admin
## Core
## Version 20.10.7.1 (25 July 2023)
## ELN
## Admin
## Version 20.10.7 (5 July 2023)
## Core
## ELN
Admin UI
ELN/Admin UI
## Version 20.10.6 (26 April 2023)
## Core
Admin UI / ELN
Admin UI
## ELN
## Version 20.10.5 (29 November 2022)
## Core
## Jupyter Integration:
Admin UI
## ELN-LIMS:
## Version 20.10.4 (3 August 2022)
## Core
## ELN
## Version 20.10.3.1 (13 June 2022)
## Core
## ELN
## Version 20.10.3 (7 March 2022)
## Core
## ELN
pyBIS
New Admin UI
## Version 20.10.2.3 (15 November 2021)
## ELN
## Version 20.10.2.2 (30 November 2021)
## Core
## ELN
## Version 20.10.2.1 (6 October 2021)
## Core
## ELN
Version 20.10.2 GA (General Availability) (22 September 2021)
## Core
## ELN
New Admin UI
Version 20.10.1 EA (Early Access) (12 March 2021)
## Core
## ELN
New Admin UI
Version 20.10.0 RC (Release Candidate) (27 October 2020)
Admin UI Currently a preview, will replace the Core UI on the future.
## ELN-LIMS UI
## Deprecated
## V3 API
## Removed
### Pending 20.10 Configuration Changes
## Version 20.10.10
1. Changes to Datastore logs configuration
## Version 20.10.9
1. Changes to ELN LIMS Dropbox, new configuration keys for DSS service.properties.
### Configuration:
2. Configuration of download-url for Application Server service.properties
## Version 20.10.6
1. Changes on ELN LIMS Dropbox, new configuration key for DSS service.properties. This change is OPTIONAL.
2. Changes to User Management Task, new configuration key for the configuration file. This change is OPTIONAL.
3. Technology Upgrade: Postgres 15. This change is OPTIONAL.
## Version 20.10.3
Version 20.10.2 GA (General Availability)
Version 20.10.1 EA (Early Access)
Release 20.10.0 RC
## Technology Upgrade: Postgres 11
Technology Upgrade: Java 11
## Technology Upgrade: Search Engine",Changelog,0,en_20.10.0-11_system-documentation_changelog_index_0,reference,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_changelog_index.txt,2025-09-30T12:09:03.844199Z,2
docs:openbis:en_20.10.0-11_system-documentation_changelog_pending-configuration-changes:0,Pending 20.10 Configuration Changes,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/changelog/pending-configuration-changes.html,openbis,"### Pending 20.10 Configuration Changes

## Version 20.10.10

1. Changes to Datastore logs configuration

Datastore server will no longer create a separate log file everytime it starts up.
In order to revert this change, uncomment
rotateLogFiles
call from
<INSTALLATION_DIR>/servers/datastore_server/datastore_server.sh
(line 184)
echo
-n
""Starting Data Store Server ""
# rotateLogFiles $LOGFILE $MAXLOGS  # <- uncomment this line to bring back log rotation
## Version 20.10.9

1. Changes to ELN LIMS Dropbox, new configuration keys for DSS service.properties.

The ELN-LIMS dropbox validation has been overhauled so that it can happen in two widely configurable steps.
First, files are allowed to be removed; this is aimed at removing files that are knowingly considered unnecessary.
Secondly, the remains were validated for acceptability. This should allow administrators to choose the level of strictness in every particular environment.
The current default configuration matches previous behavior.
### Configuration:

eln-lims-dropbox-discard-files-patterns
Allows to specify comma-separated regular expressions of filenames that will be discarded by dropbox script during dataset creation.
## Default setting:
eln-lims-dropbox-discard-files-patterns=
(no files are discarded)
eln-lims-dropbox-illegal-files-patterns
Allows to specify comma-separated regular expressions of filenames that will be considered as illegal by the dropbox script. Their presence in the dropbox directory will automatically abort the dataset creation.
## Default setting:
eln-lims-dropbox-illegal-files-patterns=desktop\\.ini,
IconCache\\.db,
thumbs\\.db,
\\..*,
.*'.*,
.*~.*,
.*\\$.*,
.*%.*
Note: regular expressions in this configuration are
Java patterns
and special characters need to be escaped
2. Configuration of download-url for Application Server service.properties

The ELN-LIMS “Print PDF” functionality has been reworked and to produce proper PDFs a
download-url
parameter (which is a base URL for Web client access to the application server) in AS service.properties needs to be set.
When the machine is behind a reverse proxy, it needs to match the actual domain of the server
download-url
- Base URL. Contains protocol, domain, and port. (e.g https://localhost:8443)
## Version 20.10.6

1. Changes on ELN LIMS Dropbox, new configuration key for DSS service.properties. This change is OPTIONAL.

mail.addresses.dropbox-errors
Allows you to set a list of mails to get notified of registration errors.
2. Changes to User Management Task, new configuration key for the configuration file. This change is OPTIONAL.

## List<String>
instanceAdmins
Allows you to set instance admins on the global configuration and they will be created.
3. Technology Upgrade: Postgres 15. This change is OPTIONAL.

openBIS can now run using Postgres 15. Upgrades of Postgres should be taken carefully and with proper backup procedures in place.
It is required to upgrade the database directory format manually. Please check the
official documentation
## Version 20.10.3

After migrating to 20.10.3 please make sure you don’t have in ELN settings properties that have BOTH custom widget set to “Word Processor” and at the same time they are listed in “Forced Disable RTF” section. If your ELN instance happens to have such properties please remove them from both “Custom Widgets” as well as “Forced Disable RTF” sections. This way they will behave as normal text fields. In case you would like to allow styling/formatting of your property values (rich text), please make sure the custom widget is set to “Word Processor” and the property is not listed in “Force Disable RTF” section.
In openBIS version 20.10.3 a mechanism for displaying property values in ELN has been changed. Starting from that version any HTML tags contained in the property values won’t be interpreted by a web browser anymore. Instead, such HTML tags will be displayed as normal text. The only exception to that rule are properties that have custom widget set to “Word Processor” in the ELN settings. HTML tags in such properties will be still recognized, interpreted and rendered by a web browser. For instance,
value
=
""<b>text</b>""
will be rendered as “text” in bold font.
In some cases, due to this change in the ELN property rendering mechanism, after the upgrade to 20.10.3 properties may contain unwanted HTML tags when displayed in ELN. What was rendered as “value” in older versions of ELN, may after the upgrade to 20.10.3 become “
value
”. If that is the case with your installation, then you can clean such problematic property values using a Java tool created for that purpose.
The tool can be downloaded here. The tools requires Java 11 or newer. It connects with a chosen openBIS server and fetches/updates the data using openBIS V3 API.
## IMPORTANT:
Please note that it is normal for rich text property values (i.e. properties with custom widget set to “Word Processor”) to contain HTML tags. HTML is used for styling/formatting of such property values. The tool should not normally be used for such properties.
## Tool usage:
Show statistics about property values that contain HTML tags (by default it searches for property values that start with: “
, contain: “
” and end with: “
”)
java
-jar
openbis-html-properties-cleaner.jar
stats
--openbis-url=https://openbis-xxx.com
List property values that contain HTML tags for a given entity kind and property type
java
-jar
openbis-html-properties-cleaner.jar
list
--openbis-url=https://openbis-xxx.com
--entity-kind=SAMPLE
--property-type=AUTHORS
Fix property values that contain HTML tags for a given entity kind and property type
java
-jar
openbis-html-properties-cleaner.jar
fix
--openbis-url=https://openbis-xxx.com--entity-kind=SAMPLE
--property-type=AUTHORS
Version 20.10.2 GA (General Availability)

The default value of “project-samples-enabled” paramenter in service.properties of the openbis project has been changed to “true”. This property controls whether it is possible to have samples related to a project directly.
Version 20.10.1 EA (Early Access)

## WARNING:
Downgrade from 20.10.1 EA to 20.10.0 RC is NOT possible (20.10.1 EA version includes database schema changes)
The default value of
entity-history.enabled
property has been changed to ‘true’. The property controls gathering information about deleted entities in ‘events’ table (for more details please check: Installation and Administrator Guide of the openBIS Server
here
).
Property collect-statistics is added with “true” as the default value. When the value is “true” (and not overridden by the environment variable) openBIS sends usage statistics to a dedicated server on startup as well as on the 1st day of the month.
In the
service.properties
file the property collect-statistics is added with “true” as the default value. When the value is “true” (and not overridden by the environment variable) openBIS sends usage statistics to a dedicated server on startup as well as on the 1st day of the month. Usage statistics include only openBIS version, country and number of users.
## DISABLE_OPENBIS_STATISTICS
environment variable, when set to “true” the statistics is not collected regardless of the value of the property collect-statistics.
Release 20.10.0 RC

This release includes migration to a new version of Postgres, as well as other technology upgrades that reduce the amount of flags and configuration keys needed (configuration files included on new installations don’t have them).
Before you upgrade, you need to take care of:
## Technology Upgrade: Postgres 11

If the installer was used: It is required to upgrade the database directory format manually. Please check the
official documentation
If the docker image was used:
Log in interactively into your 19.06.5 Docker container and make binary database dumps with the same name of the databases openbis_prod and pathinfo_prod into the ROOT of your mounted state directory.
Stop your 19.06.5 Docker container.
Rename postgresql_data into the state directory to postgresql_data_19_6 since it is not compatible with Postgres 11. It is not recommended to delete it until you are sure you are not going to need it.
Start your 20.10.0 Docker container, it should detect the presence of openbis_prod and  pathinfo_prod, automatically populating the database from them.
When the process is finish test that the system looks like you expect.
Feel free to Delete or move somewhere else the openbis_prod and  pathinfo_prod backups so the upgrade procedure is not repeated.
Technology Upgrade: Java 11

Upgrading to Java 11 forces to remove some old flags on the next files, Java 11 will fail to start with them:
openbis.conf : remove -d64
datastore_server.conf : remove -d64
## Technology Upgrade: Search Engine

Now configuration keys related with the old one are ignored, the next configuration keys will be ignored and can be removed from AS service.properties:
hibernate
.
search
.
index
-
base
hibernate
.
search
.
index
-
mode
hibernate
.
search
.
batch
-
size
hibernate
.
search
.
maxResults
hibernate
.
search
.
worker
.
execution
hibernate
.
batch
.
sessionCache
.
maxEntities",Pending 20.10 Configuration Changes,0,en_20.10.0-11_system-documentation_changelog_pending-configuration-changes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_changelog_pending-configuration-changes.txt,2025-09-30T12:09:03.912718Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_archiving-datasets:0,Archiving Datasets,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/archiving-datasets.html,openbis,"## Archiving Datasets

Manual archiving

openBIS core UI

Archiving can be triggered by doing the following steps:
go to an experiment/collection or an object.
switch to the tab “Data Sets”. There will be in the lower right corner the button ‘Archiving’.
click on the button and choose either ‘Copy to Archive’ or ‘Move to Archive’.
if you did not select any data set, all data sets will be archived. If you have selected some data sets you can choose if you want to archive only them or all the data sets accessible in the table.
Because archiving does not happens immediately the status (called ‘Archiving Status’ in data set tables) of the data sets will be changed to BACKUP_PENDING or ARCHIVE_PENDING.
To make archived data sets available again repeat the steps, but choose ‘Unarchive’.
If you want to disallow archiving, choose ‘Lock’. Remember that you can do this only for available data sets. The ‘Archiving Status’ will change to ‘AVAILABLE (LOCKED)’. To make archiving possible again, choose ‘Unlock’.
Note that the recommended way is to
not use
the core-UI for archiving, but to use the ELN-LIMS for this, as detailed below.
## ELN-LIMS

Instead of triggering archiving direclty, via the ELN archiving can only be
requested
. The maintenance task
ArchivingByRequestTask
is required. It triggers the actual archiving. For details on archiving and unarchiving via ELN UI see
archive
Automatic archiving

Archiving can be automated by the Auto Archiver. This is a
maintenance task
which triggers archiving of data sets fullfulling some conditions (e.g. not accessed since a while). Note that the auto archiver does not itself perform archiving. It only automates the selection of data sets to be archived. For all configuration parameters see
AutoArchiverTask
.
## Archiving Policies

An archiving policy can be set up to select from all non-archived data sets candidates data sets to be archived. These are either data sets not accessed since some days or data sets marked by a tag. If nothing is specified, all candidates will be archived.
The policy can be specified by
policy.class
property. It has to be the fully-qualified name of a Java class implementing
ch.systemsx.cisd.etlserver.IAutoArchiverPolicy
. All properties starting with
policy.
specify the policy further.
ch.systemsx.cisd.etlserver.plugins.GroupingPolicy

## Description
: Policy which tries to find a group of data sets with a total size from a specified interval. This is important in case of
Multi Data Set Archiving
. Grouping can be defined by space, project, collection, object, data set type or a combination of those. Groups can be merged if they are too small. Several grouping keys can be specified.
Searching for an appropriate group of data sets for auto archiving is logged. If no group could be found an admin is notified via email (email address specified in
log.xml
). The email contains the search log.
### Configuration
## :
## Property Key
## Description
minimal-archive-size
The total size (in bytes) of the selected data sets has to be equal or the larger than this value. Default: 0
maximal-archive-size
The total size (in bytes) of the selected data sets has to be equal or the less than this value. Default: Unlimited
grouping-keys
Comma separated list of grouping keys. A grouping key has the following form: <basic key 1>#<basic key 2>#…#
[:merge] A basic key is from the following vocabulary: All, Space, Project, Experiment, Sample, DataSetType, DataSet. All basic keys of a group key define a grouping of all data set candidates. In each group all data sets have the all attributes defined by the basic keys in common. Note, that basic key All means no grouping. For example: Experiment#DataSetType means that the candidates are grouped according to experiment and data set type. The optional :merge is used when no group fulfills the total size condition and there are at least two groups with total size below minimal-archive-size. In this case groups which are too small will be merged until the total size condition is fulfilled. If a grouping key doesn’t lead to a group of data set fulfilling the total size condition the next grouping key is used until a matching group is found. If for a grouping key more than one matching group is found the oldest one will be chosen. If merging applies for more than two groups the oldest groups will be merged first. The age of a group is defined by the most recent access time stamp. Examples: Grouping policy by experiment: DataSetType#Experiment, DataSetType#Project, DataSetType#Experiment#Sample Grouping policy by space: DataSetType#Space, DataSetType#Project:merge, DataSetType#Experiment:merge, DataSetType#Experiment#Sample:merge, DataSet:merge
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask
interval = 10 days
archive-candidate-discoverer.class = ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer
archive-candidate-discoverer.tags = /admin-user/archive
policy.class = ch.systemsx.cisd.etlserver.plugins.GroupingPolicy
policy.minimal-archive-size =  30000000000
policy.maximal-archive-size = 150000000000
policy.grouping-keys = Space#DataSetType, Experiment#Sample:merge
In this example the candidates are unarchived data sets which have been
tagged by the user
admin-user
with the tag
archive
. The policy tries to
find a group of data sets with total size between 30 Gb and 150 Gb. It
first looks for groups where all data sets are of the same type and from
the same space. If no group is found it tries to find groups where all
data sets are from the same experiment and sample (data set with no
samples are assigned to
no_sample
). If no matching groups are found
and at least two groups are below the minimum the policy tries to merge
groups to a bigger group until the bigger group match the size
condition. If no group can be found an email will be sent describing in
detail the several steps of finding a matching group.",Archiving Datasets,0,en_20.10.0-11_system-documentation_configuration_archiving-datasets_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_archiving-datasets.txt,2025-09-30T12:09:03.982901Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_authentication-systems:0,Authentication Systems,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/authentication-systems.html,openbis,"## Authentication Systems

openBIS currently supports 3 authentication systems: a
self-contained system based on a UNIX-like passwd file, LDAP, and Single Sign
On (e.g., through SWITCHaai). Beside these, there are also so called
stacked
authentication methods
available. Stacked authentication methods use
multiple authentication systems in the order indicated by the name. The
first authentication system being able to provide an entry for a
particular user id will be used. If you need full control over what
authentication systems are used in what order, you can define your own
stacked authentication service in the Spring application context file:
$INSTALL_PATH/servers/openBIS-server/jetty/webapps/openbis/WEB-INF/classes/genericCommonContext.xml.
The default authentication configuration

In the template service properties, we set
authentication-service
= file-ldap-caching-authentication-service
,
which means that file-based authentication, and LDAP are used for
authentication, in this order. As LDAP are not configured in
the template service properties, this effectively corresponds
to
file-authentication-service
, however when LDAP is
configured, they are picked up on server start and are used to
authenticate users when they are not found in the local
passwd
file.
Furthermore, as it is a caching authentication service, it will cache
authentication results from LDAP in file
$INSTALL_PATH/servers/openBIS-server/jetty/etc/passwd_cache
. See section
## Authentication Cache
below for details on this caching.
The file based authentication system

This authentication schema uses the file
$INSTALL_PATH/servers/openBIS-server/jetty/etc/passwd
to determine whether a login to the
system is successful or not.
The script
$INSTALL_PATH/servers/openBIS-server/jetty/bin/passwd.sh
can be used to maintain
this file. This script supports the options:
passwd
list
|
[
remove
|
show
|
test
]
<userid>
|
[
add
|
change
]
<userid>
[
option
[
...
]]
--help
## :
## Prints
out
a
description
of
the
options.
[
-P,--change-password
]
## :
## Read
the
new
password
from
the
console,
[
-e,--email
]
## VAL
## :
## Email
address
of
the
user.
[
-f,--first-name
]
## VAL
## :
## First
name
of
the
user.
[
-l,--last-name
]
## VAL
## :
## Last
name
of
the
user.
[
-p,--password
]
## VAL
## :
## The
password.
A new user can be added with
prompt>
passwd.sh
add
[
-f
<first
name>
]
[
-l
<last
name>
]
[
-e
<email>
]
[
-p
<password>
]
<username>
If no password is provided with the
-p
option, the system will ask for
a password of the new user on the console. Please note that providing a
password on the command line can be a security risk, because the
password can be found in the shell history, and, for a short time, in
the
ps
table. Thus
-p
is not recommended in normal operation.
The password of a user can be tested with
prompt>
passwd.sh
test
<username>
The system will ask for the current password on the console and then
print whether the user was authenticated successfully or not.
An account can be changed with
prompt>
passwd.sh
change
[
-f
<first
name>
]
[
-l
<last
name>
]
[
-e
<email>
]
[
## -P
]
<username>
An account can be removed with
prompt>
passwd.sh
remove
<username>
The details of an account can be queried with
prompt>
passwd.sh
show
<username>
All accounts can be listed with
prompt>
passwd.sh
list
The password file contains each user in a separate line. The fields of
each line are separated by colon and contain (in this order):
## User Id
,
## Email Address
,
## First Name
,
## Last Name
and
## Password Hash
.
## The
## Password Hash
field represents the
salted
## SHA1
hash of the user’s password in
BASE64 encoding
.
The interface to LDAP

To work with an LDAP server, you need to provide the server URL with
(example) and set the
authentication-service
=
ldap-authentication-service
ldap.server.url
=
ldap://d.ethz.ch/DC
=
d,DC
=
ethz,DC
=
ch
and the details of an LDAP account who is allowed to make queries on the
LDAP server with (example)
ldap.security.principal.distinguished.name
=
## CN
=
carl,OU
=
EthUsers,DC
=
d,DC
=
ethz,DC
=
ch
ldap.security.principal.password
=
Carls_LDAP_Password
Note: A space-separated list of URLs can be provided if distinguished
name and password  are valid for all specified LDAP servers.
## Authentication Cache

If configuring a caching authentication service like
file-ldap-caching-authentication-service
, authentication results
from remote authentication services like LDAP are cached
locally in the openBIS Application Server. The advantage is a faster
login time on repeated logins when one or more remote authentication
services are slow. The disadvantage is that changes to data in the
remote authentication system (like a changed password or email address)
are becoming known to openBIS only with a delay. In order to minimize
this effect, the authentication caching performs “re-validation” of
authentication requests asynchronously. That means it doesn’t block the
user from logging in because it is performed in different thread than
the login.
There are two service properties which give you control over the working
of the authentication cache:
authentication.cache.time
lets you set for how long (after putting
it into the cache) a cache entry (read: “user name and password”)
will be kept if the user does not have a successful login to openBIS
in this period of time (as successful logins will trigger
re-validation and thus renewal of the cache entry). The default is
28h, which means that users logging into the system every day will
never experience a delay from slow remote authentication systems. A
non-positive value will disable caching.
authentication.cache.time-no-revalidation
lets you set for how
long (after putting it into the cache) a cache entry will
not
be
re-validated if the login was successful. This allows you to reduce
the load that openBIS creates on the remote authentication servers
for successful logins of the same user. The default is 1h. Setting
it to 0 will always trigger re-validation, setting it to
authentication.cache.time
will not perform re-validation at all
and thus expire every cache entry after that time.
An administrator with shell access to the openBIS Application Server can
see and change the current cache entries in the
file
$INSTALL_PATH/servers/openBIS-server/jetty/etc/passwd_cache
. The format is the same
as for the file-based authentication system (see section
The file based
authentication system
above), but has an additional field
## Cached At
added to the end of each line.
## Cached At
is the time (in milli-seconds
since start of the Unix epoch, which is midnight
## Universal Time
## Coordinated
, 1 January 1970) when the entry was cached. Removing a line
from this file will remove the corresponding cache entry. The
authentication cash survives openBIS Application Server restarts because
of this persisted file. If you need to clear the cache altogether, it is
sufficient to remove the
passwd_cache
file at any time. No server
restart is needed to make changes to this file take effect.
You can switch off authentication caching by either
setting
authentication.cache.time
=
-1
, or by choosing an
authentication service which does not have
caching
in its name.
## Anonymous Login

In order to allow anonymous login a certain user known by openBIS has to be specified. This is
done by the property
user-for-anonymous-login
. The value is the user
ID. The display settings and the authorization settings of this user are
used for the anonymous login.
Anonymous login is possible with URL parameter
anonymous
set to
true
or by property
default-anonymous-login
in web configuration properties
(see
## Web Client Customization
). Note, that for the ELN client the property
default-anonymous-login
isn’t used. Anonymous login needs only the property
user-for-anonymous-login
for an existing user with some rights.
## Single Sign On Authentication

Currently only Shibboleth SSO is supported. For more details see
## Single Sign On Authentication
.",Authentication Systems,0,en_20.10.0-11_system-documentation_configuration_authentication-systems_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_authentication-systems.txt,2025-09-30T12:09:04.051883Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_authorization:0,Authorization,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/authorization.html,openbis,"## Authorization

Authorization is a logic that decides whether a given user is allowed to perform a given operation on a given resource. In openBIS authorization decides if entities like spaces, projects, experiments, objects, datasets, can be created, read, updated or deleted by a given user.
Similar to other IT systems, openBIS access rights can be defined only for groups of resources rather than for each individual resource separately. In openBIS the purpose of such groups is served by spaces and projects. It means that an openBIS user can be given access to a specific space or a specific project (and all the entities that belong to that space or to that project) but cannot be given access just to a single experiment, object or dataset from that space or that project.
Apart from access to a space or a project, a user can be given openBIS instance access rights. With such rights a user can access any space and any project within that openBIS installation.
Having defined the 3 scopes (i.e. instance, space and project), we need to learn how to control what operations a user can perform on entities that belong to these scopes. This aspect in openBIS is controlled with “roles”. There are 4 roles available:
OBSERVER - can see objects in a given scope
USER - as OBSERVER + can create/update objects in a given scope
POWER_USER -  as USER + can delete objects in a given scope
ADMIN - as POWER_USER + update/delete the scope itself + assign rights within the scope
The above roles together with instance, space and project scopes that we have defined earlier give us the following combinations:
PROJECT_OBSERVER - can see the project and all the entities that belong to the project
PROJECT_USER - as PROJECT_OBSERVER + can create/update entities in the project
PROJECT_POWER_USER - as PROJECT_USER + can delete entities in the project
PROJECT_ADMIN - as PROJECT_POWER_USER + can update/delete the project + assign rights to the project
SPACE_OBSERVER - can see the space and all the entities that belong to the space
SPACE_USER - as SPACE_OBSERVER + can create/update entities in the space
SPACE_POWER_USER - as SPACE_USER + can delete entities in the space
SPACE_ADMIN - as SPACE_POWER_USER + can update/delete the space + can assign rights to the space
INSTANCE_OBSERVER - can see everything
INSTANCE_ADMIN - space admin rights to all spaces + can create openBIS types (masterdata)
Please note that instance scope can be combined only with OBSERVER and ADMIN roles.
WARNING: The project scope is disabled by default. To enable it for all users you have to change openBIS service.properties as follows:
authorization.project-level.enabled = true
authorization.project-level.users = .*
The “enabled” property controls whether the project scope can be used in general, while the “users” property defines exactly which users can use it. Setting “enabled” property to “true” will only make the project roles appear in “Roles” configuration tool in openBIS generic UI. These roles can be then assigned to users and saved. Still these roles won’t be used until a name of the user they are defined for matches the “users” regexp.
Last part of the openBIS authorization puzzle are users and user groups. So far we always assumed that a scope and a role will be directly assigned to a user, e.g. “John Doe” is an ADMIN of space “TEST”. Such an approach is absolutely fine and works great until the number of users we have to manage is relatively small. As the user base grows and so the maintenance overhead, it becomes handy to find users with the same access rights, put them into a user group and assign the rights to the user group rather than to each individual user. This way by simply assigning a user to a group we give him/her all the rights that are defined for that group. It leads to a simpler, more consistent and easier to maintain configuration.",Authorization,0,en_20.10.0-11_system-documentation_configuration_authorization_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_authorization.txt,2025-09-30T12:09:04.121915Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_index:0,Advanced Configuration,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/index.html,openbis,"### Advanced Configuration

### openBIS Server Configuration
### Application Server Configuration
## Database Settings
Data Store Server Configuration
### Optional Application Server Configuration
The base URL for Web client access to the data store server.
Export data limit in bytes, default to 10Gib
## Deleted Entity History
## Login Page - Banners
## Client Customization
### Configuration
Web client customizations
Data Set Upload Client Customizations
## Examples
Full web-client.properties Example
## Configuring File Servlet
Changing the Capability-Role map
Capability Role Map for V3 API
### Optional Datastore Server Configuration
Configuring DSS Data Sources
## Authentication Systems
The default authentication configuration
The file based authentication system
The interface to LDAP
## Authentication Cache
## Anonymous Login
## Single Sign On Authentication
## Authorization
## Maintenance Tasks
## Maintenance Task Classification
## Introduction
## Feature
ArchivingByRequestTask
AutoArchiverTask
BlastDatabaseCreationMaintenanceTask
DeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask
ReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask
DeleteFromArchiveMaintenanceTask
DeleteFromExternalDBMaintenanceTask
EventsSearchMaintenanceTask
ExperimentBasedArchivingTask
HierarchicalStorageUpdater
MultiDataSetDeletionMaintenanceTask
MultiDataSetUnarchivingMaintenanceTask
MultiDataSetArchiveSanityCheckMaintenanceTask
PathInfoDatabaseFeedingTask
PostRegistrationMaintenanceTask
RevokeUserAccessMaintenanceTask
UserManagementMaintenanceTask
Consistency and other Reports
DataSetArchiverOrphanFinderTask
DataSetAndPathInfoDBConsistencyCheckTask
MaterialExternalDBSyncTask
## Mapping File
### UsageReportingTask
PersonalAccessTokenValidityWarningTask
Consistency Repair and Manual Migrations
BatchSampleRegistrationTempCodeUpdaterTask
CleanUpUnarchivingScratchShareTask
DataSetRegistrationSummaryTask
DynamicPropertyEvaluationMaintenanceTask
DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask
FillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask
PathInfoDatabaseChecksumCalculationTask
PathInfoDatabaseRefreshingTask
RemoveUnusedUnofficialTermsMaintenanceTask
ResetArchivePendingTask
SessionWorkspaceCleanUpMaintenanceTask
MaterialsMigration
## Microscopy Maintenance Tasks
MicroscopyThumbnailsCreationTask
DeleteFromImagingDBMaintenanceTask
## Proteomics Maintenance Tasks
User Group Management for Multi-groups openBIS Instances
## Introduction
### Configuration
### Static Configurations
AS service.properties
DSS service.properties
### Dynamic Configurations
## Section
globalSpaces
## Section
commonSpaces
## Section
commonSamples
## Section
commonExperiments
## Section
instanceAdmins
(since version 20.10.6)
## Section
groups
What UserManagementMaintenanceTask does
Content of the Report File sent by UsageReportingTask
Common use cases
Adding a new group
Making a user an group admin
Remove a user from a group
Adding more disk space
Manual configuration of Multi-groups openBIS instances
Masterdata and entities definition
## Spaces
## Projects
## Collections
## Objects
Rights management
## Archiving Datasets
Manual archiving
openBIS core UI
## ELN-LIMS
Automatic archiving
## Archiving Policies
ch.systemsx.cisd.etlserver.plugins.GroupingPolicy
Multi data set archiving
## Introduction
Important technical details
## Workflows
Simple workflow
Staging workflow
Replication workflow
Staging and replication workflow
Clean up
### Configuration steps
Clean up Unarchiving Scratch Share
Deletion of archived Data Sets
Recovery from corrupted archiving queues
Master data import/export
## Querying Project Database
Create Read-Only User in PostgreSQL
## Enable Querying
Configure Authorization for Querying
Share IDs
## Motivation
## Syntax
## Resolving Rules
## Example
## Sharing Databases
## Introduction
Share Databases without Mapping File
Share Databases with Mapping File
Mapping all DSSs on one
Mapping all DSSs on one per module
## Overwriting Parameters
## Overwriting Generic Settings
openBIS Sync
## Introduction
Data Source Service Configuration
Use case: One Datasource - One or more Harvester
Data Source Service Document
### Harvester Service Configuration
What HarvesterMaintenanceTask does
Master Data Synchronization Rules
openBIS Logging
Runtime changes to logging",Advanced Configuration,0,en_20.10.0-11_system-documentation_configuration_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_index.txt,2025-09-30T12:09:04.186799Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_logging:0,openBIS Logging,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/logging.html,openbis,"openBIS Logging

Runtime changes to logging

The script
$INSTALL_PATH/servers/openBIS-server/jetty/bin/configure.sh
can be used to change the logging behavior of openBIS application server while the server is running.
The script is used like this: configure.sh [command] [argument]
The table below describes the possible commands and their arguments.
## Command
Argument(s)
## Default Value
## Description
log-service-calls
‘on’, ‘off’
‘off’
Turns on / off detailed service call logging.
When this feature is enabled, openBIS will log about start and end of every service call it executes to file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_service_calls.txt
log-long-running-invocations
‘on’, ‘off’
‘on’
Turns on / off logging of long running invocations.
When this feature is enabled, openBIS will periodically create a report of all service calls that have been in execution more than 15 seconds to file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_long_running_threads.txt.
debug-db-connections
‘on’, ‘off’
‘off’
Turns on / off logging about database connection pool activity.
When this feature is enabled, information about every borrow and return to database connection pool is logged to openBIS main log in file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_log.txt
log-db-connections
no argument / minimum connection age (in milliseconds)
5000
When this command is executed without an argument, information about every database connection that has been borrowed from the connection pool is written into openBIS main log in file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_log.txt
If the “minimum connection age” argument is specified, only connections that have been out of the pool longer than the specified time are logged. The minimum connection age value is given in milliseconds.
record-stacktrace-db-connections
‘on’, ‘off’
‘off’
Turns on / off logging of stacktraces.
When this feature is enabled AND debug-db-connections is enabled, the full stack trace of the borrowing thread will be recorded with the connection pool activity logs.
log-db-connections-separate-log-file
‘on’, ‘off’
‘off’
Turns on / off database connection pool logging to separate file.
When this feature is disabled, the database connection pool activity logging is done only to openBIS main log. When this feature is enabled, the activity logging is done ALSO to file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_db_connections.txt.",Command,0,en_20.10.0-11_system-documentation_configuration_logging_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_logging.txt,2025-09-30T12:09:04.348851Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_maintenance-tasks:0,Maintenance Tasks,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/maintenance-tasks.html,openbis,"## Maintenance Tasks

## Maintenance Task Classification

## Category
## Feature
Consistency and other Reports
Consistency Repair and Manual Migrations
## Relevancy
## Default
## Relevant
## Rare
## Deprecated
## Introduction

A maintenance task is a process which runs once or in regular time intervals. It is defined by a
core plugin
of type
maintenance-tasks
. Usually a maintenance task can only run on AS or DSS but not in both environments.
The following properties are common for all maintenance tasks:
## Property Key
## Description
class
The fully-qualified Java class name of the maintenance task. The class has to implement IMaintenanceTask.
execute-only-once
A flag which has to be set to true if the task should be executed only once. Default value: false
interval
A time interval (in seconds) which defines the pace of execution of the maintenance task. Can be specified with one of the following time units: ms, msec, s, sec, m, min, h, hours, d, days. Default time unit is sec. Default value: one day.
start
A time at which the task should be executed the first time. Format: HH:mm. where HH is a two-digit hour (in 24h notation) and mm is a two-digit minute. By default the task is execute at server startup.
run-schedule
Scheduling plan for task execution. Properties execute-only-once, interval, and start will be ignored if specified.
## Crontab syntax:
## cron:
<second>
<minute>
<hour>
<day>
<month>
<weekday>
## Examples:
## cron:
0
0
*
*
*
*
: the top of every hour of every day.
## cron:
*/10
*
*
*
*
*
: every ten seconds.
## cron:
0
0
8-10
*
*
*
: 8, 9 and 10 o’clock of every day.
## cron:
0
0
6,19
*
*
*
: 6:00 AM and 7:00 PM every day.
## cron:
0
0/30
8-10
*
*
*
: 8:00, 8:30, 9:00, 9:30, 10:00 and 10:30 every day.
## cron:
0
0
9-17
*
*
## MON-FRI
: on the hour nine-to-five weekdays.
## cron:
0
0
0
25
12
?
: every Christmas Day at midnight.
## Non-crontab syntax:
Comma-separated list of definitions with following syntax:
[[<counter>.]<week
day>]
[<month
day>[.<month>]]
<hour>[:<minute>]
where
<counter>
counts the specified week day of the month.
<week
day>
is
## MO
,
## MON
,
## TU
,
## TUE
,
## WE
,
## WED
,
## TH
,
## THU
,
## FR
,
## FRI
,
## SA
,
## SAT
,
## SU
, or
## SUN
(ignoring case).
<month>
is either the month number (followed by an optionl ‘.’) or
## JAN
,
## FEB
,
## MAR
,
## APR
,
## MAY
,
## JUN
,
## JUL
,
## AUG
,
## SEP
,
## OCT
,
## NOV
, or
## DEC
(ignoring case).
## Examples:
6,
18
: every day at 6 AM and 6 PM.
## 3.FR
22:15
: every third friday of a month at 22:15.
1.
15:50
: every first day of a month at 3:50 PM.
## SAT
1:30
: every saturday at 1:30 AM.
## 1.Jan
5:15,
1.4.
5:15,
1.7
5:15,
1.
## OCT
5:15
: every first day of a quarter at 5:15 AM.
run-schedule-file
File where the timestamp for next execution is stored. It is used if run-schedule is specified. Default:
<installation
folder>/<plugin
name>_<class
name>
retry-intervals-after-failure
Optional comma-separated list of time intervals (format as for interval) after which a failed execution will be retried. Note, that a maintenance task will be execute always when the next scheduled timepoint occurs. This feature allows to execute a task much earlier in case of temporary errors (e.g. temporary unavailibity of another server).
## Feature

ArchivingByRequestTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Triggers archiving for data sets where the ‘requested
archiving’ flag is set. Waits with archiving until enough data sets for
a group come together. This is necessary for taped-base archiving where
the files to be stored have to be larger than a minimum size.
### Configuration
## :
## Property Key
## Description
keep-in-store
If true the archived data set will not be removed from the store. That is, only a backup will be created. Default: false
minimum-container-size-in-bytes
Minimum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 10 GB
maximum-container-size-in-bytes
Maximum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 80 GB
configuration-file-path
Path to the configuration file as used by User Group Management. Here only the group keys are needed. They define a set of groups. If there is no configuration file at the specified path this set is empty.A data set requested for archiving belongs the a specified group if its space starts with the group key followed by an underscore character ‘_’. Otherwise it belongs to no group. This maintenance task triggers archiving an archive container with one or more data set from the same group if the container fits the specified minimum and maximum size. Note, that data sets which do not belong to a group are handled as a group too. If a data set is larger than the maximum container size it will be archived even though the container is to large. The group key (in lower case) is provided to the archiver. The Multi Data Set Archiver will use this for storing the archive container in a sub folder of the same name.
## Default:
etc/user-management-maintenance-config.json
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.ArchivingByRequestTask
interval = 1 d
minimum-container-size-in-bytes =  20000000000
maximum-container-size-in-bytes = 200000000000
configuration-file-path = ../../../data/groups.json
## Notes:
In practice every instance using multi dataset archiving
feature and also the ELN-LIMS should have this enabled.
AutoArchiverTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Triggers archiving of data sets that have not been
archived yet.
### Configuration
## :
## Property Key
## Description
remove-datasets-from-store
If true the archived data set will be removed from the store. Default: false
data-set-type
Data set type of the data sets to be archived. If undefined all data set of all types might be archived.
older-than
Minimum number of days a data set to be archived hasn’t been accessed. Default: 30
archive-candidate-discoverer.class
Discoverer of candidates to be archived:
ch.systemsx.cisd.etlserver.plugins.AgeArchiveCandidateDiscoverer
: All data sets with an access time stamp older than specified by property older-than are candidates. This is the default discoverer.
ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer
: All data sets which are marked by one of the tags specified by the property
archive-candidate-discoverer.tags
are candidates.
policy.class
A policy specifies which data set candidates should be archived. If undefined all candidates will be archived. Has to be a fully-qualified name of a Java class implementing ch.systemsx.cisd.etlserver.IAutoArchiverPolicy.
policy.*
Properties specific for the policy specified by
policy.class
. More about policies can be found here.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask
interval = 10 days
archive-candidate-discoverer.class = ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer
archive-candidate-discoverer.tags = /admin-user/archive
policy.class = ch.systemsx.cisd.etlserver.plugins.GroupingPolicy
policy.minimal-archive-size = 1500000
policy.maximal-archive-size = 3000000
policy.grouping-keys = Space#DataSetType, Space#Experiment:merge
BlastDatabaseCreationMaintenanceTask

## Environment
## : DSS
## Relevancy:
Default (ELN-LIMS)
## Description
: Creates BLAST databases from FASTA and FASTQ files of
data sets and/or properties of experiments, samples, and data sets.
The title of all entries of the FASTA and FASTQ files will be extended
by the string
## [Data
## set:
<data
set
code>,
## File:
<path>]
## . Sequences
provide by an entity property will have identifiers of the form
<entity
kind>+<perm
id>+<property
type>+<time
stamp>
. This allows to
determine where the matching sequences are stored in openBIS. A sequence
can be a nucleic acid sequence or an amino acid sequence.
For each data set a BLAST nucl and prot databases will be created (if
not empty) by the tool
makeblastdb
. For all entities of a specified
kind and type one BLAST database (one for nucleic sequences and one
for amino acid sequences) will be created from the plain sequences
stored in the specified property (white spaces will be removed). In
addition an index is created by the tool
makembindex
if the sequence
file of the database (file type
.nsq
) is larger than 1MB. The name of
the databases are
<data
set
code>-nucl/prot
and
<entity
kind>+<entity
type
code>+<property
type
code>+<time
stamp>-nucl/prot
.
These databases are referred in the virtual database
all-nucl
## (file:
all-nucl.nal
) and
all-prot
## (file:
all-prot.pal
).
If a data set is deleted the corresponding BLAST nucl and prot databases
will be automatically removed the next time this maintenance task runs.
If an entity of specified type has been modified the BLAST databases
will be recalculated the next time this maintenance task runs.
Works only if BLAST+ tool suite has been installed. BLAST+ can be
downloaded from
ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/
## Notes:
It comes pre-configured with the ELN-LIMS but if additional
properties need to scanned they should be added to the plugin.properties
### Configuration
## :
## Property Key
## Description
dataset-types
Comma-separated list of regular expressions of data set types. All FASTA and FASTQ files from those data sets are handled. All data sets of types not matching at least one of the regular expression are not handled.
entity-sequence-properties
Comma-separated list of descriptions of entity properties with sequences. A description is of the form
<entity
kind>+<entity
type
code>+<property
type
code>
where
<entity
kind>
is either
## EXPERIMENT
,
## SAMPLE
or
## DATA_SET
(Materials are not supported).
file-types
Space separated list of file types. Data set files of those file types have to be FASTA or FASTQ files. Default:
.fasta
.fa
.fsa
.fastq
blast-tools-directory
Path in the file system where all BLAST tools are located. If it is not specified or empty the tools directory has to be in the PATH environment variable.
blast-databases-folder
Path to the folder where all BLAST databases are stored. Default:
<data
store
root>/blast-databases
blast-temp-folder
Path to the folder where temporary FASTA files are stored.  Default:
<blast-databases-folder>/tmp
last-seen-data-set-file
Path to the file which stores the id of the last seen data set. Default:
<data
store
root>/last-seen-data-set-for-BLAST-database-creation
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.BlastDatabaseCreationMaintenanceTask
interval = 1 h
dataset-types = BLAST-.+
entity-sequence-properties = SAMPLE+OLIGO+SEQUENCE, EXPERIMENT+YEAST+PLASMID_SEQUENCE
blast-tools-directory = /usr/local/ncbi/blast/bin
DeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Deletes data sets which have been deleted on AS.
## Note
If this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.
### Configuration
## :
## Property Key
## Description
last-seen-data-set-file
Path to a file which will store the code of the last data set handled. Default:
deleteDatasetsAlreadyDeletedFromApplicationServerTaskLastSeen
timing-parameters.max-retries
Maximum number of retries in case of currently not available filesystem of the share containing the data set. Default:11
timing-parameters.failure-interval
Waiting time (in seconds) between retries. Default: 10
chunk-size
Number of data sets deleted together. The task is split into deletion tasks with maximum number of data sets. Default: No chunk size. That is, all data sets to be deleted are deleted in one go.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask
interval = 60
last-seen-data-set-file = lastSeenDataSetForDeletion.txt
ReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Releases data set locks held by dead threads (e.g. threads that have been finished after a thread pool had been shrunk).
## Note
If this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.ReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask
interval = 60
DeleteFromArchiveMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Deletes archived data sets which have been deleted on
AS. This tasks needs the archive plugin to be configured in
service.properties.
## This
task
only
works
with
non
multi
data
set
archivers.
### Configuration
## :
## Property Key
## Description
status-filename
Path to a file which will store the technical ID of the last data set deletion event on AS.
chunk-size
Maximum number of entries deleted in one maintenance task run. Default: Unlimited
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DeleteFromArchiveMaintenanceTask
interval = 3600
status-filename = ../archive-cleanup-status.txt
DeleteFromExternalDBMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Deletes database entries which are related to data sets
deleted in AS. The database is can be any relational database accessible
by DSS.
### Configuration
## :
## Property Key
## Description
data-source
Key of a data source configured in
service.properties
or in a core plugin of type ‘data-sources’. A data source defines the credentials to access the database.
synchronization-table
Name of the table which stores the technical ID of the last data set deletion event on AS. This is ID is used to ask AS for all new data set deletion events. Default value:
## EVENTS
last-seen-event-id-column
Name of the column in the database table defined by property
synchronization-table
which stores the ID of the last data set deletion event. Default value:
## LAST_SEEN_DELETION_EVENT_ID
data-set-table-name
Comma-separated list of table names which contain stuff related to data sets to be deleted. In case of cascading deletion only the tables at the beginning of the cascade should be mentioned. Default value:
image_data_sets
,
analysis_data_sets
.
data-set-perm-id
Name of the column in all tables defined by
data-set-table-name
which stores the data set code. Default value:
## PERM_ID
chunk-size
Maximum number of entries deleted in one maintenance task run. Default: Unlimited
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DeleteFromExternalDBMaintenanceTask
interval = 300
data-source = proteomics-db
data-set-table-name = data_sets
EventsSearchMaintenanceTask

## Environment
## : AS
## Relevancy:
## Default
## Description
: Populates EVENTS_SEARCH database table basing on
entries from EVENTS database table. EVENTS_SEARCH table contains the
same information as EVENTS table but in a more search friendly format
(e.g. a single entry in EVENTS table may represent a deletion of
multiple objects deleted at the same time, in EVENT_SEARCH table such
entry is split into separate entries - one for each deleted object.).
This is set up automatically.
### Configuration:
There are no specific configuration parameters for this task.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.events_search.EventsSearchMaintenanceTask
interval = 1 day
ExperimentBasedArchivingTask

## Environment
## : DSS
## Relevancy:
rare, used when no MultiDataSetArchiver is used and
AutoArchiverTask is too complex.
## Description
: Archives all data sets of experiments which fulfill
some criteria. This tasks needs the archive plugin to be configured in
service.properties
.
### Configuration
## :
## Property Key
## Description
excluded-data-set-types
Comma-separated list of data set types. Data sets of such types are not archived. Default: No data set type is excluded.
estimated-data-set-size-in-KB.
Specifies for the data set type
the average size in KB. If
is DEFAULT it will be used for all data set types with unspecified estimated size.
free-space-provider.class
Fully qualified class name of the free space provider (implementing
ch.systemsx.cisd.common.filesystem.IFreeSpaceProvider
). Depending on the free space provider additional properties, all starting with prefix
free-space-provider
## .,  might be needed. Default:
ch.systemsx.cisd.common.filesystem.SimpleFreeSpaceProvider
monitored-dir
Path to the directory to be monitored by the free space provider.
minimum-free-space-in-MB
Minimum free space in MB. If the free space is below this limit the task archives data sets. Default: 1 GB
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.ExperimentBasedArchivingTask
interval = 86400
minimum-free-space-in-MB = 2048
monitored-dir = /my-data/
estimated-data-set-size-in-KB.RAW_DATA = 12000
estimated-data-set-size-in-KB.DEFAULT = 35000
If there is not enough free space the task archives all data sets
experiment by experiment until free space is above the specified limit.
The oldest experiments are archived first. The age of an experiment is
determined by the youngest modification/registration time stamp of all
its data sets which are not excluded by data set type or archiving
status.
The free space is only calculated once when the task starts to figure
out whether archiving is necessary or not. This value is than used
together with estimated data set sizes to get an estimated free space
which is used for the stopping criteria. Why not calculating the free
space again with the free space provider after the data sets of an
experiment have been archived? The reason is that providing the free
space might be an expensive operation. This is the case when archiving
means removing data from a database which have been fed by data from
data sets of certain type. In this case archiving (i.e. deleting) those
data in the database do not automatically frees disk space because
freeing disk space is for databases often an expensive operation.
The DSS admin will be informed by an e-mail about which experiments have
been archived.
HierarchicalStorageUpdater

## Environment
## : DSS
## Description
: Creates/updates a mirrot of the data store. Data set
are organized hierachical in accordance to their experiment and samples
## Relevancy:
## Deprecated
### Configuration
## :
## Property Key
## Description
storeroot-dir-link-path
Path to the root directory of the store as to be used for creating symbolic links. This should be used if the path to the store as seen by clients is different than seen by DSS.
storeroot-dir
Path to the root directory of the store. Used if storeroot-dir-link-path is not specified.
hierarchy-root-dir
Path to the root directory of mirrored store.
link-naming-strategy.class
Fully qualified class name of the strategy to generate the hierarchy (implementing
ch.systemsx.cisd.etlserver.plugins.IHierarchicalStorageLinkNamingStrategy
). Depending on the actual strategy additional properties, all starting with prefix
link-naming-strategy
## .,  mighty be needed. Default:
ch.systemsx.cisd.etlserver.plugins.TemplateBasedLinkNamingStrategy
link-source-subpath.
Link source subpath for the specified data set type. Only files and folder in this relative path inside a data set will be mirrored. Default: The complete data set folder will be mirroed.
link-from-first-child.
Flag which specifies whether only the first child of or the complete folder (either the data set or the one specified by link-source-subpath.
## ). Default: False
with-meta-data
Flag, which specifies whether directories with meta-data.tsv and a link should be created or only links. The default behavior is to create links-only. Default: false
link-naming-strategy.template
The exact form of link paths produced by TemplateBasedLinkNamingStrategy is defined by this template.
The variables
dataSet
,
dataSetType
,
sample
,
experiment
, project and space will be recognized and replaced in the actual link path.
## Default:
${space}
/
${project}
/
${experiment}
/
${dataSetType}+${sample}+${dataSet}
link-naming-strategy.component-template
If defined, specifies the form of link paths for component datasets. If undefined, component datasets links are formatted with
link-naming-strategy.template
.
Works as
link-naming-strategy.template
, but has these additional variables:
containerDataSetType
,
containerDataSet
, `containerSample.
## Default: Undefined.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.HierarchicalStorageUpdater
storeroot-dir = ${root-dir}
hierarchy-root-dir = ../../mirror
link-naming-strategy.template = ${space}/${project}/${experiment}/${sample}/${dataSetType}-${dataSet}
link-naming-strategy.component-template = ${space}/${project}/${experiment}/${containerSample}/${containerDataSetType}-${containerDataSet}/${dataSetType}-${dataSet}
MultiDataSetDeletionMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Relevant
## Description
: Deletes data sets which are already deleted on AS also
from multi-data-set archives. This maintenance task works only if the
Multi Data Set Archiver
is
configured. It does the following:
Extracts the not-deleted data sets of a TAR container with deleted
data sets into the store.
Marks them as
not present in archive
.
Deletes the TAR containers with deleted data sets.
Requests archiving of the non-deleted data sets.
The last step requires that the maintenance task
ArchivingByRequestTask
is configured.
### Configuration
## :
## Property Key
## Description
last-seen-event-id-file
File which contains the last seen event id.
mapping-file
Optional file which maps data sets to share ids and archiving folders (for details see Mapping File for Share Ids and Archiving Folders). If not specified the first share which has enough free space and which isn’t a unarchiving scratch share will be used for extracting the not-deleted data sets.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetDeletionMaintenanceTask
interval = 1 d
last-seen-event-id-file = ${storeroot-dir}/MultiDataSetDeletionMaintenanceTask-last-seen-event-id.txt
mapping-file = etc/mapping.tsv
## NOTE
: Should be configured on any instance using the multi dataset
archiver when the archive data should be deletable.
MultiDataSetUnarchivingMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Relevant
## Description
: Triggers unarchiving of multi data set archives. Is
only needed if the configuration property
delay-unarchiving
of the
Multi Data Set Archiver
is
set
true
.
This maintenance task allows to reduce the stress of the tape system by
otherwise random unarchiving events triggered by the users.
### Configuration
: No specific properties.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetUnarchivingMaintenanceTask
interval = 1 d
start = 01:00
MultiDataSetArchiveSanityCheckMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Performs sanity check of multi data set archives. The check compares archives stored in final and replica destinations with the
information kept in the path info database. The check verifies: folder structure, file sizes and checksums. In case of found inconsistencies a
notification email is sent to chosen email addresses.
The task loads a list of archives to be checked from the multi data set archiver database. The archives from the list can be checked either
chronologically (the oldest archive is checked first) or can be processed in a random order (see
check-in-random-order
property). The list of
archives can be narrowed down to a specific time window (see
check-from-date
and
check-to-date
properties). During a single run the task can check
all the archives from the list or just a chosen number of archives (see
run-size
property).
The task provides additional properties (see
run-probability
and
run-max-random-delay
) that can be used to introduce some randomness to the time
the task executes. This can be especially handy in cases when we want to have multiple instances of openBIS that share the same configuration for the
task but at the same time we want to desynchronize the task execution among these instances not to cause a peak load on the common archive storage.
## Warning
The task assumes MultiDataSetArchiver task is configured (the
task uses some of the multi data set archiver configuration properties
e.g. final destination location).
### Configuration
## :
## Property Key
## Mandatory
## Default Value
## Description
status-file
true
Path to a JSON file that keeps a list of already checked archive containers
notify-emails
true
List of emails to notify about problematic archive containers
check-from-date
false
null
“From date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm
check-to-date
false
null
“To date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm
check-in-random-order
false
false
If set to “true” then archive containers are checked in a random order. If set to “false” then the containers are checked in chronological order (from the oldest to the newest). Allowed values: true / false.
run-probability
false
1.0
Controls the probability of a task run (0.0 value means the task run will be always skipped, 1.0 value means the task run will always be executed normally). Float values between (0,1.0] are allowed.
run-max-random-delay
false
0
Maximum delay before the run. The actual delay before each run is randomly chosen from range [0, run-max-random-delay]. Different time units are allowed: sec/s, min/m, hours/h, days/d.
run-size
false
-1
Maximum number of archives to be checked in a single run, where -1 means all found archives will be checked. Integer values > 0 or equal to -1 are allowed.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetArchiveSanityCheckMaintenanceTask
interval = 1d
check-from-date = 2023-01-01 00:00:00
check-to-date = 2023-12-31 23:59:59
check-in-random-order = true
run-probability = 0.25
run-max-random-delay = 2h
run-size = 1
notify-emails = test1@email.com, test2@email.com
status-file = ../../multi-dataset-sanity-check-statuses.json
PathInfoDatabaseFeedingTask

## Environment
## : DSS
## Relevancy:
Default, is part of the post registration task
## Description
: Feeds the pathinfo database with file paths of all data
sets in the store. It can be used as a maintenance task as well as a
post registration task. As a maintenance task it is needed to run only
once if a
PostRegistrationMaintenanceTask
is configured. This task
assumes a data source with for ‘path-info-db’.
If used as a maintenance task the data sets are processed in the order
they are registered. The registration time stamp of the last processed
data set is the starting point when the task is executed next time.
### Configuration
## :
## Property Key
## Description
compute-checksum
## If
true
the CRC32 checksum (and optionally a checksum of the type specified by
checksum-type
) of all files will be calculated and stored in pathinfo database. Default value:
false
checksum-type
Optional checksum type. If specified and
compute-checksum
=
true
two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by
MessageDigest.getInstance(<checksum
type>)
. For more details see
Oracle docs
.
data-set-chunk-size
Number of data sets requested from AS in one chunk if it is used as a maintenance task. Default: 1000
max-number-of-chunks
Maximum number of chunks of size data-set-chunk-size are processed if it is used as a maintenance task. If it is <= 0 and
time-limit
isn’t defined all data sets are processed. Default: 0
time-limit
Limit of execution time of this task if it is used as a maintenance task. The task is stopped before reading next chunk if the time has been used up. If it is specified it is an alternative way to limit the number of data sets to be processed instead of specifying
max-number-of-chunks
. This parameter can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask
execute-only-once = true
compute-checksum = true
PostRegistrationMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: A tasks which runs a sequence of so-called
post-registration tasks for each freshly registered data set.
### Configuration
## :
## Property Key
## Description
ignore-data-sets-before-date
Defines a registration date. All data sets registered before this date are ignored. Format:
yyyy-MM-dd
, where
yyyy
is a four-digit year,
## MM
is a two-digit month, and
dd
is a two-digit day. Default value: no restriction.
last-seen-data-set-file
Path to a file which stores the code of the last data set successfully post-registered. Default value:
last-seen-data-set.txt
cleanup-tasks-folder
Path to a folder which stores serialized clean-up tasks always created before a post-registration task is executed. These clean-up tasks are executed on start up of DSS after a server crash. Default value:
clean-up-tasks
post-registration-tasks
Comma-separated list of keys of post-registration task configuration. Each key defines (together with a ‘.’) the prefix of all property keys defining the post-registration task. They are executed in the order their key appear in the list.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.postregistration.PostRegistrationMaintenanceTask
interval = 60
cleanup-tasks-folder = ../cleanup-tasks
ignore-data-sets-before-date = 2011-01-27
last-seen-data-set-file = ../last-seen-data-set
post-registration-tasks = eager-shuffling, eager-archiving
eager-shuffling.class = ch.systemsx.cisd.etlserver.postregistration.EagerShufflingTask
eager-shuffling.share-finder.class = ch.systemsx.cisd.openbis.dss.generic.shared.ExperimentBasedShareFinder
eager-archiving.class = ch.systemsx.cisd.etlserver.postregistration.ArchivingPostRegistrationTask
RevokeUserAccessMaintenanceTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Check if the users are available on the configured
authentication services, if they are not available, are automatically
disabled and their id renamed with the disable date.
For this to work the services should be able to list the available
users. If you use any service that doesn’t allow it, the task
automatically disables itself because is impossible to know if the users
are active or not.
## Service
## Compatible
CrowdAuthenticationService
## NO
DummyAuthenticationService
## NO
NullAuthenticationService
## NO
FileAuthenticationService
## YES
LDAPAuthenticationService
## YES
### Configuration
## :
This maintenance task automatically uses the services already configured
on the server.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.RevokeUserAccessMaintenanceTask
interval = 60 s
UserManagementMaintenanceTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Creates users, spaces, samples, projects and
experiments for all members of an LDAP authorization group or an
explicit list of user ids. A configuration file (in JSON format) will be
read each time this task is executed. All actions are logged in an audit
log file. For more details see
User Group Management for Multi-groups openBIS Instances
### Configuration:
## Property Key
## Description
configuration-file-path
Relative or absolute path to the configuration file. Default:
etc/user-management-maintenance-config.json
audit-log-file-path
Relative or absolute path to the audit log file. Default:
logs/user-management-audit_log.txt
shares-mapping-file-path
Relative or absolute path to the mapping file for data store shares. This is optional. If not specified the mapping file will not be managed by this maintenance task.
filter-key
Key which is used to filter LDAP results. Will be ignored if
ldap-group-query-template
## is specified. Default value:
ou
ldap-group-query-template
Direct LDAP query template. It should have ‘%’ character which will be replaced by an LDAP key as specified in the configuration file.
deactivate-unknown-users
## If
true
a user unknown by the authentication service will be deactivated. It should be set to
false
if no authenication service can be asked (like in Single-Sign-On). Default:
true
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.UserManagementMaintenanceTask
start = 02:42
interval = 1 day
Consistency and other Reports

DataSetArchiverOrphanFinderTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Finds archived data sets which are no longer in openBIS
(at least not marked as present-in-archive). A report will be created
and sent to the specified list of e-mail addresses (mandatory
property
email-addresses
). The task also looks for data sets which are
present-in-archive but actually not found in the archive.
This orphan finder task only works for Multi Data Set Archiver. It
doesn’t work for RsyncArchiver, TarArchiver or ZipArchiver.
### Configuration
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DataSetArchiverOrphanFinderTask
interval = 60 s
email-addresses = email1@bsse.ethz.ch, email2@bsse.ethz.ch
## Notes:
This is a consistency check task. It checks consistency for
datasets with the flag present-in-archive.
DataSetAndPathInfoDBConsistencyCheckTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Checks that the file information in pathinfo database
is consistent with the information the file system provides. This is
done for all recently registered data sets. Note, archived data sets are
skipped. After all data sets (in the specified checking time interval)
have been checked the task checks them again.
### Configuration
## :
## Property Key
## Description
checking-time-interval
Time interval in the past which defines the range of data sets to be checked. That is, all data sets with registration date between now minus checking-time-interval and now will be checked. Can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
. Default value: one day.
pausing-time-point
## Optional time point. Format:
HH:mm
. where
## HH
is a two-digit hour (in 24h notation) and
mm
is a two-digit minute.
When specified this task stops checking after the specified pausing time point and continues when executed the next time or the next day if start or
continuing-time-point
is specified.
After all data sets have been checked the task checks again all data sets started by the oldest one specified by
checking-time-interval
.
continuing-time-point
## Time point where checking continous. Format:
HH:mm
. where
## HH
is a two-digit hour (in 24h notation) and
mm
is a two-digit minute. Ignored when
pausing-time-point
isn’t specified. Default value: Time when the task is executed.
chunk-size
Maximum number of data sets retrieved from AS. Ignored when
pausing-time-point
isn’t specified. Default value: 1000
state-file
File to store registration time stamp and code of last considered data set. This is only used when pausing-time-point has been specified. Default:
<store
root>/DataSetAndPathInfoDBConsistencyCheckTask-state.txt
## Example
: The following example checks all data sets of the last ten
years. It does the check only during the night and continues next night.
plugin.properties
class = ch.systemsx.cisd.etlserver.path.DataSetAndPathInfoDBConsistencyCheckTask
interval = 1 days
start = 23:15
pausing-time-point = 5:00
checking-time-interval = 3653 days
MaterialExternalDBSyncTask

## Environment
## : AS
## Relevancy:
## Deprecated
## Description
: Feeds a report database with recently added or modified
materials.
### Configuration
## :
## Property Key
## Description
database-driver
Fully qualified name of the JDBC driver class.
database-url
URL to access the database server.
database-username
User name of the database. Default: User who started openBIS AS.
database-password
Optional password of the database user.
mapping-file
Path to the file containing configuration information of mapping material types and material properties to tables and columns in the report database.
read-timestamp-sql
The SQL select statement which returns one column of type time stamp for the time stamp of the last report. If the result set is empty the time stamp is assumed to be 1970-01-01. If the result set has more than one row the first row is used.
update-timestamp-sql
The SQL statement which updates or adds a time stamp. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp.
insert-timestamp-sql
The SQL statement to add a time stamp the first time. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp. Default: same as
update-timestamp-sql
.
## Example
## :
service.properties of AS
<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.MaterialExternalDBSyncTask
<task id>.interval = 120
<task id>.read-timestamp-sql = select timestamp from timestamp
<task id>.update-timestamp-sql = update timestamp set timestamp = ?
<task id>.insert-timestamp-sql = insert into timestamp values(?)
<task id>.mapping-file = ../report-mapping.txt
<task id>.database-driver = org.postgresql.Driver
<task id>.database-url = jdbc:postgresql://localhost/material_reporting
## Mapping File

The mapping file is a text file describing the mapping of the data (i.e.
material codes and material properties) onto the report database. It
makes several assumptions on the database schema:
One table per material type. There are only table of materials to be
reported.
Each table has a column which contains the material code.
The entries are unique.
The material code is a string not longer than 60 characters.
Each table has one column for each property type. Again, there are
only column for properties to be reported.
The data type of the column should match the data type of the
## properties:
MATERIAL:  only the material code (string) will be reported.
Maximum length: 60
CONTROLLEDVOCABULARY: the label (if defined) or the code will be
reported. Maximum length: 128
TIMESTAMP: timestamp
INTEGER: integer of any number of bits (maximum 64).
REAL: fixed or floating point number
all other data types are mapped to text.
The general format of the mapping file is as follows:
[
<
## Material
## Type
## Code
>
## :
<
table
## Name
>
,
<
code
column
name
>
]
<
## Property
## Type
## Code
>
## :
<
column
name
>
<
## Property
## Type
## Code
>
## :
<
column
name
>
...
[
<
## Material
## Type
## Code
>
## :
<
table
## Name
>
,
<
code
column
name
>
]
<
## Property
## Type
## Code
>
## :
<
column
name
>
<
## Property
## Type
## Code
>
## :
<
column
name
>
...
## Example:
mapping.txt
# Some comments
[
## GENE
## :
## GENE
,
## GENE_ID
]
## GENE_SYMBOLS
## :
symbol
[
## SIRNA
## :
si_rna
,
code
]
## INHIBITOR_OF
## :
suppressed_gene
## SEQUENCE
## :
Nucleotide_sequence
## Some rules:
Empty lines and lines starting with ‘#’ will be ignored.
Table and column names can be upper or lower case or mixed.
Material type codes and property type codes have to be in upper
case.
## Warning
If you put a foreign key constraint on the material code of one of the material properties, you need to define the constraint checking as DEFERRED in order to not get a constraint violation. The reason is that this task will
not
order the
## INSERT
statements by its dependencies, but in alphabetical order.
### UsageReportingTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Creates a daily/weekly/monthly report to a list of
e-mail recipients about the usage (i.e. creation of experiments, samples
and data sets) by users or groups. For more details see
## User Group
Management for Multi-groups openBIS
## Instances
.
In order to be able to send an e-mail the following properties in
service.properties
have to be defined:
mail
.
from
=
openbis
@<
host
>
mail
.
smtp
.
host
=
<
## SMTP
host
>
mail
.
smtp
.
user
=
<
can
be
empty
>
mail
.
smtp
.
password
=
<
can
be
empty
>
### Configuration
## :
## Property Key
## Description
interval
Determines the length of period: daily if less than or equal one day, weekly if less than or equal seven days, monthly if above seven days. The actual period is always the day/week/month before the execution day
email-addresses
Comma-separated e-mail addresses which will receive the report as an attached text file (format: TSV).
user-reporting-type
Type of reporting individual user activities. Possible values are
NONE: No reporting
ALL: Activities inside and outside groups and for all users
OUTSIDE_GROUP_ONLY: Activities outside groups and users of no groups
Default: ALL
spaces-to-be-ignored
Optional list of comma-separated space codes of all the spaces which should be ignored for the report.
configuration-file-path
Optional configuration file defining groups.
count-all-entities
## If
true
shows the number of all entities (collections, objects, data sets) in an additional column. Default:
false
## Example
## :
### class = ch.systemsx.cisd.openbis.generic.server.task.UsageReportingTask
interval = 7 days
email-addresses = ab@c.de, a@bc.de
PersonalAccessTokenValidityWarningTask

## Environment
## : AS
## Relevancy:
## Rare
### Automatic Configuration
## :
This task is automatically configured, added and run with a default interval of 1 day. If needed, the default interval can be changed. In order to do
that please configure the task just like any other maintenance task in
core-plugins
folder.
## Description
: Sends out warning emails about soon to be expired PATs (Personal Access Tokens). Emails are sent to PATs owners. Each email contains
a list of PATs that have the remaining validity period shorter than the
personal-access-tokens-validity-warning-period
defined in
## AS
service.properties
. The task does not send any information about the already expired PATs. It removes them.
For more details on Personal Access Tokens please see
## Personal Access Tokens
.
In order to be able to send an e-mail the following properties in
service.properties
have to be defined:
mail
.
from
=
openbis
@<
host
>
mail
.
smtp
.
host
=
<
## SMTP
host
>
mail
.
smtp
.
user
=
<
can
be
empty
>
mail
.
smtp
.
password
=
<
can
be
empty
>
## Example
## :
class = ch.systemsx.cisd.openbis.generic.server.pat.PersonalAccessTokenValidityWarningTask
interval = 7 d
Consistency Repair and Manual Migrations

BatchSampleRegistrationTempCodeUpdaterTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Replaces temporary sample codes (i.e. codes matching
the regular expression
TEMP\\.[a-zA-Z0-9\\-]+\\.[0-9]+
) by normal
codes (prefix specified by sample type plus number). This maintenance
task is only needed when
create-continuous-sample-codes
is set
true
in
service.properties
of AS.
## Example
## :
plugin.properties
class
=
ch.systemsx.cisd.openbis.generic.server.task.BatchSampleRegistrationTempCodeUpdaterTask
CleanUpUnarchivingScratchShareTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Removes data sets from the unarchiving scratch share
which have status ARCHIVED and which are present in archive. For more
details see
Multi data set
archiving
.
### Configuration
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.CleanUpUnarchivingScratchShareTask
interval = 60 s
## Notes:
Recommended cleanup task to run on every instance.
DataSetRegistrationSummaryTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Sends a data set summary report to a list of e-mail
recipients in regular time intervals. The report contains all new data
sets registered since the last report. Selected properties can be
included into the report. The data sets are grouped by the data set
type.
In order to be able to send an e-mail the following properties in
service.properties
have to be defined:
mail
.
from
=
openbis
@<
host
>
mail
.
smtp
.
host
=
<
## SMTP
host
>
mail
.
smtp
.
user
=
<
can
be
empty
>
mail
.
smtp
.
password
=
<
can
be
empty
>
### Configuration:
## Property Key
## Description
interval
Interval (in seconds) between regular checks whether to create a report or not. This value should be set to 86400 (1 day). Otherwise the same report might be sent twice or no report will be sent.
start
Time the report will be created. A good values for this parameter is some early time in the morning like in the example below.
days-of-week
Comma-separated list of numbers denoting days of week (Sunday=1, Monday=2, etc.). This parameter should be used if reports should be sent weekly or more often.
days-of-month
Comma-separated list of numbers denoting days of month. Default value of this parameter is 1.
email-addresses
Comma-separated list of e-mail addresses.
shown-data-set-properties
Optional comma-separated list of data set properties to be included into the report.
data-set-types
Restrict the report to the specified comma-separated data set types.
configured-content
Use the specified content as the body of the email.
A report is sent at each day which is either a specified day of week or
day of month. If only weekly reports are needed the parameter
days-of-month
should be set to an empty string.
## Example
## :
service.properties of AS
<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.DataSetRegistrationSummaryTask
<task id>.interval = 86400
<task id>.start = 1:00
<task id>.data-set-types = RAW_DATA, MZXML_DATA
<task id>.email-addresses = albert.einstein@princeton.edu, charles.darwin@evolution.org
This means that on the 1st day of every month at 1:00 AM openBIS sends
to the specified e-mail recipients a report about the data sets of types
RAW_DATA and MZXML_DATA that have been uploaded in the previous month.
DynamicPropertyEvaluationMaintenanceTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Re-evaluates dynamic properties of all entities
### Configuration
## :
## Property Key
## Description
class
ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask
interval = 3600
DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask

## Environment
## : AS
## Relevancy:
## Deprecated
## Description
: Re-evaluates dynamic properties of all samples which
refer via properties of type MATERIAL directly or indirectly to
materials changed since the last re-evaluation.
### Configuration
## :
## Property Key
## Description
class
ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask
timestamp-file
Path to a file which will store the timestamp of the last evaluation. Default value:
../../../data/DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask-timestamp.txt
.
initial-timestamp
Initial timestamp of the form
## YYYY-MM-DD
(e.g. 2013-09-15) which will be used the first time when the timestamp file doesn’t exist or has an invalid value. This is a mandatory property.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask
interval = 7 days
initial-timestamp = 2012-12-31
FillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Queries openBIS database to find data sets without a
size filled in, then queries the pathinfo DB to see if the size info is
available there; if it is available, it fills in the size from the
pathinfo information. If it is not available, it does nothing. Data sets
from openBIS database are fetched in chunks (see data-set-chunk-size
property). After each chunk the maintenance tasks checks whether a time
limit has been reached (see time-limit property). If so, it stops
processing. A code of the last processed data set is stored in a file
(see last-seen-data-set-file property). The next run of the maintenance
task will process data sets with a code greater than the one saved in
the “last-seen-data-set-file”. This file is deleted periodically (see
delete-last-seen-data-set-file-interval) to handle a situation where
codes of new data sets are lexicographically smaller than the codes of
the old datasets. Deleting the file is also needed when pathinfo
database entries are added after a data set has been already processed
by the maintenance task.
### Configuration
## :
## Property Key
## Description
last-seen-data-set-file
Path to a file that will store a code of the last handled data set. Default value: “fillUnknownDataSetSizeTaskLastSeen”
delete-last-seen-data-set-file-interval
A time interval (in seconds) which defines how often the “last-seen-data-set-file” file should be deleted. The parameter can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
. Default value: 7 days.
data-set-chunk-size
Number of data sets requested from AS in one chunk. Default: 100
time-limit
Limit of execution time of this task. The task is stopped before reading next chunk if the time has been used up. This parameter can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
.
## Example:
plugin.properties
<task id>.class = ch.systemsx.cisd.etlserver.plugins.FillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask
<task id>.interval = 86400
<task id>.data-set-chunk-size = 1000
<task id>.time-limit = 1h
## NOTE
: Useful in scenarios where the path info feeding sub task of
post registration task fails.
PathInfoDatabaseChecksumCalculationTask

## Environment
## : DSS
## Relevancy:
Rare, often the CRC32 is calculated during the post
registration.
## Description
: Calculates the CRC32 checksum (and optionally a
checksum of specified type) of all files in the pathinfo database with
unknown checksum. This task is needed to run only once. It assumes a
data source for key ‘path-info-db’.
### Configuration
## :
## Property Key
## Description
checksum-type
Optional checksum type. If specified two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by
MessageDigest.getInstance(<checksum
type>)
. For more details see http://docs.oracle.com/javase/8/docs/api/java/security/MessageDigest.html#getInstance-java.lang.String-.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseChecksumCalculationTask
execute-only-once = true
checksum-type = SHA-256
PathInfoDatabaseRefreshingTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Refreshes the pathinfo database with file metadata of
physical and available data sets in the store. This task assumes a data
source with for ‘path-info-db’.
The data sets are processed in the inverse order they are registered.
Only a maximum number of data sets are processed in one run. This is
specified by
chunk-size
.
## Warning
Under normal circumstances this maintenance task is never needed, because the content of a physical data set is
never
changed by openBIS itself.
Only in the rare cases that the content of physical data sets have to be changed this maintenance task allows to refresh the file meta data in the pathinfo database.
### Configuration
## :
## Property Key
## Description
time-stamp-of-youngest-data-set
Time stamp of the youngest data set to be considered. The format has to be
<4
digit
year>-<month>-<day>
<hour>:<minute>:<second>
.
compute-checksum
## If
true
the CRC32 checksum (and optionally a checksum of the type specified by
checksum-type
) of all files will be calculated and stored in pathinfo database. Default value: true
checksum-type
Optional checksum type. If specified and
compute-checksum
=
true
two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by
MessageDigest.getInstance(<checksum
type>)
. For more details see
Oracle doc
.
chunk-size
Number of data sets requested from AS in one chunk. Default: 1000
data-set-type
Optional data set type. If specified, only data sets of the specified type are considered. Default: All data set types.
state-file
File to store registration time stamp and code of last considered data set. Default:
<store
root>/PathInfoDatabaseRefreshingTask-state.txt
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseRefreshingTask
interval = 30 min
time-stamp-of-youngest-data-set = 2014-01-01 00:00:00
data-set-type = HCS_IMAGE
RemoveUnusedUnofficialTermsMaintenanceTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Removes unofficial unused vocabulary terms. For more details about unofficial vocabulary terms see
## Ad Hoc Vocabulary Terms
.
### Configuration:
## Property Key
## Description
older-than-days
Unofficial terms are only deleted if they have been registered more than the specified number of days ago. Default: 7 days.
## Example
## :
service.properties of AS
<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.RemoveUnusedUnofficialTermsMaintenanceTask
<task id>.interval = 86400
<task id>.older-than-days = 30
ResetArchivePendingTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: For each data set not present in archive and status
ARCHIVE_PENDING the status will be set to AVAILABLE if there is no
command in the DSS data set command queues referring to it.
### Configuration
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.ResetArchivePendingTask
interval = 60 s
SessionWorkspaceCleanUpMaintenanceTask

## Environment
## : AS
## Relevancy:
## Default
## Description
: Cleans up session workspace folders of no longer active
sessions. This maintenance plugin is automatically added by default with
a default interval of 1 hour. If a manually configured version of the
plugin is detected then the automatic configuration is skipped.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.SessionWorkspaceCleanUpMaintenanceTask
interval = 1 day
MaterialsMigration

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Migrates the Materials entities and types to use a
Sample based model using Sample Properties. It automatically creates and
assigns sample types, properties and entities.
It allows to execute the migration and to delete of the old Materials
model in separate steps.
Deleting Materials and material types requires the migration to have
been a success,  before the deletion a validation check is run.
## Example
## :
This maintenance task can be directly configured on the AS
service.properties
service.properties
maintenance-plugins = materials-migration",Maintenance Tasks,0,en_20.10.0-11_system-documentation_configuration_maintenance-tasks_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_maintenance-tasks.txt,2025-09-30T12:09:04.429877Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_maintenance-tasks:1,Maintenance Tasks,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/maintenance-tasks.html,openbis,"## Maintenance Tasks

## Maintenance Task Classification

## Category
## Feature
Consistency and other Reports
Consistency Repair and Manual Migrations
## Relevancy
## Default
## Relevant
## Rare
## Deprecated
## Introduction

A maintenance task is a process which runs once or in regular time intervals. It is defined by a
core plugin
of type
maintenance-tasks
. Usually a maintenance task can only run on AS or DSS but not in both environments.
The following properties are common for all maintenance tasks:
## Property Key
## Description
class
The fully-qualified Java class name of the maintenance task. The class has to implement IMaintenanceTask.
execute-only-once
A flag which has to be set to true if the task should be executed only once. Default value: false
interval
A time interval (in seconds) which defines the pace of execution of the maintenance task. Can be specified with one of the following time units: ms, msec, s, sec, m, min, h, hours, d, days. Default time unit is sec. Default value: one day.
start
A time at which the task should be executed the first time. Format: HH:mm. where HH is a two-digit hour (in 24h notation) and mm is a two-digit minute. By default the task is execute at server startup.
run-schedule
Scheduling plan for task execution. Properties execute-only-once, interval, and start will be ignored if specified.
## Crontab syntax:
## cron:
<second>
<minute>
<hour>
<day>
<month>
<weekday>
## Examples:
## cron:
0
0
*
*
*
*
: the top of every hour of every day.
## cron:
*/10
*
*
*
*
*
: every ten seconds.
## cron:
0
0
8-10
*
*
*
: 8, 9 and 10 o’clock of every day.
## cron:
0
0
6,19
*
*
*
: 6:00 AM and 7:00 PM every day.
## cron:
0
0/30
8-10
*
*
*
: 8:00, 8:30, 9:00, 9:30, 10:00 and 10:30 every day.
## cron:
0
0
9-17
*
*
## MON-FRI
: on the hour nine-to-five weekdays.
## cron:
0
0
0
25
12
?
: every Christmas Day at midnight.
## Non-crontab syntax:
Comma-separated list of definitions with following syntax:
[[<counter>.]<week
day>]
[<month
day>[.<month>]]
<hour>[:<minute>]
where
<counter>
counts the specified week day of the month.
<week
day>
is
## MO
,
## MON
,
## TU
,
## TUE
,
## WE
,
## WED
,
## TH
,
## THU
,
## FR
,
## FRI
,
## SA
,
## SAT
,
## SU
, or
## SUN
(ignoring case).
<month>
is either the month number (followed by an optionl ‘.’) or
## JAN
,
## FEB
,
## MAR
,
## APR
,
## MAY
,
## JUN
,
## JUL
,
## AUG
,
## SEP
,
## OCT
,
## NOV
, or
## DEC
(ignoring case).
## Examples:
6,
18
: every day at 6 AM and 6 PM.
## 3.FR
22:15
: every third friday of a month at 22:15.
1.
15:50
: every first day of a month at 3:50 PM.
## SAT
1:30
: every saturday at 1:30 AM.
## 1.Jan
5:15,
1.4.
5:15,
1.7
5:15,
1.
## OCT
5:15
: every first day of a quarter at 5:15 AM.
run-schedule-file
File where the timestamp for next execution is stored. It is used if run-schedule is specified. Default:
<installation
folder>/<plugin
name>_<class
name>
retry-intervals-after-failure
Optional comma-separated list of time intervals (format as for interval) after which a failed execution will be retried. Note, that a maintenance task will be execute always when the next scheduled timepoint occurs. This feature allows to execute a task much earlier in case of temporary errors (e.g. temporary unavailibity of another server).
## Feature

ArchivingByRequestTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Triggers archiving for data sets where the ‘requested
archiving’ flag is set. Waits with archiving until enough data sets for
a group come together. This is necessary for taped-base archiving where
the files to be stored have to be larger than a minimum size.
### Configuration
## :
## Property Key
## Description
keep-in-store
If true the archived data set will not be removed from the store. That is, only a backup will be created. Default: false
minimum-container-size-in-bytes
Minimum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 10 GB
maximum-container-size-in-bytes
Maximum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 80 GB
configuration-file-path
Path to the configuration file as used by User Group Management. Here only the group keys are needed. They define a set of groups. If there is no configuration file at the specified path this set is empty.A data set requested for archiving belongs the a specified group if its space starts with the group key followed by an underscore character ‘_’. Otherwise it belongs to no group. This maintenance task triggers archiving an archive container with one or more data set from the same group if the container fits the specified minimum and maximum size. Note, that data sets which do not belong to a group are handled as a group too. If a data set is larger than the maximum container size it will be archived even though the container is to large. The group key (in lower case) is provided to the archiver. The Multi Data Set Archiver will use this for storing the archive container in a sub folder of the same name.
## Default:
etc/user-management-maintenance-config.json
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.ArchivingByRequestTask
interval = 1 d
minimum-container-size-in-bytes =  20000000000
maximum-container-size-in-bytes = 200000000000
configuration-file-path = ../../../data/groups.json
## Notes:
In practice every instance using multi dataset archiving
feature and also the ELN-LIMS should have this enabled.
AutoArchiverTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Triggers archiving of data sets that have not been
archived yet.
### Configuration
## :
## Property Key
## Description
remove-datasets-from-store
If true the archived data set will be removed from the store. Default: false
data-set-type
Data set type of the data sets to be archived. If undefined all data set of all types might be archived.
older-than
Minimum number of days a data set to be archived hasn’t been accessed. Default: 30
archive-candidate-discoverer.class
Discoverer of candidates to be archived:
ch.systemsx.cisd.etlserver.plugins.AgeArchiveCandidateDiscoverer
: All data sets with an access time stamp older than specified by property older-than are candidates. This is the default discoverer.
ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer
: All data sets which are marked by one of the tags specified by the property
archive-candidate-discoverer.tags
are candidates.
policy.class
A policy specifies which data set candidates should be archived. If undefined all candidates will be archived. Has to be a fully-qualified name of a Java class implementing ch.systemsx.cisd.etlserver.IAutoArchiverPolicy.
policy.*
Properties specific for the policy specified by
policy.class
. More about policies can be found here.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask
interval = 10 days
archive-candidate-discoverer.class = ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer
archive-candidate-discoverer.tags = /admin-user/archive
policy.class = ch.systemsx.cisd.etlserver.plugins.GroupingPolicy
policy.minimal-archive-size = 1500000
policy.maximal-archive-size = 3000000
policy.grouping-keys = Space#DataSetType, Space#Experiment:merge
BlastDatabaseCreationMaintenanceTask

## Environment
## : DSS
## Relevancy:
Default (ELN-LIMS)
## Description
: Creates BLAST databases from FASTA and FASTQ files of
data sets and/or properties of experiments, samples, and data sets.
The title of all entries of the FASTA and FASTQ files will be extended
by the string
## [Data
## set:
<data
set
code>,
## File:
<path>]
## . Sequences
provide by an entity property will have identifiers of the form
<entity
kind>+<perm
id>+<property
type>+<time
stamp>
. This allows to
determine where the matching sequences are stored in openBIS. A sequence
can be a nucleic acid sequence or an amino acid sequence.
For each data set a BLAST nucl and prot databases will be created (if
not empty) by the tool
makeblastdb
. For all entities of a specified
kind and type one BLAST database (one for nucleic sequences and one
for amino acid sequences) will be created from the plain sequences
stored in the specified property (white spaces will be removed). In
addition an index is created by the tool
makembindex
if the sequence
file of the database (file type
.nsq
) is larger than 1MB. The name of
the databases are
<data
set
code>-nucl/prot
and
<entity
kind>+<entity
type
code>+<property
type
code>+<time
stamp>-nucl/prot
.
These databases are referred in the virtual database
all-nucl
## (file:
all-nucl.nal
) and
all-prot
## (file:
all-prot.pal
).
If a data set is deleted the corresponding BLAST nucl and prot databases
will be automatically removed the next time this maintenance task runs.
If an entity of specified type has been modified the BLAST databases
will be recalculated the next time this maintenance task runs.
Works only if BLAST+ tool suite has been installed. BLAST+ can be
downloaded from
ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/
## Notes:
It comes pre-configured with the ELN-LIMS but if additional
properties need to scanned they should be added to the plugin.properties
### Configuration
## :
## Property Key
## Description
dataset-types
Comma-separated list of regular expressions of data set types. All FASTA and FASTQ files from those data sets are handled. All data sets of types not matching at least one of the regular expression are not handled.
entity-sequence-properties
Comma-separated list of descriptions of entity properties with sequences. A description is of the form
<entity
kind>+<entity
type
code>+<property
type
code>
where
<entity
kind>
is either
## EXPERIMENT
,
## SAMPLE
or
## DATA_SET
(Materials are not supported).
file-types
Space separated list of file types. Data set files of those file types have to be FASTA or FASTQ files. Default:
.fasta
.fa
.fsa
.fastq
blast-tools-directory
Path in the file system where all BLAST tools are located. If it is not specified or empty the tools directory has to be in the PATH environment variable.
blast-databases-folder
Path to the folder where all BLAST databases are stored. Default:
<data
store
root>/blast-databases
blast-temp-folder
Path to the folder where temporary FASTA files are stored.  Default:
<blast-databases-folder>/tmp
last-seen-data-set-file
Path to the file which stores the id of the last seen data set. Default:
<data
store
root>/last-seen-data-set-for-BLAST-database-creation
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.BlastDatabaseCreationMaintenanceTask
interval = 1 h
dataset-types = BLAST-.+
entity-sequence-properties = SAMPLE+OLIGO+SEQUENCE, EXPERIMENT+YEAST+PLASMID_SEQUENCE
blast-tools-directory = /usr/local/ncbi/blast/bin
DeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Deletes data sets which have been deleted on AS.
## Note
If this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.
### Configuration
## :
## Property Key
## Description
last-seen-data-set-file
Path to a file which will store the code of the last data set handled. Default:
deleteDatasetsAlreadyDeletedFromApplicationServerTaskLastSeen
timing-parameters.max-retries
Maximum number of retries in case of currently not available filesystem of the share containing the data set. Default:11
timing-parameters.failure-interval
Waiting time (in seconds) between retries. Default: 10
chunk-size
Number of data sets deleted together. The task is split into deletion tasks with maximum number of data sets. Default: No chunk size. That is, all data sets to be deleted are deleted in one go.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask
interval = 60
last-seen-data-set-file = lastSeenDataSetForDeletion.txt
ReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Releases data set locks held by dead threads (e.g. threads that have been finished after a thread pool had been shrunk).
## Note
If this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.ReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask
interval = 60
DeleteFromArchiveMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Deletes archived data sets which have been deleted on
AS. This tasks needs the archive plugin to be configured in
service.properties.
## This
task
only
works
with
non
multi
data
set
archivers.
### Configuration
## :
## Property Key
## Description
status-filename
Path to a file which will store the technical ID of the last data set deletion event on AS.
chunk-size
Maximum number of entries deleted in one maintenance task run. Default: Unlimited
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DeleteFromArchiveMaintenanceTask
interval = 3600
status-filename = ../archive-cleanup-status.txt
DeleteFromExternalDBMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Deletes database entries which are related to data sets
deleted in AS. The database is can be any relational database accessible
by DSS.
### Configuration
## :
## Property Key
## Description
data-source
Key of a data source configured in
service.properties
or in a core plugin of type ‘data-sources’. A data source defines the credentials to access the database.
synchronization-table
Name of the table which stores the technical ID of the last data set deletion event on AS. This is ID is used to ask AS for all new data set deletion events. Default value:
## EVENTS
last-seen-event-id-column
Name of the column in the database table defined by property
synchronization-table
which stores the ID of the last data set deletion event. Default value:
## LAST_SEEN_DELETION_EVENT_ID
data-set-table-name
Comma-separated list of table names which contain stuff related to data sets to be deleted. In case of cascading deletion only the tables at the beginning of the cascade should be mentioned. Default value:
image_data_sets
,
analysis_data_sets
.
data-set-perm-id
Name of the column in all tables defined by
data-set-table-name
which stores the data set code. Default value:
## PERM_ID
chunk-size
Maximum number of entries deleted in one maintenance task run. Default: Unlimited
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DeleteFromExternalDBMaintenanceTask
interval = 300
data-source = proteomics-db
data-set-table-name = data_sets
EventsSearchMaintenanceTask

## Environment
## : AS
## Relevancy:
## Default
## Description
: Populates EVENTS_SEARCH database table basing on
entries from EVENTS database table. EVENTS_SEARCH table contains the
same information as EVENTS table but in a more search friendly format
(e.g. a single entry in EVENTS table may represent a deletion of
multiple objects deleted at the same time, in EVENT_SEARCH table such
entry is split into separate entries - one for each deleted object.).
This is set up automatically.
### Configuration:
There are no specific configuration parameters for this task.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.events_search.EventsSearchMaintenanceTask
interval = 1 day
ExperimentBasedArchivingTask

## Environment
## : DSS
## Relevancy:
rare, used when no MultiDataSetArchiver is used and
AutoArchiverTask is too complex.
## Description
: Archives all data sets of experiments which fulfill
some criteria. This tasks needs the archive plugin to be configured in
service.properties
.
### Configuration
## :
## Property Key
## Description
excluded-data-set-types
Comma-separated list of data set types. Data sets of such types are not archived. Default: No data set type is excluded.
estimated-data-set-size-in-KB.
Specifies for the data set type
the average size in KB. If
is DEFAULT it will be used for all data set types with unspecified estimated size.
free-space-provider.class
Fully qualified class name of the free space provider (implementing
ch.systemsx.cisd.common.filesystem.IFreeSpaceProvider
). Depending on the free space provider additional properties, all starting with prefix
free-space-provider
## .,  might be needed. Default:
ch.systemsx.cisd.common.filesystem.SimpleFreeSpaceProvider
monitored-dir
Path to the directory to be monitored by the free space provider.
minimum-free-space-in-MB
Minimum free space in MB. If the free space is below this limit the task archives data sets. Default: 1 GB
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.ExperimentBasedArchivingTask
interval = 86400
minimum-free-space-in-MB = 2048
monitored-dir = /my-data/
estimated-data-set-size-in-KB.RAW_DATA = 12000
estimated-data-set-size-in-KB.DEFAULT = 35000
If there is not enough free space the task archives all data sets
experiment by experiment until free space is above the specified limit.
The oldest experiments are archived first. The age of an experiment is
determined by the youngest modification/registration time stamp of all
its data sets which are not excluded by data set type or archiving
status.
The free space is only calculated once when the task starts to figure
out whether archiving is necessary or not. This value is than used
together with estimated data set sizes to get an estimated free space
which is used for the stopping criteria. Why not calculating the free
space again with the free space provider after the data sets of an
experiment have been archived? The reason is that providing the free
space might be an expensive operation. This is the case when archiving
means removing data from a database which have been fed by data from
data sets of certain type. In this case archiving (i.e. deleting) those
data in the database do not automatically frees disk space because
freeing disk space is for databases often an expensive operation.
The DSS admin will be informed by an e-mail about which experiments have
been archived.
HierarchicalStorageUpdater

## Environment
## : DSS
## Description
: Creates/updates a mirrot of the data store. Data set
are organized hierachical in accordance to their experiment and samples
## Relevancy:
## Deprecated
### Configuration
## :
## Property Key
## Description
storeroot-dir-link-path
Path to the root directory of the store as to be used for creating symbolic links. This should be used if the path to the store as seen by clients is different than seen by DSS.
storeroot-dir
Path to the root directory of the store. Used if storeroot-dir-link-path is not specified.
hierarchy-root-dir
Path to the root directory of mirrored store.
link-naming-strategy.class
Fully qualified class name of the strategy to generate the hierarchy (implementing
ch.systemsx.cisd.etlserver.plugins.IHierarchicalStorageLinkNamingStrategy
). Depending on the actual strategy additional properties, all starting with prefix
link-naming-strategy
## .,  mighty be needed. Default:
ch.systemsx.cisd.etlserver.plugins.TemplateBasedLinkNamingStrategy
link-source-subpath.
Link source subpath for the specified data set type. Only files and folder in this relative path inside a data set will be mirrored. Default: The complete data set folder will be mirroed.
link-from-first-child.
Flag which specifies whether only the first child of or the complete folder (either the data set or the one specified by link-source-subpath.
## ). Default: False
with-meta-data
Flag, which specifies whether directories with meta-data.tsv and a link should be created or only links. The default behavior is to create links-only. Default: false
link-naming-strategy.template
The exact form of link paths produced by TemplateBasedLinkNamingStrategy is defined by this template.
The variables
dataSet
,
dataSetType
,
sample
,
experiment
, project and space will be recognized and replaced in the actual link path.
## Default:
${space}
/
${project}
/
${experiment}
/
${dataSetType}+${sample}+${dataSet}
link-naming-strategy.component-template
If defined, specifies the form of link paths for component datasets. If undefined, component datasets links are formatted with
link-naming-strategy.template
.
Works as
link-naming-strategy.template
, but has these additional variables:
containerDataSetType
,
containerDataSet
, `containerSample.
## Default: Undefined.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.HierarchicalStorageUpdater
storeroot-dir = ${root-dir}
hierarchy-root-dir = ../../mirror
link-naming-strategy.template = ${space}/${project}/${experiment}/${sample}/${dataSetType}-${dataSet}
link-naming-strategy.component-template = ${space}/${project}/${experiment}/${containerSample}/${containerDataSetType}-${containerDataSet}/${dataSetType}-${dataSet}
MultiDataSetDeletionMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Relevant
## Description
: Deletes data sets which are already deleted on AS also
from multi-data-set archives. This maintenance task works only if the
Multi Data Set Archiver
is
configured. It does the following:
Extracts the not-deleted data sets of a TAR container with deleted
data sets into the store.
Marks them as
not present in archive
.
Deletes the TAR containers with deleted data sets.
Requests archiving of the non-deleted data sets.
The last step requires that the maintenance task
ArchivingByRequestTask
is configured.
### Configuration
## :
## Property Key
## Description
last-seen-event-id-file
File which contains the last seen event id.
mapping-file
Optional file which maps data sets to share ids and archiving folders (for details see Mapping File for Share Ids and Archiving Folders). If not specified the first share which has enough free space and which isn’t a unarchiving scratch share will be used for extracting the not-deleted data sets.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetDeletionMaintenanceTask
interval = 1 d
last-seen-event-id-file = ${storeroot-dir}/MultiDataSetDeletionMaintenanceTask-last-seen-event-id.txt
mapping-file = etc/mapping.tsv
## NOTE
: Should be configured on any instance using the multi dataset
archiver when the archive data should be deletable.
MultiDataSetUnarchivingMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Relevant
## Description
: Triggers unarchiving of multi data set archives. Is
only needed if the configuration property
delay-unarchiving
of the
Multi Data Set Archiver
is
set
true
.
This maintenance task allows to reduce the stress of the tape system by
otherwise random unarchiving events triggered by the users.
### Configuration
: No specific properties.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetUnarchivingMaintenanceTask
interval = 1 d
start = 01:00
MultiDataSetArchiveSanityCheckMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Performs sanity check of multi data set archives. The check compares archives stored in final and replica destinations with the
information kept in the path info database. The check verifies: folder structure, file sizes and checksums. In case of found inconsistencies a
notification email is sent to chosen email addresses.
The task loads a list of archives to be checked from the multi data set archiver database. The archives from the list can be checked either
chronologically (the oldest archive is checked first) or can be processed in a random order (see
check-in-random-order
property). The list of
archives can be narrowed down to a specific time window (see
check-from-date
and
check-to-date
properties). During a single run the task can check
all the archives from the list or just a chosen number of archives (see
run-size
property).
The task provides additional properties (see
run-probability
and
run-max-random-delay
) that can be used to introduce some randomness to the time
the task executes. This can be especially handy in cases when we want to have multiple instances of openBIS that share the same configuration for the
task but at the same time we want to desynchronize the task execution among these instances not to cause a peak load on the common archive storage.
## Warning
The task assumes MultiDataSetArchiver task is configured (the
task uses some of the multi data set archiver configuration properties
e.g. final destination location).
### Configuration
## :
## Property Key
## Mandatory
## Default Value
## Description
status-file
true
Path to a JSON file that keeps a list of already checked archive containers
notify-emails
true
List of emails to notify about problematic archive containers
check-from-date
false
null
“From date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm
check-to-date
false
null
“To date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm
check-in-random-order
false
false
If set to “true” then archive containers are checked in a random order. If set to “false” then the containers are checked in chronological order (from the oldest to the newest). Allowed values: true / false.
run-probability
false
1.0
Controls the probability of a task run (0.0 value means the task run will be always skipped, 1.0 value means the task run will always be executed normally). Float values between (0,1.0] are allowed.
run-max-random-delay
false
0
Maximum delay before the run. The actual delay before each run is randomly chosen from range [0, run-max-random-delay]. Different time units are allowed: sec/s, min/m, hours/h, days/d.
run-size
false
-1
Maximum number of archives to be checked in a single run, where -1 means all found archives will be checked. Integer values > 0 or equal to -1 are allowed.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetArchiveSanityCheckMaintenanceTask
interval = 1d
check-from-date = 2023-01-01 00:00:00
check-to-date = 2023-12-31 23:59:59
check-in-random-order = true
run-probability = 0.25
run-max-random-delay = 2h
run-size = 1
notify-emails = test1@email.com, test2@email.com
status-file = ../../multi-dataset-sanity-check-statuses.json
PathInfoDatabaseFeedingTask

## Environment
## : DSS
## Relevancy:
Default, is part of the post registration task
## Description
: Feeds the pathinfo database with file paths of all data
sets in the store. It can be used as a maintenance task as well as a
post registration task. As a maintenance task it is needed to run only
once if a
PostRegistrationMaintenanceTask
is configured. This task
assumes a data source with for ‘path-info-db’.
If used as a maintenance task the data sets are processed in the order
they are registered. The registration time stamp of the last processed
data set is the starting point when the task is executed next time.
### Configuration
## :
## Property Key
## Description
compute-checksum
## If
true
the CRC32 checksum (and optionally a checksum of the type specified by
checksum-type
) of all files will be calculated and stored in pathinfo database. Default value:
false
checksum-type
Optional checksum type. If specified and
compute-checksum
=
true
two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by
MessageDigest.getInstance(<checksum
type>)
. For more details see
Oracle docs
.
data-set-chunk-size
Number of data sets requested from AS in one chunk if it is used as a maintenance task. Default: 1000
max-number-of-chunks
Maximum number of chunks of size data-set-chunk-size are processed if it is used as a maintenance task. If it is <= 0 and
time-limit
isn’t defined all data sets are processed. Default: 0
time-limit
Limit of execution time of this task if it is used as a maintenance task. The task is stopped before reading next chunk if the time has been used up. If it is specified it is an alternative way to limit the number of data sets to be processed instead of specifying
max-number-of-chunks
. This parameter can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask
execute-only-once = true
compute-checksum = true
PostRegistrationMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: A tasks which runs a sequence of so-called
post-registration tasks for each freshly registered data set.
### Configuration
## :
## Property Key
## Description
ignore-data-sets-before-date
Defines a registration date. All data sets registered before this date are ignored. Format:
yyyy-MM-dd
, where
yyyy
is a four-digit year,
## MM
is a two-digit month, and
dd
is a two-digit day. Default value: no restriction.
last-seen-data-set-file
Path to a file which stores the code of the last data set successfully post-registered. Default value:
last-seen-data-set.txt
cleanup-tasks-folder
Path to a folder which stores serialized clean-up tasks always created before a post-registration task is executed. These clean-up tasks are executed on start up of DSS after a server crash. Default value:
clean-up-tasks
post-registration-tasks
Comma-separated list of keys of post-registration task configuration. Each key defines (together with a ‘.’) the prefix of all property keys defining the post-registration task. They are executed in the order their key appear in the list.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.postregistration.PostRegistrationMaintenanceTask
interval = 60
cleanup-tasks-folder = ../cleanup-tasks
ignore-data-sets-before-date = 2011-01-27
last-seen-data-set-file = ../last-seen-data-set
post-registration-tasks = eager-shuffling, eager-archiving
eager-shuffling.class = ch.systemsx.cisd.etlserver.postregistration.EagerShufflingTask
eager-shuffling.share-finder.class = ch.systemsx.cisd.openbis.dss.generic.shared.ExperimentBasedShareFinder
eager-archiving.class = ch.systemsx.cisd.etlserver.postregistration.ArchivingPostRegistrationTask
RevokeUserAccessMaintenanceTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Check if the users are available on the configured
authentication services, if they are not available, are automatically
disabled and their id renamed with the disable date.
For this to work the services should be able to list the available
users. If you use any service that doesn’t allow it, the task
automatically disables itself because is impossible to know if the users
are active or not.
## Service
## Compatible
CrowdAuthenticationService
## NO
DummyAuthenticationService
## NO
NullAuthenticationService
## NO
FileAuthenticationService
## YES
LDAPAuthenticationService
## YES
### Configuration
## :
This maintenance task automatically uses the services already configured
on the server.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.RevokeUserAccessMaintenanceTask
interval = 60 s
UserManagementMaintenanceTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Creates users, spaces, samples, projects and
experiments for all members of an LDAP authorization group or an
explicit list of user ids. A configuration file (in JSON format) will be
read each time this task is executed. All actions are logged in an audit
log file. For more details see
User Group Management for Multi-groups openBIS Instances
### Configuration:
## Property Key
## Description
configuration-file-path
Relative or absolute path to the configuration file. Default:
etc/user-management-maintenance-config.json
audit-log-file-path
Relative or absolute path to the audit log file. Default:
logs/user-management-audit_log.txt
shares-mapping-file-path
Relative or absolute path to the mapping file for data store shares. This is optional. If not specified the mapping file will not be managed by this maintenance task.
filter-key
Key which is used to filter LDAP results. Will be ignored if
ldap-group-query-template
## is specified. Default value:
ou
ldap-group-query-template
Direct LDAP query template. It should have ‘%’ character which will be replaced by an LDAP key as specified in the configuration file.
deactivate-unknown-users
## If
true
a user unknown by the authentication service will be deactivated. It should be set to
false
if no authenication service can be asked (like in Single-Sign-On). Default:
true
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.UserManagementMaintenanceTask
start = 02:42
interval = 1 day
Consistency and other Reports

DataSetArchiverOrphanFinderTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Finds archived data sets which are no longer in openBIS
(at least not marked as present-in-archive). A report will be created
and sent to the specified list of e-mail addresses (mandatory
property
email-addresses
). The task also looks for data sets which are
present-in-archive but actually not found in the archive.
This orphan finder task only works for Multi Data Set Archiver. It
doesn’t work for RsyncArchiver, TarArchiver or ZipArchiver.
### Configuration
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.DataSetArchiverOrphanFinderTask
interval = 60 s
email-addresses = email1@bsse.ethz.ch, email2@bsse.ethz.ch
## Notes:
This is a consistency check task. It checks consistency for
datasets with the flag present-in-archive.
DataSetAndPathInfoDBConsistencyCheckTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Checks that the file information in pathinfo database
is consistent with the information the file system provides. This is
done for all recently registered data sets. Note, archived data sets are
skipped. After all data sets (in the specified checking time interval)
have been checked the task checks them again.
### Configuration
## :
## Property Key
## Description
checking-time-interval
Time interval in the past which defines the range of data sets to be checked. That is, all data sets with registration date between now minus checking-time-interval and now will be checked. Can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
. Default value: one day.
pausing-time-point
## Optional time point. Format:
HH:mm
. where
## HH
is a two-digit hour (in 24h notation) and
mm
is a two-digit minute.
When specified this task stops checking after the specified pausing time point and continues when executed the next time or the next day if start or
continuing-time-point
is specified.
After all data sets have been checked the task checks again all data sets started by the oldest one specified by
checking-time-interval
.
continuing-time-point
## Time point where checking continous. Format:
HH:mm
. where
## HH
is a two-digit hour (in 24h notation) and
mm
is a two-digit minute. Ignored when
pausing-time-point
isn’t specified. Default value: Time when the task is executed.
chunk-size
Maximum number of data sets retrieved from AS. Ignored when
pausing-time-point
isn’t specified. Default value: 1000
state-file
File to store registration time stamp and code of last considered data set. This is only used when pausing-time-point has been specified. Default:
<store
root>/DataSetAndPathInfoDBConsistencyCheckTask-state.txt
## Example
: The following example checks all data sets of the last ten
years. It does the check only during the night and continues next night.
plugin.properties
class = ch.systemsx.cisd.etlserver.path.DataSetAndPathInfoDBConsistencyCheckTask
interval = 1 days
start = 23:15
pausing-time-point = 5:00
checking-time-interval = 3653 days
MaterialExternalDBSyncTask

## Environment
## : AS
## Relevancy:
## Deprecated
## Description
: Feeds a report database with recently added or modified
materials.
### Configuration
## :
## Property Key
## Description
database-driver
Fully qualified name of the JDBC driver class.
database-url
URL to access the database server.
database-username
User name of the database. Default: User who started openBIS AS.
database-password
Optional password of the database user.
mapping-file
Path to the file containing configuration information of mapping material types and material properties to tables and columns in the report database.
read-timestamp-sql
The SQL select statement which returns one column of type time stamp for the time stamp of the last report. If the result set is empty the time stamp is assumed to be 1970-01-01. If the result set has more than one row the first row is used.
update-timestamp-sql
The SQL statement which updates or adds a time stamp. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp.
insert-timestamp-sql
The SQL statement to add a time stamp the first time. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp. Default: same as
update-timestamp-sql
.
## Example
## :
service.properties of AS
<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.MaterialExternalDBSyncTask
<task id>.interval = 120
<task id>.read-timestamp-sql = select timestamp from timestamp
<task id>.update-timestamp-sql = update timestamp set timestamp = ?
<task id>.insert-timestamp-sql = insert into timestamp values(?)
<task id>.mapping-file = ../report-mapping.txt
<task id>.database-driver = org.postgresql.Driver
<task id>.database-url = jdbc:postgresql://localhost/material_reporting
## Mapping File

The mapping file is a text file describing the mapping of the data (i.e.
material codes and material properties) onto the report database. It
makes several assumptions on the database schema:
One table per material type. There are only table of materials to be
reported.
Each table has a column which contains the material code.
The entries are unique.
The material code is a string not longer than 60 characters.
Each table has one column for each property type. Again, there are
only column for properties to be reported.
The data type of the column should match the data type of the
## properties:
MATERIAL:  only the material code (string) will be reported.
Maximum length: 60
CONTROLLEDVOCABULARY: the label (if defined) or the code will be
reported. Maximum length: 128
TIMESTAMP: timestamp
INTEGER: integer of any number of bits (maximum 64).
REAL: fixed or floating point number
all other data types are mapped to text.
The general format of the mapping file is as follows:
[
<
## Material
## Type
## Code
>
## :
<
table
## Name
>
,
<
code
column
name
>
]
<
## Property
## Type
## Code
>
## :
<
column
name
>
<
## Property
## Type
## Code
>
## :
<
column
name
>
...
[
<
## Material
## Type
## Code
>
## :
<
table
## Name
>
,
<
code
column
name
>
]
<
## Property
## Type
## Code
>
## :
<
column
name
>
<
## Property
## Type
## Code
>
## :
<
column
name
>
...
## Example:
mapping.txt
# Some comments
[
## GENE
## :
## GENE
,
## GENE_ID
]
## GENE_SYMBOLS
## :
symbol
[
## SIRNA
## :
si_rna
,
code
]
## INHIBITOR_OF
## :
suppressed_gene
## SEQUENCE
## :
Nucleotide_sequence
## Some rules:
Empty lines and lines starting with ‘#’ will be ignored.
Table and column names can be upper or lower case or mixed.
Material type codes and property type codes have to be in upper
case.
## Warning
If you put a foreign key constraint on the material code of one of the material properties, you need to define the constraint checking as DEFERRED in order to not get a constraint violation. The reason is that this task will
not
order the
## INSERT
statements by its dependencies, but in alphabetical order.
### UsageReportingTask

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Creates a daily/weekly/monthly report to a list of
e-mail recipients about the usage (i.e. creation of experiments, samples
and data sets) by users or groups. For more details see
## User Group
Management for Multi-groups openBIS
## Instances
.
In order to be able to send an e-mail the following properties in
service.properties
have to be defined:
mail
.
from
=
openbis
@<
host
>
mail
.
smtp
.
host
=
<
## SMTP
host
>
mail
.
smtp
.
user
=
<
can
be
empty
>
mail
.
smtp
.
password
=
<
can
be
empty
>
### Configuration
## :
## Property Key
## Description
interval
Determines the length of period: daily if less than or equal one day, weekly if less than or equal seven days, monthly if above seven days. The actual period is always the day/week/month before the execution day
email-addresses
Comma-separated e-mail addresses which will receive the report as an attached text file (format: TSV).
user-reporting-type
Type of reporting individual user activities. Possible values are
NONE: No reporting
ALL: Activities inside and outside groups and for all users
OUTSIDE_GROUP_ONLY: Activities outside groups and users of no groups
Default: ALL
spaces-to-be-ignored
Optional list of comma-separated space codes of all the spaces which should be ignored for the report.
configuration-file-path
Optional configuration file defining groups.
count-all-entities
## If
true
shows the number of all entities (collections, objects, data sets) in an additional column. Default:
false
## Example
## :
### class = ch.systemsx.cisd.openbis.generic.server.task.UsageReportingTask
interval = 7 days
email-addresses = ab@c.de, a@bc.de
PersonalAccessTokenValidityWarningTask

## Environment
## : AS
## Relevancy:
## Rare
### Automatic Configuration
## :
This task is automatically configured, added and run with a default interval of 1 day. If needed, the default interval can be changed. In order to do
that please configure the task just like any other maintenance task in
core-plugins
folder.
## Description
: Sends out warning emails about soon to be expired PATs (Personal Access Tokens). Emails are sent to PATs owners. Each email contains
a list of PATs that have the remaining validity period shorter than the
personal-access-tokens-validity-warning-period
defined in
## AS
service.properties
. The task does not send any information about the already expired PATs. It removes them.
For more details on Personal Access Tokens please see
## Personal Access Tokens
.
In order to be able to send an e-mail the following properties in
service.properties
have to be defined:
mail
.
from
=
openbis
@<
host
>
mail
.
smtp
.
host
=
<
## SMTP
host
>
mail
.
smtp
.
user
=
<
can
be
empty
>
mail
.
smtp
.
password
=
<
can
be
empty
>
## Example
## :
class = ch.systemsx.cisd.openbis.generic.server.pat.PersonalAccessTokenValidityWarningTask
interval = 7 d
Consistency Repair and Manual Migrations

BatchSampleRegistrationTempCodeUpdaterTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Replaces temporary sample codes (i.e. codes matching
the regular expression
TEMP\\.[a-zA-Z0-9\\-]+\\.[0-9]+
) by normal
codes (prefix specified by sample type plus number). This maintenance
task is only needed when
create-continuous-sample-codes
is set
true
in
service.properties
of AS.
## Example
## :
plugin.properties
class
=
ch.systemsx.cisd.openbis.generic.server.task.BatchSampleRegistrationTempCodeUpdaterTask
CleanUpUnarchivingScratchShareTask

## Environment
## : DSS
## Relevancy:
## Default
## Description
: Removes data sets from the unarchiving scratch share
which have status ARCHIVED and which are present in archive. For more
details see
Multi data set
archiving
.
### Configuration
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.CleanUpUnarchivingScratchShareTask
interval = 60 s
## Notes:
Recommended cleanup task to run on every instance.
DataSetRegistrationSummaryTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Sends a data set summary report to a list of e-mail
recipients in regular time intervals. The report contains all new data
sets registered since the last report. Selected properties can be
included into the report. The data sets are grouped by the data set
type.
In order to be able to send an e-mail the following properties in
service.properties
have to be defined:
mail
.
from
=
openbis
@<
host
>
mail
.
smtp
.
host
=
<
## SMTP
host
>
mail
.
smtp
.
user
=
<
can
be
empty
>
mail
.
smtp
.
password
=
<
can
be
empty
>
### Configuration:
## Property Key
## Description
interval
Interval (in seconds) between regular checks whether to create a report or not. This value should be set to 86400 (1 day). Otherwise the same report might be sent twice or no report will be sent.
start
Time the report will be created. A good values for this parameter is some early time in the morning like in the example below.
days-of-week
Comma-separated list of numbers denoting days of week (Sunday=1, Monday=2, etc.). This parameter should be used if reports should be sent weekly or more often.
days-of-month
Comma-separated list of numbers denoting days of month. Default value of this parameter is 1.
email-addresses
Comma-separated list of e-mail addresses.
shown-data-set-properties
Optional comma-separated list of data set properties to be included into the report.
data-set-types
Restrict the report to the specified comma-separated data set types.
configured-content
Use the specified content as the body of the email.
A report is sent at each day which is either a specified day of week or
day of month. If only weekly reports are needed the parameter
days-of-month
should be set to an empty string.
## Example
## :
service.properties of AS
<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.DataSetRegistrationSummaryTask
<task id>.interval = 86400
<task id>.start = 1:00
<task id>.data-set-types = RAW_DATA, MZXML_DATA
<task id>.email-addresses = albert.einstein@princeton.edu, charles.darwin@evolution.org
This means that on the 1st day of every month at 1:00 AM openBIS sends
to the specified e-mail recipients a report about the data sets of types
RAW_DATA and MZXML_DATA that have been uploaded in the previous month.
DynamicPropertyEvaluationMaintenanceTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Re-evaluates dynamic properties of all entities
### Configuration
## :
## Property Key
## Description
class
ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask
interval = 3600
DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask

## Environment
## : AS
## Relevancy:
## Deprecated
## Description
: Re-evaluates dynamic properties of all samples which
refer via properties of type MATERIAL directly or indirectly to
materials changed since the last re-evaluation.
### Configuration
## :
## Property Key
## Description
class
ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask
timestamp-file
Path to a file which will store the timestamp of the last evaluation. Default value:
../../../data/DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask-timestamp.txt
.
initial-timestamp
Initial timestamp of the form
## YYYY-MM-DD
(e.g. 2013-09-15) which will be used the first time when the timestamp file doesn’t exist or has an invalid value. This is a mandatory property.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask
interval = 7 days
initial-timestamp = 2012-12-31
FillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Queries openBIS database to find data sets without a
size filled in, then queries the pathinfo DB to see if the size info is
available there; if it is available, it fills in the size from the
pathinfo information. If it is not available, it does nothing. Data sets
from openBIS database are fetched in chunks (see data-set-chunk-size
property). After each chunk the maintenance tasks checks whether a time
limit has been reached (see time-limit property). If so, it stops
processing. A code of the last processed data set is stored in a file
(see last-seen-data-set-file property). The next run of the maintenance
task will process data sets with a code greater than the one saved in
the “last-seen-data-set-file”. This file is deleted periodically (see
delete-last-seen-data-set-file-interval) to handle a situation where
codes of new data sets are lexicographically smaller than the codes of
the old datasets. Deleting the file is also needed when pathinfo
database entries are added after a data set has been already processed
by the maintenance task.
### Configuration
## :
## Property Key
## Description
last-seen-data-set-file
Path to a file that will store a code of the last handled data set. Default value: “fillUnknownDataSetSizeTaskLastSeen”
delete-last-seen-data-set-file-interval
A time interval (in seconds) which defines how often the “last-seen-data-set-file” file should be deleted. The parameter can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
. Default value: 7 days.
data-set-chunk-size
Number of data sets requested from AS in one chunk. Default: 100
time-limit
Limit of execution time of this task. The task is stopped before reading next chunk if the time has been used up. This parameter can be specified with one of the following time units:
ms
,
msec
,
s
,
sec
,
m
,
min
,
h
,
hours
,
d
,
days
. Default time unit is
sec
.
## Example:
plugin.properties
<task id>.class = ch.systemsx.cisd.etlserver.plugins.FillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask
<task id>.interval = 86400
<task id>.data-set-chunk-size = 1000
<task id>.time-limit = 1h
## NOTE
: Useful in scenarios where the path info feeding sub task of
post registration task fails.
PathInfoDatabaseChecksumCalculationTask

## Environment
## : DSS
## Relevancy:
Rare, often the CRC32 is calculated during the post
registration.
## Description
: Calculates the CRC32 checksum (and optionally a
checksum of specified type) of all files in the pathinfo database with
unknown checksum. This task is needed to run only once. It assumes a
data source for key ‘path-info-db’.
### Configuration
## :
## Property Key
## Description
checksum-type
Optional checksum type. If specified two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by
MessageDigest.getInstance(<checksum
type>)
. For more details see http://docs.oracle.com/javase/8/docs/api/java/security/MessageDigest.html#getInstance-java.lang.String-.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseChecksumCalculationTask
execute-only-once = true
checksum-type = SHA-256
PathInfoDatabaseRefreshingTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: Refreshes the pathinfo database with file metadata of
physical and available data sets in the store. This task assumes a data
source with for ‘path-info-db’.
The data sets are processed in the inverse order they are registered.
Only a maximum number of data sets are processed in one run. This is
specified by
chunk-size
.
## Warning
Under normal circumstances this maintenance task is never needed, because the content of a physical data set is
never
changed by openBIS itself.
Only in the rare cases that the content of physical data sets have to be changed this maintenance task allows to refresh the file meta data in the pathinfo database.
### Configuration
## :
## Property Key
## Description
time-stamp-of-youngest-data-set
Time stamp of the youngest data set to be considered. The format has to be
<4
digit
year>-<month>-<day>
<hour>:<minute>:<second>
.
compute-checksum
## If
true
the CRC32 checksum (and optionally a checksum of the type specified by
checksum-type
) of all files will be calculated and stored in pathinfo database. Default value: true
checksum-type
Optional checksum type. If specified and
compute-checksum
=
true
two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by
MessageDigest.getInstance(<checksum
type>)
. For more details see
Oracle doc
.
chunk-size
Number of data sets requested from AS in one chunk. Default: 1000
data-set-type
Optional data set type. If specified, only data sets of the specified type are considered. Default: All data set types.
state-file
File to store registration time stamp and code of last considered data set. Default:
<store
root>/PathInfoDatabaseRefreshingTask-state.txt
## Example
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseRefreshingTask
interval = 30 min
time-stamp-of-youngest-data-set = 2014-01-01 00:00:00
data-set-type = HCS_IMAGE
RemoveUnusedUnofficialTermsMaintenanceTask

## Environment
## : AS
## Relevancy:
## Rare
## Description
: Removes unofficial unused vocabulary terms. For more details about unofficial vocabulary terms see
## Ad Hoc Vocabulary Terms
.
### Configuration:
## Property Key
## Description
older-than-days
Unofficial terms are only deleted if they have been registered more than the specified number of days ago. Default: 7 days.
## Example
## :
service.properties of AS
<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.RemoveUnusedUnofficialTermsMaintenanceTask
<task id>.interval = 86400
<task id>.older-than-days = 30
ResetArchivePendingTask

## Environment
## : DSS
## Relevancy:
## Rare
## Description
: For each data set not present in archive and status
ARCHIVE_PENDING the status will be set to AVAILABLE if there is no
command in the DSS data set command queues referring to it.
### Configuration
## :
plugin.properties
class = ch.systemsx.cisd.etlserver.plugins.ResetArchivePendingTask
interval = 60 s
SessionWorkspaceCleanUpMaintenanceTask

## Environment
## : AS
## Relevancy:
## Default
## Description
: Cleans up session workspace folders of no longer active
sessions. This maintenance plugin is automatically added by default with
a default interval of 1 hour. If a manually configured version of the
plugin is detected then the automatic configuration is skipped.
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.generic.server.task.SessionWorkspaceCleanUpMaintenanceTask
interval = 1 day
MaterialsMigration

## Environment
## : AS
## Relevancy:
## Relevant
## Description
: Migrates the Materials entities and types to use a
Sample based model using Sample Properties. It automatically creates and
assigns sample types, properties and entities.
It allows to execute the migration and to delete of the old Materials
model in separate steps.
Deleting Materials and material types requires the migration to have
been a success,  before the deletion a validation check is run.
## Example
## :
This maintenance task can be directly configured on the AS
service.properties
service.properties
maintenance-plugins = materials-migration



materials-migration.class = ch.systemsx.cisd.openbis.generic.server.task.MaterialsMigration
materials-migration.execute-only-once = true
materials-migration.doMaterialsMigrationInsertNew = true
materials-migration.doMaterialsMigrationDeleteOld = true
## Microscopy Maintenance Tasks

MicroscopyThumbnailsCreationTask

## Environment
## : DSS
## Relevancy:
## Relevant
## Description
: Creates thumbnails for already registered microscopy
data sets.
### Configuration:
## Property Key
## Description
maximum-number-of-workers
If specified the creation will be parallelized among several workers. The actual number of workers depends on the number CPUs. There will be not more than 50% of CPUs used.
state-file
Name of the file which stores the registration time stamp of the last successfully handled data set. Default:
MicroscopyThumbnailsCreationTask-state.txt
script-path
Path to the jython script which specifies the thumbnails to be generated. The script should have defined the method
process(transaction,
parameters,
tablebuilder)
as for
JythonIngestionService
(see Jython-based Reporting and Processing Plugins). Note, that tablebuilder will be ignored. In addition the global variables
image_config
and
image_data_set_structure
## are defined:
image_data_set_structure: It is an object of the class
ImageDataSetStructure
. Information about channels, series numbers etc. can be requested.
image_config: It is an object of the class
SimpleImageContainerDataConfig
. It should be used to specify the thumbnails to be created. Currently only
setImageGenerationAlgorithm()
is supported.
main-data-set-type-regex
Regular expression for the type of data sets which have actual images. Default:
## MICROSCOPY_IMG
data-set-thumbnail-type-regex
Regular expression for the type of data sets which have thumbnails. This is used to test whether there are already thumbnails or not. Default:
## MICROSCOPY_IMG_THUMBNAIL
max-number-of-data-sets
The maximum number of data sets to be handle in a run of this task. If zero or less than zero all data sets will be handled. Default: 1000
data-set-container-type
Type of the data set container. Default:
## MICROSCOPY_IMG_CONTAINER
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.etl.MicroscopyThumbnailsCreationTask
interval = 1 h
script-path = specify_thumbnail_generation.py
with
specify_thumbnail_generation.py
from
ch.systemsx.cisd.openbis.dss.etl.dto.api.impl
import
MaximumIntensityProjectionGenerationAlgorithm
from
sets
import
## Set
def
_get_series_num
## ():
series_numbers
=
## Set
()
for
image_info
in
image_data_set_structure
.
getImages
## ():
series_numbers
.
add
(
image_info
.
tryGetSeriesNumber
())
return
series_numbers
.
pop
()
def
process
(
transaction
,
parameters
,
tableBuilder
## ):
seriesNum
=
_get_series_num
()
if
int
(
seriesNum
)
%
2
==
0
## :
image_config
.
setImageGenerationAlgorithm
(
MaximumIntensityProjectionGenerationAlgorithm
(
## ""MICROSCOPY_IMG_THUMBNAIL""
,
256
,
128
,
""thumbnail.png""
))
DeleteFromImagingDBMaintenanceTask

## Environment
## : DSS
## Relevancy:
## Relevant
## Description
: Deletes database entries from the imaging database.
This is special variant of
DeleteFromExternalDBMaintenanceTask
with the same configuration parameters.
### Configuration
## : See
DeleteFromExternalDBMaintenanceTask
## Example
## :
plugin.properties
class = ch.systemsx.cisd.openbis.dss.etl.DeleteFromImagingDBMaintenanceTask
data-source = imaging-db
## Proteomics Maintenance Tasks
",Maintenance Tasks,1,en_20.10.0-11_system-documentation_configuration_maintenance-tasks_1,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_maintenance-tasks.txt,2025-09-30T12:09:04.429877Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_master-data:0,Master data import/export,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/master-data.html,openbis,"Master data import/export

The master data of openBIS comprises all entity/property types, property
assignments, vocabularies etc. needed for your customized installation
to work. The system offers a way to export/import master data via Jython
scripts. More information on how to do create such scripts and run them
manually see the advanced guide
Jython Master Data Scripts
.
A master data script can be run automatically by start up of the AS if
it is defined in an AS core plugin. The script path should be
$INSTALL_PATH/servers/core-plugins/<module
name>/<version
number>/as/initialize-master-data.py
.
For more details about the folder structure of core plugins see
## Core
## Plugins
. If there are several
core plugins with master data scripts the scripts will be executed in
alphabetical order of the module names. For example, the master data
script of module
screening-optional
will be executed after the master
data script of module
screening
has been executed.
Execution of master data script can be suppressed by
disabling
initialize-master-data
core plugin. For more details see
## Core Plugins
.",Core,0,en_20.10.0-11_system-documentation_configuration_master-data_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_master-data.txt,2025-09-30T12:09:04.614767Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:0,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"Multi data set archiving

## Introduction

Multi data set archiver is a tool to archive several datasets together
in chunks of relatively large size. When a group of datasets is selected
for archive it is verified if they are all together of proper size and
then they are being stored as one big container file (tar) on the
destination storage.
When unarchiving data sets from a multi data set archive the following
## rules are obeyed:
Unarchiving of data sets from different containers is possible as
long as the maximum unarchiving cap specified in the
plugin.properties file is not exceeded.
All data sets from a container are unarchived even though
unarchiving has been requested only for a sub set.
The data sets are unarchived into a share which is marked as an
unarchiving scratch share.
In case of not enough free space in the scratch share the oldest
(defined by modification time stamp) data sets are removed from the
scratch share to free space. For those data sets the archiving
status is set back to ARCHIVED.
To test the archiver find the datasets you want to archive in openbis
GUI and “add to archive”.
Important technical details

The archiver requires configuration of three important entities.
An archive destination (e.g. on Strongbox).
A PostgreSQL database for mapping information (i.e. which data set
is in which container file).
An unarchiving scratch share.
Multi dataset archiver is not compatible with other archivers. You
should have all data available before configuring this archiver.
## Workflows

The multi data set archiver can be configured for four different
workflows. The workflow is selected by the presence/absence of the
properties
staging-destination
and
replicated-destination
.
Simple workflow

None of the properties
staging-destination
and
replicated-destination
are present.
Wait for enough free space on the archive destination.
Store the data set in a container file directly on the archive
destination.
Perform sanity check. That is, getting the container file to the
local disk and compare the content with the content of all data sets
in the store.
Add mapping data to the PostgreSQL database.
Remove data sets from the store if requested.
Update archiving status for all data sets.
Staging workflow

## Property
staging-destination
is specified but
replicated-destination
is not.
Store the data sets in a container file in the staging folder.
Wait for enough free space on the archive destination.
Copy the container file from the staging folder to the archive
destination.
Perform sanity check.
Remove container file from the staging folder.
Add mapping data to the PostgreSQL database.
Remove data sets from the store if requested.
Update archiving status for all data sets.
Replication workflow

## Property
replicated
-destination is specified but
staging
-destination is not.
Wait for enough free space on the archive destination.
Store the data set in a container file directly on the archive
destination.
Perform sanity check.
Add mapping data to the PostgreSQL database.
Wait until the container file has also been copied (by some external
process) to a replication folder.
Remove data sets from the store if requested.
Update archiving status for all data sets.
## Some remarks:
Steps 5 to 7 will be performed asynchronously from the first four
steps because step 5 can take quite long. In the meantime the next
archiving task can already be performed.
If the container file isn’t replicated after some time archiving
will be rolled back and scheduled again.
Staging and replication workflow

When both properties
staging-destination
and
replicated-destination
are present staging and replication workflow will be combined.
Clean up

In case archiving fails all half-baked container files have to be
removed. By default this is done immediately.
But in context of tape archiving systems (e.g. Strongbox) immediate
deletion might not always be possible all the time. In this case a
deletion request is schedule. The request will be stored in file. It
will be handled in a separate thread in regular time intervals (polling
time). If deletion isn’t possible after some timeout an e-mail will be
sent. Such deletion request will still be handled but the e-mail allows
manual intervention/deletion. Note, that deletion requests for
non-existing files will always be handled successfully.
### Configuration steps

Disable existing archivers
Find all properties of a form
archiver.*
in
servers/datastore_server/etc/service.properties
and remove
them.
Find all DSS core plugins of type
miscellaneous
which define
an archiver. Disable them by adding an empty marker file
named
disabled
.
Enable archiver
Configure a new DSS core plugin of type
miscellaneous
## :
multi-dataset-archiver/1/dss/miscellaneous/archiver/plugin.properties
archiver.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetArchiver",Introduction,0,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:1,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Temporary folder (needed for sanity check). Default: Value provided by Java system property java.io.tmpdir. Usually /tmp
# archiver.temp-folder = <java temp folder>",Temporary folder (needed for sanity check). Default: Value provided by Java system property java.io.tmpdir. Usually /tmp,1,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_1,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:2,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Staging folder (needed for 'staging workflow' and 'staging and replication workflow')
archiver.staging-destination = path/to/local/stage/area",Staging folder (needed for 'staging workflow' and 'staging and replication workflow'),2,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_2,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:3,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Replication folder (needed for 'replication workflow' and 'staging and replication workflow')
archiver.replicated-destination = path/to/mounted/replication/folder",Replication folder (needed for 'replication workflow' and 'staging and replication workflow'),3,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_3,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:4,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# The archiver will refuse to archive group of data sets, which together are smaller than this value
archiver.minimum-container-size-in-bytes = 15000000","The archiver will refuse to archive group of data sets, which together are smaller than this value",4,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_4,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:5,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# The archiver will refuse to archive group of data sets, which together are bigger than this value.
# The archiver will ignore this value, when archiving single data set.
archiver.maximum-container-size-in-bytes = 35000000","The archiver will refuse to archive group of data sets, which together are bigger than this value.",5,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_5,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:6,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# This variable is meant for another use case, than this archiver, but is shared among all archivers.
# For this archiver it should be specified for something safely larger than maximum-container-size-in-bytes
archiver.batch-size-in-bytes = 80000000","This variable is meant for another use case, than this archiver, but is shared among all archivers.",6,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_6,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:7,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# (since version 20.10.4) Check consistency between file meta data of the files in the store and from the pathinfo database.
# If path info db entries are missing or are different than the original files kept in the store, then the archiving is aborted.
# If ""sanity-check-verify-checksums"" is also set to ""true"", then this consistency check will also verify that the entries stored in the
# path info db do have the checksums filled in.
# Default value: true
# check-consistency-between-store-and-pathinfo-db = true",(since version 20.10.4) Check consistency between file meta data of the files in the store and from the pathinfo database.,7,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_7,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:8,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Archiving can be speed up if setting this flag to false (default value: true). But this works only if the data sets
# to be archived do not contain hdf5 files which are handled as folders (like the thumbnail h5ar files in screening/microscopy).
# archiver.hdf5-files-in-data-set = true",Archiving can be speed up if setting this flag to false (default value: true). But this works only if the data sets,8,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_8,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:9,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Whether all data sets should be archived in a top level directory of archive or with sharding (the way data sets are stored in openbis internal store)
# archiver.with-sharding = false",Whether all data sets should be archived in a top level directory of archive or with sharding (the way data sets are stored in openbis internal store),9,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_9,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:10,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Polling time for evaluating free space on archive destination
# archiver.waiting-for-free-space-polling-time = 1 min",Polling time for evaluating free space on archive destination,10,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_10,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:11,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Maximum waiting time for free space on archive destination
# archiver.waiting-for-free-space-time-out = 4 h",Maximum waiting time for free space on archive destination,11,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_11,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:12,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# If set to true, then an initial waiting time will be added before starting a sanity check.
# If the sanity check fails, it will be retried. The time between each sanity check attempt is doubled,
# starting from the initial waiting time up to the maximum waiting time (see properties below).
# Default: false
archiver.wait-for-sanity-check = true","If set to true, then an initial waiting time will be added before starting a sanity check.",12,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_12,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:13,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Initial waiting time before starting a sanity check. Works only if 'wait-for-sanity-check = true'
# Default: 10sec
archiver.wait-for-sanity-check-initial-waiting-time = 120 sec",Initial waiting time before starting a sanity check. Works only if 'wait-for-sanity-check = true',13,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_13,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:14,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Maximum total waiting time for failed sanity check attempts. Works only if 'wait-for-sanity-check = true'
# Default: 30min
archiver.wait-for-sanity-check-max-waiting-time = 5 min",Maximum total waiting time for failed sanity check attempts. Works only if 'wait-for-sanity-check = true',14,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_14,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:15,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# If set to ""true"", then the sanity check will verify that checksums of the original files and checksums of the archived files are the same.
# If ""check-consistency-between-store-and-pathinfo-db"" is also set to ""true"", then the checksums stored in the path info db will be used
# for the verification of all files. Otherwise, the checksums of the original files may be either taken from the path info db or be calculated on the fly (depending on what is available).
# Default: true
archiver.sanity-check-verify-checksums = true","If set to ""true"", then the sanity check will verify that checksums of the original files and checksums of the archived files are the same.",15,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_15,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:16,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# A template of a shell command to be executed before unarchiving. The template may use ${container-path} and ${container-id} variables which will be replaced with an absolute container path (full path of the tar file to be unarchived)
# and a container id (id of the container to be unarchived used in the archiving database). The command created from the template is executed only once for a given container (just before the first unarchiving attempt) and is not retried.
# The unarchiver waits for the command to finish before proceeding. If the command exits with status zero, then the unarchiving is started. If the command exits with a non-zero value, then the archiving is marked as failed.
#
# Example: tar -tf ${container-path}
# Default: null
archiver.unarchiving-prepare-command-template",A template of a shell command to be executed before unarchiving. The template may use ${container-path} and ${container-id} variables which will be replaced with an absolute container path (full path of the tar file to be unarchived),16,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_16,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:17,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# If set to true, then the unarchiver waits for T flag to be removed from the file in the final destination before it tries to read the file.
# Default: false
archiver.unarchiving-wait-for-t-flag = true","If set to true, then the unarchiver waits for T flag to be removed from the file in the final destination before it tries to read the file.",17,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_17,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:18,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Maximum total waiting time for failed unarchiving attempts.
# Default: null
archiver.unarchiving-max-waiting-time = 1d",Maximum total waiting time for failed unarchiving attempts.,18,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_18,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:19,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Polling time for waiting on unarchiving.
# Default: null
archiver.unarchiving-polling-time = 5 min",Polling time for waiting on unarchiving.,19,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_19,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:20,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# If set to true, then the archiver waits for T flag to be set on the file in the replicated destination. The check is done before a potential sanity check of the replicated file (see 'finalizer-sanity-check').
# Default: false
archiver.finalizer-wait-for-t-flag = true","If set to true, then the archiver waits for T flag to be set on the file in the replicated destination. The check is done before a potential sanity check of the replicated file (see 'finalizer-sanity-check').",20,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_20,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:21,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# If set to true, then a sanity check for the replicated destination is also performed (in addition to a sanity check for the final destination which is always executed).
# Default: false
archiver.finalizer-sanity-check = true","If set to true, then a sanity check for the replicated destination is also performed (in addition to a sanity check for the final destination which is always executed).",21,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_21,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:22,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Minimum required free space at final destination before triggering archiving if > 0. This threshold can be
# specified as a percentage of total space or number of bytes. If both are specified the threshold is given by
# the maximum of both values.
# archiver.minimum-free-space-at-final-destination-in-percentage
# archiver.minimum-free-space-at-final-destination-in-bytes",Minimum required free space at final destination before triggering archiving if > 0. This threshold can be,22,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_22,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:23,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Minimum free space on archive destination after container file has been added.
# archiver.minimum-free-space-in-MB = 1024",Minimum free space on archive destination after container file has been added.,23,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_23,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:24,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Polling time for waiting on replication. Only needed if archiver.replicated-destination is specified.
# archiver.finalizer-polling-time = 1 min",Polling time for waiting on replication. Only needed if archiver.replicated-destination is specified.,24,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_24,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:25,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Maximum waiting time for replication finished.  Only needed if archiver.replicated-destination is specified.
# archiver.finalizer-max-waiting-time = 1 d",Maximum waiting time for replication finished.  Only needed if archiver.replicated-destination is specified.,25,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_25,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:26,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Maximum total size (in MB) of data sets that can be scheduled for unarchiving at any given time. When not specified, defaults to 1 TB.
# Note also that the value specified must be consistent with the scratch share size.
# maximum-unarchiving-capacity-in-megabytes = 200000","Maximum total size (in MB) of data sets that can be scheduled for unarchiving at any given time. When not specified, defaults to 1 TB.",26,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_26,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:27,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Delay unarchiving. Needs MultiDataSetUnarchivingMaintenanceTask.
# archiver.delay-unarchiving = false",Delay unarchiving. Needs MultiDataSetUnarchivingMaintenanceTask.,27,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_27,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:28,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Size of the buffer used for copying data. Default value: 1048576 (i.e. 1 MB). This value is only important in case of accurate
# measurements of data transfer rates. In case of expected fast transfer rates a larger value (e.g. 10 MB) should be used.
# archiver.buffer-size = 1048576",Size of the buffer used for copying data. Default value: 1048576 (i.e. 1 MB). This value is only important in case of accurate,28,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_28,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:29,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Maximum size of the writing queue for copying data. Reading from the data store and writing to the TAR file is
# done in parallel. The default value 5 * archiver.buffer-size.
# archiver.maximum-queue-size-in-bytes = 5242880",Maximum size of the writing queue for copying data. Reading from the data store and writing to the TAR file is,29,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_29,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:30,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Path (absolute or relative to store root) of an empty file. If this file is present starting
# archiving will be paused until this file has been removed.
# This property is useful for archiving media/facilities with maintenance downtimes.
# archiver.pause-file = pause-archiving",Path (absolute or relative to store root) of an empty file. If this file is present starting,30,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_30,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:31,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Time interval between two checks whether pause file still exists or not.
# archiver.pause-file-polling-time = 10 min

#-------------------------------------------------------------------------------------------------------
# Clean up properties
#
# A comma-separated list of path to folders which should be cleaned in a separate thread
#archiver.cleaner.file-path-prefixes-for-async-deletion = <absolute path 1>, <absolute path 2>, ...",Time interval between two checks whether pause file still exists or not.,31,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_31,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:32,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# A folder which will contain deletion request files. This is a mandatory property if
# archiver.cleaner.file-path-prefixes-for-async-deletion is specified.
#archiver.cleaner.deletion-requests-dir = <some local folder>",A folder which will contain deletion request files. This is a mandatory property if,32,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_32,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:33,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Polling time interval for looking and performing deletion requests. Default value is 10 minutes.
#archiver.cleaner.deletion-polling-time = 10 min",Polling time interval for looking and performing deletion requests. Default value is 10 minutes.,33,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_33,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:34,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Time out of deletion requests. Default value is one day.
#archiver.cleaner.deletion-time-out = 24 h",Time out of deletion requests. Default value is one day.,34,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_34,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:35,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Optional e-mail address. If specified every integer multiple of the timeout period an e-mail is send to
# this address listing all deletion requests older than specified timeout.
#archiver.cleaner.email-address = <some valid e-mail address>",Optional e-mail address. If specified every integer multiple of the timeout period an e-mail is send to,35,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_35,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:36,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Optional e-mail address for the 'from' field.
#archiver.cleaner.email-from-address = <some well-formed e-mail address>",Optional e-mail address for the 'from' field.,36,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_36,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:37,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Subject for the 'subject' field. Mandatory if an e-mail address is specified.
#archiver.cleaner.email-subject = Deletion failure",Subject for the 'subject' field. Mandatory if an e-mail address is specified.,37,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_37,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:38,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of
# lines. Two lines for each deletion request. One for the absolute file path and one of the request time stamp.
# Mandatory if an e-mail address is specified.
#archiver.cleaner.email-template = The following files couldn't be deleted:\n${file-list}",Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of,38,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_38,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:39,Multi data set archiving,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html,openbis,"# Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of
# lines. Two lines for each deletion request. One for the absolute file path and one of the request time stamp.
# Mandatory if an e-mail address is specified.
#archiver.cleaner.email-template = The following files couldn't be deleted:\n${file-list}



#-------------------------------------------------------------------------------------------------------
# The following properties are necessary in combination with data source configuration
multi-dataset-archive-database.kind = prod
multi-dataset-archive-sql-root-folder = datastore_server/sql/multi-dataset-archive
You should make sure that all destination directories exist and
DSS has read/write privileges before attempting archiving
(otherwise the operation will fail).
Add the core plugin module name (here
multi-dataset-archiver
)
to the property
enabled-modules
of
core-plugin.properties
.
Enable PostgreSQL datasource
Configure a new DSS core plugin of type
data-sources
## :
multi-dataset-archiver/1/dss/data-sources/multi-dataset-archiver-db/plugin.properties
version-holder-class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.dataaccess.MultiDataSetArchiverDBVersionHolder
databaseEngineCode = postgresql
basicDatabaseName = multi_dataset_archive
urlHostPart = ${multi-dataset-archive-database.url-host-part:localhost}
databaseKind = ${multi-dataset-archive-database.kind:prod}
scriptFolder = ${multi-dataset-archive-sql-root-folder:}
owner = ${multi-dataset-archive-database.owner:}
password = ${multi-dataset-archive-database.password:}
Create a share which will be used exclusively as a scratch share for
unarchiving. To mark it for this purpose add a
share.properties
file to the share (e.g.
<mounted
share>/store/1/share.properties
)
with property
unarchiving-scratch-share
=
true
.
In addition the maximum size of the share can be specified. Example:
share.properties
unarchiving-scratch-share = true
unarchiving-scratch-share-maximum-size-in-GB = 100
It is recommended to do archiving in a separate queue in order to
avoid situation when fast processing plugin tasks are not processes
because multi data set archiving tasks can take quite long. If one
of the two workflows with replication is selected
(i.e.
archiver.replicated-destination
) a second processing plugin
## (ID
## Archiving
## Finalizer
) is used. It should run in a queue
different from the queue used for archiving. The following setting
in DSS
service.properties
## covers all workflows:
service.properties
data-set-command-queue-mapping = archiving:Archiving|Copying data sets to archive, unarchiving:Unarchiving, archiving-finalizer:Archiving Finalizer
Clean up Unarchiving Scratch Share

(Since version 20.10.4) Data sets in the unarchiving scratch share can
be removed any times because they are already present in archive.
Normally this happens during unarchving if there is not enough free
space available in the scratch share. But this may fail for some reason.
This can lead to the effect that unarchiving doesn’t work because they
are data sets in the scratch share which can be removed because they are
archived.
Therefore, it is recommended to setup a
CleanUpUnarchivingScratchShareTask
which removes data sets from the scratch share which fulfill the
## following conditions:
The data set is in state ARCHIVED and the flag
presentInArchive
is set.
The data set is found in the Multi Data Set Archive database and the
corresponding TAR archive file exists.
Deletion of archived Data Sets

(Since version 20.10.3) Archived data sets can be deleted permanently.
But they are still in the container files. In order to remove them also
from the container files a
MultiDataSetDeletionMaintenanceTask
has to be configured.
Recovery from corrupted archiving queues

In case the queues with the archiving commands get corrupted, they
cannot be used any more, they need to be deleted before the DSS starts
and a new one will be created. The typical scenario where this happens
is when you get out of space on the disk where the queues are stored.
The following steps describe how to recover from such a situation.
Finding out the data sets that are in ‘ARCHIVE_PENDING’ status.
## SELECT
id
,
size
,
present_in_archive
,
share_id
,
location
## FROM
external_data
## WHERE
status
=
## 'ARCHIVE_PENDING'
;
openbis_prod
=>
## SELECT
id
,
size
,
present_in_archive
,
share_id
,
location
## FROM
external_data
## WHERE
status
=
## 'ARCHIVE_PENDING'
;
data_id
|
size
|
present_in_archive
|
share_id
|
location
---------+-------------+--------------------+----------+-----------------------------------------------------------------------
3001
|
34712671864
|
f
|
1
|
585
## D8354
-
92
## A3
-
4
## C24
-
9621
-
## F6B7063A94AC
/
17
/
65
/
a4
/
20170712111421297
-
37998
3683
|
29574172672
|
f
|
1
|
585
## D8354
-
92
## A3
-
4
## C24
-
9621
-
## F6B7063A94AC
/
39
/
6
c
/
b0
/
20171106181516927
-
39987
3688
|
53416316928
|
f
|
1
|
585
## D8354
-
92
## A3
-
4
## C24
-
9621
-
## F6B7063A94AC
/
ca
/
3
b
/
93
/
20171106183212074
-
39995
3692
|
47547908096
|
f
|
1
|
585
## D8354
-
92
## A3
-
4
## C24
-
9621
-
## F6B7063A94AC
/
b7
/
26
/
85
/
20171106185354378
-
40002
The data sets found, can be or not in the archiving process. This is
not easy to find out instantly. It’s easier just to execute the
above statement in subsequent days.
If the data sets are still in ‘ARCHIVE_PENDING’ after a sensible
amount of time (1 week for example) and there is no other issues,
like the archiving destination is not available there is a good
change, they are really stuck on the process.
Reaching this point, the data sets are most likely still on the data
store as indicated by the combination of share ID and location
indicated. Verify this! If they are not there hope they are archived
or you are on trouble.
If they are on the store, you need to set the status to available
again using a SQL statement.
openbis_prod
=>
## UPDATE
external_data
## SET
status
=
## 'AVAILABLE'
,
present_in_archive
=
'f'
## WHERE
id
## IN
(
## SELECT
id
## FROM
data
where
code
in
(
'20170712111421297-37998'
,
'20171106181516927-39987'
));
If there is half copied files on the archive destination, these need
to be delete too, to find them run the next queries.
To find out the containers:
## SELECT
*
## FROM
data_sets
## WHERE
## CODE
## IN
(
'20170712111421297-37998'
,
'20171106181516927-39987'
,
'20171106183212074-39995'
,
'20171106185354378-40002'
);
multi_dataset_archive_prod
=>
## SELECT
*
## FROM
data_sets
## WHERE
## CODE
## IN
(
'20170712111421297-37998'
,
'20171106181516927-39987'
,
'20171106183212074-39995'
,
'20171106185354378-40002'
);
id
|
code
|
ctnr_id
|
size_in_bytes
-----+-------------------------+---------+---------------
294
|
20170712111421297
-
37998
|
60
|
34712671864
295
|
20171106185354378
-
40002
|
61
|
47547908096
296
|
20171106183212074
-
39995
|
61
|
53416316928
297
|
20171106181516927
-
39987
|
61
|
29574172672
(
4
rows
)
multi_dataset_archive_prod
=>
## SELECT
*
## FROM
containers
## WHERE
id
## IN
(
60
,
61
);
id
|
path
|
unarchiving_requested
----+---------------------------------------------+-----------------------
60
|
20170712111421297
-
37998
-
20171108
-
105339
.
tar
|
f
61
|
20171106185354378
-
40002
-
20171108
-
130342
.
tar
|
f
## Note
We have never seen it but if there is a container with data
sets in different archiving status then, you need to recover the
ARCHIVED data sets from the container and copy them manually to the
data store before being able to continue.
multi_dataset_archive_prod
=>
## SELECT
*
## FROM
data_sets
## WHERE
ctnr_id
## IN
(
## SELECT
ctnr_id
## FROM
data_sets
## WHERE
## CODE
## IN
(
'20170712111421297-37998'
,
'20171106181516927-39987'
,
'20171106183212074-39995'
,
'20171106185354378-40002'
));
After deleting the files clean up the multi dataset archiver
database.
openbis_prod
=>
## DELETE
## FROM
containers
## WHERE
id
## IN
(
## SELECT
ctnr_id
## FROM
data_sets
## WHERE
## CODE
## IN
(
'20170712111421297-37998'
,
'20171106181516927-39987'
,
'20171106183212074-39995'
,
'20171106185354378-40002'
));",Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of,39,en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving_39,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving.txt,2025-09-30T12:09:04.681768Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_openbis-sync:0,openBIS Sync,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/openbis-sync.html,openbis,"openBIS Sync

## Introduction

Sync is a service of openBIS and comes with every instance.
Sync allows to synchronize two openBIS instances using the OAI-PMH protocol.
This protocol has two participants:
One instance (called
## Data
## Source
) provides the data (types, meta-data and data sets).
Another instance (called
## Harvester
) grabs these data and makes them available.
In regular time intervals, the
## Harvester
instance will synchronize its data with the data on the
## Data
## Source
instance.
Synchronization will add and/or delete data to the
## Harvester
instance.
An openBIS instance can be one or both
## Data
## Source
and
## Harvester
since these are separate services.
An openBIS instance only needs one
## Data
## Source
service. Even with many participants since it decides what to share depending on the user requesting the information.
An openBIS instance only needs one
## Harvester
service. Even with many participants since it goes through a list of
## Data
## Source
.
Data Source Service Configuration

## The
## Data
## Source
instance provides a service based on the ResourceSync Framework Specification (see
http://www.openarchives.org/rs/1.1/resourcesync
).
This service is configured by default in all new installations as a core plugin as
core plugin
module
openbis-sync
.
So in theory all openBIS instances are by default a
## Data
## Source
. This should not worry admins, since users cannot access any data though the
## Data
## Source
endpoint that they could not already with the UI or standard API.
As a
## Data
## Source
is key to learn to configure the module
openbis-sync
, this module has two config files:
# Main Configuration File
./core-plugins/openbis-sync/2/dss/servlet-services/resource-sync/plugin.properties
# Configuration file providing an AS datasource to the DSS
./core-plugins/openbis-sync/2/dss/data-sources/openbis-db/plugin.properties
Is strongly encouraged that any overrides to the values of any configuration file should be done forwarding
such properties to AS service.properties keys.
The main configuration file defines the URL where the
## Data
## Source
will be available.
When using the default settings for the openBIS installation is not necessary to modify it.
Special attention to
request-handler.file-service-repository-path
that should point to the
file-server
.
plugin.properties
# ./core-plugins/openbis-sync/2/dss/servlet-services/resource-sync/plugin.properties
class
=
ch.systemsx.cisd.openbis.dss.generic.server.oaipmh.OaipmhServlet
path
=
${openbis-sync.servlet-services.resource-sync.path:/datastore_server/re-sync/*}
request-handler
=
${openbis-sync.servlet-services.resource-sync.request-handler:ch.ethz.sis.openbis.generic.server.dss.plugins.sync.datasource.DataSourceRequestHandler}
request-handler.server-url
=
${server-url}/openbis
request-handler.download-url
=
${download-url}
request-handler.file-service-repository-path
=
${openbis-sync.servlet-services.resource-sync.request-handler.file-service-repository-path:../../data/file-server}
authentication-handler
=
${openbis-sync.servlet-services.resource-sync.authentication-handler:ch.systemsx.cisd.openbis.dss.generic.server.oaipmh.BasicHttpAuthenticationHandler}
The second configuration file is just there to create an AS database data source.
Special attention to
databaseKind
, if is not the default
prod
.
plugin.properties
# ./core-plugins/openbis-sync/2/dss/data-sources/openbis-db/plugin.properties
#
# Data source used to determine which entities have been deleted
#
databaseEngineCode
=
${openbis-sync.data-sources.openbis-db.databaseEngineCode:postgresql}
basicDatabaseName
=
${openbis-sync.data-sources.openbis-db.basicDatabaseName:openbis}
# This needs to match the databaseKind in the openBIS service.properties
databaseKind
=
${database.kind:prod}
In practice by default the service should be available at
## <DSS
base
URL>/datastore_server/re-sync
.
Please test this is available in your instance by opening such URL in your browser, using one of the 3 verbs available:
https://openbis-sis-ci-sprint.ethz.ch/datastore_server/re-sync/?verb=about.xml
https://openbis-sis-ci-sprint.ethz.ch/datastore_server/re-sync/?verb=capabilitylist.xml
https://openbis-sis-ci-sprint.ethz.ch:443/datastore_server/re-sync/?verb=resourcelist.xml
You will be asked with the openBIS user and password you want to access the service and the service will return an XML file.
Use case: One Datasource - One or more Harvester

The key is the fact that the XML file contains only the information visible for the selected user.
It is recommended to create a
user
on the
## Data
## Source
that can only see the set of data needed for a
## Harvester
.
This way is possible to shared different sets of data, using different
users
, for example:
sync-datasource-user-inventory: This user is OBSERVER of all inventory SPACE.
sync-datasource-user-project-a: This user is OBSERVER of project A.
Is recommended that technical users are created on the file authentication system of the instance and their
rights configured on  the admin UI.
Data Source Service Document

By default, the URL of the service is
## <DSS
base
URL>/datastore_server/re-sync
and supports the verbs mentioned previously.
For example for
https://openbis-sis-ci-sprint.ethz.ch/datastore_server/re-sync/?verb=about.xml
## we get:
<urlset
xsi:schemaLocation=
""https://sis.id.ethz.ch/software/#openbis/xdterms/ ./xml/xdterms.xsd https://sis.id.ethz.ch/software/#openbis/xmdterms/""
>
<rs:ln
href=
""https://openbis-sis-ci-sprint.ethz.ch:443/datastore_server/re-sync/?verb=about.xml""
rel=
""describedby""
/>
<rs:md
capability=
""description""
/>
<url>
<loc>
https://openbis-sis-ci-sprint.ethz.ch:443/datastore_server/re-sync/?verb=capabilitylist.xml
</loc>
<rs:md
capability=
""capabilitylist""
/>
</url>
</urlset>
From capabilities described in the ResourceSync Framework Specification only
resourcelist
is supported.
The resourcelist returns an XML with all metadata of the data source openBIS instance.
This includes master data, meta data including file meta data.
Two optional URL parameters filter the data by spaces:
black_list
: comma-separated list of regular expressions. All
entities which belong to a space which matches one of the regular
expressions of this list will be suppressed.
white_list
: comma-separated list of regular expressions. If
defined only entities which belong to a space which matches one of
the regular expressions of this list will be delivered (if not
suppressed by the black list).
## Remarks:
Basic HTTP authentication is used for authentication.
The resourcelist capability returns only data visible for the user
which did the authentication.
### Harvester Service Configuration

This service needs to be configured in a case by case basis.
## A
## Harvester
can sync with one or more
## Data
## Source
openBIS instances.
For that a
## Harvester
maintenance task
has to be configured
on the
## Harvester
openBIS instance.
## The
## Harvester
is a DSS maintenance task. An example config file follows:
plugin.properties
class
=
ch.ethz.sis.openbis.generic.server.dss.plugins.sync.harvester.HarvesterMaintenanceTask
interval
=
1 d
harvester-config-file
=
../../data/harvester-config.txt
The only specific property of
HarvesterMaintenanceTask
is
harvester-config-file
which is absolute or relative path to the actual
configuration file. This separation in two configuration files has been
done because
plugin.properties
is only read once (at start up of DSS).
Thus changes in Harvester configuration would be possible without
restarting DSS.
Here is an example of a typical configuration:
harvester-config.txt
## [LAB1]
resource-list-url
=
https://<data source host>:<DSS port>/datastore_server/re-sync
data-source-openbis-url
=
https://<data source host>:<AS port>/openbis/openbis
data-source-dss-url
=
https://<data source host>:<DSS port>/datastore_server
data-source-auth-realm
=
## OAI-PMH
data-source-auth-user
=
<data source user id>
data-source-auth-pass
=
<data source password>
space-black-list
=
## SYSTEM
space-white-list
=
## LAB1.*
harvester-user
=
<harvester user id>
harvester-pass
=
<harvester user password>
keep-original-timestamps-and-users
=
false
harvester-tmp-dir
=
temp
last-sync-timestamp-file
=
../../data/last-sync-timestamp-file_HRVSTR.txt
log-file
=
log/synchronization.log
email-addresses
=
<e-mail 1>, <e-mail 2>, ...
translate-using-data-source-alias
=
true
verbose
=
true
#dry-run = true
The configuration file can have one or many section for each openBIS
instance. Each section start with an arbitrary name in square
brackets. This name becomes the alias of the instance.
Warning: This alias SHOULD only contain letters and numbers.
Other characters MIGHT look like the work but can lead to errors when parsing the prefix.
<data
source
host>
,
## <DSS
port>
and
## <AS
port>
have to be host
name and ports of the Data Source openBIS instance as seen by the
Harvester instance.
<data
source
user
id>
and
<data
source
password>
are the
credential to access the Data Source openBIS instance. Only data
seen by this user is harvested.
space-black-list
and
space-white-list
have the same meaning
as
black_list
and
white_list
as specified above in the Data
Source section.
<harvester
user
id>
and
<harvester
user
password>
are the
credential to access the Harvester openBIS instance. It has to be a
user with instance admin rights.
## Temporary
files created during harvesting are stored
in
harvester-tmp-dir
which is a path relative to the root of the
data store. The root store is specified by
storeroot-dir
in
## DSS
service.properties
. The default value is
temp
.
By default the original timestamps (registration timestamps and
modification timestamps) and users (registrator and modifier) are
synchronized. If necessary users will be created. With the
configuration property
keep-original-timestamps-and-users
=
false
no timestamps and users will be synchronized.
## The
last-sync-timestamp-file
is a relative or absolute path to the
file which store the last timestamp of synchronization.
## The
log-file
is a relative or absolute path to the file where
synchronization information is logged. This information does not
appear in the standard DSS log file.
In case of an error an e-mail is sent to the specified e-mail
addresses.
translate-using-data-source-alias
is a flag which controls whether
the code of spaces, types and materials should have a prefix or not.
If true the prefix will be the name in the square bracket followed
by an underscore. The default value of this flag is false.
verbose
flag adds to the synchronization log added, updated and
## deleted items. Default:
false
or
true
if
dry-run
flag is set.
dry-run
flag allows to run without changing Harvester openBIS
instance. This allows to check config errors or errors with the Data
Source openBIS instance. A dry run will be performed even if this
## flag is set. Default:
false
master-data-update-allowed
flag allows to update master data as
plugins, property types, entity types and entity assignments. Note,
that master data can still be added if this flag is
false
.
## Default:
false
property-unassignment-allowed
flag allows to unassign property
assignments, that is, removing property types from entity types.
## Default:
false
deletion-allowed
flag allows deletion of entities on the Harvester
## openBIS instance. Default:
false
keep-original-timestamps-and-users
flag yields that time stamps
and users are copied from the Data Source to the Harvester.
Otherwise the entities will have harvester user and the actual
## registration time stamp. Default:
true
keep-original-frozen-flags
flag yields that the frozen flags are
copied from the Data Source to the Harvester. Otherwise entities
which are frozen on the Data Source are not frozen on the Harvester.
## Default:
true
This DSS service access the main openBIS database directly in order to
synchronize timestamps and users. If the name of this database isn’t
{{openbis_prod}} the property
database.kind
in DSS service.properties
should be defined with the same value as the same property in AS service.properties.
What HarvesterMaintenanceTask does

In the first step it reads the configuration file from the file path
specified by
harvester-config-file
in
plugins.properties
. Next, the
following steps will be performed in DRY RUN mode. That is, all data are
read, parsed and checked but nothing is changed on the Harvester. If no
error occured and the
dry-run
flag isn’t set the same steps are
performed but this time the data is changed (i.e. synced) on the
## Harvester.
Read meta data from the Data Source.
Delete entities from the Harvester which are no longer on the Data
Source (if
deletion-allowed
flag is set).
Register/update master data.
Register/update spaces, projects, experiments, samples and
materials.
Register/update attachments.
Synchronize files from the file service.
Register/update data sets.
Update timestamps and users (if
keep-original-timestamps-and-users
flag is set).
Update frozen flags (if
keep-original-frozen-flags
flag is set).
Data are registered if they do not exists on the Harvester.
Otherwise they are updated if the Data Source version has a
modification timestamp which is after the last time the
HarvesterMaintenanceTask has been performed
## If
translate-using-data-source-alias
flag is set a prefix is added
to spaces, types and materials when created.
To find out if an entity already exist on the Harvester the perm ID
is used.
Master Data Synchronization Rules

Normally all master data are registered/updated if they do not exists or
they are older. But for internal vocabularies and property types
different rules apply. Internal means that the entity (i.e. a vocabulary
or a property type) is managed internally (visible by the prefix ‘$’ in
its code) and has been registered by the system user.
Internal vocabularies and property types will not be created or
updated on the Harvester.
An internal vocabulary or property type of the Data Source which
doesn’t exist on the Harvester leads to an error.
An internal property type which exists on the Data Source and the
Harvester but have different data type leads to an error.
Terms of an internal vocabulary are added if they do not exists on
the Harvester.",Introduction,0,en_20.10.0-11_system-documentation_configuration_openbis-sync_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_openbis-sync.txt,2025-09-30T12:09:05.816400Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_optional-application-server-configuration:0,Optional Application Server Configuration,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/optional-application-server-configuration.html,openbis,"### Optional Application Server Configuration

The base URL for Web client access to the data store server.

download-url = ${host-address}:${port}
Export data limit in bytes, default to 10Gib

export.data-limit=10737418240
## Deleted Entity History

Logging the history of deleted entities can be enabled / disabled in the AS service.properties using setting
entity-history.enabled = \[true | false\]
Since 20.10.1 the default value is true (meaning, entity history is enabled). Before 20.10.1 the default value was false.
Deleted entity history can be queried with script
$INSTALL_PATH/bin/show-history.sh
.
## Login Page - Banners

To add banners to the main openBIS change
loginHeader.html
page. It is
stored in the same directory as
index.html
. Note that if the height of
loginHeader.html
is too big, the content may overlap with the rest of
the openBIS login page.
Example of the
loginHeader.html
## :
<center><img src=""images/banner.gif""></center>
For announcements you have to edit the
index.html
file. Here is an example showing the tail:
<
input
style
=
""margin-left: 200px""
type
=
""submit""
id
=
""openbis_login_submit""
value
=
## ""Login""
>
<
br
>
<
br
>
<
br
>
<
br
>
<
span
style
=
""color:red""
>
Due the server maintenance openBIS
<
br
>
will not be available on 24th of
<
br
>
December 2023 from 10 am to 3 pm!
<
br
>
</
span
>
</
form
>
</
div
>
</
body
>
</
html
>
Note: the current work-around with
br
tags between the lines ensures that the login box is still centered.
## Client Customization

### Configuration

To reconfigure some parts of the openBIS Web Client and Data Set Upload
Client, prepare the configuration file and add the path to the value of
web-client-configuration-file
property in openBIS
service.properties
.
web-client-configuration-file = etc/web-client.properties
Web client customizations

Enable the trashcan. When the trashcan is enabled, deleting entities
only marks them as deleted but not deletes them physically (it is
also called “logical deletion”). When clicking on the trashcan icon
in the Web GUI, the user can see all of his deletion operations and
can revert them individually. Only an admin can empty the trashcan
and thus delete the entities physically. Only with enabled trashcan
is it possible to delete complete hierarchies (e.g. an experiment
with samples and datasets attached).
Default view mode (
## SIMPLE/NORMAL
) that should be used if user
doesn’t have it specified in a URL.
Replacement texts for ‘Experiment’ and ‘Sample’ by
experiment-text
and
sample-text
, respectively.
Anonymous login by default.
Sample, material, experiment and data set
detail
views
can be
## customized by:
hiding the sections (e.g. attachments)
## Additionally
data
set
detail
view
can be customized by:
removing
## Smart
## View
and
## File
## View
from the list of available
reports in
## Data
## View
section
Technology specific properties with property
technologies
which is
a comma-separated list of technologies. For each technology
properties are defined where the property names start with
technology name followed by a dot character.
Data Set Upload Client Customizations

It is possible to restrict the set of data set types available to the
user in the data set uploader. This is useful when there are some data
set types that a user would never upload; for example, if there are data
set types that are used only internally exist only to support
third-party software.
The restriction is specified in the web-client.properties file using
either a whitelist or a blacklist. If both are specified, the whitelist
is used. To specify a whitelist, use the key
creatable-data-set-types-whitelist
; for a blacklist, use the key
creatable-data-set-types-blacklist
. The value for the property should
be a comma-separated list of regular-expression patterns to match. In
the case of the whitelist, data set types that match the specified
patterns are shown to the user, whereas for the blacklist, the data set
types that match the specified patterns are those that are not shown to
the user.
## Examples

Specifying a whitelist
web-client.properties.
creatable-data-set-types-whitelist = .*IMAGE.*, ANALYSIS, THUMBNAIL[0-9]?
Assume we have the following data set types in our system:
## PROCESSED-DATA
,
## MICROSCOPE-IMAGE
,
## IMAGE-SCREENING
,
## ANALYSIS
,
## ANALYSIS-FEATURES
,
## THUMBNAIL1
,
## THUMBNAIL-BIG
In this case, the follwing data set types will be available to the user:
## MICROSCOPE-IMAGE
,
## IMAGE-SCREENING
,
## ANALYSIS
,
## THUMBNAIL1
Specifying a blacklist
web-client.properties.
creatable-data-set-types-blacklist = .*IMAGE.*, ANALYSIS, THUMBNAIL[0-9]?
Assume we have the following data set types in our system:
## PROCESSED-DATA
,
## MICROSCOPE-IMAGE
,
## IMAGE-SCREENING
,
## ANALYSIS
,
## ANALYSIS-FEATURES
,
## THUMBNAIL1
,
## THUMBNAIL-BIG
In this case, the follwing data set types will be available to the user:
## PROCESSED-DATA
,
## ANALYSIS-FEATURES
,
## THUMBNAIL-BIG
Full web-client.properties Example

web-client.properties
# Enable the trash can and logical deletion.
# Default value: false
enable
-
trash
=
true
# Replacement texts for 'Experiment' and 'Sample' in the UI
# sample-text = Object
# experiment-text = Collection
# Default view mode that should be used if user doesn't have it specified in URL.
# Options: 'NORMAL' (standard or application mode - default), 'SIMPLE' (read-only mode with simplified GUI)
#
default
-
view
-
mode
=
## SIMPLE
# Flag specifying whether default login mode is anonymous or not.
# If true a user-for-anonymous-login has to be defined in service.properties
# Default value: false
default
-
anonymous
-
login
=
true
# Configuration of entity (experiment, sample, data set, material) detail views.
#
# Mandatory properties:
#   - view (entity detail view id)
#   - types (list of entity type codes)
# Optional properties:
#   - hide-sections (list of section ids)
#   - hide-smart-view (removes ""Smart View"" from Data Set Detail View -> Data View) (generic_dataset_viewer)
#   - hide-file-view (removes ""File View"" from Data Set Detail View -> Data View) (generic_dataset_viewer)
# Available sections in entity-detail-views:
#   generic_dataset_viewer
#       data-set-data-section
#       data-set-parents-section
#       data-set-children-section
#       query-section
#   generic_experiment_viewer
#       data-set-section
#       attachment-section
#       query-section
#       container-sample-section
#   generic_sample_viewer
#       container-sample-section
#       derived-samples-section
#       parent-samples-section
#       data-set-section
#       attachment-section
#       query-section
#   generic_material_viewer
#       query-section
#
# Example:
#
#detail-views = sample-view, experiment-view, data-view
#
#sample-view.view = generic_sample_viewer
#sample-view.types = STYPE1, STYPE2
#sample-view.hide-sections = derived-samples-section, container-sample-section
#
#experiment-view.view = generic_sample_viewer
#experiment-view.types = ETYPE1, ETYPE2
#experiment-view.hide-sections = data-set-section
#
#data-view.view = generic_dataset_viewer
#data-view.types = DSTYPE
#data-view.hide-smart-view = false
#data-view.hide-file-view = true
#technologies = screening
#screening.image-viewer-enabled = true
#
# Only render these types when creating new data sets via the
# Data Set Upload Client
#
#creatable-data-set-types-whitelist=WHITELISTED_TYPE1, WHITELISTED_TYPE2
#
# Do not render these types in the Data Set Upload Client.
# The value of the property is only taken into account if
# creatable-data-set-types-whitelist is not configured
#
#creatable-data-set-types-blacklist=BLACKLISTED_TYPE1, BLACKLISTED_TYPE2
## Configuring File Servlet

This service is specially tailored for web applications requiring to
upload files to the system without using the DataSet concept, it was
meant to be used for small images and rich text editors like CKEditor.
## Property Key
## Default Value
## Description
file-server.maximum-file-size-in-MB
10
Max size of files.
file-server.repository-path
../../../data/file-server
Path where files will be stored, ideally should be a folder on the same NAS you are storing the DataSets.
file-server.download-check
true
Checks that the user is log in into the system to be able to download files.
Changing the Capability-Role map

openBIS uses a map of capabilities to roles to decide what role is
needed to perform a given action. The defaults can be overridden by
creating a file
etc/capabilities
. One line in this file has one of the
## following formats:
<
## Capability
>
## :
<
## Role
>
[,
<
## ROLE
>...
]
<
## Capability
>
## :
<
## Role
>
[,
<
## ROLE
>...
][;
<
## Parameter
>
=
<
## Role
>
[,
<
## Role
>...
]][;
<
## Parameter
>
=
<
## Role
>
[,
<
## Role
>
]]
...
<
## Capability
>
## :
<
## Parameter
>
=
<
## Role
>
[,
<
## Role
>...
][;
<
## Parameter
>
=
<
## Role
>
[,
<
## Role
>
]]
...
which sets a new (minimal) role for the given capability. There is a
special role
## INSTANCE_DISABLED
which allows to completely disable a
capability for an instance. Note: to set multiple roles for single
capability use multiple lines in the file.
This is the default map:
## Capability
## Parameter
## Default Role
## Comment
## WRITE_CUSTOM_COLUMN
## PROJECT_POWER_USER
## DELETE_CUSTOM_COLUMN
## PROJECT_POWER_USER
## WRITE_FILTER
## PROJECT_POWER_USER
## DELETE_FILTER
## PROJECT_POWER_USER
## WRITE_EXPERIMENT_SAMPLE
## PROJECT_USER
## WRITE_EXPERIMENT_ATTACHMENT
## PROJECT_USER
## WRITE_EXPERIMENT_PROPERTIES
## PROJECT_USER
## DELETE_EXPERIMENT
## PROJECT_POWER_USER
## DELETE_EXPERIMENT_ATTACHMENT
## PROJECT_POWER_USER
## WRITE_SAMPLE
## PROJECT_USER
## WRITE_SAMPLE_ATTACHMENT
## PROJECT_USER
## WRITE_SAMPLE_PROPERTIES
## PROJECT_USER
## DELETE_SAMPLE
## PROJECT_POWER_USER
## DELETE_SAMPLE_ATTACHMENT
## PROJECT_POWER_USER
## WRITE_DATASET
## PROJECT_POWER_USER
## WRITE_DATASET_PROPERTIES
## PROJECT_USER
## DELETE_DATASET
## PROJECT_POWER_USER
Delete datasets (this capability IS NOT enough to delete datasets with deletion_disallow flag set to true in their type - see
## FORCE_DELETE_DATASET
)
## FORCE_DELETE_DATASET
## INSTANCE_DISABLED
Delete datasets (this capability IS enough to delete datasets with deletion_disallow flag set to true in their type - see
## DELETE_DATASET
)
## ARCHIVE_DATASET
## PROJECT_POWER_USER
Move dataset from data store into archive
## UNARCHIVE_DATASET
## PROJECT_USER
Copy back dataset from archive to data store
## LOCK_DATA_SETS
## PROJECT_ADMIN
Prevent data sets from being archived
## UNLOCK_DATA_SETS
## PROJECT_ADMIN
Release locked data sets
## WRITE_EXPERIMENT_SAMPLE_MATERIAL
## INSTANCE_ADMIN
Registration / update of experiments, samples and materials in one go
## REGISTER_SPACE
## SPACE_ADMIN
The user will become space admin of the freshly created space
## DELETE_SPACE
## SPACE_ADMIN
## UPDATE_SPACE
## SPACE_ADMIN
## REGISTER_PROJECT
## SPACE_POWER_USER
## WRITE_PROJECT
## SPACE_POWER_USER
,
## PROJECT_ADMIN
## WRITE_PROJECT_ATTACHMENT
## SPACE_POWER_USER
,
## PROJECT_ADMIN
## DELETE_PROJECT
## SPACE_POWER_USER
,
## PROJECT_ADMIN
## DELETE_PROJECT_ATTACHMENT
## SPACE_POWER_USER
,
## PROJECT_ADMIN
## REGISTER_VOCABULARY
## INSTANCE_ADMIN
## WRITE_VOCABULARY
## INSTANCE_ADMIN
## DELETE_VOCABULARY
## INSTANCE_ADMIN
## WRITE_VOCABULARY_TERM
## INSTANCE_ADMIN
## WRITE_UNOFFICIAL_VOCABULARY_TERM
## PROJECT_USER
## PURGE
## PROJECT_ADMIN
Permanently delete experiments, samples and datasets in the trashcan (this capability IS NOT enough to delete datasets with deletion_disallow flag set to true in their type - see
## FORCE_PURGE
)
## FORCE_PURGE
## INSTANCE_DISABLED
Permanently delete experiments, samples and datasets in the trashcan (this capability IS enough to delete datasets with deletion_disallow flag set to true in their type - see
## PURGE
)
## REVERT_DELETION
## PROJECT_USER
Get back experiments, samples and datasets from the trashcan
## ASSIGN_EXPERIMENT_TO_PROJECT
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## ASSIGN_PROJECT_TO_SPACE
## SPACE_POWER_USER
,
## SPACE_ETL_SERVER
## ASSIGN_SAMPLE_TO_EXPERIMENT
## PROJECT_USER
,
## SPACE_ETL_SERVER
Re-assign a sample to a new experiment (called in ‘register experiment’, ‘update experiment’, ‘update sample’’)
## UNASSIGN_SAMPLE_FROM_EXPERIMENT
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## ASSIGN_SAMPLE_TO_SPACE
## SPACE_POWER_USER
,
## SPACE_ETL_SERVER
Re-assign a sample to a new space (called in ‘update sample’)
## ASSIGN_DATASET_TO_EXPERIMENT
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## ASSIGN_DATASET_TO_SAMPLE
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## SHARE_SAMPLE
## INSTANCE_ADMIN
,
## INSTANCE_ETL_SERVER
## UNSHARE_SAMPLE
## INSTANCE_ADMIN
,
## INSTANCE_ETL_SERVER
## ADD_PARENT_TO_SAMPLE
## PROJECT_USER
,
## SPACE_ETL_SERVER
## ADD_PARENT_TO_SAMPLE
## SAMPLE
## PROJECT_USER
,
## SPACE_ETL_SERVER
## ADD_PARENT_TO_SAMPLE
## PARENT
## PROJECT_USER
,
## SPACE_ETL_SERVER
## REMOVE_PARENT_FROM_SAMPLE
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## REMOVE_PARENT_FROM_SAMPLE
## SAMPLE
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## REMOVE_PARENT_FROM_SAMPLE
## PARENT
## PROJECT_USER
,
## SPACE_ETL_SERVER
## ADD_CONTAINER_TO_SAMPLE
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## REMOVE_CONTAINER_FROM_SAMPLE
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## ADD_PARENT_TO_DATASET
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## REMOVE_PARENT_FROM_DATASET
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## ADD_CONTAINER_TO_DATASET
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## REMOVE_CONTAINER_FROM_DATASET
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## ASSIGN_ROLE_TO_SPACE_VIA_DSS
## SPACE_ADMIN
,
## INSTANCE_ETL_SERVER
## CREATE_SPACES_VIA_DSS
## SPACE_ADMIN
,
## INSTANCE_ETL_SERVER
## CREATE_PROJECTS_VIA_DSS
## SPACE_POWER_USER
,
## SPACE_ETL_SERVER
## UPDATE_PROJECTS_VIA_DSS
## SPACE_POWER_USER
,
## PROJECT_ADMIN
,
## SPACE_ETL_SERVER
## CREATE_EXPERIMENTS_VIA_DSS
## PROJECT_USER
,
## SPACE_ETL_SERVER
## UPDATE_EXPERIMENTS_VIA_DSS
## PROJECT_USER
,
## SPACE_ETL_SERVER
## CREATE_SPACE_SAMPLES_VIA_DSS
## PROJECT_USER
,
## SPACE_ETL_SERVER
## UPDATE_SPACE_SAMPLES_VIA_DSS
## PROJECT_USER
,
## SPACE_ETL_SERVER
## CREATE_INSTANCE_SAMPLES_VIA_DSS
## INSTANCE_ETL_SERVER
## UPDATE_INSTANCE_SAMPLES_VIA_DSS
## INSTANCE_ETL_SERVER
## CREATE_MATERIALS_VIA_DSS
## INSTANCE_ETL_SERVER
## UPDATE_MATERIALS_VIA_DSS
## INSTANCE_ETL_SERVER
## CREATE_DATA_SETS_VIA_DSS
## PROJECT_USER
,
## SPACE_ETL_SERVER
## UPDATE_DATA_SETS_VIA_DSS
## PROJECT_POWER_USER
,
## SPACE_ETL_SERVER
## SEARCH_ON_BEHALF_OF_USER
## INSTANCE_OBSERVER
All search or list operations being performed on behalf of another user. Supposed to be used by a service user for server-to-server communication tasks.
Older versions of openBIS used to allow changing entity relationships to
regular
## SPACE_USER
. If you want to get this behavior back, put these
lines into
etc/capabilities
## :
## ASSIGN_EXPERIMENT_TO_PROJECT
## :
## SPACE_USER
## ASSIGN_EXPERIMENT_TO_PROJECT
## :
## SPACE_ETL_SERVER
## ASSIGN_SAMPLE_TO_EXPERIMENT
## :
## SPACE_USER
## ASSIGN_SAMPLE_TO_EXPERIMENT
## :
## SPACE_ETL_SERVER
## UNASSIGN_SAMPLE_FROM_EXPERIMENT
## :
## SPACE_USER
## UNASSIGN_SAMPLE_FROM_EXPERIMENT
## :
## SPACE_ETL_SERVER
## ASSIGN_SAMPLE_TO_SPACE
## :
## SPACE_USER
## ASSIGN_SAMPLE_TO_SPACE
## :
## SPACE_ETL_SERVER
## ASSIGN_DATASET_TO_EXPERIMENT
## :
## SPACE_USER
## ASSIGN_DATASET_TO_EXPERIMENT
## :
## SPACE_ETL_SERVER
## ASSIGN_DATASET_TO_SAMPLE
## :
## SPACE_USER
## ASSIGN_DATASET_TO_SAMPLE
## :
## SPACE_ETL_SERVER
## ADD_PARENT_TO_SAMPLE
## :
## SPACE_USER
## ADD_PARENT_TO_SAMPLE
## :
## SPACE_ETL_SERVER
## REMOVE_PARENT_FROM_SAMPLE
## :
## SPACE_USER
## REMOVE_PARENT_FROM_SAMPLE
## :
## SPACE_ETL_SERVER
## ADD_CONTAINER_TO_SAMPLE
## :
## SPACE_USER
## ADD_CONTAINER_TO_SAMPLE
## :
## SPACE_ETL_SERVER
## REMOVE_CONTAINER_FROM_SAMPLE
## :
## SPACE_USER
## REMOVE_CONTAINER_FROM_SAMPLE
## :
## SPACE_ETL_SERVER
## ADD_PARENT_TO_DATASET
## :
## SPACE_USER
## ADD_PARENT_TO_DATASET
## :
## SPACE_ETL_SERVER
## REMOVE_PARENT_FROM_DATASET
## :
## SPACE_USER
## REMOVE_PARENT_FROM_DATASET
## :
## SPACE_ETL_SERVER
## ADD_CONTAINER_TO_DATASET
## :
## SPACE_USER
## ADD_CONTAINER_TO_DATASET
## :
## SPACE_ETL_SERVER
## REMOVE_CONTAINER_FROM_DATASET
## :
## SPACE_USER
## REMOVE_CONTAINER_FROM_DATASET
## :
## SPACE_ETL_SERVER
Capability Role Map for V3 API

Method of IApplicationServerApi
## Default Roles
## Capability
archiveDataSets
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## ARCHIVE_DATASET
confirmDeletions, forceDeletion == false
## PROJECT_ADMIN, SPACE_ETL_SERVER
## CONFIRM_DELETION
confirmDeletions, forceDeletion == true
disabled
## CONFIRM_DELETION_FORCED
createAuthorizationGroups
## INSTANCE_ADMIN
## CREATE_AUTHORIZATION_GROUP
createCodes
## PROJECT_USER, SPACE_ETL_SERVER
## CREATE_CODES
createDataSetTypes
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## CREATE_DATASET_TYPE
createDataSets
## PROJECT_USER, SPACE_ETL_SERVER
## CREATE_DATASET
createExperimentTypes
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## CREATE_EXPERIMENT_TYPE
createExperiments
## PROJECT_USER, SPACE_ETL_SERVER
## CREATE_EXPERIMENT
createExternalDataManagementSystems
## INSTANCE_ADMIN
## CREATE_EXTERNAL_DMS
createMaterialTypes
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## CREATE_MATERIAL_TYPE
createMaterials
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## CREATE_MATERIAL
createPermIdStrings
## PROJECT_USER, SPACE_ETL_SERVER
## CREATE_PERM_IDS
createPersons
## INSTANCE_ADMIN
## CREATE_PERSON
createPlugins
## INSTANCE_ADMIN
## CREATE_PLUGIN
createProjects
## SPACE_POWER_USER, SPACE_ETL_SERVER
## CREATE_PROJECT
createPropertyTypes
## INSTANCE_ADMIN
## CREATE_PROPERTY_TYPE
createQueries
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## CREATE_QUERY
createRoleAssignments, instance role
## INSTANCE_ADMIN
## CREATE_INSTANCE_ROLE
createRoleAssignments, space role
## SPACE_ADMIN
## CREATE_SPACE_ROLE
createRoleAssignments, project role
## PROJECT_ADMIN
## CREATE_PROJECT_ROLE
createSampleTypes
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## CREATE_SAMPLE_TYPE
createSamples
## PROJECT_USER, SPACE_ETL_SERVER
## CREATE_SAMPLE
createSemanticAnnotations
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## CREATE_SEMANTIC_ANNOTATION
createSpaces
## SPACE_ADMIN, SPACE_ETL_SERVER
## CREATE_SPACE
createTags
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## CREATE_TAG
createVocabularies
## INSTANCE_ADMIN
## CREATE_VOCABULARY
createVocabularyTerms, official terms
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## CREATE_OFFICIAL_VOCABULARY_TERM
createVocabularyTerms, unofficial terms
## PROJECT_USER, SPACE_ETL_SERVER
## CREATE_UNOFFICIAL_VOCABULARY_TERM
deleteAuthorizationGroups
## INSTANCE_ADMIN
## DELETE_AUTHORIZATION_GROUP
deleteDataSetTypes
## INSTANCE_ADMIN
## DELETE_DATASET_TYPE
deleteDataSets
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## DELETE_DATASET
deleteExperimentTypes
## INSTANCE_ADMIN
## DELETE_EXPERIMENT_TYPE
deleteExperiments
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## DELETE_EXPERIMENT
deleteExternalDataManagementSystems
## INSTANCE_ADMIN
## DELETE_EXTERNAL_DMS
deleteMaterialTypes
## INSTANCE_ADMIN
## DELETE_MATERIAL_TYPE
deleteMaterials
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## DELETE_MATERIAL
deleteOperationExecutions
## PROJECT_USER, SPACE_ETL_SERVER
## DELETE_OPERATION_EXECUTION
deletePlugins
## INSTANCE_ADMIN
## DELETE_PLUGIN
deleteProjects
## SPACE_POWER_USER, PROJECT_ADMIN, SPACE_ETL_SERVER
## DELETE_PROJECT
deletePropertyTypes
## INSTANCE_ADMIN
## DELETE_PROPERTY_TYPE
deleteQueries
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## DELETE_QUERY
deleteRoleAssignments, instance role
## INSTANCE_ADMIN
## DELETE_INSTANCE_ROLE
deleteRoleAssignments, space role
## SPACE_ADMIN
## DELETE_SPACE_ROLE
deleteRoleAssignments, project role
## PROJECT_ADMIN
## DELETE_PROJECT_ROLE
deleteSampleTypes
## INSTANCE_ADMIN
## DELETE_SAMPLE_TYPE
deleteSamples
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## DELETE_SAMPLE
deleteSemanticAnnotations
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## DELETE_SEMANTIC_ANNOTATION
deleteSpaces
## SPACE_ADMIN, SPACE_ETL_SERVER
## DELETE_SPACE
deleteTags
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## DELETE_TAG
deleteVocabularies
## INSTANCE_ADMIN
## DELETE_VOCABULARY
deleteVocabularyTerms
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## DELETE_VOCABULARY_TERM
executeAggregationService
## PROJECT_OBSERVER
## EXECUTE_AGGREGATION_SERVICES
executeCustomASService
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## EXECUTE_CUSTOM_AS_SERVICE
executeProcessingService
## PROJECT_USER
## EXECUTE_PROCESSING_SERVICES
executeQuery
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## EXECUTE_QUERY
executeReportingService
## PROJECT_OBSERVER
## EXECUTE_REPORTING_SERVICES
executeSearchDomainService
## PROJECT_OBSERVER
## EXECUTE_SEARCH_DOMAIN_SERVICES
executeSql
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## EXECUTE_QUERY
getAuthorizationGroups
## PROJECT_ADMIN
## GET_AUTHORIZATION_GROUP
getDataSetTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_DATASET_TYPE
getDataSets
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_DATASET
getExperimentTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_EXPERIMENT_TYPE
getExperiments
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_EXPERIMENT
getExternalDataManagementSystems
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_EXTERNAL_DMS
getMaterialTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_MATERIAL_TYPE
getMaterials
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_MATERIAL
getOperationExecutions
## PROJECT_USER, SPACE_ETL_SERVER
## GET_OPERATION_EXECUTION
getPersons
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_PERSON
getPlugins
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_PLUGIN
getProjects
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_PROJECT
getPropertyTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_PROPERTY_TYPE
getQueries
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_QUERY
getRoleAssignments
## PROJECT_ADMIN
## GET_ROLE_ASSIGNMENT
getSampleTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_SAMPLE_TYPE
getSamples
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_SAMPLE
getSemanticAnnotations
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_SEMANTIC_ANNOTATION
getSessionInformation
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_SESSION
getSpaces
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_SPACE
getTags
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_TAG
getVocabularies
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_VOCABULARY
getVocabularyTerms
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_VOCABULARY_TERM
lockDataSets
## PROJECT_ADMIN
## LOCK_DATASET
revertDeletions
## PROJECT_USER, SPACE_ETL_SERVER
## REVERT_DELETION
searchAggregationServices
## PROJECT_OBSERVER
## SEARCH_AGGREGATION_SERVICES
searchAuthorizationGroups
## PROJECT_ADMIN
## SEARCH_AUTHORIZATION_GROUP
searchCustomASServices
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_CUSTOM_AS_SERVICES
searchDataSetTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_DATASET_TYPE
searchDataSets
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_DATASET
searchDataStores
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_DATASTORE
searchDeletions
## PROJECT_USER, SPACE_ETL_SERVER
## SEARCH_DELETION
searchExperimentTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_EXPERIMENT_TYPE
searchExperiments
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_EXPERIMENT
searchExternalDataManagementSystems
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_EXTERNAL_DMS
searchGlobally
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_GLOBALLY
searchMaterialTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_MATERIAL_TYPE
searchMaterials
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_MATERIAL
searchObjectKindModifications
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_OBJECT_KIND_MODIFICATION
searchOperationExecutions
## PROJECT_USER, SPACE_ETL_SERVER
## GET_OPERATION_EXECUTION
searchPersons
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## GET_PERSON
searchPlugins
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_PLUGIN
searchProcessingServices
## PROJECT_OBSERVER
## SEARCH_PROCESSING_SERVICES
searchProjects
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_PROJECT
searchPropertyTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_PROPERTY_TYPE
searchQueries
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_QUERY
searchReportingServices
## PROJECT_OBSERVER
## SEARCH_REPORTING_SERVICES
searchRoleAssignments
## PROJECT_ADMIN
## SEARCH_ROLE_ASSIGNMENT
searchSampleTypes
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_SAMPLE_TYPE
searchSamples
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_SAMPLE
searchSearchDomainServices
## PROJECT_OBSERVER
## SEARCH_SEARCH_DOMAIN_SERVICES
searchSemanticAnnotations
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_SEMANTIC_ANNOTATION
searchSpaces
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_SPACE
searchTags
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_TAG
searchVocabularies
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_VOCABULARY
searchVocabularyTerms
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## SEARCH_VOCABULARY_TERM
unarchiveDataSets
## PROJECT_USER, SPACE_ETL_SERVER
## UNARCHIVE_DATASET
unlockDataSets
## PROJECT_ADMIN
## UNLOCK_DATASET
updateAuthorizationGroups
## INSTANCE_ADMIN
## UPDATE_AUTHORIZATION_GROUP
updateDataSetTypes
## INSTANCE_ADMIN
## UPDATE_DATASET_TYPE
updateDataSets
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## UPDATE_DATASET
updateDataSets, properties
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## UPDATE_DATASET_PROPERTY
updateExperimentTypes
## INSTANCE_ADMIN
## UPDATE_EXPERIMENT_TYPE
updateExperiments
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_EXPERIMENT
updateExperiments, attachments
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_EXPERIMENT_ATTACHMENT
updateExperiments, properties
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_EXPERIMENT_PROPERTY
updateExternalDataManagementSystems
## INSTANCE_ADMIN
## UPDATE_EXTERNAL_DMS
updateMaterialTypes
## INSTANCE_ADMIN
## UPDATE_MATERIAL_TYPE
updateMaterials
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## UPDATE_MATERIAL
updateMaterials, properties
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## UPDATE_MATERIAL_PROPERTY
updateOperationExecutions
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_OPERATION_EXECUTION
updatePersons, activate
## INSTANCE_ADMIN
## ACTIVATE_PERSON
updatePersons, deactivate
## INSTANCE_ADMIN
## DEACTIVATE_PERSON
updatePersons, set home space
## SPACE_ADMIN
## UPDATE_HOME_SPACE
updatePlugins
## INSTANCE_ADMIN
## UPDATE_PLUGIN
updateProjects
## SPACE_POWER_USER, PROJECT_ADMIN, SPACE_ETL_SERVER
## UPDATE_PROJECT
updateProjects, attachments
## SPACE_POWER_USER, PROJECT_ADMIN, SPACE_ETL_SERVER
## UPDATE_PROJECT_ATTACHMENT
updatePropertyTypes
## INSTANCE_ADMIN
## UPDATE_PROPERTY_TYPE
updateQueries
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## UPDATE_QUERY
updateSampleTypes
## INSTANCE_ADMIN
## UPDATE_SAMPLE_TYPE
updateSamples
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_SAMPLE
updateSamples, attachments
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_SAMPLE_ATTACHMENT
updateSamples, properties
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_SAMPLE_PROPERTY
updateSemanticAnnotations
## INSTANCE_ADMIN, INSTANCE_ETL_SERVER
## UPDATE_SEMANTIC_ANNOTATION
updateSpaces
## SPACE_ADMIN, SPACE_ETL_SERVER
## UPDATE_SPACE
updateTags
## PROJECT_OBSERVER, SPACE_ETL_SERVER
## UPDATE_TAG
updateVocabularies
## INSTANCE_ADMIN
## UPDATE_VOCABULARY
updateVocabularyTerms, official terms
## PROJECT_POWER_USER, SPACE_ETL_SERVER
## UPDATE_OFFICIAL_VOCABULARY_TERM
updateVocabularyTerms, unofficial terms
## PROJECT_USER, SPACE_ETL_SERVER
## UPDATE_UNOFFICIAL_VOCABULARY_TERM",Optional Application Server Configuration,0,en_20.10.0-11_system-documentation_configuration_optional-application-server-configuration_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_optional-application-server-configuration.txt,2025-09-30T12:09:05.902418Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_optional-datastore-server-configuration:0,Optional Datastore Server Configuration,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/optional-datastore-server-configuration.html,openbis,"### Optional Datastore Server Configuration

Configuring DSS Data Sources

It is quite common that openBIS AS is using a database filled by DSS.
Depending on the DSS (specified by the DSS code) and the technology
different databases have to be used.
### Configuration is best done by AS
core
plugins
of type
dss-data-sources
. The name of the plugin is just the DSS code. The
following properties of
plugin.properties
## are recognized:
## Property Key
## Description
technology
Normally the technology/module folder of the core plugin specifies the technology/module for which this data source has to be configured. If this is not the case this property allows to specify the technology/module independently.
database-driver
Fully qualified class name of the data base driver, e.g.
org.postgresql.Driver
.
database-url
URL of the database, e.g.
jdbc:postgresql://localhost/imaging_dev
username
Optional user name needed to access database.
password
Optional password needed to access database.
validation-query
Optional SQL script to be executed to validate database connections.
database-max-idle-connections
The maximum number of connections that can remain idle in the pool. A negative value indicates that there is no limit. Default: -1
database-max-active-connections
The maximum number of active connections that can be allocated at the same time. A negative value indicates that there is no limit. Default: -1
database-max-wait-for-connection
The maximum number of seconds that the pool will wait for a connection to be returned before throwing an exception. A value less than or equal to zero means the pool is set to wait indefinitely. Default: -1
database-active-connections-log-interval
The interval (in ms) between two regular log entries of currently active database connections if more than one connection is active. Default: Disabled
database-active-number-connections-log-threshold
The number of active connections that will trigger a NOTIFY log and will switch on detailed connection logging. Default: Disabled
database-log-stacktrace-on-connection-logging
If true and logging enabled also stack traces are logged. Default:
false
## Properties
database-driver
and
database-url
can be omitted if a
etc/dss-datasource-mapping
is defined. For more see
## Sharing Databases
.",Optional Datastore Server Configuration,0,en_20.10.0-11_system-documentation_configuration_optional-datastore-server-configuration_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_optional-datastore-server-configuration.txt,2025-09-30T12:09:05.973528Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_querying-project-db:0,Querying Project Database,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/querying-project-db.html,openbis,"## Querying Project Database

In some customized versions of openBIS an additional project-specific
database is storing data from registered data sets. This database can be
queried via SQL Select statements in openBIS Web application. In order
to protect modification of this database by malicious SQL code openBIS
application server should access this database as a user which is member
of a read-only group. The name of this read-only group is project
specific.
## Note
It is possible to configure openBIS to query multiple project-specific databases.
Create Read-Only User in PostgreSQL

A new user (aka role) is created by
## CREATE
## ROLE
<
read
-
only
user
>
## LOGIN
## NOSUPERUSER
## INHERIT
## NOCREATEDB
## NOCREATEROLE
;
This new user is added to the read-only group by the following command:
## GRANT
<
read
-
only
group
>
## TO
<
read
-
only
user
>
;
The name of the read-only group can be obtained by having a look into
the list of all groups:
## SELECT
*
from
## PG_GROUP
;
Note that by default openBIS creates a user
openbis_readonly
which has read-only permissions to all database objects. You can use
this user to access the openBIS meta database through the openBIS query
interface.
## Enable Querying

To enable querying functionality for additional databases in openBIS Web  application a
core plugin
of type query-databases has to be created. The following
plugin.properties
have to be specified:
## Property
## Description
label
Label of the database. It will be used in the Web application in drop down lists for adding / editing customized queries.
database-driver
JDBC Driver of the database, e.g. org.postgresql.Driver for postgresql.
database-url
JDBC URL to the database containing full database name, e.g. jdbc:postgresql://localhost/database_name for postgresql
database-username
Above-mentioned defined read-only user.
database-password
Password of the read-only user.
Configure Authorization for Querying

In order to configure authorization, two properties can be configured:
## Property
## Description
.data-space
To which data-space this database belongs to (optional, i.e. a query database can be configured not to belong to one data space by leaving this configuration value empty).
.creator-minimal-role
What role is required to be allowed to create / edit queries on this database (optional, default: INSTANCE_OBSERVER if data-space is not set, POWER_USER otherwise).
The given parameters data-space and creator-minimal-role are used by openBIS to enforce proper authorization.
For example, if
data-space = CISD
creator-minimal-role = SPACE_ADMIN
is configured, then for the query database configured with key
db1
## :
only a
## SPACE_ADMIN
on data space
## CISD
and an
## INSTANCE_ADMIN
are allowed to create / edit queries,
only a user who has the
## OBSERVER
role in data space
## CISD
is allowed to execute a query.
For query databases that do not belong to a space but that have a column with any of the
magic column names
, the query result is filtered on a per-row basis according to what the user executing the query is allowed to see. In detail this means: if the user executing the query is not an instance admin, filter out all rows which belong to a data space where the user doesn’t have a least the observer role. The relationship between a row and a data space is established by means of the experiment / sample / data set whose
permId
is given by one of the magical column names.
For sensitive data where authorization needs to be enforced, there are
## two setups possible:
Configure a query database that
does not
belong to a data space
and set the creator-minimal-role to
## INSTANCE_ADMIN
. Any instance
admin can be trusted to understand authorization issues and ensure
that only queries are added for this query database that contain a
proper reference to an experiment / sample / data set. This way, it
can be ensured that only properly filtered query results are
returned to the user running the query.
Configure a query database that
does
belong to a specific data
space and set the creator-minimal-role to
## POWER_USER
## . The
datastore server (or whatever server maintains the query database)
ensures that only information related to the configured data space
is added to the query database. Thus whatever query the power user
writes for this database, it will only reveal information from this
data space. As only users with
## OBSERVER
role on this data space
are allowed to execute the query, authorization is enforced properly
without the need of filtering query results.",Querying Project Database,0,en_20.10.0-11_system-documentation_configuration_querying-project-db_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_querying-project-db.txt,2025-09-30T12:09:06.037517Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_required-configuration:0,openBIS Server Configuration,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/required-configuration.html,openbis,"### openBIS Server Configuration

After successful installation, the openBIS configuration files (which are extended Java property files) of the Application Server (AS) and data store server (DSS) should be checked. All necessary adjustments to those files should be made prior to running the system in production.
### Application Server Configuration

The openBIS Application Server is configured using the file
$INSTALL_PATH/servers/openBIS-server/jetty/etc/service.properties
.
Each configuration item of the default service.properties file is self-documented by means of inline comments.
## Database Settings

All properties starting with
database.
specify the settings for the openBIS database. They are all mandatory.
## Property
## Description
database.engine
Type of database. Currently only postgresql is supported.
database.create-from-scratch
If true the database will be dropped and an empty database will be created. In productive use always set this value to  false  .
database.script-single-step-mode
If true all SQL scripts are executed in single step mode. Useful for localizing errors in SQL scripts. Should be always false in productive mode.
database.url-host-part
Part of JDBC URL denoting the host of the database server. If openBIS Application Server and database server are running on the same machine this property should be an empty string.
database.kind
Part of the name of the database. The full name reads openbis_<  kind  >.
database.admin-user
ID of the user on database server with admin rights, like creation of tables. Should be an empty string if default admin user should be used. In case of PostgreSQL the default admin user is assumed to be postgres.
database.admin-password
Password for admin user. Usual an empty string.
database.owner
ID of the user owning the data. This should generally be openbis. The correspoding role (and password matching the property
database.owner-password
) needs to be created on the PostgreSQL database prior to starting openBIS. In case of an empty string, it is the same user who started up the openBIS Application Server.
database.owner-password
Password of the owner.
Additional configuration options are outlined
here
.
Data Store Server Configuration

The openBIS Data Store Server is configured using the file
$INSTALL_PATH/servers/datastore_server/etc/service.properties
.
Each configuration item of the default service.properties file is self-documented by means of inline comments.
Additional configuration options are outlined
here
.",openBIS Server Configuration,0,en_20.10.0-11_system-documentation_configuration_required-configuration_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_required-configuration.txt,2025-09-30T12:09:06.256518Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_share-ids:0,Share IDs,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/share-ids.html,openbis,"Share IDs

## Motivation

An openBIS instance for a facility often needs the possibility that each customer can have its one disk space in the data store. This means a mapping is needed to decided in eager shuffling (by using MappingBasedShareFinder) and archiving (see Archiver for Facilities) to which share and which archive the data set should go.
## Syntax

For this purpose a single mapping file is used. It is a tab-separated value file with three columns: Identifier, Share IDs, Archive Folder. The file contains a row with the headers (which can be arbitrary because they are not checked). Each following row are of the form
## TAB
## TAB
.
: This is a regular expression for an experiment identifier (/
/
/
), a project identifier (/
/
), or a space identifier (/
).
: This is a comma-separated list of zero-to-many share IDs.
: This is a comma-separated list of absolute or relative paths to the archive folders. The list can contain zero, one or two paths. When this column is empty then the row should be ignored for archive folder mapping. When the column contains exactly one path, then it is treated as a common archive folder for all data sets no matter of their size. When the column contains two paths, then the first one is an archive folder for “big” data sets and the other is an archive folder for “small” data sets. Which data sets are considered “big” and which are “small” is controlled by “small-data-sets-size-limit” archiver property (see ZIP and TAR archivers). When this column contains two paths then “small-data-sets-size-limit” property becomes mandatory.
## Resolving Rules

The mapping algorithm selects for a specified data set a line from the mapping file in four steps:
Pick the entry whose regular expression matches the identifier of the experiment to which the data set belongs. If such an entry exists and if it has a value (archive folder or share IDs, depending on what is needed) this entry will be selected.
Otherwise pick the entry whose regular expression matches the project identifier and select it if it exists and has a value.
Otherwise pick the entry whose regular expression matches the space identifier (i.e. /
) and select it if it exists and has a value.
Otherwise no entry is picked which means either no share IDs or default archive folder.
The mapping file is reloaded if it has been changed. That is, changes in the mapping do not require a restart of DSS.
## Example

Identifier	Share IDs	Archive Folder
/MAIER/DEFAULT/EXP1	7, 2	/net/miller/archive
/SMITH	6	/net/smith/openbis/archive-big, /net/smith/openbis/archive-small
## /MAIER/DEFAULT	2
/MAIER	1	/net/maier/archive
The following table shows the archive folder and the list of share IDs for various experiment identifiers:
/MAIER/DEFAULT/EXP7	2	/net/maier/archive
/MAIER/DEFAULT/EXP1	7, 2	/net/miller/archive
/MAIER/PROJECT-X/EXP1	1	/net/maier/archive
/SMITH/P786/E775	6	/net/smith/openbis/archive-big when a data set is considered ""big"" and /net/smith/openbis/archive-small when a data set is considered ""small""
/MILLER/AKZU-3/E83	 	<default archive folder>",Motivation,0,en_20.10.0-11_system-documentation_configuration_share-ids_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_share-ids.txt,2025-09-30T12:09:06.586531Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_sharing-databases:0,Sharing Databases,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/sharing-databases.html,openbis,"## Sharing Databases

## Introduction

Application server and data store server(s) can share the same database.
For example, openBIS screening uses a database for image meta data
(called imaging-db) which is used by DSS to register and delivering
images. It is also used by AS to provide information about available
images and transformations.
For configuration of the data bases
core plugin
on the AS and for each DSS have to be defined. For a DSS it is a core plugin of type
data-sources
and for AS it is a core plugin of type
dss-data-sources
. Optionally the AS can get configuration parameters from its registered DSS instances by defining a mapping file
etc/dss-datasource-mapping
for the AS.
When a DSS is registering itself at the AS all its data source
definitions are provided and stored on the AS. This allows the AS (if a
mapping file is defined)
to reduce configuration of core plugins of type
dss-data-sources
to a minimum.
to have only one core plugin of type
dss-data-sources
independent
of the number of technologies/modules and DSS instances.
The AS can have only one data source per pair defined by data store code
and module code.
Share Databases without Mapping File

Without a mapping file specified data sources are independently defined for DSS and AS. For details see
DSS Data Sources
and
AS Data Sources
, respectively. Note, that the roperties
database-driver
and
database-url
are mandatory for AS.
Share Databases with Mapping File

When a mapping file is used the configuration doesn’t change for data
sources defined for DSS. But the configuration parameters for an
actually used data source in AS can come from three sources:
AS core plugins of type
dss-data-sources
Data source definitions as provided by the data stores
Mapping file
etc/dss-datasource-mapping
AS core plugins no longer need to define the properties
database-driver
and
database-url
because they are provided by DSS
or the mapping file. The same is true for properties
username
and
password.
In fact the
plugin.properties
can be empty. Usually
only parameters for logging and connection pooling are used.
The mapping file is used to pick the right AS core plugin and the right
data source provided by the DSS. In addition database credentials can be
overwritten by the mapping file.
## Warning
Only those properties in a core plugin of type
dss-data-source
are overwritten which are
undefined
.
The mapping file is a text file with lines of the following syntax:
<data store code pattern>.<module code pattern>.<type> = <value>
where <data store code pattern> and <module code pattern>
are wildcard patterns for the data store code and module/technology
code, repectively. The <type> can have one of the following
## values:
## Type
Meaning of
config
Mapping from the actual data store and module code to an existing AS core plugin of type dss-data-sources. The value should have one of the two following forms:
[
]If a code is a star symbol the corresponding actual value of the data store code or module code is used. Note, that the first form (without module code) is usefully only if the data source is define in the service.properties of AS.
data-source-code
The data source code as provided by the DSS.
host-part
Host part of the data source URL. It contains the host name (or IP address) and optional the port. Overwrites the value provided by the DSS.
sid
Unique identifier of the database. In most cases this is the database name. Overwrites the value provided by the DSS.
username
User name. Overwrites the value provided by the DSS.
password
User password. Overwrites the value provided by the DSS.
Empty lines and lines starting with ‘#’ will be ignored.
When AS needs a data source for a specific data store and module it will
consult the mapping file line by line. For each type it considers only
the last line matching the actual data store and module code. From this
information it is able to pick the right AS core plugin of
type
dss-data-sources
, the data source definitions provided by DSS at
registration, and the values for the host part of the URL, database
name, user and password.
If there is no matching line of type
config
found the AS core plugin
with key <actual data store code>[<actual module code>] is
used.
If there is no matching line of type
data-source-code
found it is
assumed that the data store has one and only one data source. Thus data
store code has to be defined in the mapping file if the data store has
more than one data source. Remember, per data store and module there can
be only one data source for AS.
Here are some examples for various use cases:
Mapping all DSSs on one

etc/dss-datasource-mapping
*.*.config
=
dss
This means that any request for data source for data store x and
module/technology y will be mapped to the same configuration. If one of
the properties driver class, URL, user name, and password is missing it
will be replaced by the data source definition provided by data store
server x at registration. This works only, if all DSS instances have
only
one
data source specified.
The following mapping file is similar:
etc/dss-datasource-mapping
*.*.config
=
dss[*]
This means that any request for data source for data store x and
module/technology y will be mapped to AS core plugin DSS of module y.
Mapping all DSSs on one per module

etc/dss-datasource-mapping
*.
proteomics
.
config
=
dss1
[
proteomics
]
*.
proteomics
.
data
-
source
-
code
=
proteomics
-
db
*.
screening
.
config
=
dss1
[
screening
]
*.
screening
.
data
-
source
-
code
=
imaging
-
db
All DSS instances for the same module are mapped onto an AS core plugin
named DSS1 for the corresponding module. This time the data source code
is also specified. This is needed if the corrsponding DSS has more than
one data source defined. For example in screening
path-info-db
is
often used in addition to
imaging-db
to speed up file browsing in the
data store.
## Overwriting Parameters

Reusing the same AS dss-data-sources core plugin is most flexible with
the mapping file if no driver, URL, username and password have been
defined in such a core plugin. In this case all these parameters come
form the data source information provided at DSS registration. If DSS
and AS are running on the same machine AS can usually use these
parameters. In this case mapping files like  in the previous examples
are enough.
The situation is different if the DSS instances, AS and the database
server running on different machines. The following example assumes that
the AS and the database server running on the same machine but at least
one of the DSS instances are running on a different machine. In this
case the database URL for the such a DSS instances could be different
than the URL for the AS.
etc/dss-datasource-mapping
*.
screening
.
config
=
dss1
[
screening
]
*.
screening
.
data
-
source
-
code
=
imaging
-
db
*.
screening
.
host
-
part
=
localhost
Also database name (aka sid), user, and password can be overwritten in
the same way.
## Overwriting Generic Settings

etc/dss-datasource-mapping
*.screening.config = dss1[screening]
*.screening.data-source-code = imaging-db
*.screening.host-part = localhost
*.screening.username = openbis
*.screening.password = !a7zh93jP.
DSS3.screening.host-part = my.domain.org:1234
DSS3.screening.username = ob
DSS3.screening.password = 8uij.hg6
This is an example where all DSS instances except DSS3 are accessing the
same database server which is on the same machine as the AS. Username
and password are also set in order to ignore corresponding data source
definitions of all DSS instances. DSS3 uses a different database server
which could be on the same machine as DSS3. Also username and password
are different.
Note, that the generic mapping definitions (i.e. definitions with wild
cards for data store codes or module codes) should appear before the
more specific definitions.",Sharing Databases,0,en_20.10.0-11_system-documentation_configuration_sharing-databases_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_sharing-databases.txt,2025-09-30T12:09:06.653659Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:0,User Group Management for Multi-groups openBIS Instances,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html,openbis,"User Group Management for Multi-groups openBIS Instances

## Introduction

Running openBIS as a facility means that different groups share the same
openBIS instance. Therefore the following demands have to be addressed
by correct configuration of such an instance:
A user should have only access to data of groups to which he or she belongs.
Each group should have its own disk space on DSS by assigning each group to a specific
share
.
Make openBIS available for a new group.
Optional usage reports should be sent regularly.
In order to fulfill these demands
a
UserManagementMaintenanceTask
has to be configured on AS
an
EagerShufflingTask
for the
PostRegistrationTask
has to be configured on DSS.
optionally a
### UsageReportingTask
has to be configured on AS.
If a new group is added
a new share has to be added to the DSS store folder (a symbolic link to an NFS directory)
a group definition has to be added to a configuration file by added LDAP group keys or an explicit list of user ids.
### Configuration

## Two types of configurations are needed:
Static configurations: Changes in these configuration need a restart of openBIS (AS and/or DSS)
Dynamic configurations: Changes apply without the need of a restart of openBIS
### Static Configurations

The necessary static configurations have to be specified in two places:
AS and DSS service.properties.
AS service.properties

Here an LDAPAuthenticationService (only if needed) and a
## UserManagementMaintenanceTask are configured:
AS service.properties
# Authentication service.
# Usually a stacked service were first file-based service is asked (for users like etl-server, i.e. DSS)
# and second the LDAP service if the file-based service fails.
authentication-service = file-ldap-authentication-service",Introduction,0,en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances_0,concept,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances.txt,2025-09-30T12:09:06.723344Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:1,User Group Management for Multi-groups openBIS Instances,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html,openbis,"# When a new person is created in the database the authentication service is asked by default whether this
# person is known by the authentication service.
# In the case of single-sign-on this doesn't work. In this case the authentication service shouldn't be asked.
# and the flag 'allow-missing-user-creation' should be set 'true' (default: 'false')
#
# allow-missing-user-creation = false",When a new person is created in the database the authentication service is asked by default whether this,1,en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances_1,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances.txt,2025-09-30T12:09:06.723344Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:2,User Group Management for Multi-groups openBIS Instances,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html,openbis,"# The URL of the LDAP server, e.g. ""ldaps://ldaps-hit-1.ethz.ch""
ldap.server.url = <LDAP URL>
# The distinguished name of the security principal, e.g. ""CN=carl,OU=EthUsers,DC=d,DC=ethz,DC=ch""
ldap.security.principal.distinguished.name = <distinguished name to login to the LDAP server>
# Password of the LDAP user account that will be used to login to the LDAP server to perform the queries
ldap.security.principal.password = <password of the user to connect to the LDAP server>
# The search base, e.g. ""ou=users,ou=nethz,ou=id,ou=auth,o=ethz,c=ch""
ldap.searchBase = <search base>
ldap.queryTemplate = (%s)
ldap.queryEmailForAliases = true","The URL of the LDAP server, e.g. ""ldaps://ldaps-hit-1.ethz.ch""",2,en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances_2,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances.txt,2025-09-30T12:09:06.723344Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:3,User Group Management for Multi-groups openBIS Instances,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html,openbis,"# Maintenance tasks for user management
maintenance-plugins = user-management, usage-reporting

user-management.class = ch.systemsx.cisd.openbis.generic.server.task.UserManagementMaintenanceTask
# Start time in 24h notation
user-management.start = 01:15
# Time interval of execution
user-management.interval = 1 days
# Path to the file with dynamic configuration
user-management.configuration-file-path = ../../../data/user-management-maintenance-config.json
# Path to the file with information which maps groups to data store shares.
# Will be created by the maintenance task and is needed by DSS (EagerShufflingTask during post registration)
user-management.shares-mapping-file-path = ../../../data/shares-mapping.txt
# Path to the audit log file. Default: logs/user-management-audit_log.txt
# user-management.audit-log-file-path =",Maintenance tasks for user management,3,en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances_3,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances.txt,2025-09-30T12:09:06.723344Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:4,User Group Management for Multi-groups openBIS Instances,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html,openbis,"# Maintenance tasks for user management
maintenance-plugins = user-management, usage-reporting

### usage-reporting.class = ch.systemsx.cisd.openbis.generic.server.task.UsageReportingTask
# Time interval of execution and also length report period
usage-reporting.interval = 7 days
# Path to the file with group definition
usage-reporting.configuration-file-path = ${user-management.configuration-file-path}
# User reporting type. Possible values are NONE, ALL, OUTSIDE_GROUP_ONLY. Default: ALL
usage-reporting.user-reporting-type = OUTSIDE_GROUP_ONLY
# Comma-separated list of e-mail addresses for report sending
usage-reporting.email-addresses = <address 1>, <address 2>, ...",Maintenance tasks for user management,4,en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances_4,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances.txt,2025-09-30T12:09:06.723344Z,2
docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:5,User Group Management for Multi-groups openBIS Instances,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html,openbis,"# Mail server configuration is needed by UsageReportingTask
mail.from = openbis@<host>
mail.smtp.host = <SMTP host>
mail.smtp.user = <can be empty>
mail.smtp.password = <can be empty>
With this template configuration the UserManagementMaintenanceTask runs
every night at 1:15 am. It reads the configuration
file
<installation
path>/data/user-management-maintenance-config.json
and creates
<installation
path>/data/shares-mapping.txt
. Every week a
usage report file of the previous week is sent to the specified
addresses.
For the LDAP configuration
ldap.server.url
,
ldap.security.principal.distingished.name
,
ldap.security.principal.password
and
ldap.searchBase
have to be specified.
The LDAP service is not only used for authenticating users but also to
obtain all users of a group. In the later case an independent query
template can be specified by the property
ldap-group-query-template
of
the
plugin.properties
of
UserManagementMaintenanceTask
(since
20.10.1.1). The % character in this template will be replaced by the
LDAP group key.
## Active Directory

If the LDAP service is actually an Active Directory service the
configuration is a bit different. These are the changes:
## Remove
ldap.queryTemplate
. This means that the default
value
(&(objectClass=organizationalPerson)(objectCategory=person)(objectClass=user)(%s))
will be used.
It might be necessary to increase the timeout. The default value is
## 10 second. Example:
ldap.timeout
=
1
min
Add the following line to the AS service.properties:
AS service.properties
## user-management.filter-key = memberOf:1.2.840.113556.1.4.1941:
## Warning
The ldap group keys described below in section
### Dynamic Configurations
have to be full distinguished names (DN) like e.g.
CN=id-sis-source,OU=Custom,OU=EthLists,DC=d,DC=ethz,DC=ch
. To find the correct DN an LDAP browsing tool (like Apache Directory Studio <https://directory.apache.org/studio/>) might be useful.
DSS service.properties

Here the PostRegistrationMaintenanceTask has be extended for eager
shuffling.
DSS service.properties
# Lists of post registrations tasks for each data set executed in the specified order.
# Note, that pathinfo-feeding is already defined.
post-registration.post-registration-tasks = pathinfo-feeding, eager-shuffling
post-registration.eager-shuffling.class = ch.systemsx.cisd.etlserver.postregistration.EagerShufflingTask
post-registration.eager-shuffling.share-finder.class = ch.systemsx.cisd.openbis.dss.generic.shared.MappingBasedShareFinder
# Path to the file with information which maps groups to data store shares.
post-registration.eager-shuffling.share-finder.mapping-file = ../../data/shares-mapping.txt
Eager shuffling moves the just registered data set from share 1 to the share of the group as specified in
<installation
path>/data/shares-mapping.txt
. For more details about share mapping see
Mapping File for Share Ids and Archiving Folders
### Dynamic Configurations

Each time the UserManagementMaintenanceTask is executed it reads the
configuration file specified
in
user-management.configuration-file-path
of AS
service.properties
.
It is a text file in JSON format which has the following structure, that
needs to be created manually:
{
""globalSpaces"": [""<space 1>"", ""<space 2>"", ...],
## ""commonSpaces"":
{
""<role 1>"": [""<space post-fix 11>"", ""<space post-fix 12>"", ...],
""<role 2>"": [""<space post-fix 21>"", ""<space post-fix 22>"", ...],
...
},
## ""commonSamples"":
{
""<sample identifier template 1>"": ""<sample type 1>"",
""<sample identifier template 2>"": ""<sample type 2>"",
...
},
## ""commonExperiments"":
[
{
""identifierTemplate"" : ""<experiment identifier template 1>"",
""experimentType""   :  ""<experiment type 1>"",
""<property code 1>""  :  ""<property value 1>"",
""<property code 2>""  :  ""<property value 2>"",
...
},
{
""identifierTemplate"" : ""<experiment identifier template 2>"",
""experimentType""   :  ""<experiment type 2>"",
""<property code 1>""  :  ""<property value 1>"",
""<property code 2>""  :  ""<property value 2>"",
...
},
...
],
""instanceAdmins"": [""<instance admin user id 1>"", ""<instance admin user id 1>""],
## ""groups"":
[
{
""name"": ""<human readable group name 1>"",
""key"": ""<unique group key 1>"",
""ldapGroupKeys"": [""<ldap group key 11>"", ""<ldap group key 12>"", ...],
""users"": [""<user id 11>"", ""<user id 12>"", ...],
""admins"": [""<user id 11>"", ""<user id 12>"", ...],
""shareIds"": [""<share id 11>"", ""<share id 12>"", ...],
""useEmailAsUserId"": true/false (default: false),
""createUserSpace"": true/false (default: true),
""userSpaceRole"" : <role> (default: non)
},
{
""name"": ""<human readable group name 2>"",
""key"": ""<unique group key 2>"",
""ldapGroupKeys"": [""<ldap group key 21>"", ""<ldap group key 22>"", ...],
""admins"": [""<user id 21>"", ""<user id 22>"", ...],
""shareIds"": [""<share id 21>"", ""<share id 22>"", ...],
""useEmailAsUserId"": true/false (default: false),
""createUserSpace"": true/false (default: true),
""userSpaceRole"" : <role> (default: non)
},
...
]
}
## Example:
{
""globalSpaces""
## :
[
## ""ELN_SETTINGS""
],
""commonSpaces""
## :
{
## ""USER""
## :
[
## ""INVENTORY""
,
## ""MATERIALS""
,
## ""METHODS""
,
## ""STORAGE""
,
## ""STOCK_CATALOG""
],
## ""OBSERVER""
## :
[
## ""ELN_SETTINGS""
,
## ""STOCK_ORDERS""
]
},
""commonSamples""
## :
{
## ""ELN_SETTINGS/ELN_SETTINGS""
## :
## ""GENERAL_ELN_SETTINGS""
},
""commonExperiments""
## :
[
{
""identifierTemplate""
## :
## ""ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION""
,
""experimentType""
## :
## ""COLLECTION""
,
## ""$NAME""
## :
## ""Templates Collection""
,
## ""$DEFAULT_OBJECT_TYPE""
## :
null
,
## ""$DEFAULT_COLLECTION_VIEW""
## :
## ""LIST_VIEW""
},
{
""identifierTemplate""
## :
## ""ELN_SETTINGS/STORAGES/STORAGES_COLLECTION""
,
""experimentType""
## :
## ""COLLECTION""
,
## ""$NAME""
## :
## ""Storages Collection""
,
## ""$DEFAULT_OBJECT_TYPE""
## :
## ""STORAGE""
,
## ""$DEFAULT_COLLECTION_VIEW""
## :
## ""LIST_VIEW""
},
{
""identifierTemplate""
## :
""PUBLICATIONS/PUBLIC_REPOSITORIES/PUBLICATION_COLLECTION""
,
""experimentType""
## :
## ""COLLECTION""
,
## ""$NAME""
## :
## ""Publication Collection""
,
## ""$DEFAULT_OBJECT_TYPE""
## :
## ""PUBLICATION""
,
## ""$DEFAULT_COLLECTION_VIEW""
## :
## ""LIST_VIEW""
},
{
""identifierTemplate""
## :
## ""STOCK_ORDERS/ORDERS/ORDER_COLLECTION""
,
""experimentType""
## :
## ""COLLECTION""
,
## ""$NAME""
## :
## ""Order Collection""
,
## ""$DEFAULT_OBJECT_TYPE""
## :
## ""ORDER""
,
## ""$DEFAULT_COLLECTION_VIEW""
## :
## ""LIST_VIEW""
},
{
""identifierTemplate""
## :
## ""STOCK_CATALOG/PRODUCTS/PRODUCT_COLLECTION""
,
""experimentType""
## :
## ""COLLECTION""
,
## ""$NAME""
## :
## ""Product Collection""
,
## ""$DEFAULT_OBJECT_TYPE""
## :
## ""PRODUCT""
,
## ""$DEFAULT_COLLECTION_VIEW""
## :
## ""LIST_VIEW""
},
{
""identifierTemplate""
## :
## ""STOCK_CATALOG/REQUESTS/REQUEST_COLLECTION""
,
""experimentType""
## :
## ""COLLECTION""
,
## ""$NAME""
## :
## ""Request Collection""
,
## ""$DEFAULT_OBJECT_TYPE""
## :
## ""REQUEST""
,
## ""$DEFAULT_COLLECTION_VIEW""
## :
## ""LIST_VIEW""
},
{
""identifierTemplate""
## :
## ""STOCK_CATALOG/SUPPLIERS/SUPPLIER_COLLECTION""
,
""experimentType""
## :
## ""COLLECTION""
,
## ""$NAME""
## :
## ""Supplier Collection""
,
## ""$DEFAULT_OBJECT_TYPE""
## :
## ""SUPPLIER""
,
## ""$DEFAULT_COLLECTION_VIEW""
## :
## ""LIST_VIEW""
}
],
""groups""
## :
[
{
""name""
## :
## ""ID SIS""
,
""key""
## :
## ""SIS""
,
""ldapGroupKeys""
## :
[
""id-sis-source""
],
""admins""
## :
[
""abc""
,
""def""
],
""shareIds""
## :
[
""2""
,
""3""
],
""createUserSpace""
## :
false
}
]
}
## Section
globalSpaces

Optional. A list of space codes. If the corresponding spaces do not
exist they will be created. All users of all groups will have
SPACE_OBSERVER rights on these spaces. For this reason the
authorization group
## ALL_GROUPS
will be created.
## Section
commonSpaces

Optional. The following roles are allowed:
## ADMIN, USER, POWER_USER, OBSERVER.
For each role a list of space post-fix codes are specified. For each
group of the group section a space with code
<group
key>_<space
post-fix>
will be created. Normal users of the
group will have access right SPACE_<ROLE> and admin users will
have access right SPACE_ADMIN.
## Section
commonSamples

Optional. A list of key-value pairs where the key is a sample identifier
template and the value is an existing sample type. The template has the
form
<space
post-fix
code>/<sample
post-fix
code>
The space post-fix code has to be in one of the lists of common spaces.
For each group of the group section a sample with identifier
<group
key>_<space
post-fix
code>/<group
key>_<sample
post-fix
code>
of specified type will be created.
## Section
commonExperiments

Optional. A list of maps where every key represents the different
experiment attributes, allowing the not only set the type but also set
property values. The template has the form
<space
post-fix
code>/<project
post-fix
code>/<experiment
post-fix
code>
The space post-fix code has to be in one of the lists of common spaces.
For each group of the group section an experiment with identifier
<group
key>_<space
post-fix
code>/<group
key>_<project
post-fix
code>/<group
key>_<experiment
post-fix
code>
of specified type will be created.
## Section
instanceAdmins
(since version 20.10.6)

Optional. A list of users for which INSTANCE_ADMIN rights will be
established. If such users are no longer known by the authetication
service they will not be revoked
.
## Section
groups

A list of group definitions. A group definition has the following
## sections:
name
: The human readable name of the group.
key
: A unique alphanumerical key of the group that follows the same rules as openBIS codes (letters, digits, ‘-’, ‘.’ but no ‘_’), for this particular purpose using only capital letters is recommended. It is used to created the two authorization groups
<group
key>
and
<group
key>_ADMIN.
ldapGroupKeys
: A list of group keys known by the LDAP authentication service.
users
: An explicit list of user ids.
admins
: A list of user ids. All admin users have SPACE_ADMIN rights to all spaces (common and user ones) which belong to the group.
shareIds
: This is a list of ids of data store shares. This list is only needed if
shares-mapping-file-path
has been specified.
useEmailAsUserId
: (since 20.10.1) If
true
the email address will be used instead of the user ID to determine the code of the user’s space. Note, that the ‘@’ symbol in the email address will be replaced by ‘_AT_’. This flag should be used if
## Single Sign On
is used for authentication but LDAP for managing the users of a group. Default:
false.
createUserSpace
: (since 20.10.1) This is a flag that controls a creation of personal user spaces for the users of this group. By default it is set to true, i.e. the personal user spaces will be created. If set to false, then the personal user spaces won’t be created for this group.
userSpaceRole
: Optional access role (either ADMIN, USER, POWER_USER, or OBSERVER) for all users of the group on all personal user spaces. (since version 20.10.3)
What UserManagementMaintenanceTask does

Each time this maintenance task is executed (according to the scheduling
interval of
plugin.properties
) the JSON configuration file will be
read first. The task does the following:
Updates mapping file of data store shares if
shares-mapping-file-path
has been specified.
Creates global spaces if they do not exist and allows
SPACE_OBSERVER access by all users of all groups.
Revokes all users unknown by the authentication service. These users
will not be deleted but deactivated. This includes removing home
space and all authorization rights.
Does for each specified group the following:
Creates the following two authorization groups if they do not
## exist:
<group
key>
: All users of the group will a member of this
authorization group. This group has access rights to common
spaces as specified.
<group
key>_ADMIN
: All admin users of the group will be
member of this authorization group. This group has
SPACE_ADMIN rights to all common spaces and all personal
user spaces.
Creates common spaces if they do not exist and assign roles for
these space to the authorization groups.
Creates for each user of the LDAP groups or the explicit list of
user ids a personal user space with SPACE_ADMIN access right
(NOTE: since 20.10.1 creation of personal user spaces can be
disabled by setting “createUserSpace” flag in the group
configuration to false). The space code read
<group
key>_<user
id>[_<sequence
number>]
A sequence
number will be used if there is already a space with code
<group
key>_<user_id
>. There are two reason why this can
## happen:
A user leaving the group and joining it again later but was
always known by the authentication service.
A user leaving the group and the institution. That it, the
user is no longer known by the authentication service. But
later another user with the same user id is joining the
group.
Creates common samples if they do not exist.
Creates common experiments (and necessary projects) if they do
not exist.
Assigns home spaces in accordance to the following rules:
If the user has no home space the personal user space of the
first group of the JSON configuration file will become the home
space.
The home space will not be changed if its code doesn’t start
with
<group
key>_<user
id>
for all groups.
If the user leaves a group the home space will be removed.
Note, if a user is moved from one group to another group the home
space of the user will be come the personal user space of the new
group.
Content of the Report File sent by UsageReportingTask

The report file is a TSV text file with following columns:
Column header
## Description
period start
Time stamp of the begin of the reporting period.
period end
Time stamp of the end of the reporting period.
group name
It has one of the three different meanings:An empty string which indicates the summary over all groups and users.The name of the group as specified by key in the dynamic configuration file.The user id for activities outside a group or for users which do not belong to a group.
number of users
Number of users of the group.
idle users
Space-separated list of ids of those users which haven’t created a collection (i.e. experiment) or an object (i.e. sample) or registered a data set in the reporting period.
number of new collections
Number of collections created.
number of new objects
Number of objects created.
number of new data sets
Number of data sets registered.
total number of entities
Total number of all entities (collections, objects and data sets). Only shown if property count-all-entities = true.
The first line in the report (after the column headers) shows always
the summary (with unspecified ‘group name’).
## If
configuration-file-path
is specified usage for each specified
group (in alphabetic order) is listed.
Finally usage by individual users follows if
user-reporting-type
isn’t NONE
Common use cases

Here are some common uses cases. No openBIS restart is needed for these
use cases.
Adding a new group

In order to make openBIS available for a new group three things have to
be done by an administrator:
Add one or more shares to the DSS store. These are symbolic links to
(remote) disk space which belongs to the new group. Note, that
symbolic link has to be a number which is the share ID.
Define a new group in the LDAP service and add all persons which
should belong to the group. Note, a person can be in more than one
group.
Add to the above mentioned JSON configuration file a new section
under
groups
.
Making a user an group admin

Add the user ID to the
admins
list of the group in the JSON
configuration file.
Remove a user from a group

The user has to be removed from the LDAP group on the LDAP service.
Adding more disk space

Add a new share for the new disk to DSS store.
Add the share id to the
shareIds
list.
Manual configuration of Multi-groups openBIS instances

In order to reproduce the set up of a multi-group openBIS instance
handled by the maintenance task, the following steps are necessary.
Note: We do NOT recommend to use the manual set up of a multi-group
instance for productive use.
Masterdata and entities definition

## Spaces

Define a prefix for a group
Create a
group_prefix_MATERIALS
space
Create a
group_prefix_METHODS
space
Create a
group_prefix_ELN_SETTINGS
space
Create a
group_prefix_STORAGE
space
Create a
group_prefix_STOCK_CATALOG
space
Create a
group_prefix_STOCK_ORDERS
space
Create a
group_prefix_Username
space for each user of the group
## Projects

Create the /**group_prefix_ELN_SETTINGS/
group_prefix_STORAGES
project
## Collections

Create the /
group_prefix_ELN_SETTINGS/group_prefix_STORAGES/Group_prefix_STORAGES_COLLECTION
collection of type COLLECTION
## Objects

Create the  /**group_prefix_ELN_SETTINGS/group_prefix_
## ELN_SETTINGS
object of type
## GENERAL_ELN_SETTINGS
Rights management

Create a group_prefix User group in openBIS
Create a group_prefix_ADMIN User group in openBIS
Assign every group member to group_prefix User group
Assign the admin to group_prefix_ADMIN User group
Assign group_prefix User group SPACE_USER rights to the following
## spaces:
group_prefix_MATERIALS
group_prefix_METHODS
group_prefix_STORAGE
group_prefix_STOCK_CATALOG
Assign group_prefix User group SPACE_OBSERVER rights to the following spaces:
group_prefix_ELN_SETTINGS
group_prefix_STOCK_ORDERS
Assign group_prefix_ADMIN SPACE_ADMIN rights to the following folders:
group_prefix_MATERIALS
group_prefix_METHODS
group_prefix_STORAGE
group_prefix_STOCK_CATALOG
group_prefix_ELN_SETTINGS
group_prefix_STOCK_ORDERS
Assign each single user SPACE_ADMIN rights to his/her
group_prefix_Username
space",Mail server configuration is needed by UsageReportingTask,5,en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances_5,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances.txt,2025-09-30T12:09:06.723344Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_architecture:0,Architecture,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/architecture.html,openbis,"## Architecture

We recommend to run
openBIS
as a lightweight Docker container, fostering portability across environments and platforms.
### Requirements

Refer to the official documentation pages on Docker Engine (aka Docker CE) to learn more about requirements and
installation instructions
of the packages needed for running docker containers.
We recommend to run the openBIS Docker container on top of an Ubuntu server for running the application in production -
### System Requirements
.
Read more on
## Docker Architecture
to familiarize with its core concepts.
## Application Layout

openBIS can be split into distinct sub-units, which are virtualized either all-in-one or within multiple Docker containers.
Independently of the scenario, we recommend clients to communicate with the application via a reverse proxy.",Architecture,0,en_20.10.0-11_system-documentation_docker_architecture_0,concept,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_architecture.txt,2025-09-30T12:09:06.834492Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_configuration:0,Basic configuration,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/configuration.html,openbis,"Basic configuration

## Environment Variables

## Variable
Default value
## Description
## OPENBIS_ADMIN_PASS
123456789
Administrator password to openBIS instance.
## OPENBIS_DATA
/data/openbis
Directory for openBIS persistent data.
## OPENBIS_DB_ADMIN_PASS
mysecretpassword
PostgreSQL superuser password.
## OPENBIS_DB_ADMIN_USER
postgres
PostgreSQL superuser name.
## OPENBIS_DB_APP_PASS
mysecretpassword
Password for application user connecting to the database.
## OPENBIS_DB_APP_USER
openbis
Username for application user connecting to the database.
## OPENBIS_DB_HOST
openbis-db
Name of container running PostgreSQL database.
## OPENBIS_ETC
/etc/openbis
Directory for openBIS configuration files.
## OPENBIS_HOME
/home/openbis
Directory for openBIS installation binaries.
## OPENBIS_LOG
/var/log/openbis
Directory for openBIS log files.
## OPENBIS_FQDN
openbis.domain
Full qualified domain name of openBIS service.
### Configuration Files

openBIS offers the ability to pass in configuration files like, e.g., capabilities files. Those can be deployed in any directory mounted as a volume in the openBIS docker container. It needs to be ensured that the associated AS and DSS properties are pointing to the correct file paths.
## Note
It is necessary to store any data files that needs to be preserved inside the
openbis-app-data
volume, or inside some other docker volume. Likewise, any configuration files should be stored within the
openbis-app-etc
volume. Failure to do so yields undesired behavior in the sense that changes made to the files within the container are being lost when the container goes down.
## Examples

Suppy a json file for storing personal access tokens

Enable the feature
Add the following line to the AS service.properties:
personal
-
access
-
tokens
-
enabled
=
true
Ensure the AS property
personal-access-tokens-file-path
is pointing to the correct path to where the json file is located
Assuming the DSS root-dir points to the default dir,
/data/openbis
, then the personal-access-tokens.json could be stored in this directory:
personal
-
access
-
tokens
-
file
-
path
=
/
data
/
openbis
/
personal
-
access
-
tokens
.
json
Create a PAT and monitor the contents of the json file
personal-access-tokens.html#typical-application-workflow
Modify the AS capabilities file

For this, it is not needed to
## Core Plugins

It is possible to make adjustments to core-plugins shipped with the openBIS installer. To do so, just start up openBIS at least once. This will copy the contents of the core-plugins directory to a sub-directory
core-plugins
which is stored within the docker volume
openbis-app-etc
. Any customizations made here will persist restarts of the application, as well as upgrades of the openbis-docker image.
## Warning
Be careful when making changes to code of core-plugins since they might break when new releases are published. Please consider reading the
Change Log published with each release
.
If the application fails to start after changes to the core-plugins have been made, you can always revert to the original state of the core-plugins by removing the
core-plugins
folder within the
openbis-app-etc
volume.
Besides adjustments to existing plugins, it is also possible to
create new plugins from scratch
.
## Examples

Customize the InstanceProfile.js

This file is part of the
eln-lims
## core-plugin. It is located here:
<openbis-app-etc>/core-plugins/eln-lims/1/as/webapps/eln-lims/html/etc/InstanceProfile.js
Make any changes to this file
E.g., change
this.showSemanticAnnotations
=
false;
to
this.showSemanticAnnotations
=
true;
.
Restart the container
Most changes made to the configuration of the openBIS application require a restart in order to be applied. Assuming the container running the openBIS application is named
openbis-app
, this is achieved as follows:
docker
restart
openbis
-
app",Environment Variables,0,en_20.10.0-11_system-documentation_docker_configuration_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_configuration.txt,2025-09-30T12:09:06.905355Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_environments:0,Environments,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/environments.html,openbis,"## Environments

Production, testing and development

openbis-app - https://hub.docker.com/r/openbis/openbis-server

## The
openbis-app
image is designed and supported for deploying openBIS in production, testing and development environments.
The openBIS service running in container named
openbis-app
can be connected to a
containerized PostgreSQL database
or to any managed or cloud-native PostgreSQL service.
A reverse HTTP proxy is required in front. It can be
set up as a container
, as an independent ingress controller, or as a cloud-based content delivery service.
We recommend to orchestrate all containers using Docker Compose, for which we’re providing
examples
to use as a template.",Environments,0,en_20.10.0-11_system-documentation_docker_environments_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_environments.txt,2025-09-30T12:09:07.011440Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_index:0,Docker,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/index.html,openbis,"## Docker

## Quickstart
## Architecture
### Requirements
## Application Layout
## Environments
Production, testing and development
openbis-app - https://hub.docker.com/r/openbis/openbis-server
## Release Cycle
## Source Repositories
Source code
Docker images
### Usage
## Docker Containers
## Docker Compose
## Docker Network
## Storage Volumes
## Database
## Application
## Ingress
## Nginx
Apache httpd
HAProxy
Basic configuration
## Environment Variables
### Configuration Files
## Examples
Suppy a json file for storing personal access tokens
Modify the AS capabilities file
## Core Plugins
## Examples
Customize the InstanceProfile.js
## Verification
## References",Docker,0,en_20.10.0-11_system-documentation_docker_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_index.txt,2025-09-30T12:09:07.074547Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_quickstart:0,Quickstart,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/quickstart.html,openbis,"## Quickstart

Create virtual network.
$ docker network create openbis-network --driver bridge;
Run database container.
$ docker run --detach \
--name openbis-db \
--hostname openbis-db \
--network openbis-network \
-v openbis-db-data:/var/lib/postgresql/data \
-e POSTGRES_PASSWORD=mysecretpassword \
-e PGDATA=/var/lib/postgresql/data/pgdata \
postgres:15;
Run application container.
$ docker run --detach \
--name openbis-app \
--hostname openbis-app \
--network openbis-network \
--pid host \
-p 8080:8080 \
-p 8081:8081 \
-v openbis-app-data:/data \
-v openbis-app-etc:/etc/openbis \
-v openbis-app-logs:/var/log/openbis \
-e OPENBIS_ADMIN_PASS=""123456789"" \
-e OPENBIS_DATA=""/data/openbis"" \
-e OPENBIS_DB_ADMIN_PASS=""mysecretpassword"" \
-e OPENBIS_DB_ADMIN_USER=""postgres"" \
-e OPENBIS_DB_APP_PASS=""mysecretpassword"" \
-e OPENBIS_DB_APP_USER=""openbis"" \
-e OPENBIS_DB_HOST=""openbis-db"" \
-e OPENBIS_ETC=""/etc/openbis"" \
-e OPENBIS_HOME=""/home/openbis"" \
-e OPENBIS_LOG=""/var/log/openbis"" \
-e OPENBIS_FQDN=""local.openbis.ch"" \
openbis/openbis-app:20.10.7;
Run local ingress container.
$ docker run --detach \
--name openbis-ingress \
--hostname openbis-ingress \
--network openbis-network \
--pid host \
-p 443:443 \
-e OPENBIS_HOST=""openbis-app"" \
openbis/openbis-local:latest;
Verify connectivity.
$ curl -v https://local.openbis.ch/openbis/webapp/eln-lims/version.txt",Quickstart,0,en_20.10.0-11_system-documentation_docker_quickstart_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_quickstart.txt,2025-09-30T12:09:07.138486Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_references:0,References,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/references.html,openbis,"## References

openBIS: a flexible framework for managing and analyzing complex data in biology research
openBIS official documentation
openBIS official image
## Docker Engine
## Docker Compose
PostgreSQL official image
## NGINX Documentation
## HAProxy Documentation
## Apache HTTP Server Documentation",References,0,en_20.10.0-11_system-documentation_docker_references_0,reference,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_references.txt,2025-09-30T12:09:07.201022Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_release-cycle:0,Release Cycle,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/release-cycle.html,openbis,"## Release Cycle

The official
openbis-app
images available on
## Docker Hub
are tagged by major release published on the
openBIS download page
with the latest bugfix patches included. The official openBIS installation package is based on the latest official image of
Ubuntu LTS Linux
. In addition, all containers are rebuilt and republished at least on a monthly basis to include operating system patches.",Release Cycle,0,en_20.10.0-11_system-documentation_docker_release-cycle_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_release-cycle.txt,2025-09-30T12:09:07.265499Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_source-repositories:0,Source Repositories,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/source-repositories.html,openbis,"## Source Repositories

Source code

The source code of all builds and helper scripts is published in the
openBIS Continous Integration repository
. This is the only official location of openBIS source code supported by the openBIS team of ETH Zurich Scientific IT Services.
Docker images

Container images are published on
## Docker Hub
. This is the only official location of openBIS Docker images supported by the openBIS team of ETH Zurich Scientific IT Services.",Source Repositories,0,en_20.10.0-11_system-documentation_docker_source-repositories_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_source-repositories.txt,2025-09-30T12:09:07.330146Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_usage:0,Usage,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/usage.html,openbis,"### Usage

## Docker Containers

Our recommendation is to run openBIS within a
three-container setup
, in particular when aiming at
running openBIS in production
## :
openbis-ingress
: Runs a
reverse HTTP Proxy
for managing and securing HTTP requests in between the client and the application.
openbis-app
: Runs a
Java Runtime Environment
, including the openBIS Application Server (AS) and openBIS Data Store Server (DSS).
openbis-db
: Runs a
PostgreSQL
database, to handle all data transactions.
## Container
## Image
## Port
## Description
openbis-db
postgres15
5432/tcp
PostgreSQL database listens on port 5432 and accepts connection from openbis-app.
openbis-app
openbis-app
8080/tcp
Java Virtual Machine with openBIS Application Server listens on port 8080.
openbis-app
openbis-app
8081/tcp
Java Virtual Machine with openBIS Data Store Server listens on port 8081.
openbis-ingress
apache2
443/tcp
Apache HTTP server listens on port 443 and is configured as reverse proxy to ports 8080 and 8081.
## Docker Compose

Docker Compose is a tool for defining and running multi-container applications. It simplifies the control of the entire openBIS service, making it easy to control the application workflow in a single, comprehensible YAML configuration file, allows to create, start and stop all services by issuing a single command.
We are providing a basic
docker-compose.yml
, which is ready to use.
To run the application navigate to the sub-directory where you’ve downloaded the
docker-compose.yml
to and then execute
docker
-
compose
pull
docker
-
compose
up
-
d
For advanced use, consider to modify the file according to your needs (
more details
). Note that this example does not include an ingress controler. For full examples, proceed to [Ingress].
The sections below provides a brief description of the individual components used in the proposed multi-container setup.
## Docker Network

The virtual bridge network
openbis-network
allows all containers deployed with openBIS to connect to each other. The following example creates a network using the bridge network driver, which all running containers will be communicating accross.
To manually create the network, execute:
docker
network
create
openbis
-
network
--
driver
bridge
## Storage Volumes

The use of Docker volumes is preferred for
persisting data
generated and utilized by containers. For proper operation, the data directory of openBIS, main configuration files and logs are to be mounted as  persistent volumes. This ensures that data can be accessed from different containers, and allows data to persist container restarts. By utilizing the option
-v
openbis-data:/data
, a persistent storage named
openbis-data
is created and mounted as
/data
within the active container. This applies analogically to all other persistent volumes.
## Container
Persistent volume
## Mountpoint
## Description
openbis-db
openbis-db-data
/var/lib/postgresql/data
PostgreSQL database configuration and data directory.
openbis-app
openbis-app-data
/data
Application data directory for data store files.
openbis-app
openbis-app-etc
/etc/openbis
Application configuration files.
openbis-app
openbis-app-logs
/var/log/openbis
Application log files.
## Database

## The
database container
openbis-db
provides a relational database through
PostgreSQL server
to guarantee persistence for any data created while running openBIS. This includes user and authorization data, openBIS entities and their metadata, as well as index information about all datasets. It is required to have database superuser privileges.
$ docker run --detach \
--name openbis-db \
--hostname openbis-db \
--network openbis-network \
-v openbis-db-data:/var/lib/postgresql/data \
-e POSTGRES_PASSWORD=mysecretpassword \
-e PGDATA=/var/lib/postgresql/data \
postgres:15;
The running database container can be inspected to fetch logs. The database has started up successfully when the openbis-db container logs the following message: “database system is ready to accept connections”:
$ docker logs openbis-db;
2024-01-19 18:37:50.984 UTC [1] LOG:  database system is ready to accept connections
The volume created (
openbis-db-data
) can be inspected to check the mountpoint where the database data is physically stored.
$ docker volume inspect openbis-db-data;
[
{
""CreatedAt"": ""2024-01-19T19:37:48+01:00"",
""Driver"": ""local"",
""Labels"": null,
""Mountpoint"": ""/var/lib/docker/volumes/openbis-db-data/_data"",
""Name"": ""openbis-db-data"",
""Options"": null,
""Scope"": ""local""
}
]
## Application

## The
application container
openbis-app
provides Java runtime and consists of two Java processes - the
openBIS Application Server
(openBIS AS) and the -
openBIS Data Store Server
(openBIS DSS). The
openBIS AS
manages the metadata and links to the data, while the
openBIS DSS
manages the data themselves operating on a managed part of the file system.
$ docker run --detach \
--name openbis-app \
--hostname openbis-app \
--network openbis-network \
--pid host \
-p 8080:8080 \
-p 8081:8081 \
-v openbis-app-data:/data \
-v openbis-app-etc:/etc/openbis \
-v openbis-app-logs:/var/log/openbis \
-e OPENBIS_ADMIN_PASS=""123456789"" \
-e OPENBIS_DATA=""/data/openbis"" \
-e OPENBIS_DB_ADMIN_PASS=""mysecretpassword"" \
-e OPENBIS_DB_ADMIN_USER=""postgres"" \
-e OPENBIS_DB_APP_PASS=""mysecretpassword"" \
-e OPENBIS_DB_APP_USER=""openbis"" \
-e OPENBIS_DB_HOST=""openbis-db"" \
-e OPENBIS_ETC=""/etc/openbis"" \
-e OPENBIS_HOME=""/home/openbis"" \
-e OPENBIS_LOG=""/var/log/openbis"" \
-e OPENBIS_FQDN=""openbis.domain"" \
openbis/openbis-app:20.10.7;
The state of the running application container can be inspected by fetching the container logs:
$ docker logs openbis-app;
2024-01-23 11:06:19,310 INFO  [main] OPERATION.ETLDaemon - Data Store Server ready and waiting for data.
Docker volumes mounted by
openbis-app
can be inspected to check where data files, configuration files and logs are physically stored.
$ docker volume inspect openbis-app-data openbis-app-etc openbis-app-logs;
[
{
""CreatedAt"": ""2024-01-20T12:24:49+01:00"",
""Driver"": ""local"",
""Labels"": null,
""Mountpoint"": ""/var/lib/docker/volumes/openbis-app-data/_data"",
""Name"": ""openbis-app-data"",
""Options"": null,
""Scope"": ""local""
},
{
""CreatedAt"": ""2024-01-20T12:24:49+01:00"",
""Driver"": ""local"",
""Labels"": null,
""Mountpoint"": ""/var/lib/docker/volumes/openbis-app-etc/_data"",
""Name"": ""openbis-app-etc"",
""Options"": null,
""Scope"": ""local""
},
{
""CreatedAt"": ""2024-01-20T12:24:49+01:00"",
""Driver"": ""local"",
""Labels"": null,
""Mountpoint"": ""/var/lib/docker/volumes/openbis-app-logs/_data"",
""Name"": ""openbis-app-logs"",
""Options"": null,
""Scope"": ""local""
}
]
## Ingress

## An
ingress container
acts as reverse proxy and performs Transport Layer Security (TLS) termination. The examples provided below only cover the base functionality. They should be extended to handle complex access control scenarios and to comply with firewall rules. In each of the examples below, the ingress controller configures TLS, and it is configured as a reverse proxy to handle requests to the path
/openbis
(directed to port 8080) and to
/datastore_server
(directed to port 8081).
## Nginx

In order to use nginx as an ingress container, it is required to deploy the following files, as provided on our
source repository
## :
docker-compose-nginx.yml
nginx config
, to be placed in sub-directory
nginx
To run the application, you need to:
have docker and docker-compose installed
ensure that valid certificate and key files are deployed in the sub-directory
certs
from within the directory where you’ve deployed the
docker-compose-nginx.yml
, run
docker-compose
-f
docker-compose-nginx.yml
up
-d
Apache httpd

In order to use apache-httpd as an ingress container, it is required to deploy the following files, as provided on our
source repository
## :
docker-compose-httpd.yml
apache-httpd config
, to be placed in sub-directory
httpd
To run the application, you need to:
have docker and docker-compose installed
ensure that valid certificate and key files are deployed in the sub-directory
certs
from within the directory where you’ve deployed the
docker-compose-httpd.yml
, run
docker-compose
-f
docker-compose-httpd.yml
up
-d
HAProxy

In order to use haproxy as an ingress container, it is required to deploy the following files, as provided on our
source repository
## :
docker-compose-haproxy.yml
haproxy config
, to be placed in sub-directory
haproxy
To run the application, you need to:
have docker and docker-compose installed
ensure that valid certificate and key files are deployed in the sub-directory
certs
from within the directory where you’ve deployed the
docker-compose-haproxy.yml
, run
docker-compose
-f
docker-compose-haproxy.yml
up
-d",Usage,0,en_20.10.0-11_system-documentation_docker_usage_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_usage.txt,2025-09-30T12:09:07.380024Z,2
docs:openbis:en_20.10.0-11_system-documentation_docker_verification:0,Verification,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/verification.html,openbis,"## Verification

Check status of running openBIS Application Server.
$ docker exec -it openbis-app /home/openbis/servers/openBIS-server/jetty/bin/status.sh;
openBIS Application Server is running (pid 24538)
Check version of running openBIS Application Server.
$ docker exec -it openbis-app /home/openbis/servers/openBIS-server/jetty/bin/version.sh;
20.10.7.4 (r1701090021)
Check the password file for file based authentication.
$ docker exec -it openbis-app /home/openbis/servers/openBIS-server/jetty/bin/passwd.sh list;
User ID               First Name            Last Name             Email
admin
etlserver
Check connectivity to port 8080 of openBIS Application Server.
$ docker exec -it openbis-app wget -q --output-document - http://localhost:8080/openbis/webapp/eln-lims/version.txt;
20.10.7.4
Examine a process of  openBIS Data Store Server.
$ docker exec -it openbis-app pgrep -af DataStoreServer;
25503 java -server -Djavax.net.ssl.trustStore=etc/openBIS.keystore --add-exports java.xml/jdk.xml.internal=ALL-UNNAMED -Dnative.libpath=lib/native -classpath lib/slf4j-log4j12-1.6.2.jar:lib/datastore_server.jar:lib/common.jar:lib/dbmigration-20.10.7-r1688387419.jar:lib/activation-1.1.1.jar:lib/ascii-table-1.2.0.jar:lib/aspectjweaver-1.8.12.jar:lib/authentication-20.10.7-r1688387419.jar:lib/autolink-dataset-uploader-api-zip4j_1.3.2.jar:lib/autolink-dropboxReporter-jyson-1.0.2.jar:lib/autolink-eln-lims-api-htmlcleaner-2.23.jar:lib/autolink-eln-lims-api-zip4j_1.3.2.jar:lib/autolink-password-reset-api-persistentkeyvaluestore.jar:lib/autolink-zenodo-exports-api-job-scheduler.jar:lib/base64-2.3.9.jar:lib/bcel-6.0-SNAPSHOT.jar:lib/bcpg-1.59.jar:lib/bcprov-1.59.jar:lib/bioformats-6.5.1.jar:lib/builder-commons-1.0.2.jar:lib/cisd-args4j-9.11.2.jar:lib/cisd-cifex-r1550129411.jar:lib/cisd-hotdeploy-13.01.0.jar:lib/cisd-image-readers-bioformats-r1553067167.jar:lib/cisd-image-readers-imagej-r1553067167.jar:lib/cisd-image-readers-jai-r1553067167.jar:lib/cisd-image-readers-r1553067167.jar:lib/cisd-openbis-knime-server-13.6.0.r29301.jar:lib/classmate-1.3.0.jar:lib/common.jar:lib/commonbase.jar:lib/commons-cli-1.2.jar:lib/commons-codec-1.10.jar:lib/commons-collections-4.01.jar:lib/commons-collections4-4.1.jar:lib/commons-compress-1.8.jar:lib/commons-csv-1.2.jar:lib/commons-dbcp-1.3-CISD.jar:lib/commons-fileupload-1.3.3.jar:lib/commons-io-2.6.jar:lib/commons-lang3-3.11.jar:lib/commons-logging-1.1.1.jar:lib/commons-pool-1.5.6.jar:lib/commons-text-1.6.jar:lib/datastore_server-20.10.7-r1688387419.jar:lib/datastore_server_plugin-dsu-20.10.7-r1688387419.jar:lib/datastore_server_plugin-plasmid-20.10.7-r1688387419.jar:lib/datastore_server_plugin-yeastx-20.10.7-r1688387419.jar:lib/dbmigration-20.10.7-r1688387419.jar:lib/docx4j-6.1.2.jar:lib/dom4j-1.6.1.jar:lib/ehcache-2.10.0.jar:lib/eodsql-2.2-CISD.jar:lib/fast-md5-2.6.1.jar:lib/ftpserver-core-1.0.6.jar:lib/guava-25.0-jre.jar:lib/h2-1.1.115.jar:lib/hamcrest-core-1.3.jar:lib/hamcrest-integration-1.3.jar:lib/hamcrest-library-1.3.jar:lib/httpclient-4.3.6.jar:lib/httpcore-4.3.3.jar:lib/ij-1.43u.jar:lib/image-viewer-0.3.6.jar:lib/istack-commons-runtime-3.0.5.jar:lib/jackcess-1.2.2.jar:lib/jackson-annotations-2.9.10.jar:lib/jackson-core-2.9.10.jar:lib/jackson-databind-2.9.10.8.jar:lib/jandex-2.0.3.Final.jar:lib/javacsv-2.0.jar:lib/javassist-3.20.0.GA.jar:lib/javax.annotation-api-1.3.2.jar:lib/javax.jws-3.1.2.2.jar:lib/jaxb-api-2.3.0.jar:lib/jaxb-core-2.3.0.jar:lib/jaxb-runtime-2.3.0.jar:lib/jboss-logging-3.3.0.Final.jar:lib/jboss-transaction-api_1.2_spec-1.0.0.Final.jar:lib/jcommon.jar:lib/jetty-client-9.4.44.v20210927.jar:lib/jetty-deploy-9.4.44.v20210927.jar:lib/jetty-http-9.4.44.v20210927.jar:lib/jetty-io-9.4.44.v20210927.jar:lib/jetty-security-9.4.44.v20210927.jar:lib/jetty-server-9.4.44.v20210927.jar:lib/jetty-servlet-9.4.44.v20210927.jar:lib/jetty-util-9.4.44.v20210927.jar:lib/jetty-webapp-9.4.44.v20210927.jar:lib/jetty-xml-9.4.44.v20210927.jar:lib/jfreechart-1.0.13.jar:lib/jline-0.9.94.jar:lib/jsonrpc4j-1.5.3.jar:lib/jsoup-1.14.2.jar:lib/jython-2.5.2.jar:lib/log4j-1.2.15.jar:lib/mail-1.4.3.jar:lib/marathon-spring-util-1.2.5.jar:lib/mina-core-2.0.7.jar:lib/openbis-20.10.7-r1688387419.jar:lib/openbis-common.jar:lib/openbis-mobile-r29271.jar:lib/openbis_api-20.10.7-r1688387419.jar:lib/pngj-0.62.jar:lib/poi-3.17.jar:lib/poi-ooxml-3.17.jar:lib/poi-ooxml-schemas-3.17.jar:lib/postgresql-42.5.0.jar:lib/reflections-0.9.10.jar:lib/restrictionchecker-1.0.2.jar:lib/screening-20.10.7-r1688387419.jar:lib/serializer-2.7.2.jar:lib/servlet-api-3.1.0.jar:lib/sis-base-18.09.0.jar:lib/sis-file-transfer-19.03.1.jar:lib/sis-jhdf5-19.04.0.jar:lib/slf4j-1.6.2.jar:lib/slf4j-api-1.7.24.jar:lib/slf4j-log4j12-1.6.2.jar:lib/spring-aop-5.0.17.RELEASE.jar:lib/spring-beans-5.0.17.RELEASE.jar:lib/spring-context-5.0.17.RELEASE.jar:lib/spring-context-support-5.0.17.RELEASE.jar:lib/spring-core-5.0.17.RELEASE.jar:lib/spring-expression-5.0.17.RELEASE.jar:lib/spring-jcl-5.0.17.RELEASE.jar:lib/spring-jdbc-5.0.17.RELEASE.jar:lib/spring-orm-5.0.17.RELEASE.jar:lib/spring-tx-5.0.17.RELEASE.jar:lib/spring-web-5.0.17.RELEASE.jar:lib/spring-webmvc-5.0.1.RELEASE.jar:lib/sshd-common.jar:lib/sshd-core-2.7.0.jar:lib/sshd-sftp-2.7.0.jar:lib/stax-api-1.0.1.jar:lib/stax2-api-3.0.4.jar:lib/truezip-6.8.1.jar:lib/txw2-2.3.0.jar:lib/validation-api-1.0.0.GA.jar:lib/wstx-asl-4.0.0.jar:lib/xalan-2.7.2.jar:lib/xml-apis-1.3.03.jar:lib/xml-io-1.0.3.jar:lib/xmlbeans-2.6.0.jar:lib/xoai-common.jar:lib/xoai-data-provider-4.2.0.jar:ext-lib/*.jar ch.systemsx.cisd.openbis.dss.generic.DataStoreServer
Check connectivity to the database.
$ docker exec -it openbis-db psql -U openbis openbis_prod -c ""select id,user_id,email from persons"";
Password for user openbis:
id |  user_id  | email
----+-----------+-------
1 | system    |
2 | etlserver |
3 | admin     |
(3 rows)",Verification,0,en_20.10.0-11_system-documentation_docker_verification_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_docker_verification.txt,2025-09-30T12:09:07.481245Z,2
docs:openbis:en_20.10.0-11_system-documentation_standalone_index:0,Standalone,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/index.html,openbis,"## Standalone

### System Requirements
## Architecture
### Hardware Configuration
CPU and Memory Configuration
## Postgres Memory Settings
## Tuning Of Hardware Settings In Case Of Issues
## Operating System
## Third-Party Packages
### Additional Requirements
### openBIS Server Installation
Contents of openBIS Installer Tarball
### Installation Steps
Starting and Stopping the openBIS Application Server and Data Store Server
## Start Server
## Stop Server",Standalone,0,en_20.10.0-11_system-documentation_standalone_index_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_standalone_index.txt,2025-09-30T12:09:07.546179Z,2
docs:openbis:en_20.10.0-11_system-documentation_standalone_installation:0,openBIS Server Installation,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/installation.html,openbis,"### openBIS Server Installation

Contents of openBIS Installer Tarball

The server distribution is a
gzipped
tar
file named
openBIS-installation-standard-technologies-<version>.tar.gz
. It contains the following files:
## console.properties:
### Installation configuration file
## extract.sh:
helper script for installation
## jul.config:
Log configuration for the openBIS install process
openBIS-installer.jar
Java archive containing openBIS
run-console.sh
### Installation script
### Installation Steps

Create a service user account, i.e. an unprivileged, regular user account.
Do not run openBIS as root!
Gunzip the distribution on the server machine into some temporary folder:
mkdir
tmp
mv
xvfz
openBIS-installation-standard-technologies-<release-number>.tar.gz
tmp/
cd
tmp/
tar
xvfz
openBIS-installation-standard-technologies-<release-number>.tar.gz
Customize the
console.properties
file by specifying values for at least the following parameters:
## INSTALL_PATH
,
## DSS_ROOT_DIR
,
## INSTALLATION_TYPE
,
## ELN-LIMS
, and
## ELN-LIMS-LIFE-SCIENCES
. Each parameter is documented inline.
## Run installation script:
./run-console.sh
When done, openBIS is installed in the directory specified as
## INSTALL_PATH
in the
console.properties
. Within this system admin documentation pages, we’re referring this path as
## $INSTALL_PATH
.
## Note
Please be aware that the directory where openBIS is installed should not already exist. Users should specify the directory where they want to install openBIS (in the console.properties) and this directory will be created by the installation procedure. If the directory already exists, the installation will fail, except from when the installer detects that it already contains an existing openBIS installation. In the latter case, the installer will try to upgrade the existing release to the one to be installed by invoking $INSTALL_PATH/bin/upgrade.sh.",openBIS Server Installation,0,en_20.10.0-11_system-documentation_standalone_installation_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_standalone_installation.txt,2025-09-30T12:09:07.608667Z,2
docs:openbis:en_20.10.0-11_system-documentation_standalone_starting-and-stopping:0,Starting and Stopping the openBIS Application Server and Data Store Server,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/starting-and-stopping.html,openbis,"Starting and Stopping the openBIS Application Server and Data Store Server

## Start Server

The openBIS application server is started as follows:
prompt> <installation folder>/bin/bisup.sh
On startup the openBIS server creates the openBIS database (named
openbis_prod
by default) and checks the connection with the remote authentication services (if they are configured). Log files can be found in
<installation
folder>/servers/openBIS-server/jetty/logs
. Also the following command shows the log:
<installation
folder>/bin/bislog.sh
## Warning
Unless otherwise configured through running the installation script or within the database itself, the first user logged in into the system will have full administrator rights (role
## INSTANCE_ADMIN
).
Commonly, the application server is configured to access a local data store via the data store server. This has to be started after the AS:
prompt> <installation folder>/bin/dssup.sh
The application server and the data store server can also be started one after the other using a single command:
prompt> <installation folder>/bin/allup.sh
## Stop Server

The application server is stopped as follows:
prompt> <installation folder>/bin/bisdown.sh
To only stop the data store server:
prompt> <installation folder>/bin/dssdown.sh
To stop both the data store server and then the applicaiton server:
prompt> <installation folder>/bin/alldown.sh",Start Server,0,en_20.10.0-11_system-documentation_standalone_starting-and-stopping_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_standalone_starting-and-stopping.txt,2025-09-30T12:09:07.670900Z,2
docs:openbis:en_20.10.0-11_system-documentation_standalone_system-requirements:0,System Requirements,https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/system-requirements.html,openbis,"### System Requirements

## Architecture

As of today, openBIS can be deployed on the AMD64 (x86_64) architecture.
Support for ARM architecture is currently being developed.
### Hardware Configuration

Starting from openBIS version 20.10.0, openBIS memory and CPU usage requirements have remarkably dropped. The following guidelines cannot be applied to previous versions.
Below we provide recommended (virtual) hardware and database (PostgreSQL) server settings for three common use-cases:
## Parameter
## Small
## Medium
## Big
Default ELN LIMS UI using Generic or Life-Sciences Technologies
x
x
x
Old core UI still actively used
x
x
up to 5 concurrent users
x
x
x
up to 20 concurrent users
x
x
more than 20 concurrent users
x
Please bear in mind that, the more customised an openBIS installation is, the more the recommended settings may vary from the optimal ones.
CPU and Memory Configuration

## Scenario
Number of CPUs
Total memory
Memory allocated to OS
Memory allocated to PostgreSQL
Memory allocated to openBIS Application Server
Memory allocated to openBIS Data Store Server
## Small
2 modern x86 CPU cores
## 4 GB
## 1.5 GB
## 1 GB
## 1 GB
## 0.5 GB
## Medium
2-4 modern x86 CPU cores
## 8 GB
## 2 GB
## 2 GB
## 3GB
## 1 GB
## Big
4-8 modern x86 CPU cores
## 16 GB
## 3 GB
## 3 GB
## 8 GB
## 2 GB
## Postgres Memory Settings

Memory-related settings of your PostgreSQL server can be obtained from https://pgtune.leopard.in.ua/. For the “small” scenario, use the below template:
## Parameter
## Value
DB Version
15
OS Type
(depends on your infrastructure)
DB Type
## Web Applicaiton
Total Memory (RAM)
## 3 GB
Number of CPUs
2
Number of Connections
50
Data Storage
(depends on your infrastructure)
After clicking on “Generate”, you get the matching settings of the postgresql.conf displayed, jointly with the commands to be used to apply these (ALTER SYSTEM).
## Tuning Of Hardware Settings In Case Of Issues

## Symptom
## Recommended Action
Long query execution times
Increase CPU number and/or AS & Postgres memory settings, reconfigure Postgres and openBIS memory settings following the recommended settings provided by https://pgtune.leopard.in.ua/.
AS log shows out of memory errors
Increase AS Memory. This easily happens in old installations using the Legacy Core UI that requires additional memory for cache.
DSS log shows out of memory errors
Increase DSS Memory.
## Operating System

We recommend to set up openBIS on a Linux operating system. We provide support for installing and operating openBIS on supported
Ubuntu Server LTS releases
.
Operating System: Linux / MacOS X
## Third-Party Packages

The following software packages are required:
The binaries
bash
,
awk
,
sed
and
unzip
need to be installed and in the
## PATH
of the openBIS user.
Java Runtime Environment: recent versions of Oracle JRE 11 or OpenJDK 11
PostgreSQL 15
### Additional Requirements

An SMTP server needs to be accessible if you want openBIS to send out notifications via mail. We recommend to use the a local mail transfer agent such as Postfix configued for message sending.",System Requirements,0,en_20.10.0-11_system-documentation_standalone_system-requirements_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_system-documentation_standalone_system-requirements.txt,2025-09-30T12:09:07.732614Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_command-line-tool:0,openBIS Command Line Tool (oBIS),https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/command-line-tool.html,openbis,"openBIS Command Line Tool (oBIS)

oBIS is a command-line tool that makes it possible to handle data sets tracked by OpenBIS,
where users have complete freedom to structure and manipulate the data as they wish, while retaining
the benefits of openBIS.
With oBIS, it is possible not only to handle datasets stored in OpenBIS but also available to keep
only metadata send to openBIS, while the data itself is managed externally, by the user. In this
case, OpenBIS is aware of its existence and the data can be used for provenance tracking.
## 1. Prerequisites

python 3.6 or higher
git 2.11 or higher
git-annex 6 or higher
### Installation guide
### 2. Installation

pip3
install
obis
## Since
obis
is based on
pybis
, the pip command will also install pybis and all its dependencies.
3. Quick start guide

Configure your openBIS Instance
# global settings to be use for all obis repositories
obis
config
-g
set
openbis_url
=
https://localhost:8888
obis
config
-g
set
user
=
admin
## Download Physical Dataset
# create a physical (-p) obis repository with a folder name
obis
init
-p
data1
cd
data1
# check configuration
obis
config
get
is_physical
# download dataset giving a single permId
obis
download
20230228091119011
-58
## Upload Physical Dataset
# create a physical (-p) obis repository with a folder name
obis
init
-p
data1
cd
data1
# check configuration
obis
config
get
is_physical
# upload as many files or folder as you want (-f) to an existing object as type RAW_DATA
obis
upload
20230228133001314
-59
## RAW_DATA
-f
your_file_a
-f
your_file_b
### 4. Usage

4.1 Help is your friend!

$
obis
--help",1. Prerequisites,0,en_20.10.0-11_user-documentation_advance-features_command-line-tool_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_command-line-tool.txt,2025-09-30T12:09:07.801922Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_command-line-tool:1,openBIS Command Line Tool (oBIS),https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/command-line-tool.html,openbis,"## Options:
--version
## Show
the
version
and
exit.
-q,
--quiet
## Suppress
status
reporting.
-s,
--skip_verification
## Do
not
verify
cerficiates
-d,
--debug
## Show
stack
trace
on
error.
--help
## Show
this
message
and
exit.",Options:,1,en_20.10.0-11_user-documentation_advance-features_command-line-tool_1,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_command-line-tool.txt,2025-09-30T12:09:07.801922Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_command-line-tool:2,openBIS Command Line Tool (oBIS),https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/command-line-tool.html,openbis,"## Commands:
addref
## Add
the
given
repository
as
a
reference
to
openBIS.
clone
## Clone
the
repository
found
in
the
given
data
set
id.
collection
Get/set
settings
related
to
the
collection.
commit
## Commit
the
repository
to
git
and
inform
openBIS.
config
Get/set
configurations.
data_set
Get/set
settings
related
to
the
data
set.
download
## Download
files
of
a
data
set.
init
## Initialize
the
folder
as
a
data
repository.
init_analysis
## Initialize
the
folder
as
an
analysis
folder.
move
## Move
the
repository
found
in
the
given
data
set
id.
object
Get/set
settings
related
to
the
object.
removeref
## Remove
the
reference
to
the
given
repository
from
openBIS.
repository
Get/set
settings
related
to
the
repository.
settings
## Get
all
settings.
status
## Show
the
state
of
the
obis
repository.
sync
## Sync
the
repository
with
openBIS.
token
create/show
a
openBIS
token
upload
## Upload
files
to
form
a
data
set.
To show detailed help for a specific command, type
obis
<command>
--help
## :
$
obis
commit
--help
### Usage:
obis
commit
[
## OPTIONS
]
[
## REPOSITORY
]
## Options:
-m,
--msg
## TEXT
## A
message
explaining
what
was
done
.
-a,
--auto_add
## Automatically
add
all
untracked
files.
-i,
--ignore_missing_parent
## If
parent
data
set
is
missing,
ignore
it.
--help
## Show
this
message
and
exit.
5. Work modes

oBIS command line tool can work in two modes depending on how data is stored:
Standard Data Store mode
External Data Store mode
## Warning:
Each repository can work in a single mode only! Mixing modes is not supported.
Depending on the mode, some commands may be unavailable or behave differently. Please read details
in the adequate section.
Here is a short summary of which commands are available in given modes:
## Command
Standard Data Store
External Data Store
addref
❌
✅
clone
❌
✅
collection get
✅
✅
collection set
✅
✅
collection clear
❌
✅
commit
❌
✅
config get
✅
✅
config set
✅
✅
config clear
❌
✅
data_set get
❌
✅
data_set set
❌
✅
data_set clear
❌
✅
data_set search
✅
❌
download
✅
❌
init
❌
✅
init -p
✅
❌
init_analysis
❌
✅
move
❌
✅
object get
✅
✅
object set
✅
✅
object clear
❌
✅
object search
✅
❌
removeref
❌
✅
repository get
❌
✅
repository set
❌
✅
repository clear
❌
✅
settings get
❌
✅
settings set
❌
✅
settings clear
❌
✅
status
❌
✅
sync
❌
✅
token
✅
✅
upload
✅
❌
## Login
Some commands like
download
or
upload
will connect to OpenBIS instance. At that time, oBIS will
use username configured in
.obis/config.json
and will ask for password whenever session expires or
username changes.
5.1 Standard Data Store

Standard Data Store mode depicts a workflow where datasets are stored directly in the OpenBIS
instance. In this mode user can download/upload files to OpenBIS, search for objects/datasets
fulfilling filtering criteria
and get/set properties of objects/collections represented by datasets in current repository.
## 5.1.1 Commands

collection
obis
collection
get
[
key1
]
[
key2
]
...
obis
collection
set
[
key1
]=[
value1
]
,
[
key2
]=[
value2
]
...
## With
collection
command, obis crawls through current repository and gathers all data set ids and
then - if
data set is connected directly to a collection - gets or sets given properties to it in OpenBIS
Note some property names may require to be encapsulated in ‘’, e.g. ‘$name’
config
obis
config
get
[
key
]
obis
config
set
[
key
]=[
value
]
## With
config
command, obis can get/set config of a local repository, e.g. when setting access link
to OpenBIS instance
The settings are saved within the obis repository, in the
.obis
folder, as JSON files, or
in
~/.obis
for the global settings. They can be added/edited manually, which might be useful when
it comes to integration with other tools.
## Example
.obis/config.json
{
""fileservice_url""
## :
null,
""git_annex_hash_as_checksum""
## :
true,
""hostname""
## :
""bsse-bs-dock-5-160.ethz.ch""
,
""is_physical""
## :
true,
""openbis_url""
## :
""http://localhost:8888""
}
data_set
obis
data_set
search
[
## OPTIONS
]
## Options:
-object_type,
--object_type
## TEXT
## Object
type
code
to
filter
by
-space,
--space
## TEXT
## Space
code
-project,
--project
## TEXT
## Full
project
identification
code
-experiment,
--experiment
## TEXT
## Full
experiment
code
-object,
--object
## TEXT
## Object
identification
information,
it
can
be
permId
or
identifier
-type,
--type
## TEXT
## Type
code
-registration-date,
--registration-date
## TEXT
## Registration
date,
it
can
be
in
the
format
""oYYYY-MM-DD""
(
e.g.
"">2023-01-31""
,
""=2023-01-31""
,
""<2023-01-31""
)
-modification-date,
--modification-date
## TEXT
## Modification
date,
it
can
be
in
the
format
""oYYYY-MM-DD""
(
e.g.
"">2023-01-31""
,
""=2023-01-31""
,
""<2023-01-31""
)
-property
## TEXT
## Property
code
-property-value
## TEXT
## Property
value
-save,
--save
## TEXT
## Directory
name
to
save
results
-r,
--recursive
## Search
data
recursively
## With
data_set
search
command, obis connects to a configured OpenBIS instance and searches for all
data sets that fulfill given filtering criteria or by using object identification string.
At least one search option must be specified.
Search results can be downloaded into a file by using
save
option.
Recursive option enables searching for datasets of children samples or datasets
Note: Filtering by
-project
may not work when
## Project
## Samples
are disabled in OpenBIS
configuration.
download
obis
download
[
options
]
[
data_set_id
]
## Options:
-from-file,
--from-file
## TEXT
## An
output
## .CSV
file
from
`
obis
data_set
search
`
command
with
the
list
of
objects
to
download
data
sets
from
-f,
--file
## TEXT
## File
in
the
data
set
to
download
-
downloading
all
if
not
given.
-s,
--skip_integrity_check
## Flag
to
skip
file
integrity
check
with
checksums
## The
download
command downloads, the files of a given data set from the OpenBIS instance specified
in
config
. This command requires the DownloadHandler / FileInfoHandler microservices to be running
and the
fileservice_url
needs to be configured.
init
obis
init
-p
[
folder
]
If a folder is given, obis will initialize that folder as an obis repository that works in the
Standard Data Store mode.
If not, it will use the current folder.
object get / set
obis
collection
get
[
key1
]
[
key2
]
...
obis
collection
set
[
key1
]=[
value1
]
,
[
key2
]=[
value2
]
...
## With
get
and
set
commands, obis crawls through current repository and gathers all data set ids
and then - if
data set is connected directly to an object - gets or sets given properties to it in OpenBIS
Note some property names may require to be encapsulated in ‘’, e.g. ‘$name’
object search
obis
object
search
[
## OPTIONS
]
## Options:
-type,
--type
## TEXT
## Type
code
to
filter
by
-space,
--space
## TEXT
## Space
code
-project,
--project
## TEXT
## Full
project
identification
code
-experiment,
--experiment
## TEXT
## Full
experiment
-object,
--object
## TEXT
## Object
identification
information,
it
can
be
permId
or
identifier
-registration-date,
--registration-date
## TEXT
## Registration
date,
it
can
be
in
the
format
""oYYYY-MM-DD""
(
e.g.
"">2023-01-31""
,
""=2023-01-31""
,
""<2023-01-31""
)
-modification-date,
--modification-date
## TEXT
## Modification
date,
it
can
be
in
the
format
""oYYYY-MM-DD""
(
e.g.
"">2023-01-31""
,
""=2023-01-31""
,
""<2023-01-31""
)
-property
## TEXT
## Property
code
-property-value
## TEXT
## Property
value
-save,
--save
## TEXT
## File
name
to
save
results
in
csv
format
-r,
--recursive
## Search
data
recursively
## With
object
search
command, obis connects to a configured OpenBIS instance and searches for all
objects/samples that fulfill given filtering criteria or by using object identification string.
At least one search option must be specified.
Search results can be downloaded into a file by using
save
option.
Recursive option enables searching for datasets of children samples or datasets
Note: Filtering by
-project
may not work when
## Project
## Samples
are disabled in OpenBIS
configuration.
upload
obis
upload
[
sample_id
]
[
data_set_type
]
[
## OPTIONS
]
## With
upload
command, a new data set of type
data_set_type
will be created under
object
sample_id
. Files and folders specified with
-f
flag will be uploaded to a newly created
data set.
## 5.1.2 Examples

Create an obis repository to work in Standard Data Store mode
# global settings to be use for all obis repositories
obis
config
-g
set
openbis_url
=
https://localhost:8888
obis
config
-g
set
user
=
admin
# create an obis repository with a folder name
obis
init
-p
data1
cd
data1
# check configuration
obis
config
get
is_physical
# search for objects of type BACTERIA in sapce TESTID  in OpenBIS
obis
object
search
-space
## TESTID
-type
## BACTERIA
# save search results in a files
obis
object
search
-space
## TESTID
-type
## BACTERIA
-save
results.csv
obis
object
search
-space
## TESTID
-save
results_space.csv
# upload files to an existing object as type RAW_DATA
obis
upload
20230228133001314
-59
## RAW_DATA
-f
results.csv
-f
results_space.csv
download datasets of an object and check properties
# assuming we are in a configured obis repository
obis
download
20230228091119011
-58
# set object name to XYZ
obis
object
set
'$name'
=
## XYZ
# set children of an object to /TESTID/PROJECT_101/PROJECT_101_EXP_3
obis
object
set
children
=
## /TESTID/PROJECT_101/PROJECT_101_EXP_3
5.2 External Data Store

External Data Store mode allows for orderly management of data in
conditions that require great flexibility. oBIS makes it possible to track data on a file system,
where users have complete freedom to structure and manipulate the data as they wish, while retaining
the benefits of openBIS. With oBIS, only metadata is actually stored and managed by openBIS. The
data itself is managed externally, by the user, but openBIS is aware of its existence and the data
can be used for provenance tracking.
Under the covers, obis takes advantage of publicly available and tested tools to manage data on the
file system. In particular, it uses git and git-annex to track the content of a dataset. Using
git-annex, even large binary artifacts can be tracked efficiently. For communication with openBIS,
obis uses the openBIS API, which offers the power to register and track all metadata supported by
openBIS.
## 5.2.1 Settings

## With
get
you retrieve one or more settings. If the
key
is omitted, you retrieve all settings of
the
type
## :
obis
[
type
]
[
options
]
get
[
key
]
## With
set
you set one or more settings:
obis
[
type
]
[
options
]
set
[
key1
]=[
value1
]
,
[
key2
]=[
value2
]
,
...
## With
clear
you unset one or more settings:
obis
[
type
]
[
options
]
clear
[
key1
]
With the type
settings
you can get all settings at once:
obis
settings
[
options
]
get
The option
-g
can be used to interact with the global settings. The global settings are stored
in
~/.obis
and are copied to an obis repository when that is created.
## Following settings exist:
type
setting
description
collection
id
Identifier of the collection the created data set is attached to. Use either this or the object id.
config
allow_only_https
Default is true. If false, http can be used to connect to openBIS.
config
fileservice_url
URL for downloading files. See DownloadHandler / FileInfoHandler services.
config
git_annex_backend
Git annex backend to be used to calculate file hashes. Supported backends are SHA256E (default), MD5 and WORM.
config
git_annex_hash_as_checksum
Default is true. If false, a CRC32 checksum will be calculated for openBIS. Otherwise, the hash calculated by git-annex will be used.
config
hostname
Hostname to be used when cloning / moving a data set to connect to the machine where the original copy is located.
config
openbis_url
URL for connecting to openBIS (only protocol://host:port, without a path).
config
openbis_token
Token to use when connecting to openBIS. Can be either a session token or a personal access token. Alternatively, it can be a path to a file containing the token.
config
session_name
The name every personal access token is associated with.
config
obis_metadata_folder
Absolute path to the folder which obis will use to store its metadata. If not set, the metadata will be stored in the same location as the data. This setting can be useful when dealing with read-only access to the data. The clone and move commands will not work when this is set.
config
user
User for connecting to openBIS.
data_set
type
Data set type of data sets created by obis.
data_set
properties
Data set properties of data sets created by obis.
object
id
Identifier of the object the created data set is attached to. Use either this or the collection id.
repository
data_set_id
This is set by obis. Is is the id of the most recent data set created by obis and will be used as the parent of the next one.
repository
external_dms_id
This is set by obis. Id of the external dms in openBIS.
repository
id
This is set by obis. Id of the obis repository.
The settings are saved within the obis repository, in the
.obis
folder, as JSON files, or
in
~/.obis
for the global settings. They can be added/edited manually, which might be useful when
it comes to integration with other tools.
## Example
.obis/config.json
{
""fileservice_url""
## :
null,
""git_annex_hash_as_checksum""
## :
true,
""hostname""
## :
""bsse-bs-dock-5-160.ethz.ch""
,
""openbis_url""
## :
""http://localhost:8888""
}
## Example
.obis/data_set.json
{
""properties""
## :
{
## ""K1""
## :
""v1""
,
## ""K2""
## :
""v2""
}
,
""type""
## :
## ""UNKNOWN""
}
## 5.2.2 Commands

init
obis
init
[
folder
]
If a folder is given, obis will initialize that folder as an obis in the External Data Store mode.
If not, it will use the current folder.
init_analysis
obis
init_analysis
[
options
]
[
folder
]
With init_analysis, a repository can be created which is derived from a parent repository. If it is
called from within a repository, that will be used as a parent. If not, the parent has to be given
with the
-p
option.
commit
obis
commit
[
options
]
## The
commit
command adds files to a new data set in openBIS. If the
-m
option is not used to
define a commit message, the user will be asked to provide one.
sync
obis
sync
When git commits have been done manually, the
sync
command creates the corresponding data set in
openBIS. Note that, when interacting with git directly, use the git annex commands whenever
applicable, e.g. use “git annex add” instead of “git add”.
status
obis
status
[
folder
]
This shows the status of the repository folder from which it is invoked, or the one given as a
parameter. It shows file changes and whether the repository needs to be synchronized with openBIS.
clone
obis
clone
[
options
]
[
data_set_id
]
## The
clone
command copies a repository associated with a data set and registers the new copy in
openBIS. In case there are already multiple copied of the repository, obis will ask from which copy
to clone.
To avoid user interaction, the copy index can be chosen with the option
-c
With the option
-u
a user can be defined for copying the files from a remote system
By default, the file integrity is checked by calculating the checksum. This can be skipped
with
-s
.
## Note
: This command does not work when
obis_metadata_folder
is set.
move
obis
move
[
options
]
[
data_set_id
]
## The
move
command works the same as
clone
, except that the old repository will be removed.
Note: This command does not work when
obis_metadata_folder
is set.
addref / removeref
obis
addref
obis
removeref
Obis repository folders can be added or removed from openBIS. This can be useful when a repository
was moved or copied without using the
move
or
copy
commands.
token
obis
token
get
<session_name>
[
--validity-days
]
[
--validity-weeks
]
[
--validity-months
]
Gets or creates a new personal access token (PAT) and stores it in the obis configuration. If
no
session_name
is provided or is not stored in the configuration, you’ll be asked interactively.
If no validity period is provided, the maximum (configured by the server) is used. If a PAT with
this
session_name
already exists and it is going to expire soon (according to server
setting
personal_access_tokens_validity_warning_period
), a new PAT will be created, stored in the
obis configuration and used for every subsequent request.
## 5.2.3 Examples

Create an obis repository and commit to openBIS
# global settings to be use for all obis repositories
obis
config
-g
set
openbis_url
=
https://localhost:8888
obis
config
-g
set
user
=
admin
# create an obis repository with a file
obis
init
data1
cd
data1
echo
content
>>
example_file
# configure the repository
obis
data_set
set
type
=
## UNKNOWN
obis
object
set
id
=
## /DEFAULT/DEFAULT
# commit to openBIS
obis
commit
-m
'message'
Commit to git and sync manually
# assuming we are in a configured obis repository
echo
content
>>
example_file
git
annex
add
example_file
git
commit
-m
'message'
obis
sync
Create an analysis repository
# assuming we have a repository 'data1'
obis
init_analysis
-p
data1
analysis1
cd
analysis1
obis
data_set
set
type
=
## UNKNOWN
obis
object
set
id
=
## /DEFAULT/DEFAULT
echo
content
>>
example_file
obis
commit
-m
'message'
## 6. Authentication

There are 2 ways to perform user authentication against OpenBIS.
## 6.1. Login

Obis, internally, stores a session token which is used to connect with OpenBIS. Whenever this token
is invalidated, obis will ask user to provide credentials to log into OpenBIS again.
## 6.2. Personal Access Token

Session token is short-lived and its interactive generation makes it unfeasible for usage in automatic
scripts. An alternative way to authorize is to generate personal access token (PAT), which can be
configured to last for a long periods of time.
PAT generation is explained in depth in
token
command section.
7. Big Data Link Services

The Big Data Link Services can be used to download files which are contained in an obis repository.
The services are included in the installation folder of openBIS,
under
servers/big_data_link_services
. For how to configure and run them, consult
the
README.md
file.
8. Rationale for obis

Data-provenance tracking tools like openBIS make it possible to understand and follow the research
process. What was studied, what data was acquired and how, how was data analyzed to arrive at final
results for publication – this is information that is captured in openBIS. In the standard usage
scenario, openBIS stores and manages data directly. This has the advantage that openBIS acts as a
gatekeeper to the data, making it easy to keep backups or enforce access restrictions, etc. However,
this way of working is not a good solution for all situations.
Some research groups work with large amounts of data (e.g., multiple TB), which makes it inefficient
and impractical to give openBIS control of the data. Other research groups require that data be
stored on a shared file system under a well-defined directory structure, be it for historical
reasons or because of the tools they use. In this case as well, it is difficult to give openBIS full
control of the data.
For situations like these, we have developed
obis
, a tool for orderly management of data in
conditions that require great flexibility.
obis
makes it possible to track data on a file system,
where users have complete freedom to structure and manipulate the data as they wish, while retaining
the benefits of openBIS. With
obis
, only metadata is actually stored and managed by openBIS. The
data itself is managed externally, by the user, but openBIS is aware of its existence and the data
can be used for provenance tracking.
obis
is packaged as a stand-alone utility, which, to be
available, only needs to be added to the
## PATH
variable in a UNIX or UNIX-like environment.
Under the covers,
obis
takes advantage of publicly available and tested tools to manage data on
the file system. In particular, it uses
git
and
git-annex
to track the content of a dataset.
## Using
git-annex
, even large binary artifacts can be tracked efficiently. For communication with
openBIS,
obis
uses the openBIS API, which offers the power to register and track all metadata
supported by openBIS.
## 9. Literature

V. Korolev, A. Joshi, V. Korolev, M.A. Grasso, A. Joshi, M.A. Grasso, et al., “PROB: A tool for
tracking provenance and reproducibility of big data experiments”, Reproduce ‘14. HPCA 2014, vol. 11,
pp. 264-286, 2014.
http://ebiquity.umbc.edu/
file_directory
/papers/693.pdf",Commands:,2,en_20.10.0-11_user-documentation_advance-features_command-line-tool_2,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_command-line-tool.txt,2025-09-30T12:09:07.801922Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_excel-import-service:0,Excel Import Service,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/excel-import-service.html,openbis,"## Excel Import Service

## Introduction

The Excel import service reads xls definitions for both types and
entities and send them to openBIS. It is the replacement of the old
master data scripts adding support for the creation of openBIS entities.
The goals are:
For common users an import format with the following features to avoid
the shortcomings of the old format:
Recognisable labels as column names.
Multi-type imports.
Parents/Children creation and linking on a single import.
For advanced users like consultants and plugin developers a tool that
allows to specify on an Excel sheet:
Metadata model.
Basic entity structures used for navigation.
## Modes

To support different use cases the import service supports the next
modes, specifying one of them is mandatory.
UPDATE IF EXISTS: This one should be the default mode to use to make
incremental updates.
IGNORE EXISTING: This mode should be used when the intention is to
ignore updates. Existing entities will be ignored. That way is
possible to avoid unintentionally updating entities and at the same
time adding new ones.
FAIL IF EXISTS: This mode should be used when the intention is to
fail if anything is found. That way is possible to avoid making any
unintentional changes.
## Organising Definition Files

All data can be arranged according to the needs of the user, in any
number of files and any number of worksheets. All files have to be in
one directory.
The names of the files and worksheets are ignored by the service, the
user is advised to use descriptive names that they can quickly
remember/refer to later.
## Warning
If there are dependencies between files they should be submitted together or an error will be shown.
## Example:
We want to define vocabularies and sample types with properties using
these vocabularies. We can arrange our files in several ways:
put vocabulary and sample types in separate files named i.e
vocabulary.xls and sample_types.xlsx respectively
put vocabulary and sample types in different worksheets in the same
xls file
put everything in one worksheet in the same file
## Organising Definitions

## Type definitions:
The order of type definitions is not important for the Excel import
service, with exception of Vocabularies, those need to be placed before
the property types that use them.
## Entity definitions:
Type definitions for the entities should already exist in the database
at the time when entities are registered. Generally Entity definitions
are placed at the end.
Text cell formatting (colours, fonts, font style, text decorations)

All types of formatting are permitted, and users are encouraged to use
them to make their excel files more readable. Adding any non text
element (table, clipart) will cause the import to fail.
(A valid, but not easily readable, example)
Definition, rows and sheet formatting

A valid sheet has to start with definition on the first row.
Each definition has to be separated by one empty row.
Two or more consecutive empty rows mark the end of the definitions.
Empty spaces at the beginning or end of headers are silently
eliminated.
## Warning
If any content is placed after two consecutive empty rows it will result in an error. This is to alert the user and avoid silently ignoring content.
## Warning
Header rows
NEED TO BE
a valid attribute of the entity or entity type, property label or property code.
Any unintended header will result in an error. This is to avoid possible
misspellings and avoid silently ignoring content.
## Entity Types Definitions

All entity types can be created. There are differences due to the nature of the defined elements themselves.
Vocabulary and Vocabulary Term

## Vocabulary
## Headers
## Mandatory
## Code
## Yes
## Description
## Yes
## Vocabulary Term
## Headers
## Mandatory
## Code
## Yes
## Label
## Yes
## Description
## Yes
## Example
## VOCABULARY_TYPE
## Code
## Description
## $STORAGE.STORAGE_VALIDATION_LEVEL
## Validation Level
## Code
## Label
## Description
## RACK
## Rack Validation
## BOX
## Box Validation
## BOX_POSITION
## Box Position Validation
## Experiment Type

## Headers
## Mandatory
## Code
## Yes
## Description
## Yes
Validation script
## Yes
## Ontology Id
## No
## Ontology Version
## No
## Ontology Annotation Id
## No
## Example
## EXPERIMENT_TYPE
## Code
## Description
Validation script
## DEFAULT_EXPERIMENT
date_range_validation.py
## Sample Type

## Headers
## Mandatory
## Code
## Yes
## Description
## Yes
Auto generate codes
## Yes
Validation script
## Yes
Generate code prefix
## Yes
## Ontology Id
## No
## Ontology Version
## No
## Ontology Annotation Id
## No
## Example
## SAMPLE_TYPE
## Code
## Description
Auto generate codes
Validation script
Generated code prefix
## STORAGE_POSITION
## TRUE
storage_position_validation.py
## STO
## Dataset Type

## Headers
## Mandatory
## Code
## Yes
## Description
## Yes
Validation script
## Yes
## Ontology Id
## No
## Ontology Version
## No
## Ontology Annotation Id
## No
## Example
## DATASET_TYPE
## Code
## Description
Validation script
## RAW_DATA
## Property Type

A property type can exist unassigned to an entity type or assigned to an
entity type.
## Headers
## Mandatory Assigned
## Mandatory Unassigned
## Code
## Yes
## Yes
## Mandatory
## No
## Yes
Show in edit views
## No
## Yes
## Section
## No
## Yes
Property label
## Yes
## Yes
Data type
## Yes
## Yes
Vocabulary code
## Yes
## Yes
## Description
## Yes
## Yes
## Metadata
## No
## No
Dynamic script
## No
## No
## Ontology Id
## No
## No
## Ontology Version
## No
## No
## Ontology Annotation Id
## No
## No
A property type requires a data type to be defined, valid data types
are.
Data type
## Description
## INTEGER
## REAL
## VARCHAR
Text of any length but displayed as a single line field.
## MULTILINE_VARCHAR
Text of any length but displayed as a multi line field.
## HYPERLINK
## BOOLEAN
## CONTROLLEDVOCABULARY
## XML
## TIMESTAMP
## DATE
## SAMPLE
Sample of any type.
## SAMPLE:<SAMPLE_TYPE>
Sample of the indicated type.
## Example Unassigned Property
In this case, the property is registered without being assigned to a
type, and  the block of property types uses the PROPERTY_TYPE block.
## PROPERTY_TYPE
## Code
## Mandatory
Show in edit views
## Section
Property label
Data type
Vocabulary code
## Description
## $WELL.COLOR_ENCODED_ANNOTATION
## FALSE
## TRUE
Color Annotation
## CONTROLLEDVOCABULARY
## $WELL.COLOR_ENCODED_ANNOTATIONS
Color Annotation for plate wells
## ANNOTATION.SYSTEM.COMMENTS
## FALSE
## TRUE
## Comments
## VARCHAR
## Comments
## ANNOTATION.REQUEST.QUANTITY_OF_ITEMS
## FALSE
## TRUE
Quantity of Items
## INTEGER
Quantity of Items
## $BARCODE
## FALSE
## FALSE
## Custom Barcode
## VARCHAR
## Custom Barcode
## Example Assigned
In this case the property types are assigned to a sample type and the
block of property types belong to the entity type block (SAMPLE_TYPE in
this case).
## SAMPLE_TYPE
## Code
## Description
Auto generate codes
Validation script
Generated code prefix
## ENTRY
## TRUE
## ENTRY
## Code
## Mandatory
Show in edit views
## Section
Property label
Data type
Vocabulary code
## Description
## Metadata
Dynamic script
## $NAME
## FALSE
## TRUE
General info
## Name
## VARCHAR
## Name
## $SHOW_IN_PROJECT_OVERVIEW
## FALSE
## TRUE
General info
Show in project overview
## BOOLEAN
Show in project overview page
## $DOCUMENT
## FALSE
## TRUE
General info
## Document
## MULTILINE_VARCHAR
## Document
{ “custom_widget” : “Word Processor” }
## $ANNOTATIONS_STATE
## FALSE
## FALSE
## Annotations State
## XML
## Annotations State
Entity Type Validation Script and Property Type Dynamic Script

Scripts have to reside in
.py
files in the
scripts
directory within
the folder that contains the Excel files.
## Within
scripts,
files can be organised in any suitable setup:
In order to refer to a validation or dynamic script
(e.g.
storage_position_validation.py
below), the relative path (from
the
scripts
directory) to the file has to be provided in the relevant
column. See the example columns below.
## Example
## SAMPLE_TYPE
## Code
## Description
Auto generate codes
Validation scriptƒgre
Generated code prefix
## STORAGE_POSITION
## TRUE
storage_position_validation.py
## STO
## Code
## Mandatory
Show in edit views
## Section
Property label
Data type
Vocabulary code
## Description
## Metadata
Dynamic script
## $STORAGE_POSITION.STORAGE_CODE
## FALSE
## TRUE
## Physical Storage
## Storage Code
## VARCHAR
## Storage Code
## $STORAGE_POSITION.STORAGE_RACK_ROW
## FALSE
## TRUE
## Physical Storage
## Storage Rack Row
## INTEGER
Number of Rows
## $STORAGE_POSITION.STORAGE_RACK_COLUMN
## FALSE
## TRUE
## Physical Storage
## Storage Rack Column
## INTEGER
Number of Columns
## $STORAGE_POSITION.STORAGE_BOX_NAME
## FALSE
## TRUE
## Physical Storage
## Storage Box Name
## VARCHAR
## Box Name
## $STORAGE_POSITION.STORAGE_BOX_SIZE
## FALSE
## TRUE
## Physical Storage
## Storage Box Size
## CONTROLLEDVOCABULARY
## $STORAGE_POSITION.STORAGE_BOX_SIZE
## Box Size
## $STORAGE_POSITION.STORAGE_BOX_POSITION
## FALSE
## TRUE
## Physical Storage
## Storage Box Position
## VARCHAR
## Box Position
## $STORAGE_POSITION.STORAGE_USER
## FALSE
## TRUE
## Physical Storage
## Storage User Id
## VARCHAR
## Storage User Id
## $XMLCOMMENTS
## FALSE
## FALSE
## Comments
## XML
Comments log
## $ANNOTATIONS_STATE
## FALSE
## FALSE
## Annotations State
## XML
## Annotations State
## Entity Types Update Algorithm

### General Usage

For every TYPE found in the Excel sheet the next algorithm is performed:
## IF
## ITEM
## NOT
## EXISTS
in
openBIS
## :
## CREATE
## ITEM
## ELSE
## :
//
## Doesn
't exist branch
## IF
## FAIL_IF_EXISTS
## :
## THROW
## EXCEPTION
## IF
## UPDATE_IF_EXISTS
## :
## UPDATE
## ITEM
## ELSE
## IF
## IGNORE_EXISTING
## :
## PASS
//
## Ignore
as
requested
## ELSE
## :
## PASS
//
## Ignore
object
that
have
not
been
updated
## Entity Definitions

Most entities can be created, excluding DataSets. There are differences due to the nature of the defined elements themselves.
## General Rules:
Header order is arbitrary.
When referring to another entity only Identifiers are allowed.
Sample Variables are the only exception.
Vocabulary values in property value rows can be referred to by
either the vocabulary term code or the vocabulary term label.
## Warning
If a mandatory header is missing it results in an error.
## Warning
Repeated headers will result in an error, in case a Property shares Label with an Attribute is encouraged to use the property code instead.
## Space

## Headers
## Mandatory
## Code
## Yes
## Description
## Yes
## Example
## SPACE
## Code
## Description
## ELN_SETTINGS
ELN Settings
## DEFAULT_LAB_NOTEBOOK
## Default Lab Notebook
## METHODS
Folder for methods
## MATERIALS
Folder for th materials
## STOCK_CATALOG
Folder for the catalog
## STOCK_ORDERS
Folder for orders
## PUBLICATIONS
Folder for publications
## Project

## Headers
## Mandatory
## Identifier
Yes on UPDATES, ignored on INSERT
## Code
## Yes
## Space
## Yes
## Description
## Yes
## Example
## PROJECT
## Identifier
## Code
## Description
## Space
## /DEFAULT_LAB_NOTEBOOK/DEFAULT_PROJECT
## DEFAULT_PROJECT
## Default Project
## DEFAULT_LAB_NOTEBOOK
## /METHODS/PROTOCOLS
## PROTOCOLS
## Protocols
## METHODS
## /STOCK_CATALOG/PRODUCTS
## PRODUCTS
## Products
## STOCK_CATALOG
## /STOCK_CATALOG/SUPPLIERS
## SUPPLIERS
## Suppliers
## STOCK_CATALOG
## /STOCK_CATALOG/REQUESTS
## REQUESTS
## Requests
## STOCK_CATALOG
## /STOCK_ORDERS/ORDERS
## ORDERS
## Orders
## STOCK_ORDERS
## /ELN_SETTINGS/TEMPLATES
## TEMPLATES
## Templates
## ELN_SETTINGS
## /PUBLICATIONS/PUBLIC_REPOSITORIES
## PUBLIC_REPOSITORIES
## Public Repositories
## PUBLICATIONS
## Experiment

## Headers
## Mandatory
## Identifier
Yes on UPDATES, ignored on INSERT
## Code
## Yes
## Project
## Yes
## Property Code
## No
## Property Label
## No
## Example
## EXPERIMENT
Experiment type
## COLLECTION
## Identifier
## Code
## Project
## Name
Default object type
## /METHODS/PROTOCOLS/GENERAL_PROTOCOLS
## GENERAL_PROTOCOLS
## /METHODS/PROTOCOLS
## General Protocols
## GENERAL_PROTOCOL
## /STOCK_CATALOG/PRODUCTS/PRODUCT_COLLECTION
## PRODUCT_COLLECTION
## /STOCK_CATALOG/PRODUCTS
## Product Collection
## PRODUCT
## /STOCK_CATALOG/SUPPLIERS/SUPPLIER_COLLECTION
## SUPPLIER_COLLECTION
## /STOCK_CATALOG/SUPPLIERS
## Supplier Collection
## SUPPLIER
## /STOCK_CATALOG/REQUESTS/REQUEST_COLLECTION
## REQUEST_COLLECTION
## /STOCK_CATALOG/REQUESTS
## Request Collection
## REQUEST
## /STOCK_ORDERS/ORDERS/ORDER_COLLECTION
## ORDER_COLLECTION
## /STOCK_ORDERS/ORDERS
## Order Collection
## ORDER
## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION
## TEMPLATES_COLLECTION
## /ELN_SETTINGS/TEMPLATES
## Template Collection
/PUBLICATIONS/PUBLIC_REPOSITORIES/PUBLICATIONS_COLLECTION
## PUBLICATIONS_COLLECTION
## /PUBLICATIONS/PUBLIC_REPOSITORIES
## Publications Collection
## PUBLICATION
## Sample

## Headers
## Mandatory
$
## No
## Identifier
Yes on UPDATES, ignored on INSERT
## Code
## No
## Project
## No
## Experiment
## No
Auto generate code
## No
## Parents
## No
## Children
## No
## Property Code
## No
## Property Label
## No
## Example
## SAMPLE
Sample type
## ORDER
$
## Identifier
## Code
## Space
## Project
## Experiment
## Order Status
## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE
## ORDER_TEMPLATE
## ELN_SETTINGS
## /ELN_SETTINGS/TEMPLATES
## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION
Not yet ordered
Defining Parent and Children in Samples

Parent and child columns can be used to define relations between
samples. Samples can be addressed by:
$ : Variables, only really useful during batch inserts for samples
with autogenerated codes since Identifiers can’t be known. Variables
SHOULD start with $.
## Identifiers
## Warning
Parents and children SHOULD be separated by an end of line, each sample should be in its own line.
## SAMPLE
Sample type
## ORDER
$
## Parents
## Children
## Identifier
## Code
## Space
## Project
## Experiment
## Order Status
## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_A
## ORDER_TEMPLATE
## ELN_SETTINGS
## /ELN_SETTINGS/TEMPLATES
## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION
Not yet ordered
## $B
## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_B
## ORDER_TEMPLATE
## ELN_SETTINGS
## /ELN_SETTINGS/TEMPLATES
## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION
Not yet ordered
## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_A
## $B
## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_D
## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_C
## ORDER_TEMPLATE
## ELN_SETTINGS
## /ELN_SETTINGS/TEMPLATES
## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION
Not yet ordered
## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_D
## ORDER_TEMPLATE
## ELN_SETTINGS
## /ELN_SETTINGS/TEMPLATES
## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION
Not yet ordered
Properties and Sample Variables

As a general rule, properties would only accept data of the specified
type.
Sample properties would typically require an Identifier to be given but
a variable ‘$’ could be used instead for a sample declared at any point
of the document, including cyclical dependencies. This is useful for
scenarios where Sample codes are autogenerated and can’t be known in
advance.
Master Data as a Core Plugin

The master data plugin is an AS core plugin.
Directory structure
(important)
## :
Use standard initialize-master-data.py handle as it is ingested by
openbis on startup.
Excel files
should be organised
in
master-data
directory
in the same plugin and
scripts
should
be contained in
scripts
directory
under master-data.
## Contents of initialize-master-data.py:
from
ch.ethz.sis.openbis.generic.server.asapi.v3
import
ApplicationServerApi
from
ch.systemsx.cisd.openbis.generic.server
import
CommonServiceProvider
from
ch.ethz.sis.openbis.generic.asapi.v3.dto.service.id
import
CustomASServiceCode
from
ch.ethz.sis.openbis.generic.asapi.v3.dto.service
import
CustomASServiceExecutionOptions
from
ch.systemsx.cisd.openbis.generic.server.jython.api.v1.impl
import
MasterDataRegistrationHelper
import
sys
helper
=
MasterDataRegistrationHelper
(
sys
.
path
)
api
=
CommonServiceProvider
.
getApplicationContext
()
.
getBean
(
ApplicationServerApi
.
## INTERNAL_SERVICE_NAME
)
sessionToken
=
api
.
loginAsSystem
()
props
=
CustomASServiceExecutionOptions
()
.
withParameter
(
'xls'
,
helper
.
listXlsByteArrays
())
\
.
withParameter
(
'xls_name'
,
## 'ELN-LIMS-LIFE-SCIENCES'
)
.
withParameter
(
'update_mode'
,
## 'UPDATE_IF_EXISTS'
)
\
.
withParameter
(
'scripts'
,
helper
.
getAllScripts
())
result
=
api
.
executeCustomASService
(
sessionToken
,
CustomASServiceCode
(
""xls-import-api""
),
props
)
There are following parameters to fill (Easiest is to use
MasterDataRegistrationHelper to evaluate parameter values):
‘xls’: Array of excel files. It can be easily acquired by calling
helper.listXlsByteArrays or listCsvByteArrays.
‘xls_name’ - Name for the batch, it is used by versioning system.
‘update_mode’ - See “Modes” section.
‘scripts’ - if you have any scripts in your data, provide them here.
It is easiest to get it with MasterDataRegistrationHelper
getAllScripts function.
‘results’ object is a summary of what has been created.
## Example
For an complete up to date example, please check the
eln-lims-life-sciences plugin that ships with the installer or on the
## official Git repository:
https://sissource.ethz.ch/sispub/openbis/-/tree/master/openbis_standard_technologies/dist/core-plugins/eln-lims-life-sciences/1/as
Or download the complete plugin using the next link:
https://sissource.ethz.ch/sispub/openbis/-/archive/master/openbis-master.zip?path=openbis_standard_technologies/dist/core-plugins/eln-lims-life-sciences
## Known Limitations

Property type assignments to entity types cannot be updated since
the current V3 API does not support this functionality. This means
that a change in the order of assignments or group names during an
update will be ignored.",Excel Import Service,0,en_20.10.0-11_user-documentation_advance-features_excel-import-service_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_excel-import-service.txt,2025-09-30T12:09:07.896843Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_index:0,Advance Features,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/index.html,openbis,"## Advance Features

JupyterHub for openBIS
## Overview
## Nomenclature
Prerequisites for testing in a local environment
How to run the official JupyterHub for openBIS image in your local machine
How to extend the official JupyterHub for openBIS image
Modify a currently running container - From UI (for users)
## Check Available Python 2 Libraries
## Add Python 2 Library
## Check Available Octave Libraries
## Add Octave Library
## Check Available Python 3 Libraries
## Add Python 3 Library
## Check Available R Libraries
## Add R Library
Modify a currently running container - From Console (for admins)
## Add Python Library
## Add R Library
Save the state of a running container as a new image
Extend a docker image using a docker recipe (for maintenance)
How to start a jupyterhub-openbis docker image on a productive JupyterHub server
Other useful Docker commands
Save an image as a tar file to share it
Load an image from a tar file
Remove an image
Remove all stopped containers
### openBIS ELN Integration Configuration
Troubleshooting Connectivity to openBIS
Session is no longer valid. Please log in again error
Session is no longer valid. The openBIS server has a self-signed certificate
Session is no longer valid. The session has timeout
openBIS Command Line Tool (oBIS)
## 1. Prerequisites
### 2. Installation
3. Quick start guide
### 4. Usage
4.1 Help is your friend!
5. Work modes
5.1 Standard Data Store
## 5.1.1 Commands
## 5.1.2 Examples
5.2 External Data Store
## 5.2.1 Settings
## 5.2.2 Commands
## 5.2.3 Examples
## 6. Authentication
## 6.1. Login
## 6.2. Personal Access Token
7. Big Data Link Services
8. Rationale for obis
## 9. Literature
openBIS Data Modelling
## Overview
Data model in openBIS ELN-LIMS
## Inventory
## Lab Notebook
openBIS parents and children
Examples of parent-child relationships
## Excel Import Service
## Introduction
## Modes
## Organising Definition Files
## Organising Definitions
Text cell formatting (colours, fonts, font style, text decorations)
Definition, rows and sheet formatting
## Entity Types Definitions
Vocabulary and Vocabulary Term
## Experiment Type
## Sample Type
## Dataset Type
## Property Type
Entity Type Validation Script and Property Type Dynamic Script
## Entity Types Update Algorithm
### General Usage
## Entity Definitions
## Space
## Project
## Experiment
## Sample
Defining Parent and Children in Samples
Properties and Sample Variables
Master Data as a Core Plugin
## Known Limitations",Advance Features,0,en_20.10.0-11_user-documentation_advance-features_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_index.txt,2025-09-30T12:09:07.983685Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:0,JupyterHub for openBIS,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html,openbis,"JupyterHub for openBIS

## Warning
This guide is not meant to substitute the official Docker documentation. Standard Docker commands are present in sections that are not necessarily related with them.
## Overview

SIS provides a Docker image for the installation of a JupyterHub server,
available at
https://hub.docker.com/r/openbis/
.
This guide explains how to modify JupyterHub docker images and save
them. It is aimed at users who are not familiar with Docker, but it
should not be considered a substitute of the official Docker
documentation.
## Warning
We advise non expert users, to first test the instructions provided in this guide on their local machine, to familiarise themselves with the process, before making changes on the JupyterHub server.
Docker images are stateless, which means that after rebooting all
changes made will not be saved. This guarantees a stable environment,
which is particularly desirable to run services.
If a user wants to introduce changes, the docker image needs to be
updated. There are two possibilities for this:
For testing
: Login into the Docker container, modify it and save
the modified container as a new image. This method is not
recommended for production because it is not compatible with
official JupyterHub Docker image upgrades released by SIS.
For correct maintenance
: Extend the current image using a Docker
recipe that includes only your changes. This method is recommended
for production, because when a new official JupyterHub Docker image
is released by SIS, it will be possible to quickly apply the changes
to this image from the Docker recipe.
## Nomenclature

## Docker
: A computer program that performs operating-system-level
virtualisation also known as containerisation. The official website can
be found here
https://www.docker.com/
.
Docker image
: Docker images describe the environment to virtualise.
Docker images are stateless.
Docker container
: Docker containers provide the environment to
execute the images.
Prerequisites for testing in a local environment

Docker environment
. All examples shown below require a working
docker environment. Please visit
https://www.docker.com
to
download the Docker Community Edition for your OS.
JupyterHub Docker image
. The jupyterhub-openbis images can be
found at
https://hub.docker.com/r/openbis/
. They can be installed
locally like any other Docker Hub image.
openBIS installation
(optional).
How to run the official JupyterHub for openBIS image in your local machine

After downloading the jupyterhub-openbis, find the id of your image.
$
docker
images
## REPOSITORY
## TAG
## IMAGE
## ID
## CREATED
## SIZE
openbis/jupyterhub-openbis-sis-20180405
latest
585a9adf333b
23
hours
ago
4
## .75GB
Run the image with one of the two following commands:
a. if you want to connect to your productive openBIS instance (e.g.
https://openbis-elnlims.ch), use the following command:
docker run -e OPENBIS_URL=https://openbis-elnlims.ch -e JUPYTERHUB_INTEGRATION_SERVICE_PORT=8002 -e JUPYTERHUB_PORT=8000 -e CERTIFICATE_KEY=/vagrant/config/certificates/default.key -e CERTIFICATE_CRT=/vagrant/config/certificates/default.crt -p 8000:8000 -p 8081:8081 -p 8001:8001 -p 8002:8002 585a9adf333b ./vagrant/initialize/start_jupyterhub.sh
b. if you have a local openBIS installation for testing, you can run
the following command:
docker run -v /Users/juanf/jupyterhub-local/home:/home -v /Users/juanf/jupyterhub-local/config/certificates:/vagrant/config/certificates -e OPENBIS_URL=https://129.132.228.42:8443 -e JUPYTERHUB_INTEGRATION_SERVICE_PORT=8002 -e JUPYTERHUB_PORT=8000 -e CERTIFICATE_KEY=/vagrant/config/certificates/default.key -e CERTIFICATE_CRT=/vagrant/config/certificates/default.crt -p 8000:8000 -p 8081:8081 -p 8001:8001 -p 8002:8002 585a9adf333b ./vagrant/initialize/start_jupyterhub.sh
## Warning
Please note the following configuration options:
-v /Users/juanf/jupyterhub-local/home:/home
This option is only required if you want to store the changes you are making. You need to have a home directory for this. It is not necessary for testing, as the image will provide a default one. This directory should contain a “vagrant” sub directory.
-v /Users/juanf/jupyterhub-local/config/certificates:/vagrant/config/certificates
This option is only required in production environments where you need valid certificates. It is not necssary for testing, as the image will provide a default one.
OPENBIS_URL= https://129.132.228.42:8443
By defaut docker is in bridge mode, which means that your docker container accesses your local machine network directly through it. If you have a local openBIS installation please use your IP address; if you use a server installation use the typical address you use to access it.
To stop a running docker container, run “
docker kill container_ID”
.
The container_ID can be found by running the command
“docker ps”
.
How to extend the official JupyterHub for openBIS image

Modify a currently running container - From UI (for users)

Please note that libraries installed in this way are NOT permanently
saved. After upgrade of the image, the libraries need to be
re-installed.
## Check Available Python 2 Libraries

help(""modules"")
## Add Python 2 Library

It can probably be done but we are currently not supporting it.
## Check Available Octave Libraries

pkg
list
## Add Octave Library

It can probably be done but we are currently not supporting it.
## Check Available Python 3 Libraries

pip
freeze
## Add Python 3 Library

Use pip install as you would normally do. The Python 3 kernel often
doesn’t need to be restarted to pick up new libraries, but is
recommended to do so.
## Check Available R Libraries

my_packages <- library()$results
head(my_packages, 1000000)
## Add R Library

Use the install command as you would normally do. The R kernel needs
to be restarted to pick up new libraries.
Modify a currently running container - From Console (for admins)

Find the container id of the image currently running.
$
docker
ps
## CONTAINER
## ID
## IMAGE
## COMMAND
## CREATED
## STATUS
## PORTS
## NAMES
a2b76d1dd204
jupyterhub-openbis-sis-20180405
""./vagrant/initial...""
4
seconds
ago
## Up
2
seconds
0
.0.0.0:8000-8002->8000-8002/tcp,
0
.0.0.0:8081->8081/tcp
nervous_leakey
Log into the container.
$
docker
exec
-it
a2b76d1dd204
shell
## Add Python Library

Add a new library to Python 3
# First we should move to the environment used by JupyterHub
[
root@a2b76d1dd204
/
]
# export PATH=/vagrant_installation/miniconda3/bin:$PATH
[
root@a2b76d1dd204
/
]
# export LC_ALL=en_US.utf8
[
root@a2b76d1dd204
/
]
# export LANG=en_US.utf8
# Install a new python lib using pip
[
root@a2b76d1dd204
/
]
# python --version
## Python
3
.6.4
## ::
## Anaconda,
## Inc.
[
root@a2b76d1dd204
/
]
# pip install prettytable
This type of changes can be validated straightaway in JupyterHub, by
just starting a Python 3 notebook. Other changes could require to reboot
JupyterHub.
Please note that this approach should only be used for testing. To
preserve the changes, the running container should be saved as a new
image, otherwise when the container is shutdown these changes will be
lost.
## Add R Library

Add a new library to R
# First we should move to the environment used by JupyterHub
[
root@a2b76d1dd204
/
]
# export PATH=/vagrant_installation/miniconda3/bin:$PATH
[
root@a2b76d1dd204
/
]
# export LC_ALL=en_US.utf8
[
root@a2b76d1dd204
/
]
# export LANG=en_US.utf8
# Install a new r lib using conda
[
root@a2b76d1dd204
/
]
# sudo conda list r-
[
root@a2b76d1dd204
/
]
# sudo conda install -c r -y r-base64enc
This type of changes can be validated straightaway in JupyterHub, by
just starting a R notebook. Other changes could require to reboot
JupyterHub.
Save the state of a running container as a new image

If you know that you have made significant changes that you want to keep
until you build a new docker recipe, you have the option to save the
running container as a new image.
bs-mbpr28:jupyterhub_reference_installation
juanf$
docker
ps
## CONTAINER
## ID
## IMAGE
## COMMAND
## CREATED
## STATUS
## PORTS
## NAMES
a2b76d1dd204
jupyterhub-openbis-sis-20180405
""./vagrant/initial...""
37
minutes
ago
## Up
37
minutes
0
.0.0.0:8000-8002->8000-8002/tcp,
0
.0.0.0:8081->8081/tcp
lucid_stonebraker",Warning,0,en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis_0,concept,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis.txt,2025-09-30T12:09:08.051766Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:1,JupyterHub for openBIS,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html,openbis,"$
docker
commit
a2b76d1dd204
jupyterhub-openbis-sis-juanextensions-20180406
sha256:5dd0036664c75a21d6a62b80bf5780e70fcad345bb12a7ad248d01e29a3caa99
$
docker
images
## REPOSITORY
## TAG
## IMAGE
## ID
## CREATED
## SIZE
jupyterhub-openbis-sis-juanextensions-20180406
latest
5dd0036664c7
4
seconds
ago
4
## .75GB
jupyterhub-openbis-sis-20180405
latest
585a9adf333b
23
hours
ago
4
## .75GB
Extend a docker image using a docker recipe (for maintenance)

The recommended approach for maintenance purposes is to extend the
latest official docker image distributed by SIS.
Using our last example, let’s create a file called “Dockerfile” and with
the content shown below.
# vim:set ft=dockerfile:
## FROM
openbis/jupyterhub-openbis-sis-20180405
## Adding Python 3 library
## RUN
export
## PATH
=
## /vagrant_installation/miniconda3/bin:
## $PATH
&&
\
export
## LC_ALL
=
en_US.utf8
&&
\
export
## LANG
=
en_US.utf8
&&
\
pip
install
prettytable
Please change the name of the image in the file to the one you are
using.
Now we can create a new image using as a starting point the latest from
the official repository.
## Warning
It is best practice to include both the name of the user and the creation date in the image name. This will help when dealing with many versions created by different users at different times.
$
docker
build
-t
jupyterhub-openbis-sis-juanextensions-recipe-20180406
.
## Sending
build
context
to
## Docker
daemon
4
## .957GB
## Step
1
/2
## :
## FROM
openbis/jupyterhub-openbis-sis-20180405
....
## Step
2
/2
## :
## RUN
export
## PATH
=
## /vagrant_installation/miniconda3/bin:
## $PATH
&&
export
## LC_ALL
=
en_US.utf8
&&
export
## LANG
=
en_US.utf8
&&
pip
install
prettytable
....
## Successfully
tagged
jupyterhub-openbis-sis-juanextensions-recipe-20180406:latest
## The
new
image
is
now
available
and
can
be
started
as
described
above.",REPOSITORY,1,en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis_1,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis.txt,2025-09-30T12:09:08.051766Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:2,JupyterHub for openBIS,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html,openbis,"$
docker
images
## REPOSITORY
## TAG
## IMAGE
## ID
## CREATED
## SIZE
jupyterhub-openbis-sis-juanextensions-recipe-20180406
latest
a0106501b223
3
minutes
ago
4
## .75GB
openbis/jupyterhub-openbis-sis-20180405
latest
585a9adf333b
23
hours
ago
4
## .75GB
How to start a jupyterhub-openbis docker image on a productive JupyterHub server

## Warning
You can only have
## ONE
jupyterhub-openbis image running on a server at one given time, since JupyterHub makes use of certain ports on the machine that are also configured in openBIS.
1. Find the jupyterhub-openbis-start.sh file in your server (please ask
your admin).
Find the container id of the image that is currently running.
$
docker
ps
## CONTAINER
## ID
## IMAGE
## COMMAND
## CREATED
## STATUS
## PORTS
## NAMES
a2b76d1dd204
jupyterhub-openbis-sis-20180405
""./vagrant/initial...""
4
seconds
ago
## Up
2
seconds
0
.0.0.0:8000-8002->8000-8002/tcp,
0
.0.0.0:8081->8081/tcp
nervous_leakey
Stop the current container.
$
docker
kill
a2b76d1dd204
a2b76d1dd204
Edit the  jupyterhub-openbis-start.sh file in your server and update
the name of the image it runs to the one of your choice
docker
run
-v
/Users/juanf/Documents/programming/git/jupyter-openbis-integration/jupyterhub_reference_installation/home:/home
-v
/Users/juanf/Documents/programming/git/jupyter-openbis-integration/jupyterhub_reference_installation/vagrant/config/certificates:/vagrant/config/certificates
-e
## OPENBIS_URL
=
https://129.132.229.37:8443
-e
## JUPYTERHUB_INTEGRATION_SERVICE_PORT
=
8002
-e
## JUPYTERHUB_PORT
=
8000
-e
## CERTIFICATE_KEY
=
/vagrant/config/certificates/default.key
-e
## CERTIFICATE_CRT
=
/vagrant/config/certificates/default.crt
-p
8000
:8000
-p
8081
:8081
-p
8001
:8001
-p
8002
:8002
jupyterhub-openbis-sis-20180405
./vagrant/initialize/start_jupyterhub.sh
Start the new image.
$
./jupyterhub-openbis-start.sh
Other useful Docker commands

Save an image as a tar file to share it

## Warning
It is best practice to include both the name of the user and the creation date in the image name. This will help when dealing with many versions created by different users at different times.
$
docker
save
jupyterhub-openbis-sis-20180405
>
jupyterhub-openbis-sis-20180405.tar
$
ls
-lah
total
9681080
-rw-r--r--
1
juanf
1029
4
## .6G
## Apr
5
15
:38
jupyterhub-openbis-sis-20180405.tar
Load an image from a tar file

$
docker
load
<
jupyterhub-openbis-sis-20180405.tar
## 8feeda13d3ce:
## Loading
layer
[==================================================
>
]
27
.65kB/27.65kB
## 622cd2c170f3:
## Loading
layer
[==================================================
>
]
## 152MB/152MB
## 633fa40a6caa:
## Loading
layer
[==================================================
>
]
2
.048kB/2.048kB
## 7219a9159e4f:
## Loading
layer
[==================================================
>
]
223
## .9MB/223.9MB
## 678b55e862c7:
## Loading
layer
[==================================================
>
]
4
## .377GB/4.377GB
## Loaded
## image:
jupyterhub-openbis-sis-20180405:latest
$
docker
images
## REPOSITORY
## TAG
## IMAGE
## ID
## CREATED
## SIZE
jupyterhub-openbis-sis-20180405
latest
585a9adf333b
24
hours
ago
4
## .75GB
Remove an image

$
docker
rmi
jupyterhub-openbis-sis-juanextensions-recipe-20180406
Remove all stopped containers

$
docker
rm
$(
docker
ps
-aq
)
### openBIS ELN Integration Configuration

On the openBIS end, what needs to be done is to append the following
lines into your ELN instance profile:
servers/core-plugins/eln-lims/1/as/webapps/eln-lims/html/etc/InstanceProfile.js
# Ansible yml syntax, replace the variables in the double curly braces by the appropriate values:
this.jupyterIntegrationServerEndpoint = ""https://{{ openbis_jupyterhub_hostname }}:{{ openbis_jupyterhub_communication_port }}"";
this.jupyterEndpoint = ""https://{{ openbis_jupyterhub_hostname }}/"";",REPOSITORY,2,en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis_2,reference,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis.txt,2025-09-30T12:09:08.051766Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:3,JupyterHub for openBIS,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html,openbis,"# Example:
this.jupyterIntegrationServerEndpoint = ""https://jupyterhub-demo.labnotebook.ch:80"";
this.jupyterEndpoint = ""https://jupyterhub-demo.labnotebook.ch/"";

On the jupyterhub end, the docker command would then look as follows:

docker run -e OPENBIS_URL=https://{{ openbis_public_hostname }} -e JUPYTERHUB_INTEGRATION_SERVICE_PORT=8002 -e JUPYTERHUB_PORT=8000 -e CERTIFICATE_KEY=/vagrant/config/certificates/default.key -e CERTIFICATE_CRT=/vagrant/config/certificates/default.crt -p 8000:8000 -p 8081:8081 -p 8001:8001 -p {{ openbis_jupyterhub_communication_port }}:8002 585a9adf333b ./vagrant/initialize/start_jupyterhub.sh",Example:,3,en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis_3,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis.txt,2025-09-30T12:09:08.051766Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:4,JupyterHub for openBIS,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html,openbis,"# Example:
openbis_public_hostname: openbis-test.ethz.ch
openbis_jupyterhub_hostname: jupyterhub-test.ethz.ch
openbis_jupyterhub_communication_port: 80

The only port you need to open on your jupyterhub instance is the one
matching {{ openbis\_jupyterhub\_communication\_port }}. Using
firewall-cmd this would look as follows:",Example:,4,en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis_4,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis.txt,2025-09-30T12:09:08.051766Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:5,JupyterHub for openBIS,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html,openbis,"# Example:
openbis_public_hostname: openbis-test.ethz.ch
openbis_jupyterhub_hostname: jupyterhub-test.ethz.ch
openbis_jupyterhub_communication_port: 80



firewall-cmd --permanent --zone=public --add-rich-rule='rule family=""ipv4"" source address=""{{ openbis_jupyterhub_openbis_hostname }}"" port protocol=""tcp"" port=""{{ openbis_jupyterhub_communication_port }}"" accept'
Troubleshooting Connectivity to openBIS

Currently only connecting to the openBIS server used to validate your
log in is supported.
Session is no longer valid. Please log in again error

This error can be show due to two reasons:
The openBIS server has a self-signed certificates for whatever
reason, typically this is true for test servers.
The session has timeout.
For each one of them a different fix needs to be applied.
Session is no longer valid. The openBIS server has a self-signed certificate

To fix this issue please allow self signed certificates when connecting
as on the example shown below using the verify_certificates modifier.
Session is no longer valid. The session has timeout

The session token obtained during the login is stored by the Jupiter
server during its startup. This session token has most likely timeout
after a couple of days without use and needs to be refreshed. If you
just log in to JupyterHub there is a new session available that needs to
be handed over to the Jupyter server. For that just stop and start it
again.
Step 1 : Go to your control panel clicking on the button of the top
right corner.
## Step 2: Press Stop My Server
## Step 3 : Press Start My Server
## Step 4: Press Launch My Server
Step 5: Wait for the server to startup, after the startup finishes go
back to your notebook, it should connect now.",Example:,5,en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis_5,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis.txt,2025-09-30T12:09:08.051766Z,2
docs:openbis:en_20.10.0-11_user-documentation_advance-features_openbis-data-modelling:0,openBIS Data Modelling,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/openbis-data-modelling.html,openbis,"openBIS Data Modelling

## Overview

openBIS has the following data structure:
## Space
: entity with
Code and
## Description
## Project
: entity with
## Code
and
## Description
## Experiment/Collection:
entity with
user-defined properties
## Object
: entity with
user-defined properties
## Dataset
: folder where data files are stored. A dataset has
user-defined properties
## Space
is the top level. Below
## Spaces
there are
## Projects
and below
## Projects
there are
## Experiments/Collections
.
In the general openBIS data model,
## Objects
## can:
be shared across
## Spaces
(i.e. they do not belong to any Space)
belong to a
## Space
belong to a
## Project
belong to an
## Experiment/Collection
## Datasets
can be associated only to
## Experiments/Collections
or to
## Objects
.
Access to openBIS is controlled at the
## Space
level,
## Project
level or openBIS instance level (see
openBIS roles
).
Data model in openBIS ELN-LIMS

In the openBIS ELN-LIMS a simplified data model is used, as shown below.
.
In this case,
## Objects
can only belong to
## Experiments/Collections
.
## Inventory

The inventory is usually conceived to be shared by all lab members. The
inventory is used to store all materials and protocols (i.e. standard
operating procedures) used in the lab. It is possible to create
additional inventories, for example of instruments and equipment.
The picture below shows an example of an Inventory with the different openBIS levels.
## Lab Notebook

By default, the lab notebook is organised per user. Each user has a
personal folder (=
## Space
), where to create
## Projects
,
## Experiments
and Experimental Steps (=
## Objects
). Data files can be uploaded to
## Datasets
## . Example structure:
Some labs prefer to organise their lab notebook using an organization
per project rather than per user. In this case an openBIS
## Space
would
correspond to a lab Project and an openBIS
## Project
could be a
sub-project or a user folder (one folder per user working on the same project).
openBIS parents and children

## Objects
can be linked to other
## Objects
,
## Datasets
to other
## Datasets
with
N:N relationship. In openBIS these connections are known as
parents
and
children
.
Examples of parent-child relationships

One or more samples are derived from one main sample. This is the
parent of the other samples:
One Experimental step is performed following a protocol stored in the
inventory, on a sample stored in the inventory, using a given equipment. The protocol, the sample and the equipment are the parents of the Experimental step
One Experimental Step is done after another and we want to keep
track of the links between the steps:",Overview,0,en_20.10.0-11_user-documentation_advance-features_openbis-data-modelling_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_advance-features_openbis-data-modelling.txt,2025-09-30T12:09:08.377565Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_associate-file-types-to-dataset-types:0,Associate File Types to Dataset Types,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/associate-file-types-to-dataset-types.html,openbis,"Associate File Types to Dataset Types

It is possible to associate given file types to given
## Dataset
types
.
In this way when a file of this type is uploaded, the
## Dataset
type
is automatically selected. This option can be found in
## Settings
.
For example, a Jupyter notebook, which has extension
.ipynb
can
always be associated with the
## Dataset
type
## Analysis Notebook
.
Go to
## Settings
## Click
## Edit
Scroll down to the
Dataset types for filenames
section
Enter the file extension (e.g. ipynb) in the
Filename extension
field
Select the D
ataset type
with which you always want to associate
this file type (e.g. Analysis Notebook)
## Save
Updated on November 30, 2022",Dataset,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_associate-file-types-to-dataset-types_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_associate-file-types-to-dataset-types.txt,2025-09-30T12:09:08.442721Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_create-templates-for-objects:0,Create Templates for Objects,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/create-templates-for-objects.html,openbis,"Create Templates for Objects

It is possible to create templates for
## Objects
. Templates are useful
when one has to register repetitive
Experimental steps
or
measurements
where some fields should always be pre-defined.
For each
## Object
type several templates can be created. This can be
done by the lab manager, who should have admin rights for editing the
## Settings
. It is not necessary to be
Instance admin
for this. In a
multi-group set up, this can be done by the
group admin
.
## Procedure:
Go to the
## Settings
, under
## Utilities
Scroll down to the
## Templates
section
From the
## New Template
tab select select the
Object type
for
which you want to have a template.
Fill in the fields as desired.
## Save.
Your templates will be show in the table in the
## Templates
section,
as shown below
## See
Use template for Experimental Steps
for more info on how to use templates.
Updated on April 26, 2023",Objects,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_create-templates-for-objects_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_create-templates-for-objects.txt,2025-09-30T12:09:08.494469Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-materials-and-samples:0,Customise Inventory Of Materials And Samples,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-inventory-of-materials-and-samples.html,openbis,"Customise Inventory Of Materials And Samples

Create Collections of Materials

## Collections
are folders used to organise
## Objects
in the
## Materials
## Inventory. Such
## Objects
can be different types of
samples and materials (e.g. chemicals, antibodies, batteries,
environmental samples).
## Collections
need to be created inside another folder, called
## Project
, in the
## Materials
inventory.
For example, if we want to create a collection of raw samples, we need
to adopt the following steps:
Create an
## Object
type
called Sample. This can only be done by an
Instance admin
, from the admin interface, as explained here:
## New Entity Type Registration
Create a first folder called Samples inside the
## Materials
folder
## (Project)
Create a second folder called
## Raw Samples (Collection)
Create the Project folder

To create the
## Project
## folder:
Click on the
## Materials
folder
Click the
## + New Project
button in the form.
Provide a description, if wanted. This is not mandatory.
Enter the
## Code
. This will be the name of the folder, in this case SAMPLES. Codes only take alphanumeric characters and no spaces.
Create the Collection folder

To register the
## Collection
folder, inside the
## Project
## folder:
Click on the
## Project
folder, in this case
## Samples
.
Click the
## + New
button in the main form and choose
## Collection
from the dropdown.
Replace the automatically generated
## Code
with something pertinent to the collection (e.g RAW_SAMPLES)
Fill in the
## Name
field (e.g. Raw Samples). Note that by default, the navigation menu on the left shows the name. If the name is not provided, the code is shown.
Select the
Default object type
from the list of available types. This is the
## Object
for which the
## Collection
is used. In this case,
## Sample
.
Select the
Default collection view
(see
## Customise Collection View
)
Add the “+Object type” button in the Collection percentage

If you use a Collection for one Object type, you can display a button to add that type to the Collection, as shown below.
For this you need to edit the Collection form and set the Default Object type, as shown below.
## Delete Collections

To delete an existing Collection:
## Select
## Edit Collection
under the
## More..
dropdown menu
## Select
## Delete
under the
## More..
drop down menu
Enable Storage Widget on Sample Forms

When a new
Object type
is created by an
Instance admin
(see
## New Entity Type Registration)
, the storage widget is disabled by default.
If we want to track storage positions for this particular
Object type
as described in
Allocate storage positions to samples
, the
## Storage
should be enabled in the
## Settings
, under
## Utilities
. This can be done by a
group admin
.
For this, follow the steps below:
Go to
## Settings
, under
## Utilities
Click the
## Edit
button
Scroll to the last section of the Settings:
Object Type definitions Extension
Open the
Object type
for which you want to enable the storage, e.g.
## Sample
## Select
## Enable Storage
## Save
## Configure Lab Storage

Fridges and freezers can be configured in the
## Settings
, under
## Utilities
.
Go to
## Settings
## Click
## Edit
Scroll down to the
## Storages
section
Click the
## + New Storage
button above the storage table, as shown below.
Fill in the
## Storage Form
as explained below
How to fill in Storage Form:
## Code
. It is advisable to provide a meaningful code for the storage, rather than using the default, because this information is needed when registering storage positions in Batch mode. For example MINUS80_ROOM_A1
## Name
. The name is what is shown in most parts of the ELN. E.g. Minus 80°C in Room A1
Number of rows
. This is the number of shelves.
Number of columns
. This is the number of racks per shelf.
Allowed number of boxes in a rack
. This is the maximum number per rack. Enter a very high number if this is not important.
Rack space warning
. Enter space as percentage. E.g. 80, means that the system will give a warning when 80% of a rack is occupied.
Box space warning
. Enter space as percentage. E.g. 80, means that the system will give a warning when 80% of a box is occupied.
Validation level
. This is the minimum level of information required about the storage:
Rack validation
. The position in the shelf and rack needs to be specified.
Box validation
. In addition to
a
, a box name needs to be specified.
Box position validation
. In addition to
a
and
b
, the position in the box needs to be specified.
Add metadata to Storage Positions

Storage positions by default have the following metadata:
Storage code
Storage rack row
Storage rack column
Box name
Box size
Box position
User id
It is possible to add additional information. This can be done by an
## Instance Admin
by editing the
## Object
## Type
## STORAGE_POSITION
in
the admin interface (see
## New Entity Type Registration
).",Collections,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-materials-and-samples_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-materials-and-samples.txt,2025-09-30T12:09:08.564121Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-protocols:0,Customise Inventory Of Protocols,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-inventory-of-protocols.html,openbis,"## Customise Inventory Of Protocols

Create Collections of Protocols

## Collections
are folders used to organise
## Objects
in the
## Methods
Inventory. In this case,
## Objects
are protocols.
## Collections
need to be created inside another folder, called
## Project
, in the
## Methods
## Space
in the inventory.
For example, if we want to create a collection of lab protocols for
microscopy and Mass spec, we need to adopt the following steps:
Register a first
## Project
folder called PROTOCOLS in the
## Methods
## Space
.
In the
## Protocols
folder, you can register two additional
Collections called Microscopy Protocols and MS Protocols
The steps for the registration of the folders are the same as explained
in
Create Collections of
## Materials
Updated on April 26, 2023
Enable Protocols in Settings

If a new
Object type
for a protocol is created by an
Instance admin
in the admin interface, it is advisable to set the
Object type
to
## Protocol
in the
## Settings
, under
## Utilities
.
For this, follow the steps below:
Go to
## Settings
## Click
## Edit
Scroll to the last section of the
## Settings
## :
## Object Type
definitions Extension
Open the
Object type
corresponding to your protocol, e.g.
## General Protocol
## Select
Use as Protocol
## Save
This is done to be able to create local copies of protocols from the
## Inventory
inside an
## Experiment
when writing
Experimental steps,
as
described in
How to use protocols in Experimental
## Steps
Updated on April 26, 2023",Customise Inventory Of Protocols,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-protocols_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-protocols.txt,2025-09-30T12:09:08.628552Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-parents-and-children-sections-in-object-forms:0,Customise Parents and Children Sections in Object Forms,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-parents-and-children-sections-in-object-forms.html,openbis,"Customise Parents and Children Sections in Object Forms

## The
## Parents
and
## Children
sections are automatically created in
all
## Object
forms. It is possible to customise or remove these
sections, from the
## Settings
, under
## Utilities
.
Let’s consider an example. The default
## Experimental Step
, present in
all openBIS instances, looks like the picture below: in the
## Parents
section,
## General Protocol
is predefined. If we want to add a General
Protocol to the form, we use the
## Search
or
## Paste
options next to
General Protocol. If we want to add another parent, for example a
## Sample
, we need to use the
## Search Any
or
## Paste Any
next to
Parents. See also
Add parents and children to Experimental
## Steps.
Now let’s see how the
## Parents
and
## Children
sections of an
## Experimental Ste
p can be configured in the
## Object Types Definition
## Extension
in the
## Settings.
## Section Name
. Enter an alternative name for the
## Parents
or
## Children
section. If empty the default is used
## (Parents/Children).
Disable the section
for the
## Object
type. No parents/children
can be added to this
## Object
type.
Disable addition of any object type
. This removes the
+
button next to the section name, which enables to add as parent any
## Object
type. In this way only
## Objects
of types pre-defined in
the form can be added.
To define which
## Object
types should always be shown in the form of
a this
## Object
type, click the
+
button.
Select if this is a
## Parent
or
## Child
from the
drop down
.
Enter a
## Label
, which is what is shown in the
## Object
form.
Select the
## Object
type from the
drop down
.
Specify the
minimum
and
maximum
number of parents needed as
input for this
## Object
type. This can be left empty if parents are
not mandatory for this type. If a minimum is specified, this makes
the addition of those parents mandatory. As many parents as
specified in the minimum field will have to be added in order to be
able to save the form.
## Specify A
nnotations
(e.g. Comments) for this parent
## Object
type.
Click the + button on the section to add an annotation field.
Select the
## Annotation
field from the list of available fields.
Specify if the
## Annotation
is mandatory.
The figure below shows how the
## Annotation
of type
## Comments
looks
like in the
## Experimental Step
form.
Updated on November 30, 2022",The,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-parents-and-children-sections-in-object-forms_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-parents-and-children-sections-in-object-forms.txt,2025-09-30T12:09:08.691482Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-the-main-menu:0,Customise the Main Menu,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-the-main-menu.html,openbis,"Customise the Main Menu

## Main Menu Sections

The main menu can be customised from the
## Settings
, under
## Utilities
, to hide sections that are not needed by the lab.
Go to
## Settings.
## Click
## Edit.
Go to the
## Main Menu
section.
Disable the parts of the menu which are not needed.
## Save.
showLabNotebook
: if unselected, the
## Lab Notebook
section of
the main menu (
## Lab
## Notebook)
will be hidden.
showInventory
: if unselected, the
## Inventory
section of the
main menu (
Inventory of Materials and
## Methods
)
will be hidden.
showStock
: if unselected, the
## Stock
section of the main menu
(
Managing Lab Stocks and
## Orders
)
will be hidden.
showObjectBrowser
: if unselected, the
## Object Browser
under
## Utilities
in the main menu (
## Object
## Browser)
will be hidden.
showStorageManager
: if unselected, the
## Storage Manager
under
## Utilities
in the main menu
## (Storage
manager
)
will be hidden.
showAdvancedSearch
: if unselected, the
## Advanced Search
under
## Utilities
in the main menu (
## Advanced
## Search)
be hidden.
showUnarchivingHelper
: if unselected, the
## Unarchiving Helper
and
## Archiving Helper
under
## Utilities
in the main menu (
## Data
archiving)
will be hidden.
showTrashcan
: if unselected, the
## Traschcan
under
## Utilities
in the main menu
(
## Trashcan
)
will be hidden.
## showVocabularyViewer:
if unselected, the
## Vocabulary
## Browser
under
## Utilities
in the main menu (
## Vocabulary
browser
)
will be hidden.
showUserManager
: if unselected, the
## User Manager
under
## Utilities
in the main menu (
## User
## Registration
)
will be hidden.
showUserProfile
: if unselected, the
## User Profile
under
## Utilities
in the main menu (
## User
## Profile
)
will be hidden.
showZenodoExportBuilder
: if unselected, the
## Zenodo
## Export
under
## Utilities -> Exports
in the main menu
(
Export to
## Zenodo
)
will be hidden.
showBarcodes
: if unselected, the
Barcodes/QR codes Generator
under
## Utilities
in the main menu
(
## Barcodes)
will be hidden.
Lab Notebook menu

It is also possible to customise which entities should be shown under
## Experiments/Collections
in the main menu under the
## Lab Notebook
section.
By default, only the
## Object
types
## Entry
and
## Experimental Step
are shown (see picture below).
If you want to show additional custom
## Object
types in the lab notebook
main menu, they need to be enabled by editing the
## Settings
.
Go to the
Object Type definitions Extension
section in the
## Settings
. Open the relevant
## Object
type, which you would like to
see in the Main menu of the Lab Notebook and select
Show in lab
notebook main menu
, as shown below.
By default, the Object Types
## Entry
and
## Experimental Step
have
this option already selected.
Please note that this is only valid for the
## Lab Notebook
section. In
the
## Inventory
section, entries are never shown in the main menu,
because inventories can potentially have thousands of entries, which are
better visualised in tables, rather than in the main menu.
Updated on April 26, 2023",Main Menu Sections,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-the-main-menu_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-the-main-menu.txt,2025-09-30T12:09:08.755787Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_database-navigation-in-admin-ui:0,Database navigation in admin UI,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/database-navigation-in-admin-ui.html,openbis,"Database navigation in admin UI

openBIS version 20.10.5 provides a new database navigation in the admin
UI, as shown below.
This allows to navigate the openBIS hierarchy of
## Spaces, Projects,
## Experiments/Collections, Objects, Datasets
.
The same navigation menu will be used in the ELN UI in an upcoming
openBIS version.
## Features

## Filter

It is possible to filter the menu by code or name of the desired entity.
## Navigation

To navigate the menu, the nodes have to be opened individually.
When you select an entry in the node, the corresponding entry page
opens. Please note that at this stage, this is only intended as a
preview for the navigation menu and the forms show the information  of
the entity in json format.
If you open several entry pages and you switch between them, by default
the navigation menu will scroll up or down to the corresponding entry in
the menu. If you do not want to have this behaviour, you can turn off
the scrolling by selecting the button on the top right corner of the
menu, as show in the picture below.
## Sorting

Each level of the hierarchy except for
## Spaces
(
## Projects, Collections,
## Objects, Datasets
) can be sorted separately, using the buttons shown in
the pictures below.
By default, the sorting is done by
## Code
in alphabetical order. Other
options are: sorting by code in reverse alphabetical order; sorting by
ascending date; sorting by descending date.
Updated on December 5, 2022","Spaces, Projects,",0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_database-navigation-in-admin-ui_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_database-navigation-in-admin-ui.txt,2025-09-30T12:09:08.819026Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-archiving-to-long-term-storage:0,Enable archiving to Long Term Storage,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/enable-archiving-to-long-term-storage.html,openbis,"Enable archiving to Long Term Storage

openBIS supports archiving of datasets to Strongbox and StronLink
(
https://www.strongboxdata.com/
) as
described in
## Datasets
## Archiving
This needs to be set up and configured on
system level
.
Archiving can be manually triggered from the ELN interface. By default,
the archiving buttons are not shown, and they need to be enabled by an
Instance admin
or even a
group admin
in the case of a multi-group
instance. This is done in the ELN Settings, as shown below.
In addition, the Unarchiving helper tool should also be enabled in the
## ELN Settings:
More information on archiving and unarchiving datasets can be found
## here:
## Data
archiving
Updated on April 26, 2023",Datasets,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-archiving-to-long-term-storage_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-archiving-to-long-term-storage.txt,2025-09-30T12:09:08.880414Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-barcodes:0,Enable Barcodes and QR codes,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/enable-barcodes.html,openbis,"Enable Barcodes and QR codes

In order to be able to add custom barcodes and QR codes to
## Objects
, an
## Instance
## Admin
needs to add the $BARCODE property to the object type for which
barcodes/QR codes are needed.
The barcode functionality is disabled by default in the ELN UI. This can
be enabled by a
lab manager
or a
group admin
with admin right to
edit the
## Settings
, as shown below.
After enabling the option, please refresh your browser. The
Barcodes/QR Codes Generator
will be shown in the main menu under
## Utilities
and a
barcode icon will be added above the menu.
Information on how to use the Barcode functionality in openBIS can be
found
## here:
## Barcodes
Updated on April 26, 2023",Objects,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-barcodes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-barcodes.txt,2025-09-30T12:09:08.946482Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-transfer-to-data-repositories:0,Enable Transfer to Data Repositories,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/enable-transfer-to-data-repositories.html,openbis,"Enable Transfer to Data Repositories

Currently openBIS offers an integration with the
## Zenodo
data
repository (
https://zenodo.org/).
This enables data direct data transfer from openBIS to Zenodo.
This feature needs to be configured on
system level
as explained
## here:
openBIS DSS configuration
file
.
If this is done, the Zenodo Export needs to be made visible in the ELN
UI by a lab manager, who has should have admin rights for the
## Settings
. This can be done by a
group admin
, in case of a
multi-group instance set up.
## Procedure:
Edit the
## Settings
under
## Utilities.
## Select
showZenodoExportBuilder
in the
## Main Menu
section.
## Save.
## The
Export to Zenodo
functionality becomes available under the
## Utilities
menu (a refresh of the browser page may be necessary to
see the change):
Updated on April 26, 2023",Zenodo,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-transfer-to-data-repositories_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-transfer-to-data-repositories.txt,2025-09-30T12:09:09.009055Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_general-overview:0,Login,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/general-overview.html,openbis,"The admin interface of openBIS can can be accessed via a URL of this type: https://openbis-xxx/openbis/webapp/openbis-ng-ui/
where openbis-xxx is the name of the server specified in the openBIS configuration file, during the installation by a system admin.
## Login

File based and/or LDAP authentication

When file based and/or LDAP authentication are used in openBIS, the login interface is as shown below. Users need to provide their username and password to login.
Only registered users with assigned rights can login to openBIS.
SWITCHaai authentication

When SWITCHaai (SSO) authentication is used in addition to file based and/or LDAP authentication, the login interface is as shown below.
SWITCHaai is selected by default. In this case, users need to click on
## Login
and they will be redirected to the SWITCHaai login page.
If a user would like to authenticate with a file-based account or LDAP (depending on system configuration), they need to select
## Default Login Service
from the dropdown and provide username and password.",Login,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_general-overview_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_general-overview.txt,2025-09-30T12:09:09.070978Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_history-overview:0,History Overview,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/history-overview.html,openbis,"## History Overview

History of deletions

## When
## Experiments/Collections
,
## Objects
and
## Datasets
in openBIS are
permanently deleted, i.e. they are removed from the trashcan, the
information of these permanently deleted entries is stored in the
database and it is visible in the admin UI.
## Spaces
and
## Projects
are directly permanently deleted, without going
to the trashcan. Their information is also shown in the table of history
of deletions in the admin UI.
The table of history of deletions is under the
## Tools
section, as
shown below.
For each deleted entry, the table shows:
Entity type
: this can be
## Space
,
## Project
,
## Collection,
## Object, Dataset;
Entity identifier
: this is the PermID of the entity.
## Spaces
do
not have PermID, so the code of the
## Space
is shown instead;
## Entity Space
: the
## Space
to which the entity belonged;
## Entity Project
: the
## Project
to which the entity belonged;
## Entity Registrator
: the user who registered the entity;
## Entity Registration Date
: the date of registration of the
entity;
## Reason
: the reason of deletion of the entity;
## Description
: the PermID (
## Collection
,
## Object
), identifier
(
## Space
,
## Project
),  dataset path (
## Dataset
) of the entity;
## Content
: the metadata of the entity when it was deleted. This is
available for
## Projects
,
## Collections
,
## Objects
,
## Datasets
, but
not for
## Spaces
;
## User
: the user who deleted the entity;
## Date
: the date and time of deletion of the entity.
Updated on October 9, 2022
History of freezing

In the admin UI it is possible to have an overview of all frozen entries
in openBIS. Frozen entries can no longer be modified (see
## Freeze
## Entities
).
The table showing the history of freezing can be found under the
## Tools
section in the admin UI, as shown below.
Updated on March 4, 2022",History Overview,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_history-overview_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_history-overview.txt,2025-09-30T12:09:09.132055Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_import-openbis-exports:0,Imports of openBIS exports,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/import-openbis-exports.html,openbis,"Imports of openBIS exports

It is possible to export metadata and data from one openBIS instance and import them to another openBIS instance.
The export process is described
here
.
Metadata import

The exported metadata (and related masterdata) can be imported in another openBIS instance by an instance admin via the admin UI, as described in
mastedata import and export
.
Exported metadata (and masterdata) are contained in a
xlsx
folder, as shown below.
Metadata and masterdata are contained in the
metadata.xlsx
file. In addition to the metadata.xlsx file, the
xlsx
## folder might contain additional folders:
a
scripts
folder: contains scripts associated with types in the metadata.xlsx file, if these are present;
a
data
folder: holds the content of spreadsheet fields and large text fields that exceed the size of an Excel cell (32767 characters) and would not fit in the masterdata.xlsx file;
a
miscellaneous
folder: contain images embedded in text of exported entries, if present.
If a
data
,
scripts
and/or
miscellaneous
folders are present in the exported
xlsx
folder, the
xlsx
folder needs to be zipped and the
xlsx.zip
file can be imported via admin UI.
If only the
metadata.xlsx
file is contained in the
xlsx
folder, the metadata.xlsx file can be directly uploaded via admin UI.
Datasets import

Exported datasets are contained in a
data
folder in a format ready to be imported via
eln-lims default dropbox
.
The folders contained in the
data
folder need to be placed in the
eln-lims incoming directory
and from here will be uploaded to the corresponsing openBIS entities. The metadata of the datasets is read from the
metadata.json
file contained inside each dataset folder.
When importing both metadata and data in a different openBIS instance, first the metadata need to be imported and afterwards the data.",folder might contain additional folders:,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_import-openbis-exports_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_import-openbis-exports.txt,2025-09-30T12:09:09.196268Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_index:0,Admins Documentation,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/index.html,openbis,"## Admins Documentation

## Login
File based and/or LDAP authentication
SWITCHaai authentication
Inventory overview
Customise Inventory Of Materials And Samples
Create Collections of Materials
Create the Project folder
Create the Collection folder
Add the “+Object type” button in the Collection percentage
## Delete Collections
Enable Storage Widget on Sample Forms
## Configure Lab Storage
Add metadata to Storage Positions
## Customise Inventory Of Protocols
Create Collections of Protocols
Enable Protocols in Settings
Move Collections to a different Project
Customise Parents and Children Sections in Object Forms
Customise the Main Menu
## Main Menu Sections
Lab Notebook menu
Associate File Types to Dataset Types
## User Registration
Register users in ELN Interface
Default roles assigned in ELN
Register users from the admin UI
Deactivate users
Remove users
Create users groups in admin UI
openBIS roles
## Observer
## Space/Project User
## Space/Project Power User
## Space/Project Admin
## Instance Admin
## User Profile
Assign home space to a user
## New Entity Type Registration
Register a new Object Type
Registration of Properties
Property Data Types
Considerations on properties registration
## Controlled Vocabularies
Register a new Experiment/Collection type
Register a new Dataset type
Enable Rich Text Editor or Spreadsheet Widgets
Enable Objects in dropdowns
Register masterdata via Excel
Modifying existing types
Properties overview
Internal properties and vocabularies
Masterdata exports and imports
Masterdata export
Masterdata import
Masterdata version
Imports of openBIS exports
Metadata import
Datasets import
Create Templates for Objects
Enable Transfer to Data Repositories
Enable Barcodes and QR codes
Enable archiving to Long Term Storage
## History Overview
History of deletions
History of freezing
## Space Management
Create new Inventory Spaces
Create a new Inventory Space from the ELN UI
Multi-group instances
Create a new Inventory Space from the core UI
## Set Inventory Spaces
Create new ELN Spaces
Create a new Lab Notebook Space from the ELN UI
Multi-group instances
Create a new Lab Notebook Space from the core UI
## Delete Spaces
Move Spaces between Lab Notebook and Inventory
## Multi Group Set Up
General ELN Settings
## Instance Settings
## Group Settings
Group ELN Settings
Database navigation in admin UI
## Features
## Filter
## Navigation
## Sorting",Admins Documentation,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_index.txt,2025-09-30T12:09:09.258182Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_inventory-overview:0,Inventory overview,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/inventory-overview.html,openbis,"Inventory overview

The default Inventory contains two
## folders:
## Materials
and
## Methods
.
These are used to organise respectively samples and materials of any
type and lab protocols.
Samples, materials and protocols are modelled in openBIS as
## Objects
.
In the openBIS ELN-LIMS for life sciences, the following Object types
are preconfigured in the database:
Antibodies, Chemicals, Enzymes, Media, Solutions and Buffers, Plasmids, Plants, Oligos,
RNAs, Bacteria, Cell lines, Flies, Yeasts, General protocols, PCR protocols, Western blotting protocols.
## These
## Objects
are organised in
## Collections
in the
## Materials
and
## Methods
sections of the Inventory .
The generic openBIS ELN-LIMS only has one predefined
## Object
type for
the Inventory:
## General Protocol
## Additional
## Object
types and the corresponding
## Collections
must be
created by the
Instance admin
, based on the needs of the lab.
It is possible to add additional folders in the Inventory, for example
for
## Equipment
(see
Create new Inventory
## Spaces
)
.
Updated on April 26, 2023",folders:,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_inventory-overview_0,concept,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_inventory-overview.txt,2025-09-30T12:09:09.320864Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_masterdata-exports-and-imports:0,Masterdata exports and imports,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/masterdata-exports-and-imports.html,openbis,"Masterdata exports and imports

Since openBIS version 20.10.5 it is possible to export masterdata from
one openBIS instance and import it in another one via the admin UI.
Masterdata export

All types tables (
## Object Types, Collection Types, Dataset Types,
## Vocabulary Types, Property Types
) can be exported as shown below for
the
## Object Types
.
When you export you have the following options:
## Import Compatible
## :
## Yes
: in this case some columns which are incompatible
with imports (i.e. modification date) are not exported even
if selected; some columns that are required by openBIS for
imports are added to the exported file even if not selected.
## No
: in this case all columns or selected columns are
exported.
## Columns
## :
All (default order)
. All columns are exported, in accordance
with the selection explained above for import compatibility.
Selected (shown order)
. Selected columns are exported, in
accordance with the selection explained above for import
compatibility.
## Rows
: current page or all pages
Include dependencies
: yes or no. If you include dependencies,
all property types, vocabularies and associated objects are also
exported. Default is “yes”.
If the types have validation plugins or dynamic script plugins
associated with them, a zip file containing the scripts is exported from
openBIS.
Masterdata import

To import the file with the relevant masterdata that was exported as
## explained above:
Go to the
## Tools
section and select
## Import -> All
from the
menu.
Upload the file you exported before using the
## CHOOSE FILE
button.
Select one of the 3 possible options for the Update mode:
Fail if exists
: if some entries already exist in openBIS,
the upload will fail;
Ignore if exists
: if some entries already exist in openBIS,
they will be left untouched, even if their definition in the
file is different from the existing definition in openBIS;
Update if exists
: if some entries already exist in openBIS
and their definition in the file is different from the existing
definition in openBIS, they will be updated;
Since openBIS 20.10.6, the import of zip files is supported.
Masterdata version

In openBIS version 20.10.8, the masterdata version has been removed from the exported masterdata files.","Object Types, Collection Types, Dataset Types,",0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_masterdata-exports-and-imports_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_masterdata-exports-and-imports.txt,2025-09-30T12:09:09.411984Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_move-collections-to-a-different-project:0,Move Collections to a different Project,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/move-collections-to-a-different-project.html,openbis,"Move Collections to a different Project

It is possible to move one
## Collection
with it entire content
(
## Objects
+
## Datasets
) from one
## Project
to another.
## If
## Objects
contain parent/child relationships these are preserved.
This operation requires
## Space Power User
or
## Admin
rights.
To move
## Collections
in the Inventory:
Go to the
## Collection
page you want to move
Click on
## Edit Collection
## 3. Select
## Move
from the
## More..
dropdown
4. Enter the code of the
## Project
where you want to move your
## Collection
. If you start typing the code, openBIS will prompt you with
a list of available options and you can select the appropriate one from
there.
## 5. Click
## Accept
Updated on April 26, 2023",Collection,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_move-collections-to-a-different-project_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_move-collections-to-a-different-project.txt,2025-09-30T12:09:09.474644Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_multi-group-set-up:0,Multi Group Set Up,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/multi-group-set-up.html,openbis,"## Multi Group Set Up

openBIS can be configured to be used by multiple groups, where every group only sees their own group Spaces.
This configuration needs to be done on
system level
, as described
here
.
In the example below we see two groups:
## RDM
and
## ETHRDH
. For each group, in the Inventory, there area n
## Equipment
, a
## Materials
, a
## Methods
and a
## Publications
Spaces with the group prefix. In the lab notebook, each group member has a personal
## Space
where the name is the group prefix and the username of the user.
It is possible to configure the user management configuration file (
sys admin
) not to create user Spaces for a given group, in case one group prefers to organise their notebook by project, rather than by group members, as described
here
.
In a multi group instance users are automatically registered and the roles defined in the user management configuration file on the server are automatically assigned to them. There is a maintenance task that runs in the background at configured frequency. This can be once per day or several times per day. If there are new users, they will be added to openBIS when the maintenance task runs.
## An
instance admin
can assign additional roles to users from the admin interface (
## User Registration
). Default roles defined in the user management configuration file and automatically assigned cannot be removed, because they will be assigned again automatically by openBIS when the maintenance task runs.
We would recommend to assign
## SPACE_USER
rights for the Inventory Spaces to every group user and
## SPACE_ADMIN
rights for their own lab notebook. This can be specified in the user management configuration file on
system level
.
In the user management configuration file, one or more admins for each group can be designated. The
group admin
has by default
## SPACE_ADMIN
rights to all the
## Spaces
of their group. A
group admin
can customise the
Group ELN Settings
for the group.
General ELN Settings

In a multi-group instance an
Instance admin
can customise the General
ELN Settings.
The Settings can be access from the main menu, under
## Utilities
.
The General ELN Settings are Settings that are not specific to any of
the defined groups group , as shown below.
The General ELN Settings consist of two parts:
## Instance Settings
. These settings affect the whole instance, it
is not possible to customise them on a group level.
## Group Settings
. These settings affect all general
## Spaces
that
do not belong to any group defined in the configuration file
(see
openBIS set up for multi group
instances
).
This is the case, for example, if
## Spaces
are manually created and
they do not belong to any group (see
Create new ELN Spaces
).
Spaces that do not belong to any group do not have a group prefix. In
the example below
## Publications
do not belong to any group in the
## Inventory.
and
## Horizon
,
## Snf
do not belong to any group in the Lab notebook.
## Instance Settings

Custom widget
s. This section allows to enable the Rich Text
Editor or Spreadsheet component for a given field, as described
in
Enable Rich Text Editor or Spreadsheet
## Widgets;
## Forced Monospace Font
. This section allows to force the use of
monospace font (i.e. fixed width) for selected MULTILINE_VARCHAR
properties. This is useful for example for plasmid sequences.
Dataset types for filenames
. This section allows to associate
files with a given extension to a specific dataset type, as
described in
Associate File Types to Dataset
## Types
.
## Group Settings

## Storages
. In this section the storages for samples to be used in
## Spaces
not belonging to any predefined group (see above), can be
created, as described in
## Configure Lab
## Storage;
## Templates
. In this section, the templates for a given
## Object
type
to be used in
## Spaces
not belonging to any predefined group
(see above) can be created, as described in
Create Templates for
## Objects
;
Object types definition extension
. In this section, it is
## possible to:
Define if one
Object type
is a protocol. If an
Object type
is defined as a protocol, it is possible to create a local copy
of it under an Experiment, when linking to it as a parent, as
described in
Enable Protocols in
## Settings;
Enable the storage widget for an
Object type,
as described
in
Enable Storage Widget on Sample
## Forms
Define if the
Object type
should be shown in drop downs, as
described in
Enable Objects in
dropdowns
;
Define if the
Object type
should be shown in the main menu
under the Lab notebook section. By default objects are not shown
in the main menu in the Inventory section.
Customise the
## Parents
and
## Children
sections for an
## Object
type
as described in
Customise Parents and Children Sections
in Object
## Forms
;
## Inventory Spaces
. It is possible to move
## Spaces
from the
Inventory section to the Lab notebook section and vice-versa as
described in
Move Spaces between Lab Notebook and
## Inventory
Main menu
. The main menu for the
## Spaces
that do not belong to
any predefined group (see above) can be customised here, as
described in
Customise the Main
## Menu;
## Miscellaneous
. In this section it is possible to:
Show the dataset archiving buttons in
## Spaces
that do not
belong to any predefined group. Please note that this is not
available by default, but the infrastructure for
archiving to
tapes
(StrongBox/StrongLink) needs to be put in place (
Multi data set archiving
)*.
Hide sections by default in
## Spaces
that not belong to any
predefined group. By default some sections in some forms are
## hidden:
Description in
## Spaces
and
## Projects
.
Identification info in
## Spaces
,
## Projects
,
## Experiments
,
## Objects
,
## Datasets
.
By unchecking this option, these sections will be shown by default.
Updated on April 26, 2023
Group ELN Settings

In a multi group instance a
group admin
or
Instance admin
can
customise the ELN Settings for the group.
The group Settings can be selected from the
## Settings
in the main
menu.
The Settings for the relevant group can be selected from the available
dropdown, as shown below.
In the group settings the following is configurable:
## Storages
. In this section the group storages for samples can be
created, as described in
## Configure Lab
## Storage;
## Templates
. In this section, the templates for a given
## Object
type
can be created, as described in
Create Templates for
## Objects
;
Object types definition extension
. In this section, it is
## possible to:
Define if one
Object type
is a protocol. If an
Object type
is defined as a protocol, it is possible to create a local copy
of it under an Experiment, when linking to it as a parent, as
described in
Enable Protocols in
## Settings;
Enable the storage widget for an
Object type,
as described
in
Enable Storage Widget on Sample
## Forms
Define if the
Object type
should be shown in drop downs, as
described in
Enable Objects in
dropdowns
;
Define if the
Object type
should be shown in the main menu
under the Lab notebook section. By default objects are not shown
in the main menu in the Inventory section.
Customise the Parents and Children sections for an
Object type
as described in
Customise Parents and Children Sections in
## Object
## Forms
;
## Inventory Spaces
. It is possible to move Spaces from the
Inventory section to the Lab notebook section and vice-versa as
described in
Move Spaces between Lab Notebook and
## Inventory
Main menu
. The main menu for the group can be customised here,
as described in
Customise the Main
## Menu;
## Miscellaneous
. In this section it is possible to:
Show the dataset archiving buttons for the group. Please note
that this is not available by default, but the infrastructure
for
archiving to
tapes
(StrongBox/StrongLink) needs to be put in place (
Multi data set
archiving
)*.
Hide sections by default. By default some sections in some forms
## are hidden:
Description in
## Spaces
and
## Projects
.
Identification info in
## Spaces
,
## Projects
,
## Experiments
,
## Objects
,
## Datasets
.
By unchecking this option, these sections will be shown by default.
Updated on April 26, 2023",Multi Group Set Up,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_multi-group-set-up_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_multi-group-set-up.txt,2025-09-30T12:09:09.538934Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_new-entity-type-registration:0,New Entity Type Registration,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/new-entity-type-registration.html,openbis,"## New Entity Type Registration

Entity types, i.e.
## Experiment/Collection
,
## Object
and
## Dataset
types, as well as
Property types
and
## Controlled Vocabularies
constitute the openBIS
masterdata
. They can be created by someone
with
Instance admin
role in the
new Admin openBIS UI
.
## Note:
## Material
types are also part of the openBIS masterdata. However,
they are no longer supported and will be decommissioned soon. They
should NOT be used.
The new Admin openBIS UI can be accessed via a URL of this type:
https://openbis-xxx/openbis/webapp/openbis-ng-ui/
where openbis-xxx is the name of the server specified in the openBIS
configuration file, during the installation by a system admin.
Register a new Object Type

## Select
## Types -> Object Types
from the menu.
## Click
## Add
at the bottom of the page.
Enter a
## Code
. E.g.
## INSTRUMENT
. This is the name of the
Object to create and is unique. Please note that Codes should be in
capital letters, and they can only contain A-Z, a-z, 0-9 and _, -, .
Provide a description (not mandatory).
Entity validation plugin is used when we want to have validation on
some data entries. This is done via a custom script (see
## Entity Validation Scripts
)
Enter the
## Generated Code Prefix
. As a convention, we recommended
to use the first 3 letters of the
## Object
type
code (e.g.
## INS
, in
this case). This field is used by openBIS to automatically generate
Object codes: the codes will be for example INS1, INS2, INS3, etc.
## Leave
## Generate Codes
selected if you want to have codes
automatically generated by openBIS.
## Unique Subcodes
is used for contained objects, which are not
used in the ELN. Ignore this option.
## Click
## Add Section
at the bottom of the page. Sections are ways
of grouping together similar properties. Examples of sections used in
the ELN are
General info
,
Storage info
,
## Experimental Details
, etc.
Add properties inside the Section, by clicking the
## Add Property
button at the bottom of the page. To remove a property, use the
## Remove
button at the bottom of the page.
## Click
## Save
at the bottom of the page.
Please note that new
Object types
created in the admin UI, do not
automatically appear in ELN drop downs, but they have to be manually
## enabled, as described here:
Enable Objects in dropdowns
Registration of Properties

When registering new properties, the fields below need to be filled in.
## Code.
Unique identifier of the property. Codes can only contain
A-Z, a-z, 0-9 and _, -, .
Data Type.
See below for data types definitions.
## Label.
This is the property/column header that the user can see
in the ELN.
## Description
: The description is shown inside a field, to give
hints about the field itself. In most cases, label and description
can be the same.
## Dynamic Property Plugin
: Script for calculated properties.
## See
Dynamic properties
## Editable
: Editable in the ELN interface. In some cases, metadata
is automatically imported by scripts and this should not be changed
by users in the interface.
## Mandatory
: Field can be set as mandatory.
Property Data Types

## BOOLEAN
: yes or no
## CONTROLLEDVOCABULARY
: list of predefined values
## DATE
. Date field
## HYPERLINK
## : URL
## INTEGER
: integer number
## MATERIAL
. Not used in ELN. It will be dismissed.
## MULTILINE_VARCHAR
: long text. It is possible to enable a Rich
Text Editor for this type of property. This is described
## here:
Enable Rich Text Editor or Spreadsheet Widgets
## REAL
: decimal number
## OBJECT
. 1-1 connection to a specific object type.
## TIMESTAMP
: date with timestamp
## VARCHAR
: one-line text
## XML
: to be used by
## Managed Properties
(see
openBIS Managed Properties
and for
Spreadsheet component
## s, as described here:
Enable Rich Text Editor or Spreadsheet Widgets
Considerations on properties registration

If you create a property with code “PROJECT”, you should not use the
label “Project”. This will give an error if you use XLS Batch
registration/update, because openBIS considers this to be an openBIS
## Project
.
You should not assign more than 1 property with same label to the
same
## Object
type. If two or more properties with the same label
are present in the same
## Object
type, this will result in an error
in the XLS Batch registration/update.
## Controlled Vocabularies

Controlled vocabularies are pre-defined lists of values for a given
field.
Existing Vocabularies can be visualised from the Types ->
Vocabularies Tab. Vocabularies staring with the “
$
” symbol are
internal: they cannot be deleted and their terms cannot be deleted.
However, it is possible to add new terms to these vocabularies and these
can also be deleted.
New Vocabularies can be added, by clicking the
## Add
button at the
bottom of the page.
When registering a new vocabulary, a Code for the vocabulary needs to be
entered. This corresponds to the name of the vocabulary, and it is a
unique identifier. Codes can only contain A-Z, a-z, 0-9 and _, -, .
To add terms to the list click
## Add Term
at the bottom of the page.
Vocabulary terms always have a code and a label: the code is unique and
contain only alpha-numeric characters; labels are not necessarily unique
and allow also special characters. If the label is not defined, codes
are shown.
After creating the vocabulary and registering the terms, remember to
## Save
.
Register a new Experiment/Collection type

The registration of a new
## Collection
type is very similar to the
registration of
## Object
types. For Collection Types, you only need to
provide a Code (which is a unique identifier), Description and add a
validation plugin if you want to have metadata validation (see
## Entity
## Validation Scripts
).
Register a new Dataset type

The registration of a new Dataset types is similar to the registration
of object types.
It is possible to disallow deletion for a given dataset type.
Enable Rich Text Editor or Spreadsheet Widgets

For certain fields, it is possible to enable the use of a Rich Text
Editor (RTE) or a spreadsheet component.
Instance admin
rights are
necessary for this.
## The
## RTE
can be enabled for properties of type
## MULTILINE_VARCHAR
## . The
spreadsheet component
can be enabled for
properties of type
## XML
.
## Procedure:
Properties are defined when creating new entity types (
## Datasets
,
## Objects
,
## Experiments/Collections
)
To set a property as RTE or spreadsheet go to the
## Settings
,
under
## Utilities
Select /ELN_SETTINGS/GENERAL_ELN_SETTINGS
Enable editing and scroll down to the
## Custom Widgets
section
Click the + button on the same line as
## Property Type
and
## Widget
, as shown below
A new field will appear where you can select the property type and
the widget. Choices are:
## Word Processor
(=RTE) or
## Spreadsheet.
Updated on October 19, 2022
Enable Objects in dropdowns

By default, no Object shows in dropdown menus. Which object types should
show in dropdown menus can be customised from the Settings.
Navigate to the Object Type definitions Extension
Open one Object Type (e.g. Antibody)
Select show in drop downs
Save the Settings
Updated on October 19, 2022
Register masterdata via Excel

It is possible to register openBIS masterdata using an Excel template
from the admin UI.
This can be done from the Import menu under the Tools sections, as shown
below. Three options can be chosen for the import:
fail if exists
: if a type or a property already exists in the
database, the upload will fail.
ignore if exists
: if a type or a property already exists in the
database, the upload will ignore this.
update is exists
: if a type or a property already exists in the
database, the upload will update existing values.
An example template of an Excel masterdata file can be found here:
masterdata-template
Please note that in the template we used separate spreadsheets for each
type (Sample, Experiment, Dataset), but it is also possible to have
everything in one single spreadsheet.
Modifying existing types

If you wish to add a new property to an existing
## Collection/Object/Dataset
## type, you need to:
1. add the property in the file
2. increase the version number of the
## Collection/Object/Dataset
type
3. use
Ignore if exists
as upload method. In this case, only the
new property is added to the type.
The import on the admin UI allows to register entities in addition to
masterdata. An example template file for this can be found here:
masterdata-metadata
More extensive documentation on the XLS format for masterdata and
metadata registration can be found
here
.
Updated on January 13, 2023
Properties overview

The overview of all properties registered in openBIS and their
assignments to types is available under the
## Types
section in the
admin UI, as shown below.
Updated on March 1, 2022
Internal properties and vocabularies

Some properties and vocabularies are internally defined in openBIS.
These can be identified by the
$
sign.
Internal properties (e.g. $NAME, $BARCODE, etc) cannot be deleted nor
modified, not even by an instance admin.
Internal vocabularies (e.g. $DEFAULT_COLLECTION_VIEWS, etc), cannot be
deleted and their existing terms cannot be deleted nor modified, however
an instance admin can add new terms to an internal vocabulary.
Updated on January 5, 2023",New Entity Type Registration,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_new-entity-type-registration_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_new-entity-type-registration.txt,2025-09-30T12:09:09.601721Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_space-management:0,Space Management,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/space-management.html,openbis,"## Space Management

Create new Inventory Spaces

The default Inventory contains two
## folders:
## Materials
and
## Methods
. These are openBIS
## Spaces
.
## Additional
## Spaces
can be created by an
Instance admin
.
Create a new Inventory Space from the ELN UI

From openBIS version 20.10.4 it is possible to create
## Spaces
directly
from the ELN interface.
To create a new
## Space
under the Inventory:
## Select
## Inventory
in the main menu
Click on
## +New Inventory Space
in the Inventory page
3. Enter the
## Code
for the
## Space
, e.g. EQUIPMENT. Please note that
codes only accept alphanumeric characters, –, . , _.
4.
## Save
Multi-group instances

In a multi-group instance, the
Instance admin
can choose where to
create a new
## Space
## :
no group
. The new
## Space
will have no prefix and the Settings
defined in General Settings will apply (see
General ELN
## Settings
).
in one of the existing groups
. The new
## Space
will have the
group prefix and the Settings of that group will apply (see
## Group
## ELN
## Settings
).
Create a new Inventory Space from the core UI

In the core UI:
## Select
## Admin -> Spaces
## Click
## Add Space
at the bottom of the page
Enter the
## Space
## Code
, e.g.
## EQUIPMENT
## Save
## Set Inventory Spaces

When new
## Spaces
are created in the core UI, they are automatically
displayed under the
## Lab Notebook
part of the ELN main menu.
It is possible to move a new
## Space
to the Inventory, by editing the
## Settings
under
## Utilities
in the
## ELN UI:
Go to the
## Settings
and click
## Edit.
Go to the
## Inventory Spaces
section in the
## Settings
and click
the
+
button as shown below.
Select the
## Space
you want to move to the
## Inventory
from the list
of available
## Spaces
.
## Save
the Settings.
Refresh the browser.
Updated on April 26, 2023
Create new ELN Spaces

Create a new Lab Notebook Space from the ELN UI

From openBIS version 20.10.4 it is possible to create
## Spaces
directly
from the ELN interface. To create a new Space under the Inventory:
## Select
## Lab Notebook
in the main menu
Click on
## +New
## Space
in the Lab Notebook page
3. Enter the
## Code
for the
## Space
. Please note that codes only
accept alphanumeric characters, –, . , _.
4.
## Save
Multi-group instances

In a multi-group instance, the
Instance admin
can choose where to
create a new
## Space
## :
no group
. The new
## Space
will have no prefix and the Settings
defined in General Settings will apply (see
General ELN
## Settings
).
in one of the existing groups
. The new
## Space
will have the
group prefix and the Settings of that group will apply (see
## Group
## ELN
## Settings
).
Use cases where this could be useful:
in a multi-group instance with user folders in the Lab Notebook it
is desired to have in addition some
## Spaces
that are not linked to
a particular user, but maybe rather to some projects.
in a multi-group instance it is not at all desired to have the lab
notebooks organised by users, but rather by projects. A
system
admin
can configure the user management config file not to create
users folders in the lab notebook section (see
Multi group instances
).
The rights for
## Spaces
not belonging to any group need to be manually
assigned by an
Instance admin
.
Create a new Lab Notebook Space from the core UI

In the core UI:
## Select
## Admin -> Spaces
## Click
## Add Space
at the bottom of the page
Enter the Space
## Code
, e.g.
## EQUIPMENT
## Save
By default all
## Spaces
created in the core UI are shown under the Lab
Notebook part of the ELN UI.
Updated on April 26, 2023
## Delete Spaces

## Spaces
can be deleted by
Instance admins
or by
Space admins
.
To delete a
## Space
## :
Click on the
## Space
in the main menu
## Select
## Delete
from the
## More..
dropdown
3. Provide a
reason
for deletion
4.
## Accept
## Notes:
## Spaces
are not moved to the trashcan, but they are permanently deleted
straight away.
## Spaces
can only be deleted when they are empty and no entries
previously belonging to the
## Space
are in the trashcan.
Updated on April 26, 2023
Move Spaces between Lab Notebook and Inventory

If a
## Space
belongs to the Inventory, this information is stored in the
ELN Settings,
under the section
## Inventory Spaces.
To move a
## Space
from the Lab Notebook to the Inventory, click on the
+
button on top of the
## Inventory Spaces
section, select the
## Space
you want to move and
## Save
the Settings.
To move a
## Space
from the Inventory to the Lab Notebook, click on the
–
button next to the
## Space
you want to remove in the
## Inventory
## Spaces
section and
## Save
the Settings.
ELN Settings can be edited by Instance admins, group admins in
multi-group instances and by anyone with admin rights to the
## ELN_SETTINGS
## Space
.
Updated on August 2, 2022",Space Management,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_space-management_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_space-management.txt,2025-09-30T12:09:09.679870Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_user-registration:0,User Registration,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/user-registration.html,openbis,"## User Registration

Register users in ELN Interface

Users can only be registered by someone with
Instance admin
## role:
Go to the
## User Manager
, under
## Utilities
.
Click the  +
## New
## User
button.
Select the
## Authentication Service
## :
a.
## Default Authentication Service
. This can be LDAP or SSO.
b.
## File Authentication Service
. In this case a username and password need to be created.
User ID
. for LDAP authentication, this is the LDAP username; for SSO authentication this is the email address of the user.
For file-based authentication provide username and password. The password can later be changed by the user.
Default roles assigned in ELN

When a user is registered via the ELN interface, a
## Space
(folder) with
the name of the user is automatically created under the Lab Notebook
main menu. The user is also assigned some default roles:
Space admin
of the
## Space
created for him/her under the notebook.
Space user of
the Inventory
## Spaces
(MATERIALS, METHODS by
default), the STOCK_CATALOG and the STORAGE
## Spaces
.
## Space Observer
of the STOCK_ORDERS, ELN_SETTINGS and
## PUBLICATIONS S
paces
.
Modification to default rights can be granted by an
Instance admin
from the
admin UI
, as explained below.
## Overview of roles:
openBIS Roles
Register users from the admin UI

When users are registered via the admin UI no default roles are
assigned.
To register new users from the admin UI:
go to the
## Users
tab. The
## Users
and
## Groups
will show in
the main menu on left had side.
Click on
## Users
in the menu: the
## Add
button at the bottom of
the menu will become active (blue)
Click the
## Add
button
Enter the U
ser Id
. This is the LDAP username, when LDAP
authentication is used, or the email address if SSO is used. Please
note that file-based authentication (where username and password can
be created) is not supported by the admin UI.
Home space
: this sets the default folder a user sees marked as
## My Space
in the Lab Notebook.
Click the
## Add Role
button at the bottom of the page to assign a
role to the user.
Click the
## Add Group
button at the bottom of the page to assign a
user to a group of users.
To assign a role to a user, first the
## Level
needs to be selected
## (Instance, Space, Project) .
If level is Instance, you can directly select a role (Admin, Observer). If the level is Space or Project, you first need to select the Space or Project and then assign a
## Role
.
Multiple roles can be assigned to a user.
Roles can be removed from the
## Remove
button at the bottom of
the page.
After making the necessary changes, press the
## Save
button.
Note: for using the ELN interface, it is necessary to assign every user
or user group the OBSERVER role to the space ELN_SETTINGS.
Deactivate users

Users can be deactivated in the admin UI:
Select the user to deactivate in the left menu of the
## USERS
tab
Click the
## EDIT
button on the right bottom corner
Unselect the
## Active
checkbox
Remove users

Users can be removed from openBIS only if they have not registered anything in the system. If they have, they can only be deactivated, not removed.
Users can be removed in the admin UI, by selecting the user in the left menu of the
## USERS
tab and clicking the
## REMOVE
button at the bottom of the menu, as shown below.
Create users groups in admin UI

It is possible to create groups of users and assign rights to a group:
go to the
## Users
tab. The
## Users
and
## Groups
will show in
the main menu on left had side.
Click on
## Groups
in the menu: the
## Add
button at the bottom of
the menu will become active (blue)
Click the
## Add
button
Enter a
## Code
for the group. This is the equivalent of a name,
but Codes can only contain numbers, letters and the following
symbols: . – _
You can now assign registered users to the group and assign Roles as
explained above.
openBIS roles

## Observer

This role can be assigned to the whole openBIS instance (
## Instance
## Observer
) or to specific
## Spaces
or
## Projects
(
## Space
or
## Project
## Observer
). Users with this role have read-only access to the whole
openBIS (
## Instance Observer
), or to a specified
## Space
or
## Project
(
## Space
or
## Project Observer
).
An Observer can see and search everything in an openBIS instance or the
## Space/Project
which they have access to. They can also download
datasets. They cannot modify nor delete anything.
## Space/Project User

Extends Observer permissions with some creating and editing
functionality. Permissions are limited to specified
Space(s)
or
Project(s)
.
Can do everything that Observer and additionally:
create
objects
collections
edit
objects
collections
projects
## Space/Project Power User

## Extends
## Space/Projec
t User permissions with some deleting, editing and
processing functionality. Permissions are limited to specified
Space(s)
or
Project(s)
.
Can do everything that
## Space/Project
User and additionally:
create projects
delete
projects
data sets
objects
collections
Please note that this role cannot be assigned via the ELN UI, only via
admin UI.
## Space/Project Admin

Extends Space/Project Power User permissions allowing to manage roles
and projects inside given
Space(s)
or
Project(s)
.
Can do everything that Space/Project Power User and additionally:
assign and remove Space/Project roles
## Instance Admin

Has the full access to given openBIS instance.
Can do everything that Space/Project Admin and additionally:
create
space
material
person
property type
vocabulary
material type
object type
collection type
data set type
create/delete instance admin role
edit
material
property type
property type assignment
vocabulary
material type
object type
collection type
data set type
assign/unassign property type
delete
space
vocabulary terms
material type
sample type
experiment type
data set type
Please note that this role cannot be assigned via the ELN UI, only via
admin UI.
Updated on April 26, 2023
## User Profile

In the User Profile, a user who is logged in into openBIS can find the
## following information:
## First Name
## Last Name
## Email
openBIS session token
Zenodo API Token
(
Export to
## Zenodo
)
First name, last name and email are automatically filled in when LDAP or
SSO are used for authentication.
In case of file-based authentication, this information can be entered
here directly from the user.
For file-based authentication, users can also change their password
here, from the
## Change Password
option under the
## More..
dropdown.
Updated on June 28, 2022
Assign home space to a user

When a home space is assigned to a user, this becomes marked as
## My
## Space
for that user in the lab notebook, as shown below.
When users are registered via the ELN UI, a
## Space
with their username
is created (see
## User
## Registration)
and this is automatically set as home space for the user.
The same happens in multi-group instances where spaces are created for
each user in the lab notebook section.
## An
instance admin
can change the home space of a user or assign one to
a user that does not have a home space assigned from the admin UI, as
shown below.
Please note that when a user is inactivated, the home space assigned to
that user is moved in the ELN UI to the folder
Others (disabled).
## If
this is not desired, the space should be removed as home space from the
inactivated user. This can be done by an
instance admin
.
Updated on April 26, 2023",User Registration,0,en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_user-registration_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_user-registration.txt,2025-09-30T12:09:09.749210Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_custom-database-queries:0,Custom Database Queries,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/custom-database-queries.html,openbis,"## Custom Database Queries

## Introduction

openBIS application server can be configured to query any relational
database server via SQL. There are three ways to use this feature in
## openBIS Web application:
Running arbitrary SELECT statements.
Defining parametrized queries.
Running parametrized queries.
The three features correspond to three menu items of the menu
## Queries
.
The last feature can be used by any user having OBSERVER role whereas
for the first two features user needs a
query creator
role which
usually is at least POWER_USER role and is
configured
by administrator of the openBIS server. The idea is that power users
having the knowledge to write SQL queries define a query which can be
used by everybody without knowing much about SQL.
Multiple query databases may be configured for any openBIS Web
application. Database labels specified in the configuration file will be
shown in a combo box for database selection while defining new / editing
existing queries.
Note that only the first 100000 rows of the result set of a query are
shown. This restriction should prevent from running ill-designed queries
which consume all the memory of the server. There is also a time out of
5 minutes defined after which the query is canceled if it didn’t return
any result.
How it works

## Database:
is configured as a core-plugin of type “query-databases”
can be assigned to a space:
space == null : should be used for databases that contain data
from multiple spaces or data which is space unrelated
space != null : should be used for databases that contain data
from one specific space only
can be assigned a minimal query creator role:
database with space == null : by default the minimal query
creator role is INSTANCE_OBSERVER
database with space != null : by default the minimal query
creator role is POWER_USER
## Query:
can be created/updated/deleted only by a user with a database
minimal query creator role or stronger (if database space != null
then the user role has to be defined for that space or the user has
to be an instance admin)
can be seen by:
private query : a user who created it or an instance admin
public query : any user
can be executed by:
database with space == null : by users with at least
PROJECT_OBSERVER role (results are filtered by a
experiment_key/sample_key/data_set_key column values which
are expected to contain entity perm_id; WARNING: if no such
column is returned by a query then ALL results are returned)
database with space != null : by users with at least
SPACE_OBSERVER role in that space (all results are returned
without any filtering as they all belong to the space a user has
access to)
can be updated/executed/deleted only by a user who can see the query
can contain additional parameters (e.g. ${my_parameter}); values of
such parameters can be set in the UI by a user right before an
execution of a query
can be GENERIC (accessible only from the “Queries” top menu) or
EXPERIMENT/SAMPLE/DATA_SET/MATERIAL specific (accessible from the
“Queries” top menu and from Experiment/Sample/DataSet/Material view
respectively)
entity specific queries should contain ‘${key}’ parameter which will
be replaced by a permId of the displayed experiment/sample or by a
code of the displayed dataset/material before the query execution
(MATERIAL queries also have ‘${type}’ parameter which is replaced
with a type code of the material)
entity specific queries may be configured to appear only in the
views of entities of chosen types (e.g. only for samples of types
that match a given regexp)
## Arbitrary SQL:
running an arbitrary SQL is treated as a creation of a query which
is simply not stored for a future use i.e. only a user with a
minimal query creator role or stronger can do it (if database space
!= null then the user role has to be defined for that space or the
user has to be an instance admin)
## Setup

To use the custom database queries, it is necessary to define query databases. See
Installation and Administrator Guide of the openBIS Server
for an explanation on how to do this.
Running a Parametrized Query

Choose menu item
## Queries -> Run Predefined Query
. The tab
## Predefined Query
opens.
Choose a query using the query combo box. Queries specified for all
configured databases are selected transparently using the same combo
box which displays only query names.
If the query has no parameters it will be executed immediately and
the result is shown in tabular form. Otherwise text fields for each
parameter appear right of the query combo box.
Enter some values into the parameter fields and click on the
## Execute
button. The query result will be shown as a table.
Features of a query result:
The result can be browsed, exported, sorted, and filtered as most
tables in openBIS.
Values referring to permIDs of an experiment, sample, or data set
might be shown as hyperlinks. A click on such a link opens a new tab
with details.
Running a SELECT statement

This feature is only for users with
creator role
. It is useful for
exploring the database by ad hoc queries.
Choose menu item
Queries -> Run Custom SQL Query
. The tab
Custom SQL Query
opens.
Enter a SELECT statement in the text area, select database and click
on the
## Execute
button. The result appears below in tabular form.
Defining and Editing Parametrized Queries

This feature is only for users with
creator role
.
Define a Query

Choose menu item
## Queries -> Browse Query Definitions
. The tab
## Query Definitions
opens. It shows all definitions where the user
has access rights.
Click on
## Add Query Definition
for defining a new parametrized
query. A large dialog pops up.
Enter a name, database, an optional description, and a SELECT
statement.
Click on button
## Test Query Definition
to execute the query. The
result will be shown in the same dialog.
Click on button
## Save
to save the definition. The dialog
disappears and the new definition appears in the table of query
definitions.
Public flag

A query definition can be public or private depending on whether the
check box
public
is checked or not. A private query is visible only
by its creator. Public queries are visible by everybody. The idea is
that a power user first creates query definitions for their own
purposes. If he or she find it useful for other users they will set the
public flag.
## Specifying Parameters

A SQL query can have parameters which are defined later by the user
running the query. A parameter is of the form
${<parameter
name>
}.
## Example:
select * from my_table where code = ${my table code}
The parameter name will appear in the text field when running the query.
Optionally, you can provide key-value pairs which are “metadata” for the
parameter name and separated by ‘::’ from the name. These metadata keys
## are defined:
Metadata key
## Explanation
## Example
type
Sets the data type of this parameter. Valid values are VARCHAR (or STRING), CHAR, INTEGER, BIGINT, FLOAT, DOUBLE, BOOLEAN, TIME, DATE or TIMESTAMP.
${code::type=VARCHAR}
list
Coma-separated list of allowed values for the parameter.
${color::list=red,green,blue}
query
A SQL query which is run to determine the allowed values for the parameter. The query is expected to return exactly one column. You should specify only fast queries here with a reasonably small number of returned rows as the UI will block until this query has returned.
${name::query=select last_name from users}
It is possible to combine multiple keys like
## this:
${estimate::type=integer::list=1,3,7,12
}.
## Warning
Why to provide a data type
Providing a data type with
type=...
is not mandatory. In a future version of the software we may add additional client-side validation based on this value, but in the current version we don’t do that yet. If you do
not
provide a data type, openBIS will ask the database for the type of the particular query parameter. This works fine for most databases, but not for all. Oracle is a well-known example that cannot provide this information. So if your query source is an Oracle database and you do not provide a data type, you will get an error saying
## ""Unsupported
feature
”. To fix this, you have to rovide the data type.
Array Literals for PostgreSQL data sources

For PostgreSQL, there exist neat array functions
## ANY
and
## ALL
(see
PostgreSQL
documentation
).
In particularly
## ANY
comes in handy in
## WHERE
clauses to check whether
a column has one of several values. The official form for providing an
array literal as a string (which is what you have to do here) is a bit
clumsy, as you have to write for the query
""select
*
from
data
where
code
=
ANY(${codes}::text[])
” and then the
user running the query has to put the parameter value in curly braces
like “
{code1,code2,code3,...}
”.
The custom query engine has a simplification for this construct. You can
## just write:
""select
*
from
data
where
code
=
ANY({${codes}})
” for the
query and then the user running the query will be able to skip the curly
bracket and write for the parameter value: “
code1,code2,code3,...
## ”. A
user who doesn’t know that this is an array will in particular get away
with just providing a single value like “
code1
”.
Note that the most obvious way of specifying a set relationship with
""select
*
from
data
where
code
in
(${codes})
” does
not
work as
custom queries are not using simple text concatenation but prepared
queries to avoid a security problem known as “SQL Injection”.
## Hyperlinks

In order to create hyperlinks in the result table the column names in
the SQL statement should be one of the following
magic
## words:
experiment_key
sample_key
data_set_key
They should denote a perm ID of specified type.
## Example:
select
id
,
perm_id
as
data_set_key
from
data_sets
## Warning
Be careful with this feature
: The table is shown with the hyperlinks even if the value isn’t a perm ID of specified type.
Edit a Query

Choose menu item
## Queries -> Browse Query Definitions
. The tab
## Query Definitions
opens.
Select a query and click on button
## Edit
. The same dialog as for
defining a query pops up.
Entity Queries (Experiment, Sample, Material, Data Set)

By default, all custom queries are
## Generic
, which means that the user
will be able to execute them from the standard Queries menu.
Additionally it is possible to create a query containing a special
‘magic’ parameter, which will be automatically replaced by the entity
identifier (perm id in case of experiments and samples, code for data
sets and a pair (code, type) in case of materials). Those entity
specific queries will be visible only in entity details views (e.g.
experiment details) in a special
section
called
## Queries
. One can
also limit visibility of a query to a specific entity types (e.g.
experiment of type
## EXP
).
How to create/edit entity custom queries

Entity custom queries can be created and edited in the same way as
## Generic
queries (
## Queries -> Browse Query Definitions
), but the
value of
## Query
## Type
field should be set to Experiment, Sample,
Data Set or Material.
## Entity
## Type
(e.g. Experiment Type) should be changed if one wants
to limit the visibility of a query to a specific type (default option -
(all)
, doesn’t introduce such a restriction). The field accepts not
only values selected from the list but also typed text containing a
regular expression (e.g. Experiment Type
## 'EXP.*'
would mean that the
query should be visible in views of experiments of type with code
starting with
## 'EXP'
prefix).
Furthermore the sql should contain the ‘magic’ parameter
‘${key}’
(will be replaced by perm id (experiment, sample) or code (data set,
material)). In case of material custom query, additional ‘magic’
## parameter is required:
‘${type}’
(will be replaced by material type
code).
## Examples

## Warning
## Legacy Syntax:
Older versions of openBIS required to put string parameters in ticks, like ‘${param}’. Current versions of openBIS don’t need this anymore, so you can use ${param} without the ticks. However, the syntax with ticks is still accept for backward compatibility.",Custom Database Queries,0,en_20.10.0-11_user-documentation_general-admin-users_custom-database-queries_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_custom-database-queries.txt,2025-09-30T12:09:09.815903Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_index:0,General Admin Users,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/index.html,openbis,"## General Admin Users

## Admins Documentation
## Login
File based and/or LDAP authentication
SWITCHaai authentication
Inventory overview
Customise Inventory Of Materials And Samples
Create Collections of Materials
Create the Project folder
Create the Collection folder
Add the “+Object type” button in the Collection percentage
## Delete Collections
Enable Storage Widget on Sample Forms
## Configure Lab Storage
Add metadata to Storage Positions
## Customise Inventory Of Protocols
Create Collections of Protocols
Enable Protocols in Settings
Move Collections to a different Project
Customise Parents and Children Sections in Object Forms
Customise the Main Menu
## Main Menu Sections
Lab Notebook menu
Associate File Types to Dataset Types
## User Registration
Register users in ELN Interface
Default roles assigned in ELN
Register users from the admin UI
Deactivate users
Remove users
Create users groups in admin UI
openBIS roles
## Observer
## Space/Project User
## Space/Project Power User
## Space/Project Admin
## Instance Admin
## User Profile
Assign home space to a user
## New Entity Type Registration
Register a new Object Type
Registration of Properties
Property Data Types
Considerations on properties registration
## Controlled Vocabularies
Register a new Experiment/Collection type
Register a new Dataset type
Enable Rich Text Editor or Spreadsheet Widgets
Enable Objects in dropdowns
Register masterdata via Excel
Modifying existing types
Properties overview
Internal properties and vocabularies
Masterdata exports and imports
Masterdata export
Masterdata import
Masterdata version
Imports of openBIS exports
Metadata import
Datasets import
Create Templates for Objects
Enable Transfer to Data Repositories
Enable Barcodes and QR codes
Enable archiving to Long Term Storage
## History Overview
History of deletions
History of freezing
## Space Management
Create new Inventory Spaces
Create a new Inventory Space from the ELN UI
Create a new Inventory Space from the core UI
Create new ELN Spaces
Create a new Lab Notebook Space from the ELN UI
Create a new Lab Notebook Space from the core UI
## Delete Spaces
Move Spaces between Lab Notebook and Inventory
## Multi Group Set Up
General ELN Settings
## Instance Settings
## Group Settings
Group ELN Settings
Database navigation in admin UI
## Features
## Filter
## Navigation
## Sorting
## Properties Handled By Scripts
## Introduction
Types of Scripts
Defining properties
## Dynamic Properties
## Introduction
Defining dynamic properties
Creating scripts
## Simple Examples
## Advanced Examples
Data Types
Creating and Deploying Java Plugins
Dynamic properties evaluator
Entity validation scripts
## Introduction
Defining a Jython validation script
Script specification
Triggering Validation of other Entities
Script example
Activating the validation
Creating and Deploying Java Validation Plugins
When are validations performed
Good practices
## Managed Properties
## Introduction
## Defining Managed Properties
Creating scripts
## Predefined Functions
Java API
Examples of user defined functions
Storing structured content in managed properties
Unofficial API
‘Real World’ example
Creating and Deploying Java Plugins
## Custom Database Queries
## Introduction
How it works
## Setup
Running a Parametrized Query
Running a SELECT statement
Defining and Editing Parametrized Queries
Define a Query
Public flag
## Specifying Parameters
Array Literals for PostgreSQL data sources
## Hyperlinks
Edit a Query
Entity Queries (Experiment, Sample, Material, Data Set)
How to create/edit entity custom queries
## Examples",General Admin Users,0,en_20.10.0-11_user-documentation_general-admin-users_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_index.txt,2025-09-30T12:09:09.893014Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_properties-handled-by-scripts:0,Properties Handled By Scripts,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/properties-handled-by-scripts.html,openbis,"## Properties Handled By Scripts

## Introduction

One of the reasons why openBIS is easily extensible and adjustible is
the concept of generic entities like samples, experiments, materials and
data sets. By adding domain specific properties to the mentioned
objects, instance administrator creates a data model specific to given
field of study. Values of configured properties will be defined by the
user upon creation or update of the entities (samples etc.).
In most cases values of properties must be provided directly by the
user. The default way of handling a property in openBIS can be changed
by instance admin defining a property that should be handled by a script
written in
## Jython
or Predeployed Plugin written
in Java. Jython plugins use Jython version configured by the
service.properties property
jython-version
which can be either 2.5 or
2.7.
Types of Scripts

There are two types of plugins that can be used for handling properties,
and one script type to perform validations on entities:
## Dynamic Property Evaluator
(for properties referred to as
## Dynamic Properties
)
for properties that
can’t be modified by users
,
values of such properties will be
evaluated automatically
using metadata already stored in openBIS (e.g. values of other
properties of the same entity or connected entities),
the script defines an expression or a function that returns a
value for a
## Dynamic Property
specified in the script
## Managed Property Handler
(for properties referred to as
## Managed
## Properties
)
for properties that will be
indirectly modified by users
,
the script alters default handling of a property by openBIS by
defining functions that specify e.g.:
how the property should be
displayed
in entity detail
view (e.g. as a table),
input fields
for modifying the property,
translation
and/or
validation
of user input.
## Entity Validation
performed after each update or creation of a given entity type.
the user provided script performs a validation, which can cancel
the operation if the validation fails
Defining properties

To create a property that should be handled by a script perform the
following steps.
Define a property type with appropriate name and data type
## (Administration->Property Types->New).
Define a script that will handle the property
(Administration->Scripts) or deploy a Java plugin. For details
and examples of usage go to pages:
## Dynamic Properties
## Managed Properties
Entity validation scripts
Assign the created property type to chosen entity type using the
created script (e.g. for samples: Administration->Property
Types->Assign to Sample Type):
select Handled By Script checkbox,
select the appropriate Script Type
choose the Script
The validation scripts are assigned to the type in the “Edit Type”
section. (e.g Admin->Types->Samples. Select sample and click
edit.)
No labels
## Dynamic Properties

## Introduction

## Dynamic Properties
are one of two types of properties that use Jython
scripts for providing special functionality to OpenBIS. To understand
the basic concept read about
## Properties Handled By Scripts
.
Defining dynamic properties

To create a dynamic property:
Define a property type with appropriate name and data type
(
## Admin->Plugins→Add
## Plugin
)
## Choose
## Dynamic
## Property
## Evaluator
from Plugin type dropdown list
in the upper left corner.
You may evaluate script on chosen entity in Script Tester section.
Creating scripts

To edit existing dynamic property script, edit dynamic
property (
Admin→Plugins→(click
on
dynamic
property)→Edit
plugin
)
The scripts should be written using standard Jython syntax. Unlike
custom columns and filters (which also require Jython syntax), dynamic
properties can have more than one line of code. If the Script contains
only one line, it will be evaluated and used as the value of appropriate
dynamic property. If on the other hand a multi line script is needed,
the function named
""calculate""
will be expected and the the result
will be used as property value.
To access the entity object from the script, use the following syntax:
entity.<requested
method>
Currently available methods that can be called on all kinds of entities
## include:
code()
property(propertyTypeCode)
propertyValue(propertyTypeCode)
propertyRendered(propertyTypeCode)
properties()
For more details see
IEntityAdaptor
(interface implemented by
entity
) and
IEntityPropertyAdaptor
(interface implemented by each property).
It is also possible to acces the complete Java Object by calling
""entityPE()""
method, but this is appropach is not recomended, as the
returned value is not a part of a well defined API and may change at any
point. You may use it as a workaround, in case some data are not
accessible via well defined API, but you should contact OpenBIS helpdesk
and comunicate your needs, so the appropriate methods can be added to
the official API.
You can test your script on selected entities
(samples/experiments/materials/data sets) using the testing environment
part of
## Script
## Editor
.
## Simple Examples

Show a value of a Sample property which is named ‘Multiplicity’
entity
.
propertyValue
(
## 'Multiplicity'
)
Takes an existing property and multiplies the value by 1.5
float
(
entity
.
propertyValue
(
## 'CONCENTRATION_ORIGINAL_ILLUMINA'
))
*
1.5
## Advanced Examples

## Show all entity properties as one dynamic property:
def
get_properties
(
e
## ):
""""""Automatically creates entity description""""""
properties
=
e
.
properties
()
if
properties
is
## None
## :
return
""No properties defined""
else
## :
result
=
""""
for
p
in
properties
## :
result
=
result
+
""
\n
""
+
p
.
propertyTypeCode
()
+
"": ""
+
p
.
renderedValue
()
return
result
def
calculate
## ():
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
return
get_properties
(
entity
)
Calculate a new float value based some other values
import
java.lang.String
as
## String
def
calculateValue
## ():
nM
=
0
uLDNA
=
0
if
entity
.
propertyValue
(
## 'CONCENTRATION_PREPARED_ILLUMINA'
)
!=
''
and
\
entity
.
propertyValue
(
## 'FRAGMENT_SIZE_PREPARED_ILLUMINA'
)
!=
''
## :
nM
=
float
(
entity
.
propertyValue
(
## 'CONCENTRATION_PREPARED_ILLUMINA'
))
/
\
float
(
entity
.
propertyValue
(
## 'FRAGMENT_SIZE_PREPARED_ILLUMINA'
))
*
\
1000000
/
650
if
float
(
entity
.
propertyValue
(
## 'UL_STOCK'
))
!=
''
## :
uLDNA
=
float
(
entity
.
propertyValue
(
## 'UL_STOCK'
))
*
2
/
nM
uLEB
=
float
(
entity
.
propertyValue
(
## 'UL_STOCK'
))
-
uLDNA
return
## String
.
format
(
""
%16.1f
""
,
uLEB
)
return
0
def
calculate
## ():
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
return
calculateValue
()
Calculate a time difference between two time stamps:
from
datetime
import
datetime
def
dateTimeSplitter
(
openbisDate
## ):
dateAndTime
,
tz
=
openbisDate
.
rsplit
(
"" ""
,
1
)
pythonDateTime
=
datetime
.
strptime
(
dateAndTime
,
""%Y-%m-
%d
## %H:%M:%S""
)
return
pythonDateTime
def
calculate
## ():
try
## :
start
=
entity
.
propertyValue
(
## 'FLOW_CELL_SEQUENCED_ON'
)
end
=
entity
.
propertyValue
(
## 'SEQUENCER_FINISHED'
)
s
=
dateTimeSplitter
(
start
)
e
=
dateTimeSplitter
(
end
)
diffTime
=
e
-
s
return
str
(
diffTime
)
except
## :
return
## ""N/A""
Illumina NGS Low Plexity Pooling Checker: checks if the complexity of a pooled sample is good enough for a successful run:
def
checkBarcodes
## ():
'''
'parents' are a HashSet of SamplePropertyPE
'''
## VOCABULARY_INDEX1
=
## 'BARCODE'
## VOCABULARY_INDEX2
=
## 'INDEX2'
## RED
=
set
([
## 'A'
,
## 'C'
])
## GREEN
=
set
([
## 'T'
,
## 'G'
])
## SUCCESS_MESSAGE
=
## ""OK""
## NO_INDEX
=
## ""No Index""
listofIndices
=
[]
boolList
=
[]
positionList
=
[]
returnString
=
"" ""
for
e
in
entity
.
entityPE
()
.
parents
## :
for
s
in
e
.
properties
## :
if
s
.
entityTypePropertyType
.
propertyType
.
simpleCode
==
## VOCABULARY_INDEX1
## :
index
=
s
.
getVocabularyTerm
()
.
code
if
len
(
listofIndices
)
>
0
## :
for
n
in
range
(
0
,
len
(
index
)
-
1
## ):
listofIndices
[
n
]
.
append
(
index
[
n
])
else
## :
for
n
in
range
(
0
,
len
(
index
)
-
1
## ):
listofIndices
.
append
([
index
[
n
]])
# remove any duplicates
setofIndices
=
[
set
(
list
)
for
list
in
listofIndices
]
# Test whether every element in the set 's' is in the RED set
boolList
=
[
setofNuc
.
issubset
(
## RED
)
for
setofNuc
in
setofIndices
]
if
boolList
## :
for
b
in
boolList
## :
if
b
## :
positionList
.
append
(
boolList
.
index
(
b
)
+
1
)
# set the value to False, because 'index' returns only the first occurrence
boolList
[
boolList
.
index
(
b
)]
=
## False
else
## :
return
## NO_INDEX
#  if s.entityTypePropertyType.propertyType.simpleCode == VOCABULARY_INDEX2:
#   pass
if
positionList
## :
for
pos
in
positionList
## :
returnString
+=
""WARNING! Base position ""
+
str
(
pos
)
+
"" of ""
+
\
## VOCABULARY_INDEX1
+
\
"" does not contain both color channels""
+
\
""
\n
""
else
## :
returnString
=
## SUCCESS_MESSAGE
return
returnString
def
calculate
## ():
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
return
checkBarcodes
()
Data Types

Any data type that can be used by openBIS properties is supported by
dynamic properties. The script always returns just a string
representation of a property value. The value is then validated and in
special cases converted before being saved. The string formats and
validation rules are the same as in batch import/update of
samples/experiments/data sets/materials.
The non-trivial cases are properties with data type:
CONTROLLED VOCABULARY - use code of vocabulary term as string
representation,
MATERIAL - use function
material(code,
typeCode)
to create the
string representation.
Creating and Deploying Java Plugins

To create valid Java plugin for Dynamic Properties, one should create a class that is implementing
ch.systemsx.cisd.openbis.generic.server.dataaccess.dynamic_property.calculator.api.IDynamicPropertyCalculatorHotDeployPlugin
interface. The class should be annotated with
ch.ethz.cisd.hotdeploy.PluginInfo
annotation specifying the name of the plugin, and
ch.systemsx.cisd.openbis.generic.server.dataaccess.dynamic_property.calculator.api.IDynamicPropertyCalculatorHotDeployPlugin
class as a plugin type.
Such a plugin should be exported to a jar file and put
into
<<openBIS
installation
directory>>/servers/entity-related-plugins/dynamic-properties
directory. The plugin will be detected automatically and will be
automatically available to openBIS. No restart is needed.
Dynamic properties evaluator

Evaluation of dynamic properties may be very time consuming, therefore
it is not done automatically after each metadata update. To make sure
that the potential inconsistencies are repaired, the maintenance task
can be defined (
service.properties
), that runs in specified intervals:
maintenance-plugins
=
dynamic-property-evaluator
dynamic-property-evaluator.class = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask
# run daily at midnight
dynamic-property-evaluator.interval = 86400
dynamic-property-evaluator.start = 00:00
If the value of a dynamic property has not yet been calculated, it will
be shown as
(pending
evaluation)
.
No labels
Entity validation scripts

## Introduction

Entity validation scripts are a mechanism to ensure metadata
consistency. For each entity type a user can define a validation
procedure, which will be performed at each creation or update of the
entity of that type. There are two ways to define an entity validation
procedure: Jython scripts and Java plugins.
Defining a Jython validation script

Go to Admin -> Plugins -> Add Plugin.
Select “Entity Validator” as the plugin type
Choose name, entity kind, and description.
Prepare a script (see paragraph “Script specification” below)
Script specification

The script should at least include the validate function, that takes two
parameters. The first one is the entity being validated, and the second
is the boolean stating whether it is a new entity (creation) or an
existing one(update).
the function name should be ‘validate’. It will be called with two
parameters.
the first argument is the entity. It will be the object
implementing
the
IEntityAdaptor
interface
the second argument is the boolean “isNewEntity”. It will be
true if the entity is new.
The script should return None (or nothing) if the validation is
successful and a string with an error message, if the validation
fails.
Triggering Validation of other Entities

A plugin can specify that another entity needs to be validated as well.
For example, a change to a sample could require validation of its
children. The infrastructure can be informed of this dependency by
calling
requestValidation
with the entity that needs to be validated
as an argument.
Script example

Here is the example script that validates, that the newly created entity
## does not have any properties defined:
## Basic Example
## def validate(entity, isNew):
## if isNew:
## if not entity.properties() is None:
return ""It is not allowed to attach properties to new sample.""
## Triggering Example
## def validate(entity, isNew):
for s in entity.children():
requestValidation(s)
Activating the validation

To make the validation active per entity type you have to select the
validation script for each type:
## Admin -> Types ->
you selected also in the
script definition ->
Select a Sample Type and edit it
You find a property which is called ‘Validation Script’ (see screen
shot below). Just select your defined Script and hit save.
Creating and Deploying Java Validation Plugins

To create a valid Java plugin for Entity Validation, one should create a
class that is implementing
the
ch.systemsx.cisd.openbis.generic.server.dataaccess.entity_validation.IEntityValidatorHotDeployPlugin
interface.
The class should be annotated with
ch.ethz.cisd.hotdeploy.PluginInfo
annotation specifying the name of
the plugin,
and
ch.systemsx.cisd.openbis.generic.server.dataaccess.entity_validation.IEntityValidatorHotDeployPlugin
class
as a plugin type.
All classes needed to run the plugin have to be exported to a jar file
and put into
the
directory
<<openBIS
installation
directory>>/servers/entity-related-plugins/entity-validation
.
The plugin will be detected and made available automatically to openBIS.
No restart is required for that.
When are validations performed

Validations are performed at the end of the transaction, not at the
moment of the change. So if during some longer operation there are
several updates to different entities, all of them are evaluated at the
end, when all changes are available to the validation script.
Therefor it is possible to write for instance a dropbox that makes some
updates that break validation temporarily, and still succeeds as long as
the validations succeed after all the updates have been done.
Good practices

Validation scripts should be read-only.
In theory it is possible to edit the entity during the
validation. This is a bad practice. Consider using
## Dynamic
## Properties
if you
want calculations being performed after the entity updates.
Think about performance
The plugins will be executed for every creation or update of
entities of that type. This can affect the performance
drastically if the plugin will be too heavy.
No labels
## Managed Properties

## Introduction

## Managed Properties
are one of two types of properties that use Jython
scripts for providing special functionality to openBIS. To understand
the basic concept read about
## Properties Handled By
## Scripts
.
The feature is especially useful when a complex data structure should be
stored in a single property. Properties holding XML documents are a good
example. The problem with XML documents is that users may find them
difficult to work with (read, create or even modify without mistakes).
This is where managed properties come in handy. Users don’t have to be
aware of the complex format of data stored in a property. It can be a
task for instance administrator to specify how the values should be
presented to a user and how is he going to modify them using a custom
## UI.
## Defining Managed Properties

To create a Managed Property:
Define a property type with appropriate name and data type
(
## Administration->Property
## Types->New
)
Define the formula which should be used to manage your property
(
## Administration->Scripts
).
Assign created property type to chosen entity (sample etc.) type
(
## Administration->Property
## Types->Assign
to
## Sample
## Type
). In the
assignment form perform the following steps specific to managed
## properties:
select
## Handled
## By
## Script
checkbox,
select
## Managed
## Property
## Handler
radio option
choose the appropriate
## Script
that will be responsible
optionally set the
Shown in Edit Views
checkbox to true. This
has the following effects:
In registration/editing form the raw values backing the
managed properties is show.
In registration form additional input fields are provided if
the script has defined the function
batchColumnNames
or
inputWidgets
.
Creating scripts

To browse and edit existing scripts or add new ones, select
Administration->Scripts from the top menu.
The scripts should be written in standard Jython syntax. The following
## functions are invoked by openBIS, some of them are mandatory:
## Function
## Mandatory/Optional
## Description
updateFromUI(action)
optional
Updates the property value from an input form constructed by the action defined in
configureUI()
.This function has an access to a variable named ‘person’ which holds an object of type  ch.systemsx.cisd.openbis.generic.shared.basic.dto.api.IPerson. This object contains information about a user that is performing the update.
updateFromRegistrationForm(bindings)
optional
Creates the property value from input data provided by a registration forms. This function is mandatory if function  inputWidgets  are defined. The argument is list of a  java.util.Map  objects which has the values taken from the registration form. Here
<code>
is either a batch column name in upper case or a input widget code. The bound value is a string accessed with  get(
<code>
).    This function has an access to a variable named ‘person’ which holds an object of type  ch.systemsx.cisd.openbis.generic.shared.basic.dto.api.IPerson. This object contains information about a user that is performing the update.
updateFromBatchInput(bindings)
optional
Creates the property value from input data provided by a batch input file (for batch import and batch update operations). This function is mandatory if function batchColumnNames is defined. The argument is a java.util.Map which has the values of the columns with headers of the form
<property
type
code>:<code>
in the input file. Here
<code>
is either a batch column name in upper case or a input widget code. The bound value is a string accessed with
get(<code>
). If no batchColumnNames is defined in the script the input value from the file with column
<property
type
code>
is accessed by
get('')
.This function has an access to a variable named ‘person’ which holds an object of type  ch.systemsx.cisd.openbis.generic.shared.basic.dto.api.IPerson. This object contains information about a user that is performing the update.
Batch update has currently only limited support. There is no way to retrieve old value of managed property (property.getValue() always returns null). The
updateFromBatchInput(bindings)
function can set the new value though.
inputWidgets()
optional
A function which returns a list of IManagedInputWidgetDescription instances which will be used for batch update and/or input widgets in registration forms. The function
updateFromBatchInput(bindings)
assumes that values are bound to the upper-case version of the code of the widget description which is by default the upper-case version of its label.
configureUI()
mandatory
It defines output UI (e.g. table for tabular data) to be used in the detailed view of the entity owning the property. Multiple actions (of type IManagedUIAction) for update of the property value can be defined in this function. Every action specifies input UI for displaying forms with input fields and transferring the values provided by users.
batchColumnNames()
optional
A function which returns a list of column names to be used for batch update and/or input widgets in registration forms. The function
updateFromBatchInput(bindings)
assumes that values are bound to the upper-case version of the column names. The names will be the labels of non-mandatory text input widgets.
All functions (except
batchColumnNames
and
inputWidgets
)  have
access to a variable named
property
which holds an object of type
IManagedProperty
. Methods of this class are explained below. To
access the property object from the script, use the following syntax:
property
.<
requested
method
>
## Predefined Functions

The following functions are predefined and can be used everywhere in the
## script:
ISimpleTableModelBuilderAdaptor
createTableBuilder()
: Creates a
table model builder. It will be used in
configureUI
to create
tabular data to be shown in openBIS GUI.
ValidationException
ValidationException(String
message)
: Creates a
Validation Exception with specified message which should be raised
in functions
updateFromUI
and
updateFromBatchInput
in case of
invalid input.
IManagedInputWidgetDescriptionFactory
inputWidgetFactory()
## :
returns a factory that can be used to create descriptions of input
fields used for modification of managed property value (see
example
).
IElementFactory
elementFactory()
: returns a factory that can be
used to create
IElement
-s.
## See
#Storing structured content in managed
properties
.
IStructuredPropertyConverter
xmlPropertyConverter()
: returns a
converter that can translate
IElement
-s
to XML Strings and from XML or JSON Strings. See
#Storing
structured content in managed
properties
.
IStructuredPropertyConverter
jsonPropertyConverter()
: returns a
converter that can translate
IElement
-s
to JSON Strings and from XML or JSON Strings. See
#Storing
structured content in managed
properties
.
Java API

Java objects used and created in the scripts are implementing Java
## interfaces from two packages:
ch.systemsx.cisd.openbis.generic.shared.basic.dto.api
DTOs (Data Transfer Objects) that will be transferred between server
and client (web browser)
ch.systemsx.cisd.openbis.generic.shared.managed_property.api
utilities (e.g. for managing DTOs)
Examples of user defined functions

The following examples show how to implement particular script functions
that will be invoked by openBIS.
configureUI

## Example 1

This example shows how to configure a fixed table (without using value
stored in the property at all) that will be shown in detail view of an
entity.
def
configureUI
## ():
""""""create table builder and add 3 columns""""""
tableBuilder
=
createTableBuilder
()
tableBuilder
.
addHeader
(
""column 1""
)
tableBuilder
.
addHeader
(
""column 2""
)
tableBuilder
.
addHeader
(
""column 3""
)
""""""add two rows with values of types: string, integer, real""""""
row1
=
tableBuilder
.
addRow
()
row1
.
setCell
(
""column 1""
,
""v1""
)
row1
.
setCell
(
""column 2""
,
1
)
row1
.
setCell
(
""column 3""
,
1.5
)
row2
=
tableBuilder
.
addRow
()
row2
.
setCell
(
""column 1""
,
""v2""
)
row2
.
setCell
(
""column 2""
,
2
)
row2
.
setCell
(
""column 3""
,
2.5
)
""""""add a row with only value for the first column specified (two other columns will be empty)""""""
row3
=
tableBuilder
.
addRow
()
row3
.
setCell
(
""column 1""
,
""v3""
)
""""""specify that the property should be shown in a tab and set the table output""""""
property
.
setOwnTab
(
## True
)
uiDesc
=
property
.
getUiDescription
()
uiDesc
.
useTableOutput
(
tableBuilder
.
getTableModel
())
Let’s assume, that a property type with label
## Fixed Table
was assigned
to sample type CELL_PLATE as a managed property using the example
script.
The picture below shows that in detail view of CELL_PLATE sample S1
there will be a tab titled
## Fixed Table
containing a table defined by
the script. The table has the same functionality as all other openBIS
tables like sorting, filtering, exporting etc.
## Example 2

This is another example of showing how to configure a fixed table, but
this time values in the table will be displayed as clickable links to
openBIS entities (see
Linking to openBIS entities
for more details):
def
configureUI
## ():
""""""create table builder with 4 columns (any column names can be used)""""""
tableBuilder
=
createTableBuilder
()
tableBuilder
.
addHeader
(
""sample""
)
tableBuilder
.
addHeader
(
""experiment""
)
tableBuilder
.
addHeader
(
""data set""
)
tableBuilder
.
addHeader
(
""material""
)
""""""
Add rows with values of type entity link.
Use element link factory to create link cells and.
""""""
factory
=
elementFactory
()
row
=
tableBuilder
.
addRow
()
"""""" for links to samples, experiments and datasets provide the permId """"""
row
.
setCell
(
""sample""
,
factory
.
createSampleLink
(
""samplePermId""
))
row
.
setCell
(
""experiment""
,
factory
.
createExperimentLink
(
""experimentPermId""
))
row
.
setCell
(
""data set""
,
factory
.
createDataSetLink
(
""dataSetPermId""
))
"""""" for material links material code and material type code are needed """"""
row
.
setCell
(
""material""
,
factory
.
createMaterialLink
(
""materialCode""
,
""materialTypeCode""
))
""""""specify that the property should be shown in a tab and set the table output""""""
property
.
setOwnTab
(
## True
)
uiDesc
=
property
.
getUiDescription
()
uiDesc
.
useTableOutput
(
tableBuilder
.
getTableModel
())
If linked entity doesn’t exist in the database the perm id
((
code
(type)
for materials) will be shown as plain text (not
clickable).
Otherwise clickable links will be displayed with link text equal to:
identifier of samples or experiments,
code of a data set (the same as perm id),
code
(type)
of a material
## Example 3

This example shows how to configure a table representation of a property
value holding a CSV document (many lines with comma separated values):
def
configureUI
## ():
""""""get the property value as String and split it using newline character""""""
value
=
property
.
getValue
()
lines
=
[]
if
value
!=
## None
## :
lines
=
value
.
split
(
""
\n
""
)
tableBuilder
=
createTableBuilder
()
if
len
(
lines
)
>
0
## :
""""""treat first line as header - split using comma character to get column titles""""""
header
=
lines
[
0
]
.
split
(
"",""
)
tableBuilder
.
addFullHeader
(
header
)
""""""iterate over rest of lines and add them to the table as rows""""""
for
i
in
range
(
1
,
len
(
lines
## )):
row
=
lines
[
i
]
.
split
(
"",""
)
tableBuilder
.
addFullRow
(
row
)
""""""specify that the property should be shown in a tab and set the table output""""""
property
.
setOwnTab
(
## True
)
uiDesc
=
property
.
getUiDescription
()
uiDesc
.
useTableOutput
(
tableBuilder
.
getTableModel
())
## Let’s assume, that:
a multiline property type with label CSV was assigned to sample type
CELL_PLATE as managed property using the example script,
value of CSV property for CELL_PLATE sample S1 is:
col 1,col 2
r1 v1,r1 v2
r2 v1,r2 v2
r3 v1,r3 v2
The picture below shows that in detail view of sample S1 there will be a
tab titled CSV containing a table defined by the script.
Managed property value will be visible as text in the left panel
(
## Sample Properties
) only if user had enabled debugging mode in openBIS
(
## User
## Menu->Settings->Enable
## Debugging
## Mode
).
## Example 4

This is an extension of the previous example showing how to specify user
input for actions like add, edit and delete:
def
configureUI
## ():
""""""code from previous example is not repeated here""""""
factory
=
inputWidgetFactory
()
if
len
(
lines
)
>
0
## :
header
=
lines
[
0
]
.
split
(
"",""
)
""""""define an action labelled 'Add' for adding a new row to the table""""""
addAction
=
uiDesc
.
addTableAction
(
## 'Add'
)
.
setDescription
(
'Add new row to the table'
)
""""""for every header column add a text input field with the same label as column title""""""
widgets
=
[]
for
i
in
range
(
0
,
len
(
header
## )):
widgets
.
append
(
factory
.
createTextInputField
(
header
[
i
]))
addAction
.
addInputWidgets
(
widgets
)
""""""define an action labelled 'Edit' for editing a selected row of the table""""""
editAction
=
uiDesc
.
addTableAction
(
## 'Edit'
)
.
setDescription
(
'Edit selected table row'
)
editAction
.
setRowSelectionRequiredSingle
()
""""""for every header column add a text input field that is bounded with a column""""""
widgets
=
[]
for
i
in
range
(
0
,
len
(
header
## )):
columnName
=
header
[
i
]
widgets
.
append
(
factory
.
createTextInputField
(
columnName
))
editAction
.
addBinding
(
columnName
,
columnName
)
editAction
.
addInputWidgets
(
widgets
)
""""""define an action labelled ""Delete"" for deleting selected rows from the table - no input fields are needed""""""
deleteAction
=
uiDesc
.
addTableAction
(
## 'Delete'
)
\
.
setDescription
(
'Are you sure you want to delete selected rows from the table?'
)
deleteAction
.
setRowSelectionRequired
()
The picture below shows updated detail view of sample S1. For every
action defined in the script there is a button in bottom toolbar of the
table.
The screenshot was taken after a user clicked on the first table row and
then clicked on
## Edit
button. This resulted in showing a dialog with
input fields defined in the script. Every field has default value set
automatically to a value from the selected row.
Whenever an action is defined in
configureUI
there should be
updateFromUI()
function defined that handles the actions (see
next
example
). Otherwise clicking on action
buttons will cause an error.
updateFromUI()

This function should update the value of the managed property in
response to user’s action.
## Example 5

This is an extension of the previous example showing how to specify
behaviour of actions defined in
configureUI()
## function:
def
configureUI
## ():
""""""code from previous example is not repeated here""""""
def
updateFromUI
(
action
## ):
""""""get the property value as String and split it using newline character""""""
value
=
property
.
getValue
()
lines
=
[]
if
value
!=
## None
## :
lines
=
value
.
split
(
""
\n
""
)
""""""for 'Add' action add a new line with values from input fields""""""
if
action
.
getName
()
==
## 'Add'
## :
newLine
=
extractNewLineFromActionInput
(
action
)
lines
.
append
(
newLine
)
elif
action
.
getName
()
==
## 'Edit'
## :
""""""
For 'Edit' action find the line corresponding to selected row
and replace it with a line with values from input fields.
NOTE: line index is one bigger than selected row index because of header.
""""""
lineIndex
=
action
.
getSelectedRows
()[
0
]
+
1
lines
.
pop
(
lineIndex
)
newLine
=
extractNewLineFromActionInput
(
action
)
lines
.
insert
(
lineIndex
,
newLine
)
elif
action
.
getName
()
==
## 'Delete'
## :
""""""
For 'Delete' action delete the lines corresponding to selected rows.
NOTE: deletion of rows is implemented here in reversed order
""""""
rowIds
=
list
(
action
.
getSelectedRows
())
rowIds
.
reverse
()
for
rowId
in
rowIds
## :
lines
.
pop
(
rowId
+
1
)
""""""in the end update the property value concatenating all the lines""""""
value
=
""
\n
""
.
join
(
lines
)
property
.
setValue
(
value
)
def
extractNewLineFromActionInput
(
action
## ):
inputValues
=
[]
for
input
in
action
.
getInputWidgetDescriptions
## ():
inputValue
=
""""
if
input
.
getValue
## ():
inputValue
=
input
.
getValue
()
inputValues
.
append
(
inputValue
)
return
"",""
.
join
(
inputValues
)
updateFromBatchInput(), batchColumNames() and inputWidgets()

All examples assume batch upload of a CSV/TSV file in openBIS for an
entity type which has managed properties.
## Example 6

This example assumes one column in the file for the managed property.
def
updateFromBatchInput
(
bindings
## ):
property
.
setValue
(
'hello '
+
bindings
.
get
(
''
))
def
configureUI
## ():
builder
=
createTableBuilder
()
builder
.
addHeader
(
## 'Greetings'
)
row
=
builder
.
addRow
()
row
.
setCell
(
## 'Greetings'
,
property
.
getValue
())
property
.
getUiDescription
()
.
useTableOutput
(
builder
.
getTableModel
())
The following input file for a batch upload for samples of a type where
property
## MANGED-TEXT
## MANGED-TEXT
world
universe
would create in sample detailed view
## Example 7

This example takes two columns from the batch input file for creation of
one managed property.
def
batchColumnNames
## ():
return
[
## 'Unit'
,
## 'Value'
]
def
updateFromBatchInput
(
bindings
## ):
property
.
setValue
(
bindings
.
get
(
## 'VALUE'
)
+
' ['
+
bindings
.
get
(
## 'UNIT'
)
+
']'
)
def
configureUI
## ():
builder
=
createTableBuilder
()
builder
.
addHeader
(
## 'Value'
)
builder
.
addRow
()
.
setCell
(
## 'Value'
,
property
.
getValue
())
property
.
getUiDescription
()
.
useTableOutput
(
builder
.
getTableModel
())
Assuming a sample type is assigned to the property
## MANAGED-TEXT
with
this script. On the batch upload form a click on
Download file
template
would return a template file like the following one:
# The ""container"" and ""parents"" columns are optional, only one should be specified.
# ""container"" should contain a sample identifier, e.g. /SPACE/SAMPLE_1, while ""parents"" should contain comma separated list of sample identifiers.
# If ""container"" sample is provided, the registered sample will become a ""component"" of it.
# If ""parents"" are provided, the registered sample will become a ""child"" of all specified samples.
# The ""experiment"" column is optional, cannot be specified for shared samples and should contain experiment identifier, e.g. /SPACE/PROJECT/EXP_1
identifier  container   parents experiment  MANAGED-TEXT:UNIT   MANAGED-TEXT:VALUE
Instead of one property column
## MANAGED-TEXT
there are two columns for
each value of the list returned by the script function
batchColumnNames
.
Uploading the following file for such a sample type
identifier container   parents experiment  MANAGED-TEXT:UNIT   MANAGED-TEXT:VALUE
/test/sample-with-managed-property-1                mm  1.56
/test/sample-with-managed-property-2                sec 47.11
would lead to a detailed view as in the following screenshot:
If the flag
Shown in Edit Views
is set and flag
Show Raw Value in
## Forms
is not set, the registration form would have a field called
‘Managed Text’ with initially one section with input field ‘Unit’ and
## ‘Value’:
With ‘+’ and ‘Add More’ button additional sections can be created.
Existing sections can be deleted by the ‘-’ button. The section fields
are all non-mandatory single-line fields with labels specified by the
batch column names. More is possible if the function
batchColumnsNames
is replaced by function
inputWidgets
as in the following example:
def
inputWidgets
## ():
factory
=
inputWidgetFactory
()
unit
=
factory
.
createComboBoxInputField
(
## 'Unit'
,
[
'cm'
,
'mm'
])
.
setMandatory
(
## True
)
value
=
factory
.
createTextInputField
(
## 'Value'
)
.
setMandatory
(
## True
)
return
[
unit
,
value
]
The field ‘Managed Text’ in the registration form will be as shown in
the following screen shot:
Both fields are mandatory and the first field is a combo box with the
two elements ‘cm’ and ‘mm’.
HTML Output

In addition to table output, a managed property may produce HTML output.
Here is a (overly) simple example:
## Example 8

def
configureUI
## ():
property
.
getUiDescription
()
.
useHtmlOutput
(
""<p>hello<br>foo</p>""
)
Accessing information about a person that performs an update operation

## Example 9

This example shows how information about a person that performs an
update operation can be access in a managed property script. The
information is stored in the ‘person’ variable that is available in both
‘updateFromUI’ and ‘updateFromBatchInput’ functions.
def
updateFromBatchInput
(
bindings
## ):
property
.
setValue
(
'userId: '
+
person
.
getUserId
()
+
', userName: '
+
person
.
getUserName
())
Storing structured content in managed properties

By “structured” properties we understand properties holding complex data
structures in their values. Typically, the Jython scripts handling such
properties need to implement a conversion strategy transforming the data
structure held into a property value and vice versa. To facilitate this
task, openBIS offers an API allowing the users to easily create an
abstract data structure and persist it as a single property.
Supported structure elements

The abstract data structure supported by the API is basically a list of
IElement
-s.
IElements are named objects optionally having associated key-value
attributes and optionally containing other IElements. Additionally,
IElements can also be used as containers for larger chunks of raw data.
To construct concrete instances of IElement one has to use
IElementFactory
available via the predefined function
elementFactory()
.
Linking to openBIS entities

Astute readers may have already noticed that the
IElementFactory
also offers methods that create
IEntityLinkElement
instances. An IEntityLinkElement denotes a link to another object in
openBIS.
Currently the entity links can lead to entities that don’t exist in the
database. Such links are displayed as text instead of html links in
grids defined for managed properties.
Converting to property value

Once you a have created the desired data structure in form of
IElement
-s,
you can use an
IStructuredPropertyConverter
to convert it to a property value. An instance of
IStructuredPropertyConverter
can be created from the
#Predefined
## Functions
.
Managed properties can be stored either as XML Strings or as JSON
Strings. The script writer makes the decision for a serialization type
by either calling
xmlPropertyConverter()
or
jsonPropertyConverter()
.
Note that both converters can convert from XML or JSON Strings
to
IElement
lists, detecting automatically which type of String they
get. The two converters only differ in what type of serialization they
use when converting from
List<IElement>
to a String. By this mechanism
it is even possible to change the serialization type after values of the
managed property have been created and stored in the database without
breaking the functionality of managed properties. To maintain this
transparency it is recommended that API users avoid parsing the XML or
JSON Strings themselves and let the converter do the job.
Jython example script

Sometimes a few lines of code are worth a thousand words. Have a look at
the example code below. It is extracted from a Jython script and
demonstrates the basics of constructing and serializing structured
content within a managed property.
factory
=
elementFactory
()
converter
=
xmlPropertyConverter
()
def
initialCreationOfPropertyValue
## ():
""""""
Create an element data structure containing
1) A link to Sample with specified perm id
2) A link to Material with specified type and typCode
3) An application specific element ""testElement""
""""""
elements
=
[
factory
.
createSampleLink
(
""samplePermId""
),
factory
.
createMaterialLink
(
""type""
,
""typeCode""
),
factory
.
createElement
(
""testElement""
)
.
addAttribute
(
""key1""
,
""value1""
)
.
addAttribute
(
""key2""
,
""value2""
)
]
# save the created data structure into the property value
property
.
value
=
converter
.
convertToString
(
elements
)
def
updateDataStructure
## ():
""""""
This function imitates an update procedure. The content of the property
is parsed to a list of elements, several modifications are made on the elements
and these are then saved back in the property.
""""""
# read the stored data structure
elements
=
list
(
converter
.
convertToElements
(
property
))
# we assume, the contents from the ""create..."" method above
elements
[
0
]
=
factory
.
createSampleLink
(
""modifiedLink""
)
elements
[
1
]
.
children
=
[
factory
.
createElement
(
""nested1""
)
.
addAttribute
(
""na1""
,
""na2""
)
]
# replaces the old value of the ""key2"" attribute
elements
[
2
]
.
addAttribute
(
""key2""
,
""modifiedvalue""
)
# update the property value to reflect the modified data structure
property
.
value
=
converter
.
convertToString
(
elements
)
At the end of the function
initialCreationOfPropertyValue()
, the
variable
property.value
will contain an XML representation of the
created data structure, which will look like
<root>
## <Sample
permId=
""samplePermId""
/>
## <Material
permId=
""type (typeCode)""
/>
<testElement
key1=
""value1""
key2=
""value2""
/>
</root>
The function
updateDataStructure()
assumes that the
initialCreationOfPropertyValue()
has already been called and modifies
the data structure to what would translate to the following XML snippet:
<root>
## <Sample
permId=
""modifiedLink""
/>
## <Material
permId=
""type (typeCode)""
>
<nested1
na1=
""na2""
/>
## </Material>
<testElement
key1=
""value1""
key2=
""modifiedvalue""
/>
</root>
Unofficial API

In addition to the variable
property
, the variable
propertyPE
is
also available to managed property scripts. Its use is not officially
supported and code that uses it is not guaranteed to work after an
upgrade of openBIS, but it can be used to get access to useful
information that is not available through the official API.
‘Real World’ example

The following example shows a complete implementation of a managed
property script for handling list of log entries. The property value is
stored as an XML document.
from
java.util
import
## Date
""""""
Example XML property value handled by this script:
<root>
<logEntry date=""2011-02-20 14:15:28 GMT+01:00"" person=""buczekp"" logType=""INFO"">Here is the 1st log entry text.<logEntry>
<logEntry date=""2011-02-20 14:16:28 GMT+01:00"" person=""kohleman"" logType=""WARN"">Here is the 2nd log entry text - a warning!<logEntry>
<logEntry date=""2011-02-20 14:17:28 GMT+01:00"" person=""tpylak"" logType=""ERROR"">Here is the 3rd log entry text - an error!!!<logEntry>
<logEntry date=""2011-02-20 14:18:28 GMT+01:00"" person=""brinn"" logType=""ERROR"">Here is the 4th log entry text - an error!!!<logEntry>
<logEntry date=""2011-02-20 14:19:28 GMT+01:00"" person=""felmer"" logType=""WARN"">Here is the 5th log entry text - a warning!<logEntry>
</root>
""""""
## LOG_ENTRY_ELEMENT_LABEL
=
'logEntry'
## LOG_TYPES
=
[
## 'INFO'
,
## 'WARN'
,
## 'ERROR'
]
"""""" labels of table columns and corresponding input fields """"""
## DATE_LABEL
=
## 'Date'
## PERSON_LABEL
=
## 'Person'
## LOG_TYPE_LABEL
=
## 'Log Type'
## LOG_TEXT_LABEL
=
## 'Log Text'
"""""" names of attributes of XML elements for log entries """"""
## DATE_ATTRIBUTE
=
'date'
## PERSON_ATTRIBUTE
=
'person'
## LOG_TYPE_ATTRIBUTE
=
'logType'
"""""" action labels (shown as button labels in UI) """"""
## ADD_ACTION_LABEL
=
## 'Add Log Entry'
## EDIT_ACTION_LABEL
=
## 'Edit'
## DELETE_ACTION_LABEL
=
## 'Delete'
def
configureUI
## ():
""""""Create table builder and add headers of columns.""""""
builder
=
createTableBuilder
()
builder
.
addHeader
(
## DATE_LABEL
,
250
)
# date and log text values are long, override default width (150)
builder
.
addHeader
(
## PERSON_LABEL
)
builder
.
addHeader
(
## LOG_TYPE_LABEL
)
builder
.
addHeader
(
## LOG_TEXT_LABEL
,
300
)
""""""
Extract XML elements from property value to a Python list.
For each element (log entry) add add a row to the table.
""""""
elements
=
list
(
xmlPropertyConverter
()
.
convertToElements
(
property
))
for
logEntry
in
elements
## :
row
=
builder
.
addRow
()
row
.
setCell
(
## DATE_LABEL
,
## Date
(
long
(
logEntry
.
getAttribute
(
## DATE_ATTRIBUTE
))))
row
.
setCell
(
## PERSON_LABEL
,
logEntry
.
getAttribute
(
## PERSON_ATTRIBUTE
))
row
.
setCell
(
## LOG_TYPE_LABEL
,
logEntry
.
getAttribute
(
## LOG_TYPE_ATTRIBUTE
))
row
.
setCell
(
## LOG_TEXT_LABEL
,
logEntry
.
getData
())
""""""Specify that the property should be shown in a tab and set the table output.""""""
property
.
setOwnTab
(
## True
)
uiDescription
=
property
.
getUiDescription
()
uiDescription
.
useTableOutput
(
builder
.
getTableModel
())
""""""
Define and add actions with input fields used to:
1. specify attributes of new log entry,
""""""
addAction
=
uiDescription
.
addTableAction
(
## ADD_ACTION_LABEL
)
\
.
setDescription
(
'Add a new log entry:'
)
widgets
=
[
inputWidgetFactory
()
.
createComboBoxInputField
(
## LOG_TYPE_LABEL
,
## LOG_TYPES
)
\
.
setMandatory
(
## True
)
\
.
setValue
(
## 'INFO'
),
inputWidgetFactory
()
.
createMultilineTextInputField
(
## LOG_TEXT_LABEL
)
\
.
setMandatory
(
## True
)
]
addAction
.
addInputWidgets
(
widgets
)
""""""
2. modify attributes of a selected log entry,
""""""
editAction
=
uiDescription
.
addTableAction
(
## EDIT_ACTION_LABEL
)
\
.
setDescription
(
'Edit selected log entry:'
)
# Exactly 1 row needs to be selected to enable action.
editAction
.
setRowSelectionRequiredSingle
()
widgets
=
[
inputWidgetFactory
()
.
createMultilineTextInputField
(
## LOG_TEXT_LABEL
)
.
setMandatory
(
## True
)
]
editAction
.
addInputWidgets
(
widgets
)
# Bind field name with column name.
editAction
.
addBinding
(
## LOG_TEXT_LABEL
,
## LOG_TEXT_LABEL
)
""""""
3. delete selected log entries.
""""""
deleteAction
=
uiDescription
.
addTableAction
(
## DELETE_ACTION_LABEL
)
\
.
setDescription
(
'Are you sure you want to delete selected log entries?'
)
# Delete is enabled when at least 1 row is selected.
deleteAction
.
setRowSelectionRequired
()
def
updateFromUI
(
action
## ):
""""""Extract list of elements from old value of the property.""""""
converter
=
xmlPropertyConverter
()
elements
=
list
(
converter
.
convertToElements
(
property
))
""""""Implement behaviour of user actions.""""""
if
action
.
name
==
## ADD_ACTION_LABEL
## :
""""""
For 'add' action create new log entry element with values from input fields
and add it to existing elements.
""""""
element
=
elementFactory
()
.
createElement
(
## LOG_ENTRY_ELEMENT_LABEL
)
""""""Fill element attributes with appropriate values.""""""
element
.
addAttribute
(
## DATE_ATTRIBUTE
,
str
(
## Date
()
.
getTime
()))
# current date
element
.
addAttribute
(
## PERSON_ATTRIBUTE
,
action
.
getPerson
()
.
getUserId
())
# invoker the action
""""""Retrieve values from input fields filled by user on the client side.""""""
element
.
addAttribute
(
## LOG_TYPE_ATTRIBUTE
,
action
.
getInputValue
(
## LOG_TYPE_LABEL
))
""""""Set log text as a text element, not an attribute.""""""
element
.
setData
(
action
.
getInputValue
(
## LOG_TEXT_LABEL
))
""""""Add the new entry to the end of the element list.""""""
elements
.
append
(
element
)
elif
action
.
name
==
## EDIT_ACTION_LABEL
## :
""""""
For 'edit' action find the log entry element corresponding to selected row
and replace it with an element with values from input fields.
""""""
selectedRowId
=
action
.
getSelectedRows
()[
0
]
elements
[
selectedRowId
]
.
setData
(
action
.
getInputValue
(
## LOG_TEXT_LABEL
))
elif
action
.
name
==
## DELETE_ACTION_LABEL
## :
""""""
For 'delete' action delete the entries that correspond to selected rows.
NOTE: As many rows can be deleted at once it is easier to delete them in reversed order.
""""""
rowIds
=
list
(
action
.
getSelectedRows
())
rowIds
.
reverse
()
for
rowId
in
rowIds
## :
elements
.
pop
(
rowId
)
else
## :
raise
ValidationException
(
'action not supported'
)
""""""Update value of the managed property to XML string created from modified list of elements.""""""
property
.
value
=
converter
.
convertToString
(
elements
)
Creating and Deploying Java Plugins

To create valid Java plugin for Managed Properties, one should create a
class that is
implementing
ch.systemsx.cisd.openbis.generic.shared.managed_property.api.IManagedPropertyHotDeployEvaluator
interface.
The class should be annotated
with
ch.ethz.cisd.hotdeploy.PluginInfo
annotation specifying the name
of the plugin,
and
ch.systemsx.cisd.openbis.generic.shared.managed_property.api.IManagedPropertyHotDeployEvaluator
class
as a plugin type.
Such a plugin should be exported to a jar file and put
into
<<openBIS
installation
directory>>/servers/entity-related-plugins/managed-properties
directory.
The plugin will be detected automatically and will be automatically
available to openBIS. No restart is needed.
No labels",Properties Handled By Scripts,0,en_20.10.0-11_user-documentation_general-admin-users_properties-handled-by-scripts_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-admin-users_properties-handled-by-scripts.txt,2025-09-30T12:09:09.971680Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_additional-functionalities:0,Additional Functionalities,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/additional-functionalities.html,openbis,"## Additional Functionalities

Print PDF

For every entity in openBIS it is possible to generate a pdf using the
Print PDF
option from the
## More..
dropdown menu.
The generated pdf file can be printed or downloaded from the browser.
An example for a Space is shown in the picture below.
## Visualise Relationships

Parent-child relationships between
## Objects
can be visualised as trees
or tables in the ELN.
To see the genealogical tree, select the
## Hierarchy Graph
option from the
## More…
dropdown in an entity form.
Large trees can be pruned, by selecting how many levels of parents
and/or children and which types to show.
To view the genealogy of an
## Object
in a tabular format, select the
## Hierarchy Table
option from the
## More…
dropdown.
## Tables

All tables in the ELN have a similar format and functionalities. The
tables have been redesigned for the 20.10.3 release of openBIS.
Here we give an overview of the main functionalities of the tables.
## Filters

Two filter options are available form the
## Filters
## button:
## Filter
## Per Column
and
## Global Filter
. The first allows to filter on
individual columns, or multiple columns, whereas the second filters
terms across the entire table using the
## AND
or
## OR
operator.
## Sorting

It is possible to sort individual columns or also multiple columns. For
multi-column sorting, you should click on the column header and press
the
## Cmd
keyboard key. The order of sorting is shown by a number in
each column, as shown below.
## Exports

Tables can be exported in different ways, using the export button shown
below.
## Import Compatible
## :
## Yes
: in this case some columns which are incompatible
with imports (i.e. registration date, registrator,
modification date, modifier) are not exported even if
selected; some columns that are required by openBIS for
imports are added to the exported file even if not
selected (i.e. code, identifier, $ column). Moreover text
fields are exported in HTML, to keep the formatting upon
import.
## No
: in this case all columns or selected columns are
exported.
2.
## Columns
## :
All (default order)
. All columns are exported, in
accordance with the selection explained above for import
compatibility.
Selected (shown order)
. Selected columns are exported,
in accordance with the selection explained above for
import compatibility.
3.
## Rows
## :
## All Pages
. All pages of the table are exported.
## Current Page
. Only the currently visible page of
the table is exported.
## Selected Rows
. Only selected rows in the table are
exported.
4.
## Value
## :
## Plain Text
. Text fields are exported in plain text,
without any formatting. This option is not available if
the export is import-compatible.
## Rich Text
. Text fields are exported in HTML format.
Tables are exported to
## XLS
format. Exported tables can be used for
updates via the
XLS Batch Update Objects
.
Note: Excel has a character limit of 32767 characters in each cell. If you export entries where a field exceeds this limit, you get a warning and the exported Excel file will not contain the content of the cell which is above this limit and the cell is highlighted in red, as shown below.
## Columns

Users can select which properties to display in the table clicking on
the
## Columns
button. It is also possible to show all properties or
hide all properties. The position of the columns can also be changed by
placing the cursor next to the = sign in the list and moving the fields.
This information is stored in the database for each user.
## Spreadsheets

If a table contains
## Objects
which have a spreadsheet field which is
filled in, a spreadsheet icon is displayed in the table. Upon clicking
on the icon, the content of the spreadsheet can be expanded.
Text fields

If a table contains Objects which have long text fields, only the
beginning of the text is shown and can be expanded. If the text contains
a picture or a table, an icon is shown in the table and the content of
the text becomes visible by clicking on the icon.
Selection of entries in table

Single entries in a table can be selected using the checkbox in the row.
By clicking the checkbox in the table header, all entries of the table
## are selected. After selection of entries, some actions become available:
## Delete
: allows to move the selected entries to the trashcan.
## Move
: allows to move the selected entries to a different existing
## Collection/Experiment
or to a new one.
Generate barcodes
: allows to generate custom barcodes for the
selected entries.
Update custom barcodes/QR codes
: allows to update existing custom
barcodes of the selected entries.
Clear selection
: allows to clear the selection made.
## In
## Object
tables inside
## Experiments/Collections
there is an
## Operations
column, which allow users to perform certain tasks on an
## Object
## :
Upload a file to the
## Object
Move the
## Object
to another exiting
## Experiment/Collection
.
Update Barcode/QR code.
Open the hierarchy graph. This is the graph showing parent/child
connections of the
## Object
.
Open the hierarchy table. This is the table showing parent/child
connections of the
## Object
.
Browse Entries by Type

## The
## Object Browser
under the
## Utilities
main menu allows to see
all entries of the same type and all
## Experimental Steps
, which may be
contained in different
## Experiments/Collections
and
## Projects
.
This is useful when there are entries of a certain type that belong to
different
## Collections
(e.g. protocols of the same type stored in two
different protocol collections), or to have an overview of all
## Experimental Steps
, independently of the
## Experiment
they belong to.
From the
## Object Browser
page, it is also possible to
## Batch
register
or
Batch update
## Objects
using an XLS or TSV template.
## Trashcan

## When
## Experiments
,
## Objects
and
## Datasets
are deleted, they are moved
to the openBIS
trashcan
, under the
## Utilities
main menu. Items
can be removed from the trashcan only by someone with
Space admin
or
Instance admin
role. Deletion from the trashcan is
## IRREVERSIBLE
.
## Note:
## Spaces
and
## Projects
are directly permanently deleted, they are
not moved to the trashcan first.
To empty the whole trashcan, click the blue
## Empty Trash
button above the table.
To delete permanently single entries choose one of two options from the
## Operations dropdown:
delete permanently
: deletes permanently only the selected entry.
delete permanently (including dependent deletions)
: if the
selected entry had children which are also in the trashcan, this
option allows to permanently delete both the entry and its children.
If one entity was unintentionally deleted, the operation can be reverted
at this stage by choosing the
## Revert Deletions
option from
the
## Operations
drop down in the table.
## Visualize Available Storage Space

The storage space available in an openBIS instance can be visualized by navigating to
## Other Tools
in the navigation menu and clicking on the
Show available storage space
button.
Before uploading large datasets, the available storage space should always be checked.
## Vocabulary Browser

## The
Vocabulary browser
is accessible from the
## Utilities
main
menu. This shows all controlled vocabularies registered in openBIS and
the terms they contain. Vocabularies are predefined lists of values to
choose from in given fields. Vocabularies can be created/modified by an
openBIS
Instance admin
(see
## New Entity Type
## Registration
).
This information is needed for filling the forms for
## Batch
## Upload
or
## Batch Update
of
## Objects
via TSV file. If an
## Object
has a property of type
## Controlled Vocabulary
, the codes of the
vocabulary have to be entered in the .tsv template file. This is not the
case for XLS Batch registration or update, where labels can be used.
## Freeze Entities

Each level of the openBIS hierarchy (Space, Project,
Experiment/Collection, Object, Dataset) can be frozen, so it can be no
longer edited and/or deleted.
At every level, everything contained underneath is selected by default
to be frozen. E.g. if I choose to freeze a Space, everything contained
in the Space is automatically selected to be frozen. Single entities can
be manually unselected.
A Space admin role is necessary to freeze entities in a given Space.
IMPORTANT: the freezing is IRREVERSIBLE!
This operation cannot be undone from any UI, not even by an
Instance admin.
Please freeze entities only when you are absolutely sure that
they should not be further modified!
How to freeze an entity

At each level of the openBIS hierarchy (
## Space, Project,
## Experiment/Collection, Object, Dataset
) the
## Freeze Entity
option is
available under the
## More..
dropdown menu. See the example for a
## Space
below.
If you select this, a list of entities contained or connected to the one
selected will be presented to you, as shown below. By default everything
is selected, so you need to unselect entries that you do not want to
freeze.
To freeze one or several entities, you need to provide your login
password and save.
Rules for freezing
Freeze Space only
## Allowed
Not allowed
Create new Project
x
Create new Experiment/Collection
x
Create new Object
x
Create new Dataset in existing Experiment/Collection
x
Create new Dataset in existing Object
x
Edit existing Project
x
Edit existing Experiment/Collection
x
Edit existing Object
x
Edit existing Dataset
x
## Delete Space
x
## Delete Project
x
## Delete Experiment/Collection
x
## Delete Object
x
## Delete Dataset
x
## Move Experiment/Collection
x
## Move Object
x
## Copy Object
x
## Export
x
Freeze Project only
## Allowed
Not allowed
Create new Experiment/Collection
x
Create new Object
x
Create new Dataset in existing Experiment/Collection
x
Create new Dataset in existing Object
x
## Edit Project
x
Edit existing Experiment/Collection
x
Edit existing Object
x
Edit existing Dataset
x
## Delete Project
x
## Delete Experiment/Collection
x
## Delete Object
x
## Delete Dataset
x
## Move Experiment/Collection
x
## Move Object
x
## Copy Object
x
## Export
x
3. Freeze Experiment/Collection only
## Allowed
Not allowed
Create new Object
x
Create new Dataset in existing Experiment/Collection
x
Create new Dataset in existing Object
x
Edit existing Experiment/Collection
x
Edit existing Object
x
Edit existing Dataset
x
## Delete Experiment/Collection
x
## Delete Object
x
## Delete Dataset
x
## Move Experiment/Collection
x
## Move Object
x
## Copy Object
x
## Export
x
4. Freeze Object only
## Allowed
Not allowed
Create new Dataset in existing Object
x
Edit existing Object
x
Edit existing Dataset in Object
x
## Delete Object
x
## Delete Dataset
x
## Move Object
x
## Copy Object
x (only if the Experiment is not frozen)
## Export
x
5. Freeze Dataset only
## Allowed
Not allowed
Edit existing Dataset
x
## Delete Dataset
x
## Move Dataset
x
## Export
x
Navigation menu

openBIS 20.10.6 features a new navigation menu.
This has the following functionalities:
## 1.Filter
. You can filter the menu by names or codes.
2. Root nodes
. If you do not want to navigate the full menu, but
only a section of it, you can set the section you want to navigate as
root node, by clicking the icon shown in the picture below.
This now becomes the root node, as shown below. To restore the full menu
view, you can click on the root node icon shown below.
## 3. Sorting
. The default sorting of the menu is in alphabetical. It
is now possible to sort separately individual sections of the menu
(
ELN, Inventory, Stock
) and individual nodes inside those sections. It
is possible to do a custom sorting by moving around (drag&drop) entities
in the menu. Please note that this is only possible inside a given
level, i.e. you can re-organise
## Objects
inside a
## Collection/Experiment
;
## Collections/Experiments
inside a
## Project
;
## Projects
inside a
## Space
. However, you cannot move entities from one
level to another, i.e. you cannot move an
## Object
to a different
## Collection/Experiment
; a
## Collection/Experimen
t to a different
## Project
; a
## Project
to a different
## Space
. This can only be done
from the
## Move
option under the
## More..
dropdown menu in the
forms.
## 4. Collapse/Expand.
The full menu or individual nodes can be
expanded or collapsed, with the button shown below.
5. Scroll to selected node
. In some cases, the view in the main ELN
page does not correspond to an entry selected in the menu. You can
scroll to the selected node in the menu, using the button shown below.
The state of the menu is saved. Every time you change something in the
menu, this change will be saved and when you login next time you will
see the menu in the state you last saved it.
## Custom Imports

From openBIS version 20.10.4, Custom Imports, previously available only
in the core UI, are available in the ELN UI.
Custom imports allow users to import metadata in a custom way, by using
a dropbox script in the background. You can use this if you want to
parse a file in a given format and import the information from this file
as metadata in openBIS.
Custom imports are not available by default, but need to be enabled on
the server side by a
system admin
, and a dropbox script needs to be
associated with an import (see
## Custom
## Imports
).
If one or more custom imports are configured in openBIS, the
## Custom
## Import
option is available under the
## Utilities
in the
main
menu
.
The available custom imports can be selected from the
## Custom Import
## Service
drop down menu in the Custom Import page (see below).
If the available custom import provides a template that can be used as
input for the import, the template will be available to download from
the Custom Import page.
If the custom import is not configured to provide a template, no
download link is shown in the Custom Import page.
Entity history

Whenever an entity of type
## Collection/Experiment
,
## Object
or
## Dataset
is modified in openBIS, the changes are stored in the
database. The stored changes are modifications to property fields,
addition and deletion of parents/children for
## Objects
and
## Datasets
,
changes of
## Space/Project/Experiment/Object
ownership if an entity is
moved.
## The
## History
of changes of each entity is now available in the ELN
UI. In versions prior to openBIS 20.10.3 this was only available in the
core UI.
History table for Collections

In a
## Collection
page, the
## History
can be accessed from the
## More..
dropdown list.
## The
## History
table shows the version number of the changes, the
author of the changes, the changes made (with the values before- in red,
and after the change – in green), and the timestamp, i.e. the time when
the changes were made.
For a
## Collection
, the
PermID
(Permanent Identifier) of the
## Project
it belongs to is shown. If a
## Collection
is moved from one
## Project
to another, the PermID of the old and new
## Projects
are shown
in the history table.
## The
show
option in
## Full Document
shows the full metadata of the
entry (not only the changed fields) when changes were applied. This is
displayed in JSON format.
History table for Objects

For every
## Object
, the history of changes can be accessed from the
## More..
dropdown on the
## Object
page.
For an
## Object
, the
PermID
(Permanent Identifier) of the
## Collection
it belongs to is shown. If an
## Object
is moved from one
## Collection
to another, the PermID of the old and new
## Collections
are
shown in the history table.
History table for Datasets

For every dataset, the history of changes can be accessed from the
## More..
dropdown on the
## Dataset
page.
For a
## Dataset
, the
PermID
(Permanent Identifier) of the
## Object
/
## Collection
it belongs to is shown. If a
## Dataset
is moved
from one
## Object
/
## Collection
to another, the PermID of the old and new
## Objects
/
## Collections
are shown in the history table.
## Spreadsheet

The spreadsheet component needs to be enabled by a group admin or lab manager who can edit the ELN Settings, as described here:
Enable Rich Text Editor or Spreadsheet Widgets
The spreadsheet supports some basic Excel functionalities, such as mathematical formulas (e.g. =SUM(A1+A2)).
It is possible to import an openBIS Object into the spreadsheet, with the
import
button, on the spreadsheet itself:
Please note that if the Object is updated in openBIS, it will NOT be automatically updated in the spreadsheet.
## Session Token

When users log in to openBIS, a session token is generated. The session token is visible in the ELN UI, under the
## User Profile
, in the
navigation menu
.
The session token is needed to connect to openBIS via pyBIS or obis, in cases where SSO (e.g. SWITCHaai) is used for authentication. See
pyBIS
.",Additional Functionalities,0,en_20.10.0-11_user-documentation_general-users_additional-functionalities_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_additional-functionalities.txt,2025-09-30T12:09:10.043391Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_barcodes:0,Barcodes and QR codes,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/barcodes.html,openbis,"Barcodes and QR codes

Barcodes and QR codes

The barcode functionality must be enabled in openBIS by a
lab manager
or
group admin
## :
## Enable
Barcodes and QR codes
.
Barcodes for individual samples

When a sample is registered, a barcode is automatically generated by
openBIS. This is found in the
Identification info
section, as shown
below.
This barcode can be printed and the label can be added to the vial
containing the sample. The option to print the barcode is under the
## More..
menu
If a sample already has its own barcode or QR code, it is possible to scan this with
a scanner or the camera of a mobile device and assign it to the sample.
This can be done after registration of a sample, with the
## Custom
Barcode/QR Code Update
option under the
## More..
drop down.
The custom barcode will appear in the
## Identification Info
. If a custom
barcode/QR code is registered, the print function shown above will print the
custom barcode /QR code, instead of the default one.
Generate batches of barcodes / QR codes

In some cases there is the need to generate several barcodes/QR codes that can be
later on assigned to samples registered in openBIS.
To generate new barcodes, go to the
Barcodes/QR codes Generator
in the main
menu under
## Utilities
.
Users can select:
The type of barcode to generate:
## Code 128
QR Code
Micro QR code
The number of barcodes to generate
The layout:
## Split
: one barcode per page
## Continuous
: several barcodes in one page
The width of the barcode
The length of the barcode
After selecting the desired parameters, click the
## Generate Custom
## Barcodes
button.
To print the barcodes use the
print icon
on the form, next to
Generate Custom Barcodes/QR Codes
. These barcodes can be printed on labels to
be attached to vials. When the samples are registered in openBIS, these
barcodes can be scanned and assigned to the samples as explained above.
Scan barcodes from mobile devices

It is also possible to scan barcodes and QR codes using the scan button
on top of the main menu, as shown below. In this way, you can scan a
barcode or QR code already associated with an entry and this will open
the entry page in openBIS. You can use a scanner or the camera of a
mobile device. The selection you make is saved.
Updated on July 5, 2023
Printer and Barcode Scanner Requirements

## Printers

There are several manufacturers of printers and different kinds of
barcodes and paper to adapt to different use cases. Most manufacturers
have their own proprietary printer driver and language for labels.
To allow freedom of choice for the barcode printer, the openBIS ELN-LIMS
allows to configure both the type of barcodes and the layout and size of
the labels. openBIS uses this information to produce a PDF document,
thus having as single requirement that the printer driver used allows to
print PDF documents using applications such as Adobe Acrobat Reader or
## Preview (Mac).
### Printer Configuration

There are different types of printer drivers. The two types we can
define as generic are
## PS
(PostScript) (recommended) and
## PCL
(Printer Command Language). Printers with these drivers are likely to
print PDF documents and other types of documents with embedded fonts,
images, etc…
The printer paper type needs to be configured for each printer. Two
## layouts are supported:
## Split
: The PDF will contain separate pages with each barcode.
## Continuous
: The PDF will contain a continuous layout with the
barcodes. More uncommon for this applications.
The printer paper size needs to be configured for each printer. It is
possible to indicate the size of the barcode, so it can fit.
Printer testing

We provide two example documents that can be used to test the printer.
## Split barcodes example PDF:
printer-test-code128-split-50-15
## Continuous barcodes example PDF:
printer-test-code128-continuous-50-15
Please consider that these examples likely do not correspond to the
particular paper size of the printer being evaluated and as such the
barcodes may look squashed. In order to obtain optimal results, the
paper size would need to be configured. However, for the test it is
enough to verify that the printer can print those files.
Printer Advice before purchasing

Before purchasing a printer, we recommend to check with the manufacturer
that the barcode printer provides a general driver and that it can print
one of the documents provided as example above.
## Tested Printers

Zebra ZD420
## Scanners

There are several manufacturers of barcode scanners. In most cases
scanners act as a keyboard for the computer, so when the barcode scanner
scans a barcode it will type whatever has been scanned.
### Scanner Configuration

The scanner keyboard layout should be the same as the computer used. If
not this could cause problems if there are any special characters.
Scanner testing

Open a notepad and scan the barcodes provided in the examples below. The
scanner should read them and type the correct output.
## Barcode Code 128.
scanner-test-code128-50-15
.
This should give as output “20210720122856003-454071” without
quotes.
Barcode QR Code.
scanner-test-qrcode-50-50
.
This should give as output “20210720122856003-454071” without
quotes.
Barcode Micro QR Code.
scanner-test-microqrcode-30-30
.
This should give as output “20210720122856003-454071” without
quotes.
Scanner Advice before purchasing

Before purchasing a scanner, ensure that the barcode scanner provides a
keyboard driver and ask the manufacturer’s support to scan the examples
above.
## Tested Scanners

Honeywell 1902G-BF
Updated on July 27, 2022",:,0,en_20.10.0-11_user-documentation_general-users_barcodes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_barcodes.txt,2025-09-30T12:09:10.116184Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_data-archiving:0,Data archiving,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-archiving.html,openbis,"Data archiving

Dataset archiving

openBIS supports archiving of datasets to Strongbox
(
https://www.strongboxdata.com/
) as
described in
## Datasets
## Archiving
This needs to be set up and configured on
system level
.
To trigger archiving manually from the ELN, navigate to a dataset and
use the
Request or disallow archiving
button, as shown below.
Please note that the strongbox has a minimum size requirement of
## 10GB
. If a single dataset is below this threshold it will be queued
for archiving and it will be archived only when additional datasets in
the same
## Space/Project/Experiment
are selected for archiving and the
minimum size is reached. All datasets are bundled together and archived
together. This implies that if unarchiving is requested for one dataset
in a bundle, all other datasets will also be unarchived.
Dataset archiving helper tool

If you wish to archive multiple datasets, you can use the
## Archiving
## Helper
tool under
## Utilities
in the main menu. You can search for
datasets and select multiple ones to be archived, by clicking the
## Request Archiving
button on the top of the page.
It is possible to search datasets by size, by selecting
## Property
in
the
## Field Type
,
Size (bytes)[ATTR.SIZE]
in the
## Field Name
and the desired
Comparator Operator
, as shown below.
Dataset unarchiving

Once the dataset is archived on tapes, the button on the dataset page
changes to
## Unarchive
, as shown below. Datasets can be unarchived by
using this button.
Dataset unarchiving helper tool

To unarchive several datasets it is possible to use the
## Unarchiving
## Helper
tool, under
## Utilities
in the main menu, as shown below. You
can search for datasets and select multiple ones to be unarchived, using
the
## Unarchive
button on tope of the page.
Updated on April 25, 2023",Datasets,0,en_20.10.0-11_user-documentation_general-users_data-archiving_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_data-archiving.txt,2025-09-30T12:09:10.178585Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_data-export:0,Export,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-export.html,openbis,"## Export

Export to File

## Export Lab Notebooks & Inventory Spaces

All levels of the
## Lab Notebook
and
## Inventory
can be exported, using
the
## Export
option in the
## More..
drop down, as shown below.
## Space
## Project
## Experiment/Collection
## Object
## Dataset
In each case, the following export options are available:
Make import compatible
. If selected, datasets are exported in a
data
folder and are in a format ready to be uploaded in openBIS using the default eln-lims dropbox; the metadata are exported in a
xlsx
folder which contains information in a format ready to be uploaded via the openBIS admin UI.
Export metadata as PDF
. Metadata are exported in a
hierarchy
folder that keeps the folder structure of the ELN. At each level, one pdf file for each exported entity is generated.
Export metadata as XLSX
. Metadata are exported in one
xlsx
folder. The folder contains the metadata of all exported entities and the corresponding masterdata in a
metadata.xlsx
file. If
Make import compatible
is selected, this file is suitable for re-import in openBIS. If not, the file contains some fields which are not compatible with re-imports. These fields are: PermId of entities, registrator, registration date, modifier, modification date. In addition to the metadata.xlsx file, the
xlsx
## folder might contain additional folders:
a
scripts
folder, which contains scripts associated with types in the metadata.xlsx file, if these are present;
a
data
folder which holds the content of spreadsheet fields and large text fields that exceed the size of an Excel cell;
a
miscellaneous
folder which contain images embedded in text of exported entries, if present.
Export data
. The default maximum size of all datasets to be exported is 10GB. This can be configured by a system admin in the
AS service.properties file
. We recommend to use
sftp
to download large datasets.
## If
Make import compatible
is selected, datasets are exported in a
data
folder in a format ready to be uploaded in openBIS using the default eln-lims dropbox. If not, the datasets are exported in a
hiearchy
folder that matches the ELN hierarchy.
Include levels below from same space
. If selected, all hierachy levels below the selected entity and belonging to the same Space are exported.
Include Object and Dataset parents from same space
. If selected, Object parents and Dataset parents from the same Space are exported. Example: I export Object A, in Experiment A, in Space 1. Object B in Experiment B also in Space 1 is parent of Object A. When this option is selected, Object B is also exported, otherwise it is not.
Include Objects and Datasets parents and children from different spaces
. This allows to export Object and Dataset parents and children that belong to a different Space than the Space from where Objects and Datasets are being exported. Example: I export Object A in Space 1, which has parents in Space 2. If this option is selected, the parents in Space 2 are also exported, otherwise they are not.
Wait for download to complete in browser
. This is suitable when exporting only metadata or small datasets. When the dowload is ready, a zip file will be available to download from the browser.
Note: ensure that pop-ups are not disabled in your browser
.
Receive results by email
. If this option is selected, when the export is ready, you will receive an email notification with a download link.  Email notification needs to be configured on
system level
during or after installation, as explained in
Configure Data Store
## Server
We provide below a couple of examples of the export, to clarify how it works.
1. Import-compatible export of a Space selecting all options

We select all options from the export widget, as shown below.
We export a Space called CATERINA in the Lab Notebook with all its sublevels (see below).
One Object in this Space has a parent in a Space called METHODS (see below).
The exported zip file contains 3 folders:
## A.
data
folder
This contains the datasets in the correct format to be uploaded via eln-lims dropbox, as shown below.
## B.
hiearchy
folder
This contains folders that match the openBIS hierarchy (Space/Project/Experiment/Object).
In this case 2 Space folders are present:
## CATERINA
: is the exported space.
## METHODS
: contains an Object which is parent of an Object in the Space CATERINA. This was exported because the option
Include Objects and Datasets parents and children from different spaces
was selected for export.
Inside each folder, there is a pdf of the corresponding entity. Example:
in the Space folder
## CATERINA
there is a
CATERINA.pdf
file that contains the metadata of the Space;
in the Project folder
## PROJECT_1
there is a
PROJECT_1.pdf
file that contains the metadata of the Project;
in the Experiment folder
My second experiment (PROJECT_1_EXP_1)
there is a
My second experiment (PROJECT_1_EXP_1).pdf
file with the metadata of the Experiment;
in the Object folder
Step A (EXP4)
there is a
Step A(EXP4).pdf
file with the metadata of the Object and a
20240726094631217-68.pdf
file that contains the metadata of the dataset that belongs to this Object.
## C.
xlsx
folder.
This contains:
a
metadata.xlsx
file which has the metadata of the exported entities and the corresponding masterdata (types and properties) in the correct format to be re-imported in another openBIS instance;
a
scripts
folder that contains evaluation plugins associated to two types defined in the metadata.xlsx file. This folder is present only if the exported types have plugins associated with them.
a
data
folder that contains the information stored in the spreadsheet field of one of the Objects in this Space. This folder is present only if the exported entities contain information in spreadsheet or if there are text fields with more than 32,767 characters (this is the limit of the Excel cells).
a
miscellaneous
folder that contains images that are embedded in text fields of the exported entities. This folder is present only if exported entities contain images embedded in text.
2. Non import-compatible export of a Space selecting all options

We export the same Space as described in Example 1, with all options selected, but the export this time is not import-compatible, as shown below.
In this case the exported zip file contains only 2 folders:
hierarchy
and
xlsx
. Data are exported inside the
hierachy
folder, instead of being in a separate
data
folder.
## A.
hierarchy
folder
This contains the same folder structure as described above. In addition, in this case, inside the Object
Step A (EXP4)
folder there is a
data
folder that contains the dataset belonging to this Object, as shown below. The metadata of the dataset is provided as a metadata.json file inside the data folder and as pdf file inside the Object folder (
Step A (EXP4)
).
## B.
xlsx
folder
This contains the same files and folders as described in Example 1 (see below). The only difference in this case is that the metadata.xlsx is not import-compatible. It contains some fields which are not compatible with openBIS re-import, as explained above.
Export to Zenodo

openBIS provides an integration with the
## Zenodo
data
repository (
https://zenodo.org/).
This enables data direct data transfer from openBIS to Zenodo. First of
all the connection to Zenodo needs to be configured on
system level
in the DSS service.properties (see
How to configure the openBIS
## DSS)
If this is configured, a lab manager, who has admin rights for the
## Settings,
needs to enable it in the ELN, as explained in
## Enable
Transfer to Data
## Repositories
.
## Create Zenodo Personal Access Token

In order to be able to export data to Zenodo, you need a valid Zenodo
account. You also need to create a
personal access token.
This can
be done from the
## Applications
under
## Settings
in Zenodo, as shown
## below:
Save Zenodo Personal Access Token in openBIS

After creating the personal access token in Zenodo, this needs to be
stored in openBIS, with the following procedure:
Go to
## User Profile
under
## Utilities
in the main menu.
Enable editing.
Add the personal access token from Zenodo.
## Save.
Export data to Zenodo

To export data to Zenodo:
Go to
## Exports
->
Export to Zenodo
under
## Utilities
in
the main menu.
Select the data you want to export from the menu.
Enter a
## Submission
## Title.
## Click
## Export Selected
on top of the export form.
The selected data are transferred as a zip file to Zenodo. You are
now redirected to Zenodo, where you should fill in additional
metadata information.
Publish the entry in Zenodo.
The data exported to Zenodo is a .zip file that contains the metadata of the exported entries in 4 formats (.txt, .html, .doc, .json) and the data. The hiearchy (i.e.folder structure) used in the ELN is preserved in the exported .zip file.
After you hit the
## Publish
button in Zenodo, a new entry with the
details of this submission will be created in the
## Publications
folder in the
## Inventory
. Please note that this may take a few
minutes.
Export data to Zenodo in a multi-group instance

If you export data from a multi-group instance where you have access to more than one group, you need to select the group under which the new publication entry should be created.
In the example below we see 3 group names: GENERAL, DEMO, TEST.
If you select GENERAL, the publication entry will be created under the PUBLICATION Space (if present).
If you select DEMO, the publication entry will be created under the DEMO_PUBLICATION Space.
If you select TEST, the publication entry will be created under the TEST_PUBLICATION Space.
Export to ETH Research Collection

## The
ETH Research Collection
is a FAIR repository for publications and research data provided by ETH
Zurich to its scientists.
Data can be uploaded to the ETH Research Collection
only by members of
ETH Zurich
. This export feature is only available to ETHZ members.
To export data to the ETH Research Collection:
Go to
## Utilities
->
## Exports
->
Export to Research
## Collection
.
Select what to export from the tree.
Select the
## Submission Type
from the available list:
## Data
collection, Dataset, Image, Model, Sound, Video, Other Research
## Data
.
Select the
## Retention Period
that will be used in the ETH
## Research Collection:
10 years, 15 years, indefinite.
This is time
for which the data will be preserved in the Research Collection.
Click the
## Export Selected
button on top of the page.
The selected data are transferred as zip file to the ETH Research
Collection. You will be redirected to the ETH Research Collection
and will need to complete the submission process there.
The data exported to the Research Collection is a .zip file that contains the metadata of the exported entries in 4 formats (.txt, .html, .doc, .json) and the data. The hiearchy (i.e.folder structure) used in the ELN is preserved in the exported .zip file.
A new entry with the details of this submission will be created in the
## Publications
folder in the
## Inventory
after the submission
process in complete. This may take a few minutes.
The size limit for one single export to the ETH Research Collection is
## 10GB.
Export data to the ETH Research Collection in a multi-group instance

If you export data from a multi-group instance where you have access to more than one group, you need to select the group under which the new publication entry should be created. See explanation in section
Export data to Zenodo in a multi-group instance
above.",Export,0,en_20.10.0-11_user-documentation_general-users_data-export_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_data-export.txt,2025-09-30T12:09:10.247410Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_data-upload:0,Data Upload,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-upload.html,openbis,"Data Upload

Data can be uploaded to Datasets in openBIS to
## Experiments
and
## Objects
(e.g.,
## Experimental Steps
). openBIS is agnostic of file formats and types.
Small data files can be uploaded via the web user interface, larger data files can be uploaded via dropbox mechanism.
Data upload via web UI

To upload data via the web interface:
1.Click the
## Upload
button in the form, as shown below.
2. Select the dataset type (e.g. Attachment).
3. Fill in the relevant fields in the form. It is advisable to always
enter a
## Name
, because this is shown in the menu. If the name is not
provided, the dataset code is shown.
4. Drag and drop files in the
## Files
## Uploader
area or browse for
files.
5. When uploading a zip file, the option to
uncompress before
import
will be presented in the form.
6.
## Save.
Note for MacOS users:
the default MacOS archiver generates hidden
folders that become visible in openBIS upon unarchive. To avoid this
## there are two options:
Zip using  the following command on the command-line:
zip
-r folder-name.zip
folder-name/\*
-x
“\.DS\_Store”
Use an external archiver (e.g. Stuffit Deluxe).
Updated on March 23, 2023
Data upload via dropbox

Web upload of data files is only suitable for files of limited size (few GB). To upload larger data, openBIS uses dropbox scripts that run in the background (see
## Dropboxes
). A default dropbox script is provided with the openBIS ELN-LIMS plugin, and the dropbox folder needs to be set up by a
system admin
.
If this is available, users need to organise their data in a specific way:
## Folder 1
## Data
(can be single files or folders)
## Folder 1
needs to have a specific name that encodes the information
of where the data should be uploaded to openBIS.
The name of
## Folder 1
can be generated from the ELN interface:
From the page where you want to upload data, select
Dataset upload
helper tool for eln-lims dropbox
from the
## More…
dropdown and
follow the instructions on screen.
## Select:
The dataset type from the list of available types (mandatory);
Enter the name of your dataset (optional, but recommended);
Copy the generated name of the folder using the copy to clipboard icon.
3. In your finder/explorer, create a new folder and paste the name you
copied from openBIS. Place your data in this folder.
4. Place this folder containing your data inside the
eln-lims-dropbox
folder. openBIS continuously monitors this folder
and when data are placed here, they are
moved
to the final storage.
The move happens after a predefined (and customisable) inactivity period
on the eln-lims-dropbox folder.
Dropbox with markerfile

In case of uploads of data >100GB we recommend to configure the
eln-lims-dropbox-marker
. The set up and configuration need to be
done by a
system admin
. The process of data preparation is the same as
described above, however in this case the data move to the openBIS final
storage only starts when a markerfile is placed in the
eln-lims-dropbox-marker folder. The marker file is an empty file with
this name:
.MARKER_is_finished_
.
Please note the “.” at the start of the name, which indicates that this is a hidden file. This file should also not have any extension. For example, if the folder to be uploaded has the following name:
## O
+
## BARILLAC
+
## PROJECT
\
_1
+
## EXP1
+
## RAW
\
## _DATA
+
test
The marker file should be named:
.MARKER_is_finished_O+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test
How to create the Marker file in Windows

You can create the Marker file in Windows using a text editor such as
## Editor
. Any other text editor will  also work.
open
## Editor.
Save the file with a name such as
.
MARKER_is_finished_O+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test.
The file is automatically saved with a “.txt” extension. This needs
to be removed.
Use the
## Rename
option to remove the extension from the file.
How to create the Marker file on Mac

If you are not familiar with the command line, you can create an empty
text file using for example the
TextEdit
application in a Mac. Any
other text editor will also work.
Open the
TextEdit
application and save an empty file with a name
such as
.MARKER_is_finished_O+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test
.
Save to any format.
You will get a message to say that files starting with “.” are
reserved for the system and will be hidden. Confirm that you want to
use “.”
To show these hidden files, open the Finder and press
Command +
Shift + . (period)
.
The file you saved before has an extension, that needs to be
removed. If the extension is not shown in your Finder, go to Finder
Preferences menu, select the Advanced tab, and check the “Show
all filename extensions” box.
Remove the extension from the file.
Dropbox monitor

It is possible to check the status of the upload via dropbox using the
## Dropbox Monitor
under
## Utilities
in the main menu.
The Dropbox Monitor shows a table with all available dropboxes for a
given openBIS instance. By default,
default-dropbox, eln-lims-dropbox
and eln-lims-dropbox-marker
are shown.
If data are uploaded in a dropbox folder, users can see the status of
the data upload in the table. A red face in the column
## Last Status
indicates a failure of data import, a green face indicates successful
data import.
If you click on the row of the table above, you can see the details of
every upload attempt for a given dropbox, as shown below. For failures,
the log with the error is shown.
Registration of metadata for datasets via dropbox

Starting from openBIS version 20.10.2, the default eln-lims dropbox
supports the registration of metadata for datasets. The metadata needs
to be provided in a file called
metadata.json.
This file should be
placed inside the folder with the openBIS-generated name described
above, together with the data. This is shown in the example below.
O+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test
is the folder with the openBIS-generated name. Inside this folder there
is the metadata.json file, and the data, which consists of a few files
and 2 folders.
For example, the metadata.json file for the default RAW_DATA dataset
## type would be:
## { “properties” :",Experiments,0,en_20.10.0-11_user-documentation_general-users_data-upload_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_data-upload.txt,2025-09-30T12:09:10.310248Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_data-upload:1,Data Upload,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-upload.html,openbis,"{ “$NAME” : “my raw data”,

“NOTES” : “This is a test for metadata upload via dropbox” }

}
It is possible to download the template metadata.json file for each
dataset type from the
## Other Tools
section under the
## Utilities
in
the main menu.
## In
## Other Tools
, there is also the
Show available storage space
button, which shows the available storage space on the openBIS instance.
This is helpful in calculating how much space one might require for
future data upload, especially large data.
Updated on April 26, 2023",Other Tools,1,en_20.10.0-11_user-documentation_general-users_data-upload_1,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_data-upload.txt,2025-09-30T12:09:10.310248Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_ELN-types:0,ELN types,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/ELN-types.html,openbis,"ELN types

Standard types

When the eln-lims plugin is enabled the following types are installed by default.
Object types

General protocol
## Storage
Storage position
## Product
## Supplier
## Order
## Request
## Publication
Collection types

## Collection
Dataset types

ELN preview
Raw data
Processed data
Analyzed data
## Attachment
Other data
Source code
Analysis notebook
Publication data
Basic default types

The following Object types are created if the
eln-lims-template-types
is enabled in core plugins. This can be enabled by a
system admin
when openBIS is first installed (see
installation steps
) or at any time afterwards.
## Entry
## Experimental Step
## Default Experiment
Life science types

The following Object types are provided with the
eln-lims-life-science
data model which can be downloaded from
Community data model
. An openBIS
instance admin
can upload these types from the admin UI, as explained
here
.
## Antibodies
## Chemicals
## Enzymes
## Media
Solutions and Buffers
## Plasmids
## Plants
## Oligos
## RNA
## Bacteria
Cell lines
## Flies
## Yeasts
General protocols
PCR protocol
Western blotting protocols",Storage,0,en_20.10.0-11_user-documentation_general-users_ELN-types_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_ELN-types.txt,2025-09-30T12:09:10.576330Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_general-overview:0,General Overview,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/general-overview.html,openbis,"## General Overview

The openBIS platform has three primary functionalities:
Inventory management
of laboratory samples, materials,
protocols, equipment.
Laboratory notebook
, to document lab experiments.
Data management
, to store all data related to lab experiments
(raw, processed, analysed data, scripts, Jupyter notebooks, etc.).
It is possible to use all functionalities or only selected ones.
In the most general use-case, the
## Inventory
is shared by all lab
members, so everyone can access information about available lab
materials and regularly used protocols.
In addition, every lab member has a personal folder in the
## Lab
notebook
, where to organise projects and experiments. This folder can
be shared with other lab members or collaborators with openBIS access.
Experimental steps described in the lab notebook can be linked to
protocols and samples stored in the inventory. Experimental steps can
also be linked to each other.
## Data
of any sort can be attached to the corresponding Experimental
step in different ways, depending on the size.
Data can be exported to data repositories, such as
## Zenodo
or the
ETH Research
## Collection
(for ETHZ users
only).
This allows to have the complete overview of workflows and information,
from initial data generation to data analysis and publication.
The openBIS ELN interface can be accessed via a URL of this type:
https://openbis-xxx/openbis/webapp/eln-lims/
where
openbis-xxx
is the name of the server specified in the openBIS
configuration file, during the installation by a system admin.
## Login

File based and/or LDAP authentication

When file based and/or LDAP authentication are used in openBIS, the login interface is as shown below. Users need to provide their username and password to login.
Only registered users with assigned rights can login to openBIS.
SWITCHaai authentication

When SWITCHaai (SSO) authentication is used in addition to file based and/or LDAP authentication, the login interface is as shown below.
SWITCHaai is selected by default. In this case, users need to click on
## Login
and they will be redirected to the SWITCHaai login page.
If a user would like to authenticate with a file-based account or LDAP (depending on system configuration), they need to select
## Default Login Service
from the dropdown and provide username and password.
openBIS also supports SWITCH edu-id authentication and the login process is the same as described in this section.",General Overview,0,en_20.10.0-11_user-documentation_general-users_general-overview_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_general-overview.txt,2025-09-30T12:09:10.649716Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_index:0,General Users,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/index.html,openbis,"## General Users

## General Overview
## Login
File based and/or LDAP authentication
SWITCHaai authentication
ELN types
Standard types
Object types
Collection types
Dataset types
Basic default types
Life science types
Inventory Of Materials And Methods
## Customise Collection View
Register single entries in a Collection
Batch register entries in a Collection
Batch registration via Excel template file
## Codes
Controlled vocabularies
Assign parents
Date format
Register storage positions and samples in the same XLS file
Batch registration via TSV template file
Rules to follow to fill in the template .tsv file
Advantages of XLS batch registration vs the old batch registration
Batch register entries in several Collections
XLS Batch Register Objects
TSV Batch Register Objects
Batch update entries in a Collection
XLS Batch Update Objects
TSV Batch Update Objects
Batch update entries in several Collections
XLS Batch Update Objects
TSV Batch Update Objects
Copy entries
Move entries to a different Collection
Move from entry form
Move from Collection Table
Register Protocols in the Methods Inventory
LINKS TO SAMPLES, MATERIALS, OTHER PROTOCOLS
## Managing Storage Of Samples
Allocate storage positions to samples
Register storage position for a single sample
Add additional metadata to storage positions
Batch register storage positions
XLS Batch Registration
Batch Registration with TSV file
Batch update storage positions
Delete storage positions
Delete single storage positions
Remove one of multiple positions in the same box
Delete multiple storage positions
## Overview of lab storages
## Overview of lab Storages
Change storage position of samples
Barcodes and QR codes
Barcodes and QR codes
Barcodes for individual samples
Generate batches of barcodes / QR codes
Scan barcodes from mobile devices
Printer and Barcode Scanner Requirements
## Printers
### Printer Configuration
Printer testing
Printer Advice before purchasing
## Tested Printers
## Scanners
### Scanner Configuration
Scanner testing
Scanner Advice before purchasing
## Tested Scanners
## Lab Notebook
## Register Projects
## Register Experiments
Register a Default Experiment:
Register a Collection:
## Register Experimental Steps
## Comments Log
Add parents and children to Experimental Steps
Adding a parent
Adding a parent of a predefined type in the form
Adding parent of any available type
Adding parent via barcodes
Removing a parent
Adding and Removing Children
## Children Generator
Parent-child relationships between entries in lab notebook
How to use protocols in Experimental Steps
## Move Experimental Steps
## Copy Experimental Steps
Use templates for Experimental Steps
Datasets tables
Data Access
## Example of SFTP Net Drive connection:
Example of Cyber Duck configuration
Example of  Dolphin File Manager configuration
SFTP access via session token
## Move Datasets
Move one Experiment to a different Project
## Project Overview
Edit and Delete Projects, Experiments, Experimental Steps
Share Lab Notebooks and Projects
## Rich Text Editor
## EMBED IMAGES IN TEXT FIELDS
Data Upload
Data upload via web UI
Data upload via dropbox
Dropbox with markerfile
How to create the Marker file in Windows
How to create the Marker file on Mac
Dropbox monitor
Registration of metadata for datasets via dropbox
## Export
Export to File
## Export Lab Notebooks & Inventory Spaces
1. Import-compatible export of a Space selecting all options
2. Non import-compatible export of a Space selecting all options
Export to Zenodo
## Create Zenodo Personal Access Token
Save Zenodo Personal Access Token in openBIS
Export data to Zenodo
Export data to Zenodo in a multi-group instance
Export to ETH Research Collection
Export data to the ETH Research Collection in a multi-group instance
Data archiving
Dataset archiving
Dataset archiving helper tool
Dataset unarchiving
Dataset unarchiving helper tool
## Search
Advanced search
Search for: All
Search for: Experiment/Collection
Search for: Object
Search for: Dataset
Search for: specific Object Type (e.g Experimental Step)
## Search Collection
## Search
Global search
BLAST search
Data Set File search
Save and reuse searches
## Additional Functionalities
Print PDF
## Visualise Relationships
## Tables
## Filters
## Sorting
## Exports
## Columns
## Spreadsheets
Text fields
Selection of entries in table
Browse Entries by Type
## Trashcan
## Visualize Available Storage Space
## Vocabulary Browser
## Freeze Entities
How to freeze an entity
Navigation menu
## Custom Imports
Entity history
History table for Collections
History table for Objects
History table for Datasets
## Spreadsheet
## Session Token
Managing Lab Stocks and Orders
## STOCK CATALOG
Building the catalog of products and suppliers
Catalog of suppliers
Catalog of products
Creating requests for products to order
## STOCK ORDERS
Processing product orders from requests
Tools For Analysis Of Data Stored In Openbis
## Jupyter Notebooks
How to use Jupyter notebooks from openBIS
## Overview of Jupyter notebook opened from openBIS.
What to do in case of invalid session token
Using a local Jupyter installation with openBIS
MATLAB toolbox",General Users,0,en_20.10.0-11_user-documentation_general-users_index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_index.txt,2025-09-30T12:09:10.851918Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_inventory-of-materials-and-methods:0,Inventory Of Materials And Methods,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/inventory-of-materials-and-methods.html,openbis,"Inventory Of Materials And Methods

The default Inventory contains two folders:
## Materials
and
## Methods
.
These are used to organise respectively samples and materials of any type and lab protocols.
Samples, materials and protocols are modelled in openBIS as
## Objects.
In the
openBIS ELN-LIMS for life sciences
, the following Object types are provided:
Antibodies, Chemicals, Enzymes, Media, Solutions and Buffers, Plasmids, Plants, Oligos, RNAs, Bacteria, Cell lines, Flies, Yeasts, General protocols, PCR protocols, Western blotting protocols.
These Objects are organised in
## Collections
in the
## Materials
and
## Methods
sections of the Inventory.
The generic openBIS ELN-LIMS only has one predefined Object type for the Inventory,
## General Protocol
, in the General Protocols Collection in the Methods folder. The Material folder is empty. Additional Object types and Collections must be created by an openBIS instance admin, based on the needs of the lab(s).
## Customise Collection View

It is possible customise the view of
## Collections
in the ELN.
The default
## Collection
can have a
## Form View
or a
## List View
.
Depending on this selection, the collection view will be different.
## Form View:
This shows the metadata of the
## Collection
along with
the table of objects. This view is useful when a user wants to see
specific metadata for a
## Collection
.
If you do not see the table with the Objects in the form, you need to
enable this by selecting
## Show Objects
from the
## More..
dropdown
## List View:
The metadata of the
## Collection
is not shown in this
view, but only the table of objects is shown.
In this case a user would need to click on
## More..
, and
## Edit
## Collection
in order to see the metadata and be able to edit the
## Collection
.
Updated on April 25, 2023
Register single entries in a Collection

In this example, we will see how to register one
## Object
of type
## Sample
in the
## Raw Samples
## Collection.
The same procedure
should be followed to register any other
## Object
in other
## Collections
.
Click on the
## Raw Samples
## Collection
folder in the main menu.
Click the
## New Sample
in the main page
Fill in the form
## Save
Please note that the
Object type
shown in the
## +New
button (in this
case
## Sample
), is what is defined as
default object type
for the
## Collection
. If this is missing in the
## Collection,
the button will
not be present.
To register a different object type in the Collection:
## Select
## New Object
from the
## More
drop down menu (as shown
below)
Select the relevant
Object type
from the list
## (Sample,
in this case).
Fill in the form
## Save
Updated on April 25, 2023
Batch register entries in a Collection

It is possible to register several samples at once via file upload. Two
## methods are currently available:
Batch registration via Excel template file (XLS Batch Register
## Objects)
Batch registration via TSV template file (TSV Batch Register
## Objects)
## Warning
In openBIS versions prior to 20.10.6, the XLS batch registration is not
recommended to register several hundreds of entries. The use of the TSV
batch upload to register several hundreds of entries is recommended in
those cases.
Batch registration via Excel template file

To register several entries of the same type with an Excel file:
Navigate to the relevant collection (e.g.
## Samples
).
## Select
XLS Batch Register Objects
from the
## More
drop-down menu (see figure above)
Download the
template
file and fill in the relevant information.
## (Example file:
SAMPLE-COLLECTION-REGISTRATION-SAMPLE-STORAGE_POSITION-template
)
Upload the file.
## Codes

In most cases,
## Object
types have the option to auto-generate codes set
to true in the admin UI. In this case, openBIS automatically generates
codes and identifiers when
## Objects
are registered. If that is not the
case, the code needs to be manually entered by the users in the Excel
template. The current template does not have a
## Code
column. This can
however be manually added if codes should be provided by the user and
not automatically generated by openBIS.  If codes should be manually
entered and are missing, openBIS will show the error message
“
UserFailureExceptionmessage: Code cannot be empty for a non auto
generated code.
”
Controlled vocabularies

For Controlled Vocabularies fields, i.e. fields with a drop down menu,
you can enter either the
code
or the
label
of the terms in the
Excel file.
Please note that codes are not case-sensitive, but labels are.
Codes and labels of vocabulary terms can be seen under
## Utilities -> Vocabulary Browser
.
Assign parents

Assign already existing parents
If the parents you want to assign to your Objects are already registered
in openBIS, in the
## Parents
column of the Excel file, you can assign
the relationship, by providing the identifier of the parent (i.e. /SPACE
code/PROJECT code/OBJECT code). If you want to add multiple parents to
one Object, every identifier should be in a new line in the
corresponding Excel cell. A new line in an Excel cell is entered with
the keyboard shortcuts
## Alt
+
## Enter.
## Example file:
SAMPLE-COLLECTION-REGISTRATION-ANTIBODY-STORAGE_POSITION-template
## Note:
no other separators (e.g “,” or  “;”) should be used,
otherwise an error will be thrown.
Register Objects and assign parents in the same batch registration
process.
If you want to register a few
## Objects
and at the same time establish a
parent-child relationship between some of them, you can do so by using
the
$
and
## Parents
columns. In the example below we want to
register 2
## Objects
, antibody 1 and antibody 2. We want to assign
antibody 1 as parent of antibody 2. In the
$ column
corresponding to
antibody 1 we need to enter numbers or letters proceeded by the $ symbol
(i.e. $1, or $parent1). In the
## Parents
column of antibody 2, we need
to use the same value used in the
$ column
for antibody 1.
Date format

For date fields, the expected format is YYYY-MM-DD.
Register storage positions and samples in the same XLS file

## A
sample
and its
storage
position
can be registered
together, as shown in the template provided above:
The info in the
$
column of the
sample
spreadsheet should
match the
## Parents
column in
## Storage Positions
spreadsheet.
In the $ column you can enter numbers or letters proceeded by the $
symbol (i.e. $1, $2 or $parent1, $parent2)
.
Batch registration via TSV template file

## Select
## TSV
## Batch Register Objects
from the
## More
drop-down menu
Select the
## Object
type (E.g. Sample or Storage)
Download the
template
file and fill in the relevant information
Upload the file
Rules to follow to fill in the template .tsv file

## Identifiers
## :
Identifiers are given by
/SPACE code/PROJECT code/OBJECT
code
, e.g
## /MATERIALS/EQUIPMENT/INS1
. Users can provide
their own identifiers, or these can be automatically generated
by openBIS.
To have identifiers automatically generated by openBIS,
completely remove the
identifier
column from the file.
## Lists
. In fields that have lists to choose from (called
## Controlled Vocabularies
), the code of the term needs to be
entered. Term codes can be seen under
## Utilities -> Vocabulary
## Browser
.
## Parents
. Use the following syntax to enter parents:
identifier1, identifier2, identifier3.
Parents annotations
. Use the following syntax to annotate
## parents:
identifier:xxx;COMMENTS:xxxx\identifier:yyy;COMMENTS:yyyy
## . Where
## COMMENTS
is the property used for the annotation in this case
(to be replaced with the actual property used).
Date fields
. The expected syntax for dates is YYYY-MM-DD.
Advantages of XLS batch registration vs the old batch registration

XLS batch registration uses labels instead of codes in the column
headers in the template file.
Fields which are Controlled Vocabularies can use labels instead of
codes.
The template can be used as it is, and no modifications are
necessary by removing the identifier column, as it was in case of
the old batch registration.
Upload of samples and storage positions can now be performed using
single template file.
## The
old
batch register mode is being maintained for backward
compatibility and will be phased out.
Updated on April 25, 2023
Batch register entries in several Collections

It is possible to batch register
## Objects
that belong to different
## Collections
.
This can be done from the
## Object Browser
page, under
## Utilities
.
## Two options are available:
XLS Batch Register Objects
: batch registration via Excel
template file.
TSV Batch Register Objects
: batch registration via .tsv template
file.
XLS Batch Register Objects

This option for batch registration is available since openBIS version
20.10.3. It allows to register
## Objects
of different types to multiple
## Collections
.
You can select which types you want to register from the list of
available types.
You can then download the template that will allow you to register
## Objects
of the selected types to single or multiple
## Collections
## . The
## Space, Project, Collection
need to be entered in the file. The
complete path for
## Projects
and
## Collections
need to be used, as shown
in this example file:
SAMPLE-GENERAL-REGISTRATION-EXPERIMENTAL_STEP-MASS_MEASUREMENT-SAMPLE-template
TSV Batch Register Objects

The batch registration via .tsv file allows to batch register only one
type of
## Object
at a time.
## Objects
however can be registered to
several
## Collections
.
This batch upload method is kept for backward compatibility, but it will
be phased out.
In this case, if
## Objects
are to be registered to multiple
## Collections
, an
identifier
for the
## Objects
needs to be provided,
as shown below. This is not the case with the XLS batch registration,
where identifiers can be automatically generated by openBIS.
Updated on April 25, 2023
Batch update entries in a Collection

It is possible to modify the values of one or more fields in several
objects simultaneously via batch update. This can be done in two ways:
XLS Batch Update Objects
TSV Batch Update Objects
XLS Batch Update Objects

Navigate to the relevant collection (e.g.
## Raw Samples
).
In the Collection table, from the
## Columns,
select
## Identifier
and the field(s) you want to update (e.g.
## Source
), as shown
below
3. If you have several entries you can filter the table
(see
## Tables
)
4.
## Export
the table choosing the options
Import Compatible= YES;
Selected Columns; All pages/Current page/Selected rows
(depending on
what you want to export)
.
5. Modify the file you just exported and save it.
## 6. Select
XLS Batch Update Objects
from the
## More..
dropdown
6. Upload the file you saved before and click
## Accept
. Your entries
will be updated.
## Note
## :
If a column is removed from the file or a cell in a column is left empty
the corresponding values of updated samples will be preserved.
To delete a value or a parent/child connection from openBIS one needs to
enter
into the corresponding cell in the XLS file.
TSV Batch Update Objects

Navigate to the relevant collection (e.g.
## Raw Samples
).
## Select
## TSV
## Batch Update Objects
from the
## More…
dropdown.
Select the relevant
## Object
type
, e.g.
## Sample
Download the available
template
Fill in the
identifiers
of the objects you want to update
(identifiers are unique in openBIS. This is how openBIS knows what to
update). You can copy the identifiers from the identifier column in the
table and paste them in the file. Identifiers have this format:
## /MATERIALS/SAMPLES/SAMPLE1.
Fill in the values in the columns you want to update
Save the file and upload it via the
XLS Batch Update
## Objects
from the
## More..
dropdown
## Note
## :
If a column is removed from the file or a cell in a column is left empty
the corresponding values of updated samples will be preserved.
To delete a value/connection from openBIS one needs to enter
## _ _DELETE_ _
into the corresponding cell in the file.
Updated on April 25, 2023
Batch update entries in several Collections

It is possible to batch update
## Objects
that belong to different
## Collections
.
This can be done from the
## Object Browser
page, under
## Utilities
.
## Two options are available:
XLS Batch Update Objects
: batch update via Excel template file.
TSV Batch Update Objects
: batch update via .tsv template file.
XLS Batch Update Objects

This option for batch update is available since openBIS version 20.10.3.
It allows to update
## Objects
of different types that belong to
different
## Collections
.
You can select which types you want to update from the list of available
types.
You can then download the template that will allow you to update
## Objects
of the selected types to single or multiple
## Collections
## . The
## Space, Project, Collection
need to be entered in the file. The
complete path for
## Projects
and
## Collections
need to be used. In
addition, identifiers for the
## Objects
need to be provided: identifiers
are unique in openBIS, by providing them openBIS will know which
## Objects
have to be updated. Example file:
SAMPLE-GENERAL-REGISTRATION-EXPERIMENTAL_STEP-MASS_MEASUREMENT-SAMPLE-template
TSV Batch Update Objects

The batch update via .tsv file allows to batch update only one type of
## Object
at a time. However, it is possible to update
## Objects
that
belong to several
## Collections
.
This batch update method is kept for backward compatibility, but it will
be phased out.
## The
## Space, Project, Collection
need to be entered in the file. The
complete path for
## Projects
and
## Collections
need to be used. In
addition, identifiers for the
## Objects
need to be provided: identifiers
are unique in openBIS, by providing them openBIS will know which
## Objects
have to be updated.
Updated on April 25, 2023
Copy entries

To create a copy of an existing entry, select
## Copy
from the
## More..
drop down menu in the
## Collection
page.
When an entry is copied, the user has the option to
link parents
,
copy children into the Parents’ collection
and
copy the comments
log
.
All these options are disabled by default.
Updated on July 27, 2022
Move entries to a different Collection

You can move entries to a different
## Collection
either from the e
ntry
form or from a
## Collection
table.
Move from entry form

To move entries to a different
## Collection
, select
## Move
from the
## More…
drop down menu in the entry form.
You have the option to move to an existing
## Collection
or to create a
new
## Collection
.
Move from Collection Table

It is also possible to move objects from
## Collection
tables. You can
select one or multiple entries from a table and click on the
## Move
button.
Also in this case you can move to an existing
## Collection
or create a
new one.
Updated on July 27, 2022
Register Protocols in the Methods Inventory

Protocols are standard operating procedures (SOPs) used in the lab. If such procedures are in place, they should be organised in folders in the Methods Inventory which, by default, is accessible by all lab members.
openBIS provides a General Protocol Object type that can be used. If different specific metadata is needed for protocols, new Object types can be created by an Instance admin in the admin UI and the corresponding Collections can be created in the ELN UI.
To register a new General Protocol in the General Protocols folder, follow these steps:
Go to the General Protocols Collection in the Methods folder.
Click the + New General Protocol button in the main page.
Fill in the relevant fields in the form or choose from available templates.
## Save
LINKS TO SAMPLES, MATERIALS, OTHER PROTOCOLS

When writing a protocol, it is possible to create links to samples, materials or other protocols stored in the Inventory. These are parent-child relationships in openBIS.
Everything that is used in the protocol can be added as Parent of the protocol itself. This can be done as described fo Experimental Steps:
Add parents and children to Experimental Steps",Materials,0,en_20.10.0-11_user-documentation_general-users_inventory-of-materials-and-methods_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_inventory-of-materials-and-methods.txt,2025-09-30T12:09:10.920508Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_lab-notebook:0,Lab Notebook,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/lab-notebook.html,openbis,"## Lab Notebook

In the most common use-cases, the
## Lab Notebook
part of the openBIS ELN-LIMS contains a personal Space (i.e. folder) for each scientist. Within this
## Space
, scientists can organise their work using the openBIS
Projects, Experiments and Objects.
An openBIS
## Experiment
is defined as a specific scientific question. The individual attempts to answer this question, are
## Objects
of type
## Experimental Step
. At this level, the user can create links to
materials
and
methods
registered in the Inventory that were used to perform the
## Experimental Step
. These are entered as
## Parents
of the
## Experimental Step
. All data produced in the
## Experimental Step
and further processed and analysed can be added at this level.
It is also possible to organise the Lab Notebook on
## Projects
, rather than on personal Spaces. This should be configured by an
Instance admin
.
## Register Projects

In a personal folder, users can register one or more
## Projects
they
currently work on.
## Projects
in openBIS only have a
## Description
field, no additional fields can be added.
Navigate to the relevant
## Space
in the
## Lab Notebook
menu and click the
## + New Project
Should you have an empty page, select
## Show Identification Info
and
## Show Description
from the
## More…
dropdown
## Projects
do not have a
## Name
field, but only
## Code
. Codes can only take alphanumeric characters and no spaces. Codes are prettified in the Main Menu.
Enter a
## Description
for the project.
## Click
## Save
on top of the form.
In the
## More…
dropdown you have additional options on what you can do
in the Project folder, as shown below.
## Register Experiments

Inside one
## Project
, a user can register several
## Experiments
, which can in turn be divided into single
## Experimental Steps.
openBIS provides by default 2 options for registering Experiments:
## Default Experiment
: The form of the Default Experment contains several metedata fields that can be filled in by the user.
## Collection
: This form has limited metadata fields. It should be considered as a folder, to be used in cases where a user only needs to group subsequent steps, and does not need any relevant information at this folder level.
Register a Default Experiment:

Navigate to the relevant
## Project
in the
## Lab Notebook
menu
## Select
## Default Experiment
from the
## +New
dropdown, as shown below.
Please note
that your openBIS instance might have different types of Experiments, depending on how it has been configured by the Instance admin.
Fill in the relevant fields in the form.
## Select
Show in project overview = true
if the
## Experiment
is
important and should be shown in the
## Project
form.
## Click
## Save
on top of the form.
Register a Collection:

Navigate to the relevant
## Project
in the
## Lab Notebook
menu
## Select
## Collection
from the
## +New
dropdown, as shown below.
Please note
that your openBIS instance might have different types of Experiments, depending on how it has been configured by the Instance admin.
Fill in the
## Name
of the Collection and choose the
## Default Object Type
and
Default collection view
. For more info about Collections, see
## Customize Collection View
and
Collections of Materials
## Click
## Save
on top of the form.
## Register Experimental Steps

As mentioned above, the various steps executed when performing an
Experiment in the lab can be registered in openBIS as
## Experimental
Steps or Entries.
The default
## Experimental Step
## has pre-defined fields, as shown below:
## An
## Entry
, is a blank page, with no pre-defined fields:
To register a default
## Experimental Step
or
## Entry
## :
Navigate to the relevant
## Experiment
in the
## Lab Notebook
menu and click the
## + New
button, as shown below.
## Select
## Experimental Step
or
## Entry
Fill in the relevant information or select an available template from the list (see below).
## If
Show in project overview
is set to true, this
## Experimental Step
or
## Entry
will be displayed on the
## Project
page.
## Click
## Save
on top of the form.
## Comments Log

Several comments can be added by different users who have write-access to a given user Space:
Click the button in the
## Comments
section.
Enter the
comment
.
## Click
## Save.
Add parents and children to Experimental Steps

In the default
## Experimental Step
and in the
## Entry
, there is a
## Parents
section where it is possible to specify links to materials
and methods from the
## Inventory
or to any other
## Object
, e.g. another
## Experimental Step
or
## Entry
.
## Parents
are all samples/materials used in an experimental procedure,
standard protocols from the inventory followed in the experimental
procedure, the equipment used. It is also possible to set one
## Experimental Step/Entry
as parent of a second
## Experimental
## Step/Entry,
to keep the connection between the two.
The name of this section and which parents should be shown in the form,
is customisable by the
lab manager
or
group admin
as described in
Customise Parents and Children Sections in Object
## Forms
Adding a parent

Adding a parent of a predefined type in the form

In the screenshot above,
General protocol
is predefined as parent
type in the form. We have two options to add a parent of this predefined
## type:
## 1. Search

Click on the
## Search
button.
Enter the
name
or
code
of the entry you want to add as
parent.
Select the entry you want to add from the list presented to you.
The parent will be added only when you
save
the entity.

## 2. Paste

You may copy the identifier of an entry you want to add as
parent from a file, or from an advanced search or from another
ELN page. You can paste the identifier(s) in the
## Paste
text
field.
Click the
## +Add
button
Adding parent of any available type

If you want to add a parent that is not specified in the
## Experimental
## Step
form, you can use the
## Search Any
or
## Paste Any
options next
to
## Parents.
## 1. Search Any

## Click
## Search Any
Select the
## Object
type for which you want to add a parent
Search by
code
or
name
as explained above
Click the
## +Add
button
## 2. Paste Any

There are cases where you may want to add several parents of the same
type or also of different types. In this case, we recommend to use the
## Advanced Search
to find the entries you want to add. You can select
the desired entries from the table and the
## Copy Identifiers
button
will become visible. You can copy the identifiers and paste them in the
## Paste Any
field in the
## Experimental Step
page, as shown below.
Adding parent via barcodes

If you want to add a parent that is registered in openBIS and has a
barcode associated with it by scanning the barcode:
1.Click on the
barcode
icon in the Parents section
## A
Barcode/QR code reader
window opens
Scan the barcode/QR code of the entry you want to add as parent with
a scanner or with the camera of a mobile device
Click on the
## Add Objects
button
## Close
Removing a parent

To remove a parent, choose
## Remove
from the
## Operations
drop down in the parent table, as shown below.
Adding and Removing Children

Children of
## Experimental Steps
are usually derivative
## Experimental
## Steps,
or products of the
## Experimental Step.
As for the
## Parents
section, this section can also be customised by a
group admin
or
lab
manager
in the
ELN Settings
(
Customise Parents and Children Sections in Object Forms)
.
The procedure for adding and removing children is the same as explained
for parents.
## Children Generator

## The
## Children Generator
creates a matrix of all the parents entered
in the
## Experimental Step
, as shown below. Combinations of parents
needed to generate children can then be selected by the user. The
## Object
type to assign to the children and the number of replicas need
to be specified. The children will then be automatically generated by
openBIS upon registration of the
## Experimental Step
.
Parent-child relationships between entries in lab notebook

In the Lab Notebook section, if you create a new
## Object
from an
existing
## Object
, independently of the type, this will be automatically
set as parent of the new Object. For example, if you create a new
Experimental Step (measurement 4) from an existing Experimental Step
(measurement 3), this will be automatically set as child of measurement
3, as shown below.
If you do not wish to have this relationship established, you need to
create the new Object starting from the Experiment level, as shown
below.
How to use protocols in Experimental Steps

When adding protocols to an
## Experimental Step
, two options are
## available:
Link to a
## Protocol
stored in the
## Inventory
. This can be used
if the protocol was followed exactly in all steps as described.
Create a
local copy of the Protocol
from the
## Inventory
in the
current
## Experiment
. This should be done if some steps of the main
protocol were modified. These modifications can be edited in the
local copy of the protocol, while the template is left untouched.
To create a local copy under the current Experiment of a template protocol stored in the
## Inventory
## :
Add a protocol as parent.
From the
## Operations
dropdown in the parents table select
Copy to Experiment.
Provide the
Object code
for the new protocol.
A copy of the protocol is created under the current
## Experiment
, where the user can modify it. This copy has the original protocol set as parent, so that connection between the two is clear.
## Move Experimental Steps

To move an
## Experimental Step
to a different
## Experiment
, choose
## Move
from the
## More..
drop down, as shown in the picture above.
It is possible to move
## Experimental Steps
from the
## Object
table
which is presented on an
## Experiment
or
## Collection
page.
Select the entries to move and use the
## Move
button on the table. You
can move to an existing
## Experiment
/
## Collection
or create a new one.
## Copy Experimental Steps

To copy an
## Experimental Step
, select
## Copy
from the
## More…
drop
down menu, as shown below.
When an
## Experimental Step
is copied, the user has the option to
link
parents, copy children to the current Experiment
and
copy the
comments log.
## The
## Experimental Step
is copied inside the same
## Experiment
.
Use templates for Experimental Steps

Templates need to be defined by the lab manager in the
ELN Settings
. If templates have been created for a given
## Experimental Step
, you can choose from the list of available templates by clicking the
## Template
button on the
## Object
form, as shown below.
A template of an
## Experimental Step
is an
## Experimental Step
with
pre-filled values. Templates are useful when you need to repeat an
## Experimental Step
with the same parameters several times and you wold
like to have default values for those parameters.
Datasets tables

Since openBIS version 20.10.7, a dataset table has been added to the Experiment/Collection and Object pages.
This table shows the metadata of the datasets. The content of the datasets can be navigated through the main menu.
Data Access

## Datasets
are displayed on the left hand-side of the
## Experiment/Object
form, as shown below.
To navigate and open data registered in openBIS via Finder or Explorer, open the
## Dataset
folder and click on the drive icon next to the Dataset type name (see above).
## If
SFTP has been configured on system level, you will be provided with a link to copy/paste in an application such as
## Cyberduck
or other.
Please check our documentation for SFTP server configuration:
Installation and Administrators Guide of the openBIS Data Store Server
For native access through Windows Explorer or Mac Finder we recommend
the following:
## Windows
## 10:
https://www.nsoftware.com/sftp/netdrive/
Mac OS X Yosemite and
## higher:
https://mountainduck.io
Kubuntu: Default Dolphin File Manager with SFTP support
## Example of SFTP Net Drive connection:

1. open SFTP Net Drive and click on
## New
## :
2. Edit the drive with the following info, as shown below:
a.
Drive name
: choose any name you want. Can be the same as
your openBIS server, but does not have to be.
b.
## Remote Host
: the name of your openBIS. For example, if the
url of your openBIS is https://openbis-
demo.ethz.ch/openbis/webapp/eln-lims, then openbis-demo.ethz.ch is the
name you want to enter.
c.
Remote por
t: enter 2222.
d.
Authentication type
: Password (this is selected by default).
e.
## Username
: the username you use to login to openBIS.
f.
## Password
: the password you use to login to openBIS.
g.
Root folder on server
: you can leave the default, User’s home
folder.
h. Press
## OK
after filling in all the information above.
3. After saving the drive, select it in the drivers’ window and click
## Connect
.
3. openBIS will now appear as a drive in your Explorer window. Click on
the
## ELN-LIMS
folder and navigate to the folder containing the data
you want to access.
Note: if you encounter the error message “
SSH connection failed: Could
not find a part of the path
.” you can fix this by disabling the cache
(Drives -> Advanced -> Enable Caching), and disabling log files.
The error is caused by an attempt to create files in a folder not
available to Windows.
Example of Cyber Duck configuration

Create a new connection in cyberduck:
select
SFTP (SSH File Transfer Protocol)
## Nickname
: the name you want to use for the server
## Server
: the name of the server you want to connect to. In the
example below openbis-training.ethz.ch. Replace this with the name
of your own openBIS server.
## Port
: 2222
## Username
: this is the username with which you connect to your
openBIS
## Password
: this is the password you use to connect to your
openBIS
## SSH
private Key: none
Save the specifications and connect to the server.
You will see the folders of your own openBIS in the Cyberduck window and
you can navigate to your data from there.
Example of  Dolphin File Manager configuration

To access the Dataset form and edit the Dataset metadata, click on the
Dataset code or Name (if provided).
SFTP access via session token

To access via session token (for example when using SSO authentication)
you need to provide the following credentials:
## Username: ?
Password: session token
.
The session token can be copied from the
## User Profile
under
## Utilities
in the main menu, as shown below.
## Move Datasets

It is possible to move a
## Dataset
from one
## Experiment/Object
to
another
## Experiment/Object
.
Click on the
## Dataset
in the main menu
In the
## Dataset
page select
## Move
from the
## More..
dropdown
Enter the name or code of the
## Experiment
or
## Object
where you
want to move the
## Dataset
to. If you start typing, openBIS will
show you a list of possible entries that match what you entered.
Press the
## Accept
button.
Move one Experiment to a different Project

It is possible to move one Experiment and all contained Objects and
Datasets from one Project to another.
If Objects contain parent/child relationships these are preserved.
To move one Experiment from one Project to another:
Select the Experiment you want to move from the main menu
## Select
## Move
from the
## More…
dropdown
3. Enter the code of the Project where you want to move your
Experiment. If you start typing the code, openBIS will prompt you with a
list of available options and you can select the appropriate one from
there.
## 4. Click
## Accept
## Project Overview

In the Project page you have the options to see:
Default Experiments and Experimental Steps with the field
Show in project overview = true
. This is a way to mark the most    relevant Experiments and Experimental steps and see them at a glance on the project page (
## Show Overview
).
All experiments belonging to the project (
## Show Experiments/Collections
).
The two options are available from the
## More..
dropdown on the Project
page.
Below you see an example of an overview in a Project page.
Below you see an example of the visualisation of Experiments and
Collections in a Project page.
Edit and Delete Projects, Experiments, Experimental Steps

## Projects
,
## Experiments
and
## Experimental Steps
can be edited at any
time, by selecting the
## Edit
icon from the toolbar of the relevant
page.
## Projects
,
## Experiments
and
## Experimental Steps
can be deleted using
the
## Delete
option under
## More
tab in the toolbar.
## Experiments
and
## Experimental Steps
are moved to the
trashcan
,
from where they need to be removed in order to be permanently deleted
from the database.
## Projects
are directly deleted, they are not moved
to the trashcan first.
## Projects
can be deleted only after deleting all
the
## Experiments
they contain.
Please be aware that, by default, only users with
Space Admin and
## Instance Admin
role have permission to delete. Default permissions can
be modified on
system level
(see
Changing the openBIS
capability role
map
)
Share Lab Notebooks and Projects

It is possible to share either a complete lab notebook or single
## Projects
, using the
## Manage Access
option in the
## More..
dropdown of a
## Space
or
## Project
page, as shown below.
## Available roles are:
## Observer
: read-only access to Space or Project
## User
: can create and modify entities in Space or Project
## Admin
: can create, modify and delete entities in Space or
## Project
The roles can be granted to:
## User
: the user needs to be already registered in openBIS. The
username of the user needs to be entered.
## Group
: the name of a user group existing in openBIS needs to be
entered.
## Rich Text Editor

## EMBED IMAGES IN TEXT FIELDS

To embed an image in the a text field with the Rich Text Editor (RTE) enabled, you can simply drag & drop a .png or .jpg file and resize the image by clicking on and dragging the corners.",Lab Notebook,0,en_20.10.0-11_user-documentation_general-users_lab-notebook_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_lab-notebook.txt,2025-09-30T12:09:10.989758Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_managing-lab-stocks-and-orders-2:0,Managing Lab Stocks and Orders,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/managing-lab-stocks-and-orders-2.html,openbis,"Managing Lab Stocks and Orders

It is possible to use openBIS to manage stocks of products and create
orders of products to buy for the lab.
Every lab member can register products and place requests of products to
buy. The requests can be converted into orders by the lab manager or the
person responsible for purchases in the lab. The orders created with
openBIS contain the information that can be sent to the suppliers.
In the
## Stock Catalog
folder, a lab can create one collection of all
products purchased in the lab and one collection of all suppliers used
for purchasing. Each product must be linked to 1 supplier.
Every lab member by default has
## Space User
access rights to the
## Stock Catalog
folder and is able to register products, suppliers and
place requests for products to buy.
## The
## Stock Orders
folder is visible to all lab members, who have by
default
## Space Observer
rights to it.  The lab manager, or person
responsible for purchases, has
## Space Admin
rights to this Space.
Orders can be created based on the requests placed in the
## Stock
## Catalog
.
## STOCK CATALOG

Building the catalog of products and suppliers

Catalog of suppliers

To build the catalog of all suppliers used for purchasing products by
the lab:
Go to the
## Supplier Collection
folder under
## Stock
->
## Stock Catalog
->
## Suppliers
in the main menu.
Click on the
## + New Supplier
button in the
## Collection
page.
Follow the steps explained in the
## Register Entries
documentation page.
To register several suppliers at once, follow the steps described in
Batch register entries in a Collection.
Catalog of products

To build the catalog of all products purchased in the lab:
Go to the
## Product Collection
folder under
## Stock
->
## Stock Catalog
->
## Products
in the main menu.
Click the
## + New Product
button in the
## Collection
page.
For each product it is necessary to register one supplier as parent. Select the correct supplier from the list of suppliers registered in the
## Supplier Collection.
The process for adding parents is the same as described for Experimental Steps:
Add parents
.
To register several suppliers at once, follow the steps described in
Batch register entries in a Collection.
Creating requests for products to order

Every lab member can create requests for products that need to be
## ordered:
Go to the
## Request Collection
folder under
## Stock
->
## Stock Catalog
->
## Requests
in the main menu.
Click the
## + New Request
button in the
## Collection
page.
When you fill in the form the following information needs to be provided:
## Order Status
. Options  are
## Delivered
,
## Paid
,
## Ordered
,
Not yet ordered
. When you create a request set this field to
Not yet ordered.
Only requests with this
## Order Status
can be processed to orders.
Add the product you for which you want to place a request for order. This can be done in two ways:
add a product that is already present in the catalog. This process is the same as described for adding parents in Experimental steps:
Add parents
. The quantity, i.e. how many units of the product are requested, needs to be specified.
add a product that is not yet registered in the Catalog. In this case the information shown in the picture below needs to be provided. After creating the request, the product entered here is automatically registered in the Product Catalog.
Please note that only 1 product can be added to 1 request.
## Click
## Save
on top of the form.
## STOCK ORDERS

This section is accessible by default by every lab member. However, by
default, only the person in charge of lab purchases can process orders
based on the requests created in the Stock Catalog by every lab member.
Processing product orders from requests

To create orders of products from requests created in the Stock Catalog:
Go to the
## Order Collection
folder under
## Stock
->
## Stock Orders
->
## Orders
in the main menu.
Click the
## + New Order
button in the
## Collection
page.
If you do not see the
## Code
in the form, select
## Show Identification Info
from the
## More..
dropdown
Enter a
## Code
for the order
If an
order
template
form is available (see
Create Templates for Objects
), this template can be used and most fields will be automatically filled (see
Use templates for Experimental Steps
). If no template is available, the relevant fields in the form need to be filled in with the relevant information.
Enter the
## Order Status.
This field is mandatory. Available options are
## Delivered
,
## Paid
,
## Ordered
,
Not yet ordered
. When you first create the order, you should set the status to
Not yet ordered
.
Add one or more requests to the Order. Only requests with Order Status set to
Not yet ordered
will be displayed and can be selected.
## Click
## Save
on top of the form.
If the price information is available in the products, the total cost of
the order is calculated by openBIS and displayed in the order form, as
shown above.
By using the
## Print Order
button in the order form, the order can be
printed as text file that can be sent to the suppliers for purchasing
the products.
To simplify the process of ordering multiple products from the same
supplier, all information related to the same supplier is grouped in one
single text file.
In the example presented in the picture above, there are 2 products to
buy from fluka and 1 product to buy from Sigma-Aldrich. In this case the
two attached files have been printed from the Order form in openBIS,
using the
## Print Order
## button:
order_ORD1_p0;
order_ORD1_p1
Once the order is processed, you should change the
## Order Status
to
## Ordered
. This will automatically change the
## Order Status
in all
connected requests. Requests with this oder status cannot be added to
additional orders.
Updated on April 25, 2023",Stock Catalog,0,en_20.10.0-11_user-documentation_general-users_managing-lab-stocks-and-orders-2_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_managing-lab-stocks-and-orders-2.txt,2025-09-30T12:09:11.062529Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_managing-storage-of-samples:0,Managing Storage Of Samples,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/managing-storage-of-samples.html,openbis,"## Managing Storage Of Samples

Allocate storage positions to samples

If we want to track the storage position of samples, openBIS provides a
graphical overview of lab storages.
Lab storages need to be configured by a
lab manager
or
group admin
,
## as explained here:
## Configure Lab
## Storage
This can be done in two ways:
add storage information on the sample form during (or after) sample
registration
batch register storage positions for several samples
Register storage position for a single sample

1. Navigate to the
## Storage
section, at the bottom of the sample
form. Click the
## + New Storage Positions
above the table, as shown
## below:
In the widget that opens, select the appropriate
## Storage
from the
dropdown menu. Storage must be configured by a lab manager or group
admin as explained in
## Configure Lab
## Storages
3. Select the
position
in the storage (shelf and rack).
4. If the sample is in a box, provide a
## Box Name.
5. Select the
## Box Size
form the list of configured sizes (the list
can be configured by an
## Instance Admin)
.
6. Select the
## Position
in the box.
## 7. Click
## Accept.
Add additional metadata to storage positions

By default, the storage only keeps track of locations. If the
## Storage
## Position
has been configured by an
Instance admin
to have additional
metadata (e.g. freezing date), these can be added by clicking on the
link in the storage table, as shown below. The link becomes available
after saving the sample.
The additional information can be entered in the
## Storage Position
## Object
form.
Batch register storage positions

XLS Batch Registration

With the new XLS batch registration, samples and their storage positions
can be registered in one transaction using the XLS template file, as
explained in
Batch register entries in a
## Collection
.
Batch Registration with TSV file

Storage positions are modelled in openBIS as children of other entries.
To register the positions for several samples with the Batch
Registration using the .tsv template, first the parent samples need to
be registered in openBIS. In a second step, the positions are assigned.
To assign storage positions in batch mode follow the steps below:
## Select
Storage positions
from the
## Batch Registration
drop
down menu.
Download the
template file
.
Remove the
identifier
column from the file (identifiers need
to be automatically generated by openBIS).
Fill in the
parents
column. These are the identifiers of the
samples for which we want to register the storage
positions(/MATERIALS/PROJECT/OBJECT_CODE).
Fill the remaining information about the storage positions.
Save the file and upload with the
## Batch Registration
.
An example file can be found
## here:
SAMPLE-STORAGE_POSITION-template
Updated on April 26, 2023
Batch update storage positions

To update several storage positions, we can use the batch update option
from the Object Browser:
Go to the
## Object Browser
under
## Utilities
in the main menu
Select the object type
## Storage Position
from the dropdown menu
(see picture)
Use the table
## Filter
to select the storage positions you want to
update
(see
## Tables
)
Export the table (see
## Tables
)
Edit the file to make the changes needed (e.g. change the name of a
box, change the storage, change a box position, change box size etc)
## Select
XLS Batch Update Objects
from the
## More..
dropdown.
7. Import the file you modified before and update the storage
positions.
Updated on April 25, 2023
Delete storage positions

Delete single storage positions

To delete a single storage position from a sample:
Edit the sample for which you want to deleted the storage position
Navigate to the
## Storage
section at the end of the page
Use the “
–
” button in the
## Storage Position
table, as shown
in the picture
Save the sample
Please note that the storage position deleted in this way is moved to
the trashcan. To delete the position permanently, this has to be deleted
from the trashcan (see
## Trashcan
).
Remove one of multiple positions in the same box

If one sample has been assigned to multiple positions in the same box
and you need to remove only one or some of them, you can follow these
## steps:
## Edit
the sample for which you need to remove the storage
position in the box
Navigate to the
## Storage
section at the end of the page
Click on the
table row
(see picture below)
## Unselect
the position you want to remove (eg. A5 in the example below)
## Click
## Accept
## Save
the sample
Delete multiple storage positions

To delete multiple storage positions from multiple samples we can use
the
## Object Browser
.
Go to the
## Object Browser
under
## Utilities
in the main menu
## Select
## Storage Position
from the
## Object Type
dropdown
3.
## Filter
the table to find the storage positions you want to
delete
(see
## Tables
)
4. Select the positions you want to delete from the table and click the
## Delete
button (see picture below)
5. You will be asked to provide a reason for deletion
6. The deleted storage positions will be moved to the trashcan and
should be removed from there to be permanently deleted (see
## Trashcan)
Updated on May 2, 2023
## Overview of lab storages

## The
## Storage Manager
, under
## Utilities
, provides an overview of
each single storage configured for the lab, by the lab admin.
Select the storage containing the samples to visualise from the
## Storage
drop down menu.
Click on a box to view its content.
When hovering with the mouse over a sample inside a box, the info
about the sample is shown.
## Overview of lab Storages

Change storage position of samples

## The
## Storage Manager
can also be used to move samples from one
storage position to another, if the location of the sample is changed:
Click on
## Toggle Storage B
(see figure above).
Select the destination storage, from the
## Storage
drop down
menu.
Drag and drop the box or sample to move from
## Storage A
to the
desired position in
## Storage B
. Please note that the move
operation for samples with multiple positions in the same box or
in different boxes is not supported.
Changes are visualised at the bottom of the page. To save them,
click
## Save Changes
on top of the
## Storage Manager
form.
Updated on April 25, 2023",Managing Storage Of Samples,0,en_20.10.0-11_user-documentation_general-users_managing-storage-of-samples_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_managing-storage-of-samples.txt,2025-09-30T12:09:11.131836Z,2
docs:openbis:en_20.10.0-11_user-documentation_general-users_tools-for-analysis-of-data-stored-in-openbis:0,Tools For Analysis Of Data Stored In Openbis,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/tools-for-analysis-of-data-stored-in-openbis.html,openbis,"Tools For Analysis Of Data Stored In Openbis

## Jupyter Notebooks

Jupyter notebooks are web applications that combine text, code and
output (
https://jupyter.org/
). Jupyter supports
over 40 programming languages.
Jupyter notebooks can be used to analyze data stored in openBIS.
It is possible to connect to a JupyterHub server and launch Jupyter
notebooks directly from the openBIS interface. This feature is not
available by default, but needs to be enabled and configured by a
system admin
. JupyterHub docker containers are available from our
## download page:
openBIS
download.
Further documentation can be found here:
JupyterHub for
openBIS
How to use Jupyter notebooks from openBIS

Jupyter notebooks can be opened at every level of the openBIS hierarchy
(
## Space, Project, Experiment/Collection, Object, Dataset
) from the
## More…
dropdown menu, as shown below.
If you get a similar error as the one shown below when you try to launch
a notebook from an entity, you need to start the JupyterHub server by
going to the main menu
## Utilities
->
## Jupyter Workspace
## . This
error appears when the JupyterHub server is restarted (e.g. after an
upgrade), because the user profile needs to be recreated.
If you go to the Jupyter workspace, the user profile is re-created on
the server. After this, you can open a notebook from any entity of the
openBIS hierarchy as explained above (
## Space, Project,
## Experiment/Collection, Object, Dataset
).
Jupyter notebooks can also be launched from the main menu, under
## Utilities
, as shown below.
## Note
: if you use SSO for authentication (eg. Switch aai), the first
time you want to work with a Jupyter notebook, you first need to open
the
## Jupyter Workspace
and then launch a notebook from wherever you
want to open it.
When you launch a notebook from the
## New Jupyter Notebook
in the main
menu under
## Utilities
, it is necessary to enter:
## The
dataset(s)
needed for the analysis.
## The
owner
of the Jupyter notebook. Jupyter notebooks are saved
back to openBIS as datasets, and these belong either to an
## Experiment/Collection
or to an
## Object
. The owner is the
## Experiment/Collection
or
## Object
where the notebook should be
stored.
## The
directory name
. This is the name of the folder that will be
created on the JupyterHub server.
Notebook name
. This is the name of the Jupyter notebook.
Jupyter notebooks can also be opened from a
## Project
,
## Experiment
,
## Experimental Step
choosing the corresponding option in the
## More
drop down menu. When opening notebooks from an
## Experiment
or
## Experimental Step
, all connected datasets are automatically selected.
If some are not needed, they can be deselected.
## Overview of Jupyter notebook opened from openBIS.

The Jupyter notebooks running on the JupyterHub server for openBIS
support the following kernels:
## Bash, Octave, Python 2, Python 3, R,
SoS
(
Script of Scripts).
When you open a Jupyter notebook from openBIS, the default kernel used
is Python 3, but you can change to another language as shown below.
The Jupyter notebook opened from the openBIS interface contains some
pre-filled cells. All cells need to be run. The information of two cells
## should be modified:
Name of the dataset
where the notebook will be
stored and
## Notes
(in red below).
If you are running a JupyterHub version released after July 2021
(available at
https://hub.docker.com/u/openbis
)
you do not need to enter username and password, as authentication uses
the openBIS session token.
What to do in case of invalid session token

If your session token is not automatically renewed you will see a long
error message when you try to retrieve information of a dataset. At the
bottom of the  error message you can see:
In such case, the session token can be manually entered in the cell as
## shown below:
The session token can be copied from the
## User Profile
under the
## Utilities
Main Menu in the ELN.
Enter the session token, run the cell above and then move to the next
cell to get the dataset(s) information.
Alternatively you can go to the Jupyter Workspace under
## Utilities
and restart the server.
Your script should be written in the section named
Process your data
here
, that contains one empty cell (see below). You can, of course, add
additional cells.
After the analysis is done, the notebook can be saved back to openBIS,
by running the last few cells which contain the information about where
the notebook will be stored (as shown below).
The last pre-filled cell in the notebook, contains the information on
where to upload the Jupyter notebook in openBIS. After you run this
cell, you can go back to the ELN interface, refresh the webpage and you
will see your Jupyter notebook uploaded to the Object or Experiment you
specified. By default the Jupyter notebook are save to datasets of type
ANALYSIS_NOTEBOOK. If you prefer to use a different type, you can edit
the pre-filled cell shown above.
Using a local Jupyter installation with openBIS

It is also possible to use a local Jupyter installation with openBIS. In
this case, it is possible to download an extension for JupyterLab that
adds 3 buttons to a default notebook:
connect to an openBIS instance;
download datasets from the openBIS instance;
upload the notebook to openBIS.
The JupyterLab openBIS extension is available from:
JupyterLab openBIS
extension
Updated on April 25, 2023
MATLAB toolbox

The MATLAB toolbox for openBIS allows to access data stored in openBIS
directly from MATALB. Full documentation can be found here:
## MATLAB
## API
Updated on April 17, 2023",Jupyter Notebooks,0,en_20.10.0-11_user-documentation_general-users_tools-for-analysis-of-data-stored-in-openbis_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_general-users_tools-for-analysis-of-data-stored-in-openbis.txt,2025-09-30T12:09:11.196225Z,2
docs:openbis:en_20.10.0-11_user-documentation_legacy-advance-features_index:0,Legacy Advance Features,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/legacy-advance-features/index.html,openbis,"## Legacy Advance Features

openBIS KNIME Nodes
## Introduction
### Installation
### Usage
## Nodes
Definining openBIS URLs
Defining User Credentials for Authentication
openBIS Query Reader
openBIS Report Reader
openBIS Data Set File Importer
openBIS Data Set Registration (Flow Variable Port)
### Usage
openBIS Data Set Registration (URI Port)
openBIS Aggregation Service Report Reader
openBIS Aggregated Data File Importer
KNIME Aggregation Service Specifications
KNIME Aggregation Service Helper API
Example for an Aggregation Service Report Reader
Example for an Aggregated Data File Importer",Legacy Advance Features,0,en_20.10.0-11_user-documentation_legacy-advance-features_index_0,concept,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_legacy-advance-features_index.txt,2025-09-30T12:09:11.259837Z,2
docs:openbis:en_20.10.0-11_user-documentation_legacy-advance-features_openbis-kinme-nodes:0,openBIS KNIME Nodes,https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/legacy-advance-features/openbis-kinme-nodes.html,openbis,"openBIS KNIME Nodes

## Introduction

## KNIME
is a powerful workflow system. It allows
to import data from some sources and process them in a workflow
graphically designed by the user.
There are special openBIS KNIME nodes for importing/exporting data
from/to openBIS. KNIME version 2.7.2 or higher is required.
### Installation

Start KNIME application.
Click on menu item ‘Install New Software…’ of menu ‘Help’. An
installation dialog pops up.
Click on the add button. A dialog titled ‘Add Repository’ pops up.
Enter a name like ‘KNIME Community Nodes’ and the URL
http://update.knime.org/community-contributions/3.1
Check the check box of ‘openBIS Knime Nodes’ in section ‘Community
Contributions - Bioinformatics & NGS’ and click twice the next
button.
Accept the license agreements.
Click the finish button.
Ignore the security warning and restart KNIME application.
### Usage

All openBIS KNIME nodes can be found in Node Repository under Community
## Nodes -> openBIS:
Drag and drop a node into the project and double-click on the node. A
node setting dialog opens for entering parameters.
## Nodes

All nodes need
URL of openBIS server, like
https://sprint-openbis.ethz.ch/openbis
.
User credentials
When configuring a node in the node setting dialog the user is asked for
these parameters in the section ‘Connection Parameters’:
After pressing the button
connect
a connecting to the openBIS server
will be established. This is needed for editing additional node
parameters. For example, the combo boxes of the reader nodes have to be
populated.
## Warning
For a data set registration node the credentials combo box is only filled if all nodes of the upstream part of the workflow are successfully configured.
The OK button closes the node setting dialog. The connection parameters
and all other parameters will be stored and used when executing a
workflow.
Definining openBIS URLs

Contrary to the previous version of openBIS KNIME nodes (Version 13.04.0
and earlier) the URL field in the node setting dialog is no longer a
text field but a combo box with URLs. This list of predefined URLs is
initially empty. It has to be created by the following preference page:
Defining User Credentials for Authentication

For security reasons it is not recommended to specify user ID and
password directly for each openBIS node. Instead named credentials
should be used. This has the advantage to enter user ID and password
only once for a workflow with several openBIS nodes.
Named credentials are defined for a particular workflow. They are called
workflow credentials and can be specified via the context menu of the
## workflow:
Each set of credentials has a name (which is used in the combo box), a
user ID (called ‘Login’) and a password:
The credentials are saved with the workflow except of the passwords. The
user will be asked for the passwords after loading a workflow.
## Warning
If user ID and password are entered directly in the node setting dialog the KNIME master key on the preferences page
KNIME -> Master Key
should be activated. Otherwise passwords will be stored unencrypted!
openBIS Query Reader

This node allows to run parametrized SQL queries on openBIS. The combo
box shows a list of available queries. After choosing one additional
parameters have to be entered.
openBIS Report Reader

This node allows to get a report for a specified data set. The combo box
shows a list of available report. After choosing a report a data set
should be entered. The button with three dots lets pop up a dialog for
convenient way to choose a data set.
openBIS Data Set File Importer

This nodes allows to download a particular file from a specified data
set. Data set code, file path and a local folder for downloads have to
be specified. The output of the node is not a table put an object of
type
org.knime.core.data.uri.URIPortObject
. Other nodes with input
ports of this type can access the downloaded file. Such nodes exist in
GenericKnimeNodes of the Community Nodes (which are a part of openMS
KNIME Nodes). Also ‘openBIS Data Set Registration (URI Port)’ is such a
node.
The absolute path of the downloaded file is also available as a flow
variable
absolute-file-path
. This allows to connect a openBIS Data Set
File Importer with a file reader which supports absolute file paths in
flow variables like the CSV Reader node. The mechanism of connecting
both nodes via flow variable ports is explained in the next section
where a CSV Writer node is connected with an openBIS Data Set
Registration node.
This importer node also creates the following KNIME flow variables:
openbis.DATA_SET
,
openbis.EXPERIMENT
, and
optionally
openbis.SAMPLE
. These variables contain data set code,
experiment identifier, and sample identifier, respectively. The flow
variable
openbis.SAMPLE
identifier only appears if the data set is
directly link to a sample. KNIME flow variables are available to other
nodes downstream.
openBIS Data Set Registration (Flow Variable Port)

This node allows to register a file as a data set. The path of the file
to be registered is the value a flow variable specified in the node
settings dialog. In addition the user has to specify owner type and data
set type.
The owner identifier (which is either a data set code, an experiment
identifier, or a sample identifier depending on the chosen owner type)
can be chosen by a chooser dialog. If the owner field is empty one of
the flow variables s
openbis.DATA_SET
,
openbis.EXPERIMENT
, or
openbis.SAMPLE
will be used.
### Usage

This node is usually used in combination with a writer node which stores
data (e.g. data table) in a file. Writer nodes are end nodes of a
workflow. But it is possible to append another node downstream by using
the flow variable port. Normally the flow variable ports are not
visible. To make them visible choose item ‘
## Show Flow Variable Ports
’
of the context menu of the node. Two red circle will appear at the upper
corners of the node symbol:
add a node of type ‘openBIS Data Set Registration (Flow Variable Port)’
and connect the upper right circle of the writer node with the input
node of the registration node. A click on ‘Hide Flow Variable Ports’ of
the context menu of the writer node hides the upper left circle:
you need to tell the registration node which flow variable has the path
to the file to be registered. This needs two steps:
The configuration parameter of the writer has to be made available
as a flow variable. This can be done in tab ‘Flow Variables’ of the
node settings dialog. It lists all configuration parameters. If a
name is specified in the text field of a certain parameter its value
will be available as a flow variable of specified name for the
downstream nodes. Here is the example for CSV Writer:
This works for all writers. There is an easier way for CSV Writer:
On the Settings tab there is small button named ‘
v=?
’. Clicking on
this button opens a dialog where the flow variable for the file name
can directly be specified by using ‘Create Variable’:
In the registration node the flow variable specified in the first
step has to be chosen as the file variable:
openBIS Data Set Registration (URI Port)

This nodes allows to register a file as a data set. The file to be
registered is the first one in the list of URIs of the port object of
type
org.knime.core.data.uri.URIPortObject
. The user has to specify
owner type and data set type in the node settings dialog.
The owner identifier (which is either a data set code, an experiment
identifier, or a sample identifier depending on the chosen owner type)
can be chosen by a chooser dialog. If the owner field is empty one of
the flow variables
openbis.DATA_SET
,
openbis.EXPERIMENT
, or
openbis.SAMPLE
will be used.
openBIS Aggregation Service Report Reader

This nodes allows to get an
aggregation
service
report. Only
aggregation services where the service key starts with
knime-
can be
chosen by the user in the node settings dialog.  After the service has
been chosen the aggregation service will be invoked with the parameter
## _REQUEST_
set to
getParameterDescriptions
. The service has to return
a table where each row defines the name of the parameter and optionally
its type. This is used to created an appropriated form in the node
settings dialog. The values specified by the user will be used to invoke
the aggregation service when the node is executed. The result will be
available as a KNIME table. See also section
KNIME Aggregation Service
## Specifications
.
openBIS Aggregated Data File Importer

This nodes allows to invoke an
aggregation
service
which returns a name
of a file in the session workspace which will be downloaded and made
available for nodes with input ports of type
org.knime.core.data.uri.URIPortObject
. Such nodes exist in
GenericKnimeNodes of the Community Nodes. Also ‘openBIS Data Set
Registration (URI Port)’ is such a node.
Only aggregation services where the service key starts
with
knime-file-
can be chosen by the user in the node settings
dialog.  The communication protocol between this node and openBIS is as
for nodes of type ‘openBIS Aggregation Service Report Reader’. The only
difference is that the returned table has only one row with one cell
which contains the file name.
KNIME Aggregation Service Specifications

Nodes of type ‘openBIS Aggregation Service Report Reader’ and ‘openBIS
Aggregated Data File Importer’ rely on
aggregation
services
which follow a
certain protocol. In order to distinguish these services from other
aggregation services the service key (i.e.
core
plugins
ID) has to start
with
knime-
. The specifications of such services are the following:
If there is a parameter
## _REQUEST_
with
value
getParameterDescriptions
descriptions of all parameters will
be returned in the form specified as follows:
The table has the columns
name
and
type
.
Each row has a non-empty unique value of column
name
## . It
specifies the name of the parameter. It is also shown in node
settings dialog.
The type columns contains either an empty string or
## VARCHAR
,
## VOCABULARY
,
## EXPERIMENT
,
## SAMPLE
, or
## DATA_SET.
The default
type is
## VARCHAR
which is represented in the node settings
dialog by a single-line text field. The types
## EXPERIMENT
,
## SAMPLE
, and
## DATA_SET
are also single line text field with an
additional button to open an appropriate chooser.
The type
## VOCABULARY
isn’t useful without a list of terms in
the following form:
VOCABULRY:<term
1>,
<term
2>,
...
.
## Example:
VOCABULARY:Strong,
## Medium,
## Weak
If there is no parameter
## _REQUEST_
or its value
isn’t
getParameterDescriptions
the aggregation service can assume
that all parameters as defined by the parameters description are
present. Some of them might have empty strings as values.
An exception should be returned as a table with five columns where
the first column is
## _EXCEPTION_
. If such a table is returned an
exception with stack trace will be created and thrown in KNIME. It
will appear in KNIME log. For each row either the first cell isn’t
empty or the five other cells are not empty. In the first case the
value of the first column is of the form
## :
. If the first column is empty
the row represents a stack trace entry where the other columns are
interpreted as class name, method name, file name, and line number.
In order to simplify KNIME aggregation services a Helper API in Java is
available
openbis-knime-server.jar
.
It should be added to openBIS installation in
folder
<installation
folder>/servers/datastore_server/ext-lib
.
KNIME Aggregation Service Helper API

The helper API contains the two
classes
ch.systemsx.cisd.openbis.knime.server.AggregationCommand
and
ch.systemsx.cisd.openbis.knime.server.AggregationFileCommand
which
should be extend when writing an aggregation service for nodes of type
‘openBIS Aggregation Service Report Reader’ and ‘openBIS Aggregated Data
File Importer’, respectively.
The subclasses should override the method
defineParameters()
## . Its
argument is a
ParameterDescriptionsBuilder
which simplifies creation
of parameter descriptions.
## If
AggregationCommand
/
AggregationFileCommand
is subclassed the
method
aggregate()/createFile()
should be overridden. The
aggregate()
methods gets the original arguments which are the
parameters binding map and the ISimpleTableModelBuilderAdaptor. The
createFile()
methods gets only the parameters binding map. It returns
the name of the file in the session workspace.
The aggregation service should instanciate the subclass and
invoke
handleRequest()
with the parameters binding map and the table
model builder adaptor.
## The
ParameterDescriptionsBuilder
has the method
parameter()
## . It
creates a
ParameterDescriptionBuilder
based on the specified parameter
name. The
ParameterDescriptionBuilder
has the
methods
text()
,
vocabulary()
,
experiment()
,
sample()
,
dataSet()
which specify the parameter type. Only
vocabulary()
has an
argument: The string array of vocabulary terms.
Example for an Aggregation Service Report Reader

from
ch.systemsx.cisd.openbis.knime.server
import
AggregationCommand
from
ch.systemsx.cisd.openbis.generic.shared.api.v1.dto
import
SearchCriteria
from
ch.systemsx.cisd.openbis.generic.shared.api.v1.dto
import
SearchSubCriteria
from
ch.systemsx.cisd.openbis.generic.shared.api.v1.dto.SearchCriteria
import
MatchClause
from
ch.systemsx.cisd.openbis.generic.shared.api.v1.dto.SearchCriteria
import
MatchClauseAttribute
## EXPERIMENT
=
## 'Experiment'
## DATA_SET_COLUMN
=
'Data Set'
## PATH_COLUMN
=
## 'Path'
## SIZE_COLUMN
=
## 'Size'
def
scan
(
tableBuilder
,
dataSetCode
,
node
## ):
if
node
.
isDirectory
## ():
for
child
in
node
.
childNodes
## :
scan
(
tableBuilder
,
dataSetCode
,
child
)
else
## :
row
=
tableBuilder
.
addRow
()
row
.
setCell
(
## DATA_SET_COLUMN
,
dataSetCode
)
row
.
setCell
(
## PATH_COLUMN
,
node
.
relativePath
)
row
.
setCell
(
## SIZE_COLUMN
,
node
.
fileLength
)
class
MyAggregationCommand
(
AggregationCommand
## ):
def
defineParameters
(
self
,
builder
## ):
builder
.
parameter
(
## EXPERIMENT
)
.
experiment
()
def
aggregate
(
self
,
parameters
,
tableBuilder
## ):
experiment
=
searchService
.
getExperiment
(
parameters
.
get
(
## EXPERIMENT
))
searchCriteria
=
SearchCriteria
()
subCriteria
=
SearchCriteria
()
subCriteria
.
addMatchClause
(
MatchClause
.
createAttributeMatch
(
MatchClauseAttribute
.
## PERM_ID
,
experiment
.
permId
))
searchCriteria
.
addSubCriteria
(
SearchSubCriteria
.
createExperimentCriteria
(
subCriteria
))
dataSets
=
searchService
.
searchForDataSets
(
searchCriteria
)
tableBuilder
.
addHeader
(
## DATA_SET_COLUMN
)
tableBuilder
.
addHeader
(
## PATH_COLUMN
)
tableBuilder
.
addHeader
(
## SIZE_COLUMN
)
for
dataSet
in
dataSets
## :
dataSetCode
=
dataSet
.
dataSetCode
try
## :
content
=
contentProvider
.
getContent
(
dataSetCode
)
scan
(
tableBuilder
,
dataSetCode
,
content
.
rootNode
)
finally
## :
if
content
!=
## None
## :
content
.
close
()
def
aggregate
(
parameters
,
tableBuilder
## ):
MyAggregationCommand
()
.
handleRequest
(
parameters
,
tableBuilder
)
Example for an Aggregated Data File Importer

import
os.path
from
java.util
import
## Date
from
ch.systemsx.cisd.openbis.knime.server
import
AggregationFileCommand
class
MyAggregationFileCommand
(
AggregationFileCommand
## ):
def
defineParameters
(
self
,
builder
## ):
builder
.
parameter
(
## 'Greeting Type'
)
.
vocabulary
([
## 'Hi'
,
## 'Hello'
])
builder
.
parameter
(
## 'Name'
)
builder
.
parameter
(
## 'Sample'
)
.
sample
()
def
createFile
(
self
,
parameters
## ):
sessionWorkspace
=
sessionWorkspaceProvider
.
getSessionWorkspace
()
filename
=
""report.txt""
output
=
open
(
os
.
path
.
join
(
sessionWorkspace
.
getAbsolutePath
(),
filename
),
""w""
)
name
=
parameters
.
get
(
## 'Name'
)
sample
=
searchService
.
getSample
(
parameters
.
get
(
## 'Sample'
))
output
.
write
(
str
(
parameters
.
get
(
## 'Greeting Type'
))
+
"" ""
+
str
(
name
)
+
""!
\n\n
""
+
## Date
()
.
toString
()
+
""
\n
""
)
output
.
write
(
sample
.
getSampleType
())
output
.
close
()
return
filename
def
aggregate
(
parameters
,
tableBuilder
## ):
MyAggregationFileCommand
()
.
handleRequest
(
parameters
,
tableBuilder
)",Introduction,0,en_20.10.0-11_user-documentation_legacy-advance-features_openbis-kinme-nodes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\openbis\en_20.10.0-11_user-documentation_legacy-advance-features_openbis-kinme-nodes.txt,2025-09-30T12:09:11.325380Z,2
docs:datastore:en_concepts:0,Concepts of Data Store and openBIS,https://datastore.bam.de/en/concepts,datastore,"concepts
# Concepts of Data Store and openBIS
## Page Contents
## Collection
## Controlled Vocabulary
## Dataset
Data Structure
## Dropbox
Entity and Entity Types
## Inventory
Intances in the Data Store
## Jupyter Notebook
## Lab Notebook
## Masterdata
## Metadata
## Object
## Parent-Child Relationship
## Project
## Property
pyBIS
Roles defined in openBIS
Roles and Rights
Roles defined per default in the Data Store
Roles Management: Access to Spaces and Projects
## Space
## Demidova, Caroline
¶
## Collection
In openBIS, a
## Collection
is a folder with user-defined
## Properties
located on the third level of the hierarchical data structure (Space/Project/
## Collection
## /Object). A
## Collection
is always part of a
## Project
.
## Collections
of the same type are described by the same set of
## Properties
.
## Collection
types are defined as part of the openBIS
masterdata
.
## Datasets
can be attached to
## Collections
.
## A
## Collection
can logically group an unlimited number of
## Object
of one or more
## Object
types. For instance, a
## Collection
of the type ""Measurement Devices"" can be used to organize
## Objects
of the type ""Instrument"" in the
## Inventory
## . A
## Collection
of the type ""Default Experiment"" can be used to organize
## Objects
of the type ""Experimental Step"" in the
## Lab Notebook
.
¶
## Controlled Vocabulary
A controlled vocabulary is an established list of terms to provide consistency and uniqueness in the description of a given domain, e.g., a list of room labels, SI units or purity grades. In openBIS, controlled vocabularies are a possible data type for metadata
## Properties
. Each term in a controlled vocabulary has a code, a label, and a description. All existing controlled vocabularies and their terms are listed in the Vocabulary Browser in the Utilities.
¶
## Dataset
In openBIS, a
## Dataset
is a folder with user-defined
## Properties
that can contain individual files of arbitrary formats (e.g., images, csv files, xml files, etc.) as well as complex folder structures with subfolders and many files (of potentially different formats). The content of
## Datasets
(but not their metadata) is immutable, i.e., it cannot be edited after creation.
## Datasets
of the same type are described by the same set of
## Properties
.
## Dataset
types are defined as part of the openBIS
masterdata
.
## A
## Dataset
has to be attached to either an
## Object
or to a
## Collection
## . Different
## Datasets
can be connected via
parent-child relationships
.
¶
Data Structure
The openBIS data structure is hierarchically organized in five (sub)folders called
## Space
,
## Project
,
## Collection
,
## Object
and
## Dataset
. Folder names can be customized by users.
To digitally represent an
## Object
, a
## Space
,
## Project
and
## Collection
need to be created first.
¶
## Dropbox
The Dropbox is a core openBIS plugin that allows the upload of (large) data files to an openBIS instance. Instead of using the user interface for
## Dataset
registration, users move their data files (+ information about the
## Object
or
## Collection
they should be attached to) to a dedicated Dropbox folder in a file-service at BAM that is continously monitored. Once new data files are detected inside the Dropbox folder, they are automatically uploaded as
## Datasets
to the openBIS instance.
It is possible to control the
## Dataset
registration process via Dropbox scripts written in Python. The script can register new
## Datasets
,
## Objects
,
## Properties
and
parent-child relationships
as part of its processing. The Dropbox framework also provides tools to track file operations and, if necessary, revert them, ensuring that the incoming file or directory is returned to its original state in the event of an error.
The Dropbox is not related to the commercial file hosting service.
¶
Entity and Entity Types
An Entity is an item of the ""real world"" (tangible/non tangible) that is uniquely identified by attributes (
## Properties
). An Entity Type is a collection of Entities with similar properties. An Entity Type is an object in a data model. In openBIS relevant entity types are
## Collection
,
## Object
and
## Dataset
types. Entity types can only be created by someone with the Instance admin role.
¶
## Inventory
The Inventory is one of the two main components of openBIS. It is used for the digital representation of shared laboratory inventory and the storage of related data files such as measuring instruments, chemical substances, and samples, but can also be user for storing protocols, standard operating procedures (SOPs) and publications. The Inventory is organized into
## Spaces
,
## Projects
and
## Collections
according to the hierarchical
data structure
of openBIS.
By default, each BAM division gets
private
## Inventory
## Spaces
(Equipment, Materials, Methods, Publications) that are only accessible to division members
public
## Inventory
## Projects
(Equipment, Materials, Methods) that are accessible to every user of the Data Store.
¶
Intances in the Data Store
In the Data Store, various instances are available to support users during different stages of the onboarding and data management process. Below are two key instances and their roles:
## Instance
## Users
### Configuration
Access time
Training Data Store
DSSt during the onbodring
Multi-group instance
Only during onboarding
Main Data Store
Onboarded divisions
Multi-group instance
After onboarding continously
¶
## Jupyter Notebook
Jupyter Notebook is a web-based interactive computing platform that combines live code, equations, narrative text, visualizations, interactive dashboards and other media. Jupyter Notebooks can be used to analyze data stored in an openBIS instance.
¶
## Lab Notebook
The Lab Notebook is one of the two main components of openBIS. It is the digital version of a paper lab notebook and can be used for the digital representation and documentation of experimental procedures and analyses and the storage of related data files according to good scientific practice.
By default, each user gets their own personal
## Space
in the Lab Notebook where they can represent multiple research
## Projects
. Within a given
## Project
,
## Collections
can be used to represent comprehensive experiments which comprise individual
## Objects
, e.g., of the type ""Experimental Step"". Access to the personal Lab Notebook
## Space
or individual
## Projects
can be shared with colleagues.
¶
## Masterdata
The term ""Masterdata"" describes all information structures and plugins that are used to define metadata in openBIS (i.e., masterdata = ""meta-metadata""). Masterdata is comprised of Entity types, i.e.,
## Collection
,
## Object
and
## Dataset
types, as well as
## Property
types,
controlled vocabularies
and related scripts (e.g., dynamic property plugins and entity validation scripts). Domain-specific masterdata have to be defined by the Data Store Stewards of the BAM divisions, but can only be imported to the openBIS instance (and edited) by Instance Admins.
¶
## Metadata
Metadata is ""data about data"" that provides the information needed to find, interpret and understand research data. This includes general
## Properties
such as Code, Name, Description and more specific
## Properties
defined by users within Entity Types (for the Data Store starting from rollout phase IV, the only Entity types that are defined are
## Object
## Types
which might include controlled vocabularies).
The following table provides overview on the metadata generated along the openBIS data structure (
## Space
,
## Projects
,
## Collection
,
## Object
and
## Dataset
).
¶
## Object
In openBIS, an
## Object
is an entity with user-defined
## Properties
located on the fourth level of the hierarchical data structure (Space/Project/Collection/
## Object
).
## An
## Object
is always part of a
## Collection
.
## Objects
of the same type are described by the same set of
## Properties
.
## Object
types are defined as part of the openBIS
masterdata
.
## Datasets
can be attached to
## Objects
.
## An
## Object
can be used to represent any kind of physical or intangible entity. For instance, an
## Object
of the type ""Chemical"" can be used to represent a batch of ethanol in the
## Inventory
## . An
## Object
of the type ""Experimental Step"" can be used to represent a measurement or an analysis in the
## Lab Notebook
.
¶
## Parent-Child Relationship
A parent-child relationship is a directed link (or ""directed edge"" in graph theory) between two
## Objects
(Object1 --> Object2) or between two
## Datasets
(Dataset1 --> Dataset2) in an openBIS Instance. For a given relationship between two
## Objects
(or
## Datasets
), the
## Object
with the outgoing edge is called the ""parent"" and the
## Object
with the incoming edge is called the ""child"". It is not possible to have parent-child relationships between
## Objects
and
## Datasets
or between other entity types (e.g.,
## Collection
).
Parent-child relationships can be used to represent different kinds of logical connections between
## Objects
(or
## Datasets
## ), e.g.:
a partition of an entity:
## Object
""Sample 1"" is parent of
## Objects
""Sample 1A"" and ""Sample 1B"" because the original sample was broken up into two smaller sub-samples,
context in a research process, e.g., Object ""Experimental Step 1"" is child of the Objects ""Sample 1A"" and ""Measurement Device"" because during the experimental step, the measurement device was used to measure some properties of the sub-sample,
a temporal sequence of different steps in a workflow: Object ""Experimental_Step_1"" is parent of ""Experimental Step 2"" because the first experimental step was conducted prior to the second.
When all of these
## Objects
and their connections to each other are combined, we get a hierarchy tree (or a ""directed acyclic graph"" (DAG) in graph theory):
Parent-child relationships are not allowed to form cycles within the graph (e.g., an
## Object
cannot be both parent and child of another
## Object
), otherwise an error will be reported.
Parent-child relationship can also be used to represent relations between
## Datasets
, e.g., ""Dataset_v2"" being the parent of ""Dataset_v1"" because the second
## Dataset
is a newer version of the first one.
Parent-child relationships between
## Objects
(or
## Datasets
) are independent of the folder hierarchy, i.e.,
## Objects
(or
## Datasets
) can be connected across different
## Spaces
and
## Projects
and irrespective of whether they are located in the
## Inventory
or the
## Lab Notebook
.
By default, every
## Object
(or
## Dataset
) can have a unlimited number of parents and/or children or none (N:N relationships with N being any number from 0 to N). For a given
## Object
type, group admins can set a minimum and maximum number of children and parents of a certain type in the settings.
¶
## Project
In openBIS, a
## Project
is a folder located on the second level of the hierarchical data structure (Space/
## Project
## /Collection/Object). A
## Project
is always part of a
## Space
## . A
## Project
can logically group an unlimited number of
## Collections
.
For instance, a
## Project
""Reagents"" can be used to organize
## Collections
of the type ""Chemicals"" in the
## Inventory
. A Project ""Master Thesis"" can be used to organize
## Collections
of the type ""Experiment"" in the
## Lab Notebook
.
Apart from a code (PermId) and a description,
## Projects
have no metadata. User access rights can be defined at the
## Project
-level.
¶
## Property
In openBIS, a
## Property
is a metadata field that can be used to describe a
## Collection
, an
## Object
or a
## Dataset
.
## Properties
can be of different
data types
, e.g., numbers (Boolean, real, integer), text, hyperlink, date,
controlled vocabularies
but also tabular data.
¶
pyBIS
pyBIS is a Python module for interacting with openBIS. Most actions that can be carried out in the openBIS graphical user interface (GUI) can also be done via pyBIS. pyBIS is designed to be most useful in a
## Jupyter Notebook
or IPython environment.
¶
Roles defined in openBIS
The openBIS roles defined the rights that users get assigned to manage the research data stored in the BAM Data Store. In openBIS, there are four different types of roles, in descending order of rights:
## Admin
## User
## Observer
openBIS roles are assigned to users at different levels:
for the complete openBIS instance (only Admin or Observer role)
for a
## Space
or a
## Project
## : Adin, User, Observer
Roles assigned to
## Spaces
and
## Projects
also apply to the corresponding subfolders (
## Collections
,
## Objects
and
## Datasets
).
Since users with the role of Instance Admin  have full access to the entire instance and all
## Spaces
contained therein, this role is the sole responsibility of the core team of the Data Store team. Only Instance Admins can make changes to the Masterdata, create new
## Spaces
, and edit the settings of the ELN-LIMS User Interface (UI).
¶
Roles and Rights
The corresponding rights to openBIS User roles are summarized in the table below.
For additional information on roles and permissions, please refer to the official openBIS docs
here
.
## Role
## Rights
Instance Admin (Data Store Team)
- Full access to the complete openBIS Instance
-
## Space
/
## Project
Admin rights
- Create and edit Masterdata
- Create and edit
## Spaces
- Create/manage
## Space
Admin role
Group Admin (Division Head, DSSt)
-
## Space
/
## Project
Admin rights
- Customise the group‘s ELN Settings
- Revert deletions
## Space/Project Admin
-
## Space
/
## Project
User rights
- Assign and remove
## Space
/
Project roles
## -Create
## Projects
## -Delete
## Project
s,
## Collections
,
## Objects
,
## Datasets
- Save searches
## Space
/
## Project
## User
- Observer rights
## - Create
## Collections
and
## Objects
## - Edit
## Projects
,
## Collections
and
## Objects
## Observer
- Read-only access
## - Download
## Datasets
¶
Roles defined per default in the Data Store
Default roles are based on the
## Lab Notebook
and
## Inventory
structure and the multi-groups set up of the Data Store. By default, all division members / users (who are not DSSt or division leads) have the following roles:
## Space/Project Admin
in their personal Lab Notebook
## Space
(e.g., X.1 Amueller)
## Observer
in the personal Lab Notebook of colleagues from the same division
## Space/Project User
in the private Inventory of their devision (e.g., X.1 EQUIPMENT, X.1 MATERIALS, X.1 METHODS, X.1 PUBLICATIONS)
## Observer
in the public Inventory of the other divisions (BAM EQUIPMENT, BAM MATERIALS, BAM METHODS, BAM PUBLICATIONS)
## Space/Project User
in the public Inventory of their group (X.1 EQUIPMENT OPEN, X.1 MATERIALS OPEN, X.1 METHODS OPEN, X.1 PUBLICATIONS OPEN)
By default, all Data Store Stewards (DSSt) and division leads/Group Admins have following roles:
## Space/Project Group Admin
in their personal Lab Notebook
## Space
and the Lab Notebook of all division members.
## Space/Project Group Admin
in all Inventory Spaces of their division (X.1 EQUIPMENT, X.1 MATERIALS, X.1 METHODS, X.1 PUBLICATIONS).
## Space/Project User
in all public Inventory BAM Spaces (X.1 EQUIPMENT OPEN, X.1 MATERIALS OPEN, X.1 METHODS OPEN, X.1 PUBLICATIONS OPEN)
¶
Roles Management: Access to Spaces and Projects
openBIS roles can be assigned to individual users or groups. The main instance of the BAM Data Store is organized as a multi-group instance, with each BAM division (“Fachbereich”) corresponding to a group.
Group Admins in openBIS are assigned by default to division heads, their deputies, and Data Store Stewards (DSSt) for all Spaces within their division (group). They are responsible for managing user roles within their group(division), specifically for the Spaces and Projects where they hold Admin rights.
Please note that
access can only be managed at the
## Space
and
## Project
level
and NOT on the level of individual
## Collections
,
## Objects
and/or
## Datasets
. The rights granted for a
## Space
apply to all subfolders/entities in the openBIS hierarchical data structure (
## Project
,
## Collection
,
## Object
,
## Dataset
), while rights granted at the Project level apply to all subfolders of the same (
## Collection
,
## Object
,
## Dataset
).
To manage access rights at the
## Space
/
## Project
level in the ELN-LIMS UI, click on the “More” button and select “Manage access”. Note that roles can only be assigned to users or groups that have been granted access by the Instance Admins (Data Store Team).
¶
## Space
In openBIS, a
## Space
is a folder located on the first level of the hierarchical data structure (
## Space
## /Project/Collection/Object). A
## Space
is either located in the
## Inventory
or in the
## Lab Notebook
## . A
## Space
can logically group an unlimited number of
## Projects
.
For instance, a
## Space
""Materials"" can include the
## Project
""Reagents"" in the Inventory. A
## Space
""Master Students"" can include the
## Project
""Master Thesis"" in the Lab Notebook.
Apart from the permanent ID (PermId) and a description,
## Spaces
have no metadata. User access rights can be defined at the
## Space
-level.",Concepts of Data Store and openBIS,0,en_concepts_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_concepts.txt,2025-09-30T12:09:11.472948Z,1
docs:datastore:en_faq:0,Frequently Asked Questions (FAQ),https://datastore.bam.de/en/faq,datastore,"faq
# Frequently Asked Questions (FAQ)
## Page Contents
## General
IT Infrastructure
Data Formats
## Metadata
## Traceability
Data Analysis
Data Import/Export
## Interfaces
## Development
## Rollout
## Pilot Phase
## Meindl, Kristina
¶
## General
Who can use the Data Store? Is the use of the Data Store mandatory?
In the future, all BAM employees who work with research data will be able to use the Data Store. Divisions that have registered for the rollout commit themselves to using the Data Store after the end of the onboarding phase. It is the responsibility of the division head to ensure that the Data Store is used appropriately.
Who is responsible for maintaining the Data Store?
The Data Store is operated as a central service by the central IT (VP.2). Training and consulting is provided by the eScience section (VP.1). The divisions themselves are responsible for the content of their group and maintenance of metadata and data.
What kind of data should be stored and what data should NOT be stored in the Data Store?
The Data Store is primarily a system for the internal storage of research data produced at BAM. According to the BAM Data Policy, research data include all digital data that are the basis, object, work steps or result of research processes as well as data that serve to describe the former. Typical examples of research data are measurement and observation data, experimental data obtain in the laboratory, audiovisual information, methodological test procedures and protocols. Simulations, software source code, algorithms, and derivations are also research data that can be stored in the Data Store. However, since the Data Store does not include a version management system, it is recommended to use a more suitable service for software development projects, for example the
BAM GitHub
(more information can be found
here
in the Infoportal). It is not allowed to store private data, in particular no personal data, in the Data Store. If you are not sure whether your data may be stored in the Data Store, please contact
datastore@bam.de
.
How is the Data Store organized?
The Data Store is divided into groups that correspond to the BAM divisions. By default, each group receives its own Inventory for the digital representation of laboratory inventory such as measuring instruments, chemical substances, samples, protocols, etc., as well as associated documents (e.g. technical data sheets). By default, each user additionally receives their own Lab Notebook with personal folders for the documentation of research activities and related data. If required, project-based folders (for several project members) can also be created in the Lab Notebook in addition to the personal folders.
Can closely collaborating divisions have common Spaces in the Data Store Inventory?
Each division is assigned open Spaces (readable by all Data Store users) and closed Spaces (readable and editable only by division members) in the Data Store Inventory. Group Admins can grant read/write access to the Spaces to members of other divisions to enable joint work. If required, dedicated shared Spaces can be created as well.
How are access rights defined within the Data Store?
The Data Store is divided into groups, which correspond to BAM divisions. Within a group, roles can be assigned and access rights can be further defined. Users with the appropriate rights can also grant specific access to Spaces and Projects to non-group members. More information about the rights and role system in the Data Store can be found
here
.
Can changes in the BAM organization (e.g. merging of divisions) be represented in the Data Store?
The user accounts in the Data Store as well as the organizational affiliation of the users are derived from the central user directory of BAM (LDAP). If your division is affected by upcoming organizational changes (e.g. renaming or merging) and if the division is  already using the Data Store, please contact
datastore@bam.de
.
Is the Data Store/the underlying software openBIS multilingual?
No, the user interface of openBIS as well as the official documentation of the ETHZ are only available in English. Training materials for the Data Store are also provided in English. If you need a translation of certain documents/articles, please contact us via datastore@bam.de. Except for justified exceptions, we also recommend documenting user-defined metadata in openBIS exclusively in English to ensure better interoperability.
Is the Data Store also suitable for the management of data generated in the context of testing tasks/scientific and technical services (Wissenschaftlich-Technisches Dienstleistungen, WTD)?
In principle, any type of data can be stored in the Data Store and described with metadata. Whether the Data Store is also suitable for managing test data will be investigated during the rollout.
My data files are very large, can the Data Store handle them?
The actual data files are stored in the Data Store as files on disk storage, only the metadata is stored in the database. This means that even very large data volumes are possible, as long as the corresponding storage resources are available. In addition, there is the possibility via git-annex in openBIS to manage only references to datasets when they become too large.
¶
IT Infrastructure
What happens to the existing file services after the introduction of the Data Store?
The personal file services (drive ""M:"") will still be provided. The necessity of shared file services by divisons, project groups, or departments will be reviewed. If required, they will be provided as a supplement to the Data Store.
Where is the data stored in the Data Store physically located? Is there be a backup of the Data Store?
All data of the Data Store are exclusively stored on servers in the computing center of BAM (UE). The datasets/metadata are backed up regularly/continuously in a multi-stage process.
Will the Data Store be permanently available?
As a central RDM system, the Data Store is designed for permanent operation as far as this is technically possible. However, it will not be possible to avoid interruptions  for maintenance work and updates.
Will there be access to the Data Store from the lab networks (""Labornetze"")?
Yes, access from laboratory networks will be enabled where necessary.
Will there be access for external parties to the Data Store?
No, the Data Store is designed as a system for internal research data management at BAM and requires access from internal BAM networks (including VPN). Therefore, external users cannot access the Data Store. However, it is possible to export data from the Data Store, e.g., to the public repository
## Zenodo
.
¶
Data Formats
Which file formats can be stored in the Data Store?
openBIS works independently of file formats: The data are stored as files in the file system, the associated metadata in a database.
Can I read/work with proprietary file formats in the Data Store?
The Data Store is not a live environment for reading or editing files, independent of whether their format is open or proprietary. To read or edit files stored in the Data Store using specialized software, they must be downloaded locally. The modified files can then be re-saved in the Data Store and linked to the original dataset. If the underlying file format allows programmatic access, files stored in the Data Store can also be read and analyzed via APIs (for example, with the Python module
pyBIS
and Jupyter Notebook), but not modified.
¶
## Metadata
What kind of metadata standards are used in the Data Store?
In the Data Store, individual metadata standards can be defined depending on the research domain. Some basic standards are already offered (e.g. for the description of instruments, chemicals and experimental steps). The definition of additional (domain) metadata standards is an important part of the onboarding process. The divisions are supported in this task by the Data Store team.
Can ontologies be represented in the Data Store?
There is currently no function to import ontologies to openBIS. However, it is possible to add so-called semantic annotations when defining metadata Object and Property Types. However, these are currently not visible in the UI and can only be accessed via API.
¶
## Traceability
Can I edit data once it is stored in the Data Store?
In the Data Store, there is a clear technical seperation between data (in the form of files) and descriptive metadata. Files are stored in folders, so-called Datasets, which can contain one or more file(s) and whose content is immutable and stored on disk storage. However, Datasets be deleted in their entirety. Metadata entities, on the other hand, are stored in a separate database and can be both edited and deleted.
Can edits of (meta)data be tracked in the Data Store?
All edits to the metadata in the Data Store are saved (which change was made when and by whom), so that the entire history of a metadata entry can be traced back if necessary. Files stored in the Data Store, on the other hand, are immutable: They cannot be edited after they have been saved, but they can be deleted.
Can metadata/data stored in the Data Store be deleted and can deleted metadata/data be recovered?
Users with appropriate rights can edit entities as well as delete metadata Objects and associated files (Datasets). Deleted Objects and Datasets are first moved to the openBIS trashcan from where they can be restored if necessary. If Objects/Datasets are removed from the trashcan, they are permanently deleted from the underlying database and cannot be restored. By default, only users with Space Admin and Instance Admin role have permission to delete. If you want to completely prevent editing/deleting an entity, you can irreversibly ""freeze"" individual Objects or Datasets,  as well as entire folders in the Data Store.
Can chronological relationships be represented in the Lab Notebook of the Data Store?
Yes, by means of directed links between Objects, so-called ""parent-child relationships"", the chronological sequence of, e.g., several experimental steps can be represented in a hierarchy tree. You can find more information about parent-child relationships
here
.
¶
Data Analysis
What are Jupyter Notebooks and how can they be used to analyze data in the Data Store?
## Jupyter Notebook
is a web-based interactive computing platform that combines live code, equations, narrative text, visualizations etc. The Jupyter system currently supports over 100 programming languages including Python, Java, R, Julia, Matlab, Octave and many more. Jupyter Notebooks can be used to analyze data stored in an openBIS instance, e.g., by connecting a local Jupyter installation with the Data Store. The output of the analysis and the notebooks themselves can then be saved in the Data Store and connected to the dataset they are based on. It is possible to download an
extension for JupyterLab
that adds three buttons to a default notebook to
connect to an openBIS instance,
download datasets from the openBIS instance,
upload the notebook to the openBIS instance. For researchers using Python for data analysis, we recommend to use
pyBIS
, a Python module for interacting with openBIS, which is designed to be most useful in a Jupyter Notebook.
¶
Data Import/Export
What are the upload options to the Data Store?
Files can be uploaded to the Data Store in several ways:
via the graphical user interface (GUI),
via script, e.g., via the Python module
pyBIS
,
via the Dropbox mechanism, where files are copied to a specially created Dropbox folder.
You can find more information about the upload via GUI
here
.
Will there be support for implementing Dropbox scripts for automated data import?
It is planned to develop a ""meta dropbox script"" that offers a number of core functionalities (e.g. search for metadata Objects in the Data Store to which the datasets are to be attached; create new Objects and set metadata properties) as well as a template for entering the required metadata. The division must fill this template with the appropriate metadata: either manually or automatically via a parser script that is tailored to a specific data format.
Can warnings or events be triggered when importing data via Dropbox?
Yes, the Dropbox scripts can validate incoming data and act accordingly.
Can continuous data streams from measuring devices be included in the Data Store?
No, openBIS works with files only. Continuous data streams must be split into individual data files (e.g. per week, day, hour) which can be saved in openBIS as immutable data sets. The integration of openBIS and measuring devices is possible via the so-called Dropbox mechanism. For this, the data files must be saved in a dedicated Dropbox folder; from there they are uploaded to the Data Store. The Dropbox can be controlled via scripts that contain the logic for the data upload. You can find more information
here
.
Can data be exported/published from the Data Store?
Yes, files and descriptive metadata can be exported, both locally as well as to the public research data repository
## Zenodo
.
¶
## Interfaces
What interfaces (to devices and software) does the Data Store offer?
The APIs of openBIS are described
here
in the openBIS documentation of the ETHZ. There are programming interfaces to Java, Javascript and Python. For data analysis, there is the possibility to connect a local Jupyter installation to the Data Store (Jupyter Notebook, Hub and Lab). Jupyter itself provides numerous kernels to support a variety of programming languages for analyzing data in openBIS.
Will there be an interface between the Data Store to the hazardous substances database sigmaBAM in order to avoid duplicate work when entering hazardous substances?
There is currently no interface between the Data Store and sigmaBAM. Although both systems are intended for the digital representation of chemicals or hazardous substances, the perspective differs: sigmaBAM is used for the documentation of handling permits for hazardous substances as well as for a yearly updated total quantity of a hazardous substance. In the Data Store, on the other hand, it is recommended to represent the specific batch of a chemical and link it to experimental steps to ensure traceability. However, the metadata format for chemicals in the Data Store is based on the metadata format of hazardous substances in sigmaBAM. For divisions that have already entered hazardous substances in sigmaBAM, it is therefore possible to export these as Excel lists and import them (after some adjustments) into the Data Store.
Will there be an interface from the Data Store to the E-Akte (or vice versa)?
At the moment, there are no direct interfaces between the Data Store and the E-Akte, because the two systems are used to store different kinds of data (research data in the Data Store vs. (other) record-relevant documents in the E-Akte). If necessary, files in the E-Akte can be hyperlinked in the Data Store.
Can data stored in the Data Store be linked to publications in Publica?
There are currently no interfaces between the Data Store and Publica. However, it is possible to reference publications in the Data Store, e.g. via hyperlinks to their DOIs.
¶
## Development
I have an idea for a new/improved openBIS feature or plugin for the Data Store. Where can I submit this?
Please send your ideas for improved and/or additional features to datastore@bam.de. Depending on whether your idea is a BAM-specific or a general openBIS feature, we will include your suggestion in our feature request list or forward it to the openBIS development team at ETHZ. Please note, however, that due to limited development resources we cannot implement every feature request and therefore prioritize them according to effort/added value. Since openBIS is an open source software, the development team at ETHZ also handles openBIS feature requests at its own discretion.
Can I develop my own plugins for the Data Store?
If you would like to participate in the development of plugins for the Data Store, please contact
datastore@bam.de
.
¶
## Rollout
When will the rollout of the Data Store begin?
The rollout of the Data Store takes place in phases. In each phase, several divisions will be onboarded to the Data Store at the same time. The first rollout phase started in May 2023.
How can I register my division for the rollout?
All division heads were contacted by the project team in December 2022 as part of a survey to gauge their interest in rollout participation. Based on the responses, the rollout sequence for the first three phases was determined. In February 2025, another survey was conducted, which will now serve as the basis for further phase planning. The divisions involved in phase 4 have already been contacted. If you are a head of division and did not receive the survey, or if you wish to change your response, please contact us at
datastore@bam.de
.
What is a Data Store Steward/Group Admin and what are the responsibilities associated with the role?
Before the Data Store can be used operationally, research group-specific data structures and metadata schemas have be developed. To this end, at least one Data Store Steward must be appointed from each division to take on this task in consultation with colleagues and the division head. The Data Store Stewards are trained and supported in their work by the Data Store project team. Once the Data Store has been implemented for a division, the Data Store Steward acts as Group Admin. Group Admins can access all data and metadata within their own group and (in the case of metadata) edit it, as well as adjust access rights and other settings to meet the requirements of the division. Data Store Stewards are also the point of contact for research data management (RDM) issues within the division and are responsible for introducing new employees to the Data Store once onboarding has been completed. Data Store Stewards should be familiar with methodology and processes within the division and its research domain. Prior IT and RDM experience is helpful, but not required. Ideally, Data Store Stewards should have permanent positions in the division to reduce the risk of knowledge loss upon departure. For larger divisions with subgroups, it is recommended that at least two Data Store Stewards be assigned.
I would like to take on the role of Data Store Steward/Group Admin for my division, where can I apply for this?
To do so, please talk first with the head of your division and then contact us at
datastore@bam.de
.
Will there be training and/or manual on how to use the Data Store?
Yes, during onboarding, target group-specific training (for Data Store Stewards and for normal users) will take place. In addition to the openBIS documentation of the ETHZ, accompanying training material will be provided here in the Data Store Wiki.
How much time should a division invest for the rollout?
A rollout phase lasts approx. 2-3 months. During this time, onboarding events are held for the Data Store Steward(s) and for the members of the division. The effort required for adjustments to the Data Store and, if necessary, the setup of interfaces depends on the research subject, the equipment, the working methods, and the FDM requirements of the department and cannot be specified in general terms. The obligations of DSSts include carrying out the onboarding process (approx. 5 working days) and scheduling additional time for system adjustments, as well as integration into daily workflows after onboarding.
How can I/my division prepare for the rollout?
In preparation for the rollout (and for good research data management in general), it is recommended to consider the following points:
How to define uniform rules for naming files within the division (e.g., YYYYMMDD_someMeasurement_Initials_v1.csv)?
What kind of entities should be represented in the Inventory of the Data Store? Can the properties of these entities be structured as standardized metadata in lists? These lists can later be customized and imported into the Data Store.
What kind of information is needed in a typical experiment description? Can this also be standardized in the form of metadata? Can templates for experiment descriptions be created?
What are the links between experimental steps and inventory elements (e.g. an experimental step should be linked with the measuring instrument that was used)?
What kind of output data formats are generated by measurements? Are they open or proprietary (can only be read with special software from the manufacturer)? If open, what kind of measurement metadata are included? Is it possible to write a parser in the form of a short script that automatically reads this metadata?
Will we get additional staff for the rollout?
Unfortunately not, the subject-specific implementation and customization of research data management in the Data Store is to be conducted by the divisions themselves. Each division must appoint one or more Data Store Steward(s) for this task. However, the Data Store team from the sections VP.1 (eScience) and VP.2 (IT) will closely accompany the onboarding process and support the Data Store Stewards and the users by offering webinars, Q&A sessions and training materials.
Can I test openBIS before my division is onboarded to the Data Store?
Three demo openBIS instances of ETHZ and EMPA are available
here
.
In future, we will provide a demo instance of the Data Store. If you are interested in testing it, please contact the Data Store team at
datastore@bam.de
.
The openBIS installer as well as a pre-installed virtual machine image is available
here
. For the download, a registration at the SIS helpdesk is necessary. A
container image
is also provided. It should be noted, however, that the entire functionality only becomes visible after a complex configuration process and not all features can be easily grasped in a demo installation.
Can I participate in the Data Store rollout as an individual employee?
No, the rollout of the Data Store is designed for divisions and not for individual employees.
¶
## Pilot Phase
My research group was part of the pilot project. What happens to the pilot instance during the rollout?
As agreed at the beginning of the pilot project, the pilot instances will continue to be supported, provided with security updates and kept operational, but not provided with new features (which go beyond the regular openBIS updates) or further customised. In the medium term, it is planned to transfer the data of the pilot instances to the BAM-wide system.
Will more pilot groups be onboarded as test cases for the Data Store?
No, the pilot project was completed in February 2022 and no additional pilot groups will be included. All further groups will be included in the rollout, i.e. the central implementation of the Data Store, which started in May 2023.",Frequently Asked Questions (FAQ),0,en_faq_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_faq.txt,2025-09-30T12:09:11.540889Z,1
docs:datastore:en_Feedback:0,Feedback,https://datastore.bam.de/en/Feedback,datastore,"# Feedback
# Feedback
## Demidova, Caroline
In case of any questions or suggestions, please leave your feedback in this
## Onboarding
MsTeams channel.",Feedback,0,en_Feedback_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_Feedback.txt,2025-09-30T12:09:11.626833Z,1
docs:datastore:en_home:0,Welcome to BAM Data Store,https://datastore.bam.de/en/home,datastore,"# Welcome to BAM Data Store
## Page Contents
Welcome to the Data Store Wiki
## Wiki Structure:
## 💡 Concepts
📖 How-to guides
## ❓ FAQ
👥 Use cases
What is the Data Store?
What is openBIS?
What is the Data Store Project?
What is the Data Store rollout process?
## Demidova, Caroline
¶
Welcome to the Data Store Wiki
This Wiki provides information on the BAM Data Store - the central system for research data management at the Bundesanstalt für Materialforschung und -prüfung (BAM).
The Wiki is not intended to replace the openBIS documentation by the ETHZ (
User docs
,
Admin docs
). It provides conscise guidance and should serve as an additional source of openBIS and Data Store documentation for BAM employees.
Some articles of this Wiki are currently under construction. If you have further questions that are not yet answered here, please contact
datastore@bam.de
.
¶
## Wiki Structure:
¶
## 💡 Concepts
Explanation about terms and concepts.
## Explore Concepts
¶
📖 How-to guides
Step-by-step instructions for openBIS functions.
Go to Guides
¶
## ❓ FAQ
Frequently asked questions about Data Store and openBIS.
View FAQ
¶
👥 Use cases
Discover Use cases of the Data Store.
Discover Use cases
¶
What is the Data Store?
The Data Store is the central system for research data management (RDM) at BAM.
The Data Store is the central system for research data management (RDM) at BAM.
It enables divisions to digitally organize and describe laboratory inventory -such as instruments, samples, standard operating procedures (SOPs), using customizable metadata and linked documentation..
Integrated with electronic lab notebook (ELN), the Data Store allows experimental steps to be connected with inventory items, ensuring centralized and traceable documentation of research processes.  This structure linkage supports the FAIR principles
[1]
(Findable, Accessible, Interoperable, Reusable), facilitating both internal and external reuse of research data in line with funding bodies.
By storing data and metadata in a unified system, the Data Store enhances interdisciplinary collaboration and lays the foundation for advanced data analysis, including big data and Artificial Intelligence (AI) applications.
¶
What is openBIS?
openBIS (open Biology Information System) is the underlying platform of the Data Store.
It is an open-source software solution for Research Data Management (RDM) and Electronic Lab Notebook (ELN).
Developed and maintained by the Scientific IT Services (SIS) at ETHZ Zurich (ETHZ), openBIS was originally designed for life sciences
[2]
[3]
, it is now increasingly used materials science and other research domains.
openBIS provides a browser-based graphical user interface (GUI) for the managing digital laboratory inventories and documenting experiments in a standardized way.  Data files can be imported into via the GUI or through programming interfaces and linked to inventory items and experimental steps.
openBIS also supports integration with external tools and services, such as exporting data to
## Zenodo
repository and analyzing scientific data in
## Jupyter Notebook
.
For more information on openBIS visit the official website (
https://openbis.ch/
).
¶
What is the Data Store Project?
The introduction of a RDM system does not happen overnight.
To evaluate openBIS suitability, a pilot phase was conducted from 01.12.2020 to 28.02.2022. During this period, five BAM research groups from diverse domains successfully implemented openBIS, confirming its effectiveness for managing data in various materials science domains.
Following the pilot’s success, the Data Store project was approved to establish the system based on openBIS as a central RDM system across all BAM divisions. The project, led by VP.1 eScience and VP.2 Information Technology, began in October 2022 and is scheduled to run for 3.5 years.
The initial phase focused on developing the necessary IT infrastructure and preparing for the software rollout, included an analysis of RDM needs across BAM. The rollout of the Data Store began in 2023.
For more information about the Data Store project visit the BAM infoportal (
About the Project
)
¶
What is the Data Store rollout process?
Throughout all phases, project management and communication play a crucial role in supporting a smooth and efficient implementation.
The actual Data Store rollout began in May 2023 and is being carried out in successive phases, with several divisions being trained at the same time. Lessons learned are collected at the end of each rollout phase to implement improvements in subsequent rollout phases. The order of the rollout is determined based on the interest expressed by division heads in surveys done in December 2022 and February 2025.
The current onboarding concept lasts 2 to 3 months for assigned Data Store Stewards (DSSt), including 1 day for division heads and all employees working with research data.
The Data Store Stewards are one or two members of the division appointed by the division head. They are ideally permanently employed for sustainability and are familiar with inventory, experimental processes and the (digital) workflows of the division. DSSts are trained to use main functions and customize the group settings.
All other division members who handle research data receive introductory training in the use of the system. The head of division takes part in information events and coordinates the completion of the rollout phase together with the DSSts.
After the rollout, the heads of the division and the DSSts, together with the users, are responsible for storing newly generated research data and the continuous implementation of the Data Store within their division.
Mark D. Wilkinson et al. (2016). ""The FAIR Guiding Principles for scientific data management and stewardship"". Scientific Data. 3 (1): 160018. doi:
## 10.1038/SDATA.2016.18
.
↩︎
Angela Bauch et al. openBIS: a flexible framework for managing and analyzing complex data in biology research. 12, 468 (2011). doi:
10.1186/1471-2105-12-468
.
↩︎
Caterina Barillari et al. openBIS ELN-LIMS: an open-source database for academic laboratories. Bioinformatics. 32 (4), Feb 2016, 638–640. doi:[10.1093/bioinformatics/btv606].(
https://doi.org/10.1093/bioinformatics/btv606
).
↩︎",Welcome to BAM Data Store,0,en_home_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_home.txt,2025-09-30T12:09:11.693900Z,1
docs:datastore:en_How_to_guides:0,How-to Guides,https://datastore.bam.de/en/How_to_guides,datastore,"How_to_guides
# How-to Guides
## Page Contents
How to use the Data Store - main functions for Users
How to start
Register data in the Lab Notebook
Connect Experimental Steps in the Lab Notebook
Edit, Delete and Move functions
Register data in the Inventory
## Connect Inventory Objects
Manage Storage of Objects
## Barcodes
## Additional Function
Export data
Customize Group Settings - Admin/Data Store Stewards
Group settings
## Templates
Object types
Parents and Children sections
Storage for Objects
Onboarding Data Store Stewards
Login training instance
Share Code in BAM research GitHub
## Demidova, Caroline
Here you find an answer to ""How do I …?"" related questions on how to use the BAM Data Store and its  underlying software - openBIS. These goal-oriented instructions should help you accomplish specific tasks.  If the function you are looking for is missing, please contact the Data Store Team at
datastore@bam.de
.
¶
How to use the Data Store - main functions for Users
¶
How to start
Log in to the BAM Data Store - main instance
Represent research data - Conceptual data model
Manage Access to Spaces and Projects
¶
Register data in the
## Lab Notebook
Register a Project
Register Collection of the type Default Experiment
Register non-sequential Experimental Steps
Register sequential Experimental Steps
Upload data
¶
Connect Experimental Steps in the
## Lab Notebook
Define Parents and Children of Experimental Steps
Display connections of Experimental Steps - Hierarchy Graph
Add multiple Children to an Experimental Steps - Children generator
¶
Edit, Delete and Move functions
## Project Overview
Edit Projects, Collections, Objects and Datasets
Delete Projects, Collections, Objects and Datasets
Revert deletions of Collection, Objects and Datasets
Move Projects, Collections, Objects and Datasets
Move Experimental Step - Object with descendants
History of Changes
¶
Register data in the
## Inventory
## Register Projects
## Register Collections
## Register Objects
Batch Registration of Objects
Batch Update of Objects
¶
## Connect
## Inventory
## Objects
Connect Inventory Objects with Experimental Steps from Lab Notebook
Define Parents and Children of Inventory Objects
¶
Manage Storage of Objects
Allocate Storage position to an Object
Batch registration/update of storage position(s)
## Verify Storage Position
## Delete Storage Position
¶
## Barcodes
Use Barcodes and QR Codes
Print Barcodes or QR codes
Scan Barcodes and QR Codes
¶
## Additional Function
Embedding Images in Text Fields
Filter Objects within a Collection
Search - Global and Advanced search across all fields of all Entities
Search - Objects in the Inventory
Search - Objects - Experimental Steps in the Lab Notebook and save search queries
Search queries - Use saved
## Search - Datasets
¶
Export data
Export (meta)data to file
Export data to Zenodo
¶
Customize Group Settings - Admin/Data Store Stewards
¶
Group settings
Customize the Main menu
## Enable Barcodes
¶
## Templates
Create Templates for Experimental Steps and other Objects
¶
Object types
Enable Object Types in drop-downs
¶
Parents and Children sections
Customize Parents and Children sections in Object Forms
Add a hint in Parents and Children sections
¶
Storage for Objects
Enable Storage Widget on Object Form
Configure Storage of Objects
¶
Onboarding Data Store Stewards
¶
Login training instance
Log in to the BAM Data Store-training instance
Checklist Group Settings customization
Checklist Use Case implementation
¶
Share Code in BAM research GitHub
Share Code in BAM research GitHub",How-to Guides,0,en_How_to_guides_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides.txt,2025-09-30T12:09:11.763344Z,1
docs:datastore:en_How_to_guides_Add_hint_parents_and_children_sections:0,Add a hint in Parents and Children sections,https://datastore.bam.de/en/How_to_guides/Add_hint_parents_and_children_sections,datastore,"How_to_guides
/
Add_hint_parents_and_children_sections
# Add a hint in Parents and Children sections
## Demidova, Caroline
In the left main menu, under
## Utilities
select
## Settings
. The Select Group Settings drop-down menu will appear, select your
division number
to open your group settings. Click on the
## Edit
tab, navigate to the Object type definitions Extension section, options to modify those sections within an Object type will be displayed. In the
Hints for
part of settings there is an option to set a particular Object type(s) as Parent(s) or Child(ren) and limit the number of them. By pressing the
+
tab on the right corner of the
Hints for
row, you can extend the number of the Parents and Children. In case, if minimal number of Parents and/or Children is specified, the required number of those is mandatory to enter and the form cannot be saved until this condition is satisfied. Annotation to these connections can be supported by using the Properties. To add Property press
+
left to the Parent(s)/Child(ren) field. Review the entries and
## Save
.
## Under Utilities
## Select Settings
Select division number
Click on Edit tab
Navigate to Object type definitions Extension section
Select an Object type
In the Hints for part set Parent(s) and/or Child(ren) to the particular Object type
Specify minimal and maximal number of Parents and Children
Press + tab to add annotations
Review the entries and Save.",Add a hint in Parents and Children sections,0,en_How_to_guides_Add_hint_parents_and_children_sections_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Add_hint_parents_and_children_sections.txt,2025-09-30T12:09:11.828337Z,1
docs:datastore:en_How_to_guides_Batch_registration_Inventory:0,Batch register Objects in a Collection,https://datastore.bam.de/en/How_to_guides/Batch_registration_Inventory,datastore,"How_to_guides
/
Batch_registration_Inventory
# Batch register Objects in a Collection
Ariza de Schellenberger, Angela
Multiple Objects can be registered in a
## Collection
via import
## Excel Template
. Navigate to the relevant
## Collection
, open the
## More
drop-down menu, and select
XLS Batch Register Objects
.
## Download
the
## Template
, fill out the form and enter values for all properties, upload it and click
## Accept
. To confirm batch registration, navigate to updated
## Collection
.
To assign
## Parents
to objects during registration, first assign numbers  to the Parents in the
$
column (e.g., $1). Then assign the number of a Parent ($1) to an Object in the
## Parents
column to establish the relationship.
## Select Collection
Open More drop-down menu
Select XLS Batch Register Objects
Click on Template Download, update the file
and enter values for all mandatory (*) Properties
Upload the file
Review the entries and Accept.",Batch register Objects in a Collection,0,en_How_to_guides_Batch_registration_Inventory_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Batch_registration_Inventory.txt,2025-09-30T12:09:11.854472Z,1
docs:datastore:en_How_to_guides_Batch_registration_storage_position:0,Batch registration/update of storage position(s),https://datastore.bam.de/en/How_to_guides/Batch_registration_storage_position,datastore,"How_to_guides
/
Batch_registration_storage_position
# Batch registration/update of storage position(s)
## Demidova, Caroline
To allocate storage position to multiple Objects, the Data Store Steward(s) for your group must customize the
storage for Objects
and
Enable object types in drop-downs
so that
Allowed object types
are displayed in Excel spreadsheet for batch registration.
To assign a storage location to multiple objects, navigate to the relevant
## Collection
and select
XLS Batch Register Objects
from the
## More
drop-down menu. Ensure that the required Object Types are displayed in the
## Register Objects
window and click on
## Download
. The Excel Template should contain at least two sheets (the SAMPLE and STORAGE_POSITION metadata). To link these sheets, the information in the $ column of the SAMPLE spreadsheet must match the
## Parents
column in STORAGE_POSITION spreadsheet. Enter the numbers or letters proceeded by the $ symbol (i.e., $1, $2) in the $ column. Upload the updated file and click
## Accept
.
To find the
storage Code
required in the STORAGE_POSITION spreadsheet, navigate to
## Utilities
,
## Settings
(choose your division’s number), open the
## Storages
section and select relevant storage to view its metadata. Enter the required information and save the file locally on your device.
## Select Collection
Open More drop-down menu
Select XLS Batch Register Objects
Click on Template Download
(The Excel Template should contain at least two sheets,
the SAMPLE and STORAGE_POSITION metadata.)
Upload the file
Review the entries and click Accept.",Batch registration/update of storage position(s),0,en_How_to_guides_Batch_registration_storage_position_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Batch_registration_storage_position.txt,2025-09-30T12:09:11.904601Z,1
docs:datastore:en_How_to_guides_Batch_update_Inventory:0,Batch Update of Objects,https://datastore.bam.de/en/How_to_guides/Batch_update_Inventory,datastore,"How_to_guides
/
Batch_update_Inventory
# Batch Update of Objects
Ariza de Schellenberger, Angela
## Multiple
## Objects
can be updated in a
## Collection
using an Excel Template. Navigate to the relevant
## Collection
(in the Inventory or Lab Notebook) and click on the
## COLUMNS
tab in the Collection form. Select the
## Identifier
of the Properties you want to update. If you have multiple Objects, you can filter the table. To export the Excel table for the selected Properties, click on the
## EXPORTS
tab and select
## Import Compatible
## (Yes);
## Columns
(All) (default order);
## Rows
(All) pages/Current page/Selected rows (depending on the Objects you want to export and update). Click
## EXPORT
to download the table. Modify the file and save it. In the Object form, click the
## More
drop-down menu and select
XLS Batch Update
. Upload the file and press
## Accept
. To confirm batch update, navigate to updated
## Collection
.
## Select Collection
Click on Columns tab
Press show all
## Click on Exports tab:
## Select Import Compatible (Yes),
Columns(All) (default order),
## Rows(All) Pages
Click on Export
Update the Excel Template file and save it
Click on More drop-down menu (Object form)
Select XLS Batch Update Objects
Upload the Excel Template file
Review the entries and Accept.",Batch Update of Objects,0,en_How_to_guides_Batch_update_Inventory_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Batch_update_Inventory.txt,2025-09-30T12:09:11.965889Z,1
docs:datastore:en_How_to_guides_Checklist_group_settings:0,Checklist Group Settings Customization,https://datastore.bam.de/en/How_to_guides/Checklist_group_settings,datastore,"How_to_guides
/
Checklist_group_settings
# Checklist Group Settings Customization
## Page Contents
## ✅ Group Settings Customization Checklist
Objects registration
Parent-child relationships
## Barcodes
## Lab Storage
Group Settings: Data Store -Multi-group Instance
Ariza de Schellenberger, Angela
¶
## ✅ Group Settings Customization Checklist
This checklist helps you keep track of all group settings and configurations for your department that can be customized by the Data Store Steward.
¶
Objects registration
Enable Object Types in drop-down menus
Create Templates for Experimental Steps and other Objects
¶
Parent-child relationships
Customize Parents and Children sections in Object Forms
Add a hint in Parents and Children sections
¶
## Barcodes
## Enable Barcodes
¶
## Lab Storage
Enable Storage for an Object Type
Configure Lab storage
¶
Group Settings: Data Store -Multi-group Instance
## Storages
## Templates
Object Type definition extension
## Inventory Spaces
Customize the Main menu
## Miscellaneous
## Optional:
Manage Access to Spaces and Projects - FB Inventory folders
Roles defined per default in the Data Store
Group settings should adopt the data policy of projects, when required.
Define naming conventions for entities (Project, Collections, Object, Data sets); templates, etc. to ensure consistency and facilitate findability.",Checklist Group Settings Customization,0,en_How_to_guides_Checklist_group_settings_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Checklist_group_settings.txt,2025-09-30T12:09:12.029015Z,1
docs:datastore:en_How_to_guides_Checklist_implementation:0,Checklist Use Case implementation,https://datastore.bam.de/en/How_to_guides/Checklist_implementation,datastore,"How_to_guides
/
Checklist_implementation
# Checklist Use Case implementation
## Page Contents
## Example 1
## 🎯 Define Realistic Goals
📊 Select Data to Represent
🗂️ Represent research data in the Data Store - Create a Conceptual Model
📝 Create Templates for Experimental Steps
## 🚀 Use Case Implementation
## 🧪 Define Lab Storage
## 🔍 Define Search Queries
Ariza de Schellenberger, Angela
¶
## Example 1
This checklist contains implementation steps that DSSt should consider in order to implement a use case for the division with the goal to improve the discoverability and reusability of research data and simplify the use of Data Store for users.
¶
## 🎯 Define Realistic Goals
Establish clear and realistic objectives for using the Data Store.
This check list focuses on improving findability and reusability of research data.
¶
📊 Select Data to Represent
Choose new research data.
Choose structured, tabular data that is easily accessible.
¶
🗂️ Represent research data in the Data Store - Create a Conceptual Model
Use the
Represent_research_data
.  Use the
template
in (
draw.io
) to Identify key steps and data points in the workflow; map data in the folder structure for the Data Store and define parent-child relationships between objects.
Manage roles and rights at the Space and Project level to comply with agreements and contracts of related research projects.
¶
📝 Create Templates for Experimental Steps
Develop templates that include sufficient information to describe datasets attached to experimental steps.
¶
## 🚀 Use Case Implementation
Automate metadata import from instruments (e.g., generate barcodes).
Automate metadata import from measurements (e.g., structured tabular data, reuse templates for experimental steps).
Batch register objects such as inventory items and samples.
¶
## 🧪 Define Lab Storage
Set up lab storage for samples and batch register/update samples.
Ensure storage locations are well-documented and are easily found.
¶
## 🔍 Define Search Queries
Create and save search queries to improve data traceability.
Ensure queries are easy to use and provide relevant results.",Checklist Use Case implementation,0,en_How_to_guides_Checklist_implementation_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Checklist_implementation.txt,2025-09-30T12:09:12.090519Z,1
docs:datastore:en_How_to_guides_Children_generator:0,Add multiple Children to an Experimental Steps - Children generator,https://datastore.bam.de/en/How_to_guides/Children_generator,datastore,"How_to_guides
/
Children_generator
# Add multiple Children to an Experimental Steps - Children generator
## Demidova, Caroline
The children generator function allows to register multiple children simultaneously during the registration of the Object or while editing it. At first, you need to register an Object of Type Experimental Step and defined at least one parent for it (e.g. instruments, chemicals, a sample, etc.).  The child generator will allow you to generate Children (e.g., subsequent Experimental Steps) with a defined combination of Parents.
To do this, select the
## Experimental Step
with registered Parents. Click on the
## Edit
tab, scroll down to the
## Parents
and
## Children
section in the Experimental Step form, and click on the
## Generate Children
tab. A matrix of Parents is displayed in the
## Children Generator
form. Select all Parents, choose the
## Experimental Step
in the
Object type
drop-down menu and enter the number of replicates of newly generated Children, and click on
## Generate
. Review the entries and
## Save
.
Note, to ensure the traceability of the data in the Data Store, a minimum set of mandatory Properties has been defined for some Object Types. The child generator only allows the registration of Objects with
non-mandatory
Properties, as the Object form cannot be edited. It is therefore only possible to register multiple Objects of various Object Types with the child-parents relationship simultaneously using the
Batch registration
function.
Select Experimental Step with registered Parents
Click on Edit tab
Scroll down to the Parents and Children section
Click on Generate Children tab
Choose Experimental Step in the Object type drop-down menu
Enter the number of new child(ren)
Review and click on Generate",Add multiple Children to an Experimental Steps - Children generator,0,en_How_to_guides_Children_generator_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Children_generator.txt,2025-09-30T12:09:12.152332Z,1
docs:datastore:en_How_to_guides_Connect_Inventory_Lab_Notebook:0,Connect Objects of Inventory with Experimental Steps from  Lab Notebook,https://datastore.bam.de/en/How_to_guides/Connect_Inventory_Lab_Notebook,datastore,"How_to_guides
/
## Connect_Inventory_Lab_Notebook
# Connect Objects of Inventory with Experimental Steps from  Lab Notebook
Ariza de Schellenberger, Angela
To connect an Inventory Object to an Experimental Step, select the relevant inventory Object and click on the
## Edit
tab. In the
## Parent
and
## Children
section, click
## Search Any
and select
Experimental step
from the
Select an object type
drop-down menu. You will be returned to the
## Update-Object
form. Enter the
Code or Name of the Object
in the text box that will appear below the Parents section. Start typing the Code or Name of the Parent-Object to display available Objects for your group, select the appropriate Object and
## Save
.
To display linked Objects in a hierarchy graph, navigate to the edited Inventory item, click the
## More
drop-down menu, and select
## Hierarchy Graph
.
It is also possible to define multiple Parents and Children to  Objects at the same time. To do this, use the
## Paste Any
option, enter the Code or Name of respective Objects, review the entries and
## Save
. You can copy the Code or Name of Objects from another ELN page (Log in to the BAM Data Store in another/private browser window). Paste the Codes(s) or name(s) in the text fields, review and
## Save
.
Select Object in the Inventory
Click on Edit tab
Navigate to the Parent and Children sections
## Click Search Any
## Select Object Type - Experimental Step
Enter Code or Name of Experimental Step to connect
Review the entries and Save.",Connect_Inventory_Lab_Notebook,0,en_How_to_guides_Connect_Inventory_Lab_Notebook_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Connect_Inventory_Lab_Notebook.txt,2025-09-30T12:09:12.220000Z,1
docs:datastore:en_How_to_guides_Customize_Main_menu:0,Main menu,https://datastore.bam.de/en/How_to_guides/Customize_Main_menu,datastore,"How_to_guides
/
Customize_Main_menu
# Main menu
## Page Contents
# Main menu
## ⚙️ Step 1: Access Group Settings
## 🖱️ Step 2: Edit Main Menu Sections
## 💾 Step 3: Save Changes
🔄 Step 4: Reload the Interface
Ariza de Schellenberger, Angela
¶
# Main menu
¶
## ⚙️ Step 1: Access Group Settings
In the
left menu
, go to
## Utilities
.
Click on
## Settings
.
In the
## Select Group Settings
drop-down, choose your
division number
.
¶
## 🖱️ Step 2: Edit Main Menu Sections
Click the
## Edit
tab.
Navigate to the
## Main Menu
section.
Modify the sections by checking or unchecking the checkboxes to add or hide sections.
¶
Available Sections to Show/Hide:
showLabNotebook
showInventory
showStock
showObjectBrowser
showExports
showStorageManager
showAdvancedSearch
showArchivingHelper
showUnarchivingHelper
showTrashcan
showVocabularyViewer
showUserManager
showUserProfile
showZenodoExportBuilder
showBarcodes
showDatasets
¶
## 💾 Step 3: Save Changes
Review your changes.
## Click
## Save
.
¶
🔄 Step 4: Reload the Interface
Refresh the openBIS webpage to see the updated main menu sections.",Main menu,0,en_How_to_guides_Customize_Main_menu_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Customize_Main_menu.txt,2025-09-30T12:09:12.281771Z,1
docs:datastore:en_How_to_guides_Customize_parents_and_children_sections:0,Customize Parents and Children sections in Object Forms,https://datastore.bam.de/en/How_to_guides/Customize_parents_and_children_sections,datastore,"How_to_guides
/
Customize_parents_and_children_sections
# Customize Parents and Children sections in Object Forms
## Demidova, Caroline
In the left main menu, under
## Utilities
select
## Settings
. The Select Group Settings drop-down menu will appear. Select your
division number
to open your group settings. Click on the
## Edit
tab, navigate to the
Object type definitions
Extension section and select the
## Object Type
you want to customize. To deselect sections for Parents or Children, select the
Disable section
checkbox, to disable the addition of any Object type as Parent or Child, select the disable addition of any object type) checkbox.
To make adding Parents and Children more convenient to users, enter a name under Section name (e.g., Parents - Section name: Instruments for Experimental Step).  Review the changes and Save.
## Under Utilities
## Select Settings
Select division number
Click on Edit tab
Navigate to Object type definitions Extension section
Select an Object type
If required: Select Disable section, Disable addition of any object type checkboxes
Enter Section name for Parents and Children
Review changes and Save.",Customize Parents and Children sections in Object Forms,0,en_How_to_guides_Customize_parents_and_children_sections_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Customize_parents_and_children_sections.txt,2025-09-30T12:09:12.344003Z,1
docs:datastore:en_How_to_guides_Delete_ELN:0,"Delete Projects, Collections, Objects and Datasets",https://datastore.bam.de/en/How_to_guides/Delete_ELN,datastore,"How_to_guides
/
Delete_ELN
# Delete Projects, Collections, Objects and Datasets
## Demidova, Caroline
Users can delete Projects, Default Experiments, Experimental Steps and Datasets in their
## Lab Notebook
-
## My Space
. In the division’s private
## Inventory
, users do not have
sufficient rights
(Space/Project User ) to delete contents and need to contact Data Store Steward(s) (DSSt(s)). The DSSt(s) manage the division’s private
## Inventory
and assign roles to users that support Inventory management.
To delete contents, navigate to the relevant folder, click on the
## More
drop-down menu, and select
## Delete
. In the
## Confirm Delete
window, enter the *
## Reason (
mandatory)
for deletion. Click
## Accept
to confirm.
Deletion of Default Experiments, Experimental Steps and Datasets can be
reverted
. Deleting Projects, on the other hand is
irreversible
.
Note that deletions from the Trashcan are
irreversible
.
## Select Project, Default Experiment,
Experimental Steps or Dataset
Click on More drop-down menu
## Select Delete
Enter Reason (*mandatory) for deletion
## Click Accept.","Delete Projects, Collections, Objects and Datasets",0,en_How_to_guides_Delete_ELN_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Delete_ELN.txt,2025-09-30T12:09:12.412824Z,1
docs:datastore:en_How_to_guides_Delete_storage_position:0,Delete Storage Positions,https://datastore.bam.de/en/How_to_guides/Delete_storage_position,datastore,"How_to_guides
/
Delete_storage_position
# Delete Storage Positions
Ariza de Schellenberger, Angela
To delete storage information from an Object, navigate to the relevant
## Object
, click
## Edit
and navigate to the
## Storage
section in the Object form.  Scroll to the right in the Storage table and click on the
-
icon to delete the storage.  The deleted Storage will be moved to the trashcan. To delete it permanently, delete it from the
trashcan
.
## Select Object
Click on Edit tab
Navigate to Storage sections
Click - icon",Delete Storage Positions,0,en_How_to_guides_Delete_storage_position_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Delete_storage_position.txt,2025-09-30T12:09:12.473341Z,1
docs:datastore:en_How_to_guides_Edit_ELN:0,"Edit – Projects, Collections, Objects and Datasets",https://datastore.bam.de/en/How_to_guides/Edit_ELN,datastore,"How_to_guides
/
Edit_ELN
# Edit – Projects, Collections, Objects and Datasets
## Demidova, Caroline
To edit Projects, Default Experiments - Collections and Experimental Steps - Objects , navigate to the relevant folder, click on the
## Edit
tab, enter the changes, review and
## Save
.
Select relevant folder (Default Experiment or Experimental Steps)
Click on Edit tab
Enter the changes
Review the entries and Save.","Edit – Projects, Collections, Objects and Datasets",0,en_How_to_guides_Edit_ELN_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Edit_ELN.txt,2025-09-30T12:09:12.533428Z,1
docs:datastore:en_How_to_guides_Embedding_Images:0,Embedding Images in Text Fields,https://datastore.bam.de/en/How_to_guides/Embedding_Images,datastore,"How_to_guides
/
## Embedding_Images
# Embedding Images in Text Fields
## Demidova, Caroline
To embed an image (.jpeg, .png formats) in the description of the Entity during the editing, drag an image in the description field. Another way is to click on the picture icon (
) in the rich text editor field. Alternatively, the image (.jpeg, .png, .pdf, .svg formats) could be embedded as an
ELN Preview Dataset
. For that, select relevant
## Default Experiment (Collections)
or
## Object
, and
upload
the Dataset. In the
## Create Dataset
form, in the Data Set Type (*) drop-down menu select
ELN Preview
. Fill out the relevant information and click on select files to upload, review the files and
## Save
.
Select Default Experiment or Object
Click Upload button
Select ELN Preview in the Dataset Type (*)
Select files to upload
Review the files and Save.",Embedding_Images,0,en_How_to_guides_Embedding_Images_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Embedding_Images.txt,2025-09-30T12:09:12.593853Z,1
docs:datastore:en_How_to_guides_Enable_barcodes:0,Enable Barcode / QR-Code Functionality,https://datastore.bam.de/en/How_to_guides/Enable_barcodes,datastore,"How_to_guides
/
Enable_barcodes
# Enable Barcode / QR-Code Functionality
## Page Contents
🧩 Enable Barcodes / QR-Code Functionality
## ✅ Prerequisites
🪪 Step 1: Understand Default Barcode Behavior
⚙️ Step 2: Enable Barcode Display for Your Group
🔄 Step 3: Reload the Interface
Ariza de Schellenberger, Angela
¶
🧩 Enable Barcodes / QR-Code Functionality
To make barcodes and QR codes visible and usable for your group in openBIS.
¶
## ✅ Prerequisites
You must have Group Admin role (e.g., be Data Store Steward (DSSt)
Your group must be registered in the Data Store.
At least one object (e.g., sample, instrument, chemical) must be registered.
¶
🪪 Step 1: Understand Default Barcode Behavior
When an object is registered in openBIS, a
default barcode
is automatically generated.
This barcode is visible in the
## Identification Info
section.
## If it's not visible:
Click the
## More
drop-down menu.
## Select
## Show Identification Info
.
¶
⚙️ Step 2: Enable Barcode Display for Your Group
In the
left main menu
, go to
## Utilities
.
Click on
## Settings
.
In the
## Select Group Settings
drop-down, choose your
division number
.
Click the
## Edit
tab.
Scroll down to the
Main menu
section.
## Select
## Show Barcodes
.
¶
🔄 Step 3: Reload the Interface
Refresh your browser. The Barcodes/QR Codes Generator will be shown in the main menu under Utilities and a barcode icon will be added above the main menu.",Enable Barcode / QR-Code Functionality,0,en_How_to_guides_Enable_barcodes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Enable_barcodes.txt,2025-09-30T12:09:12.656882Z,1
docs:datastore:en_How_to_guides_Export_to_file:0,Export (meta)data to file,https://datastore.bam.de/en/How_to_guides/Export_to_file,datastore,"How_to_guides
/
Export_to_file
# Export (meta)data to file
## Demidova, Caroline
All (meta)data from the Lab Notebook and Inventory for which you have observer rights can be exported. Navigate to the relevant file, open the
## More
drop-down menu, and select
## Export
. The export window will open. Select the appropriate export options and
Receive results by email
instead of downloading in the browser to enable more efficient software performance. You will receive an email notification from
datastore@bam.de
with a download link.
Navigate to the relevant file
Open the More drop-down menu
## Select Export
Select the appropriate export options
Select to receive results by email
Download file via link in email from datastore@bam.de",Export (meta)data to file,0,en_How_to_guides_Export_to_file_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Export_to_file.txt,2025-09-30T12:09:12.724699Z,1
docs:datastore:en_How_to_guides_Export_to_zenodo:0,Export data to Zenodo,https://datastore.bam.de/en/How_to_guides/Export_to_zenodo,datastore,"How_to_guides
/
Export_to_zenodo
# Export data to Zenodo
## Demidova, Caroline
To export data to Zenodo, you need a Zenodo account to generate a personal access token. To do this, log in to your Zenodo account, select
## Settings
,
## Applications
and copy the Zenodo token.  Return to openBIS, select
## Utilities
and then
## User Profile
and copy the
Zenodo API token
.
Select then
## Exports
and
Export to Zenodo
from
## Utilities
, enter the
title of the submission (*)
, select the files to be exported, and click
## Export Selected
. The selected data will then be transferred to Zenodo as a ZIP file. The files exported to Zenodo can be private or public, before publishing, check the entries and ensure that no personal data has been exported, authors are listed correctly, and the project's data policy is being followed.
## Select Utilities
## Select Export
Select Export to Zenodo
Enter title of the submission
Click on Export Selected",Export data to Zenodo,0,en_How_to_guides_Export_to_zenodo_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Export_to_zenodo.txt,2025-09-30T12:09:12.795832Z,1
docs:datastore:en_How_to_guides_Filter:0,Filter Objects within a Collection,https://datastore.bam.de/en/How_to_guides/Filter,datastore,"How_to_guides
/
## Filter
# Filter Objects within a Collection
## Demidova, Caroline
Select relevant
## Collection
, click on the
## FILTERS
tab and select
## Filter Per Column
. In the appeared line of the Properties fields enter a specific
## Property
value(s) to narrow the filtering.
## Select Collection
Click the FILTERS button
## Choose Filter Per Column
Enter the Property value in the Property Filter fields.",Filter,0,en_How_to_guides_Filter_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Filter.txt,2025-09-30T12:09:12.858355Z,1
docs:datastore:en_How_to_guides_Global_advanced_search:0,Search - Global and Advanced search across all fields of all Entities,https://datastore.bam.de/en/How_to_guides/Global_advanced_search,datastore,"How_to_guides
/
Global_advanced_search
# Search - Global and Advanced search across all fields of all Entities
## Demidova, Caroline
To search across all database fields, enter the search term in the
## Global Search
window in the top left-hand corner of the main menu. The
## Advanced Search
form opens and displays the first results. To continue an advanced search across all fields of all entities (Experiments/Collections, Objects, Datasets), select an option from
## Search For
drop-down menu and the
## AND
or
## OR
operator, then click the
## Search
icon. Narrow the search by selecting an option from the
## Field Type
drop-down menu (the options displayed will vary depending on the search). Select
## Property
from the Field Type drop-down menu and enter the values required by the system in
## Field Name
,
Comparator Operator
and
## Field Value
. To narrow down the search further, click on the
+
icon and enter values. Click on the
## Search
icon to activate the search.
Enter the search term in Global Search
## Advanced Search
Add Filters and Conditions; Search For drop-down menu
and the AND or OR operator
Select an option from the Field Type menu
Fill out additional search parameters
Click the + icon to narrow down the search
Click on search icon.",Search - Global and Advanced search across all fields of all Entities,0,en_How_to_guides_Global_advanced_search_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Global_advanced_search.txt,2025-09-30T12:09:12.919691Z,1
docs:datastore:en_How_to_guides_Hierarchy_graph:0,Display connections of Experimental Steps (Hierarchy Graph),https://datastore.bam.de/en/How_to_guides/Hierarchy_graph,datastore,"How_to_guides
/
Hierarchy_graph
# Display connections of Experimental Steps (Hierarchy Graph)
## Demidova, Caroline
To display Parent-Child connections in a hierarchy graph, navigate to one of the Experimental steps, click on the
## More
drop-down menu, select
## Hierarchy Graph
. The number of displayed Objects (Parents and Children) and
## Types
can be adjusted for better visualization.
In the hierarchy graph, the
eye
icon shows or hides an Object and connected Entities, the
arrow
shows the metadata, and the
plus
icon can be used to insert Parents and/or Children directly into the hierarchy graph.
## Select Object-Experimental Step
Click on More drop-down menu
Select Hierarchy Graph and adjust the number of displayed Objects.",Display connections of Experimental Steps (Hierarchy Graph),0,en_How_to_guides_Hierarchy_graph_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Hierarchy_graph.txt,2025-09-30T12:09:12.983119Z,1
docs:datastore:en_How_to_guides_History_ELN:0,History of Changes,https://datastore.bam.de/en/How_to_guides/History_ELN,datastore,"How_to_guides
/
History_ELN
# History of Changes
## Demidova, Caroline
To access the history of Projects, Default Experiment - Collections, Experimental Steps - Objects or Datasets, navigate to the relevant folder, open the
## More
drop-down menu, and select
## History
## . The
## History
form shows previous values in
red
, updated values in
green
.
Note that
## Samples
was the former openBIS term for
## Objects
, and
## Experiment
for Default Experiment - Collections.
Select relevant folder
Open More drop-down menu
## History",History of Changes,0,en_How_to_guides_History_ELN_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_History_ELN.txt,2025-09-30T12:09:13.047707Z,1
docs:datastore:en_How_to_guides_How_to_log_in:0,Log in to the BAM Data Store,https://datastore.bam.de/en/How_to_guides/How_to_log_in,datastore,"How_to_guides
/
How_to_log_in
# Log in to the BAM Data Store
## Demidova, Caroline
Log in to the
main BAM Data Store
.
BAM employees are granted access to the main BAM Data Store instance, after their onboarding. To access the main instance, open
main BAM Data Store
in the browser, click on the
ELN (Electronic Lab Notebook)
icon and you will be redirected to the login page. Log in with your
BAM username
and
password
. If you encounter any problems, check your internet connection or contact us at
datastore@bam.de
.
main BAM Data Store Instance
Click on the ELN (Electronic Lab Notebook) icon
Enter BAM username and password",Log in to the BAM Data Store,0,en_How_to_guides_How_to_log_in_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_How_to_log_in.txt,2025-09-30T12:09:13.108511Z,1
docs:datastore:en_How_to_guides_How_to_log_in_train:0,Log in to the Bam Data Store,https://datastore.bam.de/en/How_to_guides/How_to_log_in_train,datastore,"How_to_guides
/
How_to_log_in_train
# Log in to the Bam Data Store
## Demidova, Caroline
Log in to the
training BAM Data Store
.
BAM employees who are assigned as Data Store Stewards for their division receive access to a training instance of the BAM Data Store for the duration of the
rollout
. To access the training instance, open
training BAM Data Store
in the browser, click on the
ELN (Electronic Lab Notebook)
icon and you will be redirected to the login page. Log in with your
BAM username
and
password
. If you encounter any problems, check your internet connection or contact us at
datastore@bam.de
.
training BAM Data Store Instance
Click on the ELN (Electronic Lab Notebook) icon
Enter BAM username and password",Log in to the Bam Data Store,0,en_How_to_guides_How_to_log_in_train_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_How_to_log_in_train.txt,2025-09-30T12:09:13.170628Z,1
docs:datastore:en_How_to_guides_Manage_Access:0,Manage Access to Spaces and Projects,https://datastore.bam.de/en/How_to_guides/Manage_Access,datastore,"How_to_guides
/
## Manage_Access
# Manage Access to Spaces and Projects
## Page Contents
## Lab Notebook
## Inventory
## Demidova, Caroline
¶
## Lab Notebook
Users can manage access in their Lab Notebook -
## My Space
at the Space and Project level (access applies to the underlying content: Collections, Objects, Datasets). To grant access to a Space or a Project, navigate to relevant folder, click on the
## More
drop-down menu, select
Manage access
to open the form. Select
## Role
and set
grant to
User or Group, enter the
username
(BAM username – lower case only) or
division number
(e.g. 1.0). Click Grant access to apply settings.
To get an overview of who has access in your Lab Notebook, Space or Projects, navigate to the relevant folder, click on the
## More
drop-down menu, select
Manage access
. In the
Manage access to
window, an overview (User, Group, Role) of the assigned access rights is displayed.
¶
## Inventory
DSSt(s) of the division can also manage access in the division’s private Inventory, but not for the BAM public Inventory. By providing the Users with the
sufficient rights
DSSt(s) can gain help from the division members to maintain the Inventory.
Select Space or Project
Click on More drop-down menu
Select Manage access
Select Role and set grant to options
Enter BAM username or division number
Click on Grant access",Manage_Access,0,en_How_to_guides_Manage_Access_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Manage_Access.txt,2025-09-30T12:09:13.232643Z,1
docs:datastore:en_How_to_guides_Move_ELN:0,"Move Projects, Collections, Objects and Datasets",https://datastore.bam.de/en/How_to_guides/Move_ELN,datastore,"How_to_guides
/
Move_ELN
# Move Projects, Collections, Objects and Datasets
## Demidova, Caroline
To move content, navigate to the relevant folder, open the
## More
drop-down menu, and select
## Move
. Start typing the
## Name
or
## Code
of the folder you want to move the content to and select it from the available options. Click on
## Accept
.
To move an Object to a another Collection, navigate to relevant Collection and select the Collection List view. Select the Object to be moved and click on the
## Move
tab that appears. In the
## Move Object
window, select to move to an Existing Collection and enter the
## Code
or
## Name
of the Collection in which you want to move the Object. Otherwise, select New Collection, fill in the mandatory(*) fields, review them and click
## Accept
.
Navigate to relevant folder
Open More drop-down menu
## Select Move
Enter Code or Name of the target Collection
in the search field
## Click Accept.","Move Projects, Collections, Objects and Datasets",0,en_How_to_guides_Move_ELN_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Move_ELN.txt,2025-09-30T12:09:13.298940Z,1
docs:datastore:en_How_to_guides_Move_with_descendants_ELN:0,Move Experimental Step - Object with descendants,https://datastore.bam.de/en/How_to_guides/Move_with_descendants_ELN,datastore,"How_to_guides
/
Move_with_descendants_ELN
# Move Experimental Step - Object with descendants
## Demidova, Caroline
An Object can also be moved together with its descendant Objects (i.e. children, grandchildren, etc.) if all Objects (Parent and Children) belong to one and the same Collection of the Object to be moved. To do this, click on the Object in the Collection, the
## Object
Form opens, open the
## More
drop-down menu, and select
## Move
. Select the option
move the Object with all descendants
and start typing the
## Name
of the (existing) Collection to which you want to move the Objects, review, and click
## Accept
.
Select Experimental Step – Object with descendants
Open More drop-down menu
## Select Move
Select option (to move all descendants)
Enter Code or Name of the target Collection in the search field
Check the box
## Click Accept.",Move Experimental Step - Object with descendants,0,en_How_to_guides_Move_with_descendants_ELN_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Move_with_descendants_ELN.txt,2025-09-30T12:09:13.362268Z,1
docs:datastore:en_How_to_guides_New_storage:0,Configure Storage of Objects,https://datastore.bam.de/en/How_to_guides/New_storage,datastore,"How_to_guides
/
New_storage
# Configure Storage of Objects
Ariza de Schellenberger, Angela
To configure Fridges and freezers, navigate to the left main menu, under
## Utilities
select
## Settings
. The Select Group Settings drop-down menu will appear, select your
division number
to open your group settings. Click on the
## Edit
tab, and navigate to the
## Storages
section, click on
## + New Storage
tab. Fill in the
## New Storage
form.
## The
## Code
in the
## Identification Info
(if hidden, open the
## More
drop-down menu and select
## Show Identification Info
) and
## Name
should be meaningful and descriptive to represent the storage location.
## The
Number of Row
s (shelves/horizontal position), **
Number of Columns
**(racks per shelf/vertical position) and
Number of Boxes
( number of Boxes per  rack) must be specified.
## The
## Rack Space Warning
and the
## Box Space Warning
can be set (in percentage) to get a warning from the system when e.g., 80% of rack or a box is occupied.
## The
Validation level
is the minimum information required about the storage, must be set. The options are from the lowest to the highest validation level
a. Rack validation
;
b. Box validation
or
c. Box position validation
. Review the entries and
## Save
.
Generated Storages are listed in the left menu, under Utilities, Storage Manager
## Under Utilities
## Select Settings
Select division number
Click on Edit tab
Navigate to Storages section
Click on + New Storage tab
Fill out the New Storage form
Review the entries and Save.",Configure Storage of Objects,0,en_How_to_guides_New_storage_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_New_storage.txt,2025-09-30T12:09:13.424791Z,1
docs:datastore:en_How_to_guides_Object_types_in_drop-downs:0,Enable Object Types in drop-downs menus,https://datastore.bam.de/en/How_to_guides/Object_types_in_drop-downs,datastore,"How_to_guides
/
Object_types_in_drop-downs
# Enable Object Types in drop-downs menus
## Demidova, Caroline
In the left main menu, under
## Utilities
select
## Settings
. The Select Group Settings drop-down menu will appear, select your division number to open your group settings. Click on the
## Edit
tab and scroll down to the
Object type definitions Extension
section, open the corresponding object type, and enable the
Show in drop-downs
option. You can edit several object types at the same time, review the changes and click on the Save.
## Under Utilities
## Select Settings
Select division number
Click on Edit tab
Scroll down to the section: Object type definitions Extension
Select an Object type
Enable Show in drop downs
Review the changes and Save.",Enable Object Types in drop-downs menus,0,en_How_to_guides_Object_types_in_drop-downs_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Object_types_in_drop-downs.txt,2025-09-30T12:09:13.486948Z,1
docs:datastore:en_How_to_guides_Parents_and_Children_Experimental_Steps:0,Define Parents and Children of Experimental Steps,https://datastore.bam.de/en/How_to_guides/Parents_and_Children_Experimental_Steps,datastore,"How_to_guides
/
Parents_and_Children_Experimental_Steps
# Define Parents and Children of Experimental Steps
## Demidova, Caroline
To assign
Parent and Children
to Objects of the type - Experimental Step, select relevant Object, click on the
## Edit
tab. In the
## Parent
and
## Children
section, click
## Search Any
and select the
## Type
of the
## Object
you want to add from the drop-down menu. Enter the
## Code
or
## Name
of the Object in the field and start typing to display available options for your group, select accordingly and
## Save
.
To define multiple Parents and Children at the same time, select the
## Paste Any
option, add the Code or Name of respective Objects, review the entries, and
## Save
. You can copy the Code/Name of Objects from another ELN page (Log in in the BAM Data Store in another browser window). Paste the Codes(s) or name(s) in the text fields, review and
## Save
.
## Select Object
Click on Edit tab
Navigate to the Parent and Children sections
## Click Search Any
## Select Object Type
Enter Code or Name of the Objects to connect
Review the entries and Save.",Define Parents and Children of Experimental Steps,0,en_How_to_guides_Parents_and_Children_Experimental_Steps_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Parents_and_Children_Experimental_Steps.txt,2025-09-30T12:09:13.553757Z,1
docs:datastore:en_How_to_guides_Parents_and_Children_Inventory:0,Define Parents and Children of Inventory Objects,https://datastore.bam.de/en/How_to_guides/Parents_and_Children_Inventory,datastore,"How_to_guides
/
Parents_and_Children_Inventory
# Define Parents and Children of Inventory Objects
Ariza de Schellenberger, Angela
To assign Parent and Children to Objects of a division’s private Inventory, select relevant
## Object
, click on the
## Edit
tab. In the
## Parent
and
## Children
section, click
## Search Any
and select the
Type of the Object
(e.g., Instrument of the public BAM inventory) you want to add from the drop-down menu. Enter the
## Code
or
## Name
of the Object in the field, start typing to display available options for your group. Select and
## Save
.
To define multiple Parents and Children at the same time, select the
## Paste Any
option, add the
## Code
or
## Name
of respective Objects, review the entries and
## Save
. You can copy the Code or Name of Objects from another ELN page (Log in to the BAM Data Store in another/private browser window). Paste the Codes(s) or name(s) in the text fields, review and
## Save
.
Parent-Children connections are displayed in the
## Hierarchy Graph
available under the
## More
drop-down menu of each Object.
## Select Object
Click on Edit tab
Navigate to the Parent and Children sections
## Click Search Any
## Select Object Type - Instrument
Enter Code or Name of the Objects to connect
Review the entries and Save.",Define Parents and Children of Inventory Objects,0,en_How_to_guides_Parents_and_Children_Inventory_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Parents_and_Children_Inventory.txt,2025-09-30T12:09:13.621264Z,1
docs:datastore:en_How_to_guides_Print_barcode:0,Print the Barcode,https://datastore.bam.de/en/How_to_guides/Print_barcode,datastore,"How_to_guides
/
Print_barcode
# Print the Barcode
Ariza de Schellenberger, Angela
Select an Object and open its
## More
drop-down menu, select B
arcode/QR Code Print
. In the Print Barcode/QR Code window, select the code type and size and click on the
## Download
tab. The code will be saved on your computer as a PDF file that you can print.
## Select Object
Open More drop-down menu
Select Barcode/QR Code Print
Select the code type and size
Download the code
## Print.",Print the Barcode,0,en_How_to_guides_Print_barcode_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Print_barcode.txt,2025-09-30T12:09:13.689056Z,1
docs:datastore:en_How_to_guides_Print_barcodes:0,Print the Barcodes,https://datastore.bam.de/en/How_to_guides/Print_barcodes,datastore,"How_to_guides
/
Print_barcodes
# Print the Barcodes
## Demidova, Caroline
Select an Object and open its
## More
drop-down menu, select
Barcode/QR Code Print
. In the Print Barcode/QR Code window, select the code type and size and click on the
## Download
tab. The code will be saved on your computer as a PDF file that you can print.
## Select Object
Open More drop-down menu
Select Barcode/QR Code Print
Select the code type and size
Download the code
## Print.",Print the Barcodes,0,en_How_to_guides_Print_barcodes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Print_barcodes.txt,2025-09-30T12:09:13.756592Z,1
docs:datastore:en_How_to_guides_Project_overview:0,Project Overview,https://datastore.bam.de/en/How_to_guides/Project_overview,datastore,"How_to_guides
/
Project_overview
# Project Overview
## Demidova, Caroline
To generate a Project overview navigate to Project, open the More drop-down menu and click on Show Overview.
Navigate to Project
Open the More drop-down menu
## Click on Show Overview.",Project Overview,0,en_How_to_guides_Project_overview_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Project_overview.txt,2025-09-30T12:09:13.819981Z,1
docs:datastore:en_How_to_guides_Register_collection_Inventory:0,Register Collections in the Inventory,https://datastore.bam.de/en/How_to_guides/Register_collection_Inventory,datastore,"How_to_guides
/
Register_collection_Inventory
# Register Collections in the Inventory
## Demidova, Caroline
To register a
## Collection
in the Inventory, navigate to relevant Project, click on
## + New
tab and select
## Collection
from the Experiment/Collection type drop-down menu. The
## Collection
form opens. Fill out the
## Identification Info
(if hidden, open the
## More
drop-down menu and select Show Identification Info). Enter the
## Code
, a meaningful
## Name
, select
## Empty
in
Default object type
and
## List
view in
Default collection view
drop-down menu.
Note that Collections can contain Objects of one or many types. Arrange Objects in the Inventory in a meaningful way for the group.
Objects can be moved together with descendants, only if they are in the same Collection (
Move Objects to a different Collection
).
## Select Project
Click on + New tab
Select Collection from the Experiment/Collection type drop-down menu
Enter Code and Name
Select in Default Object Type – (empty)
Select in Default Collection view – List view
Review the entries and Save.",Register Collections in the Inventory,0,en_How_to_guides_Register_collection_Inventory_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_collection_Inventory.txt,2025-09-30T12:09:13.884466Z,1
docs:datastore:en_How_to_guides_Register_default_Experiment:0,Register Collection of type Default Experiment,https://datastore.bam.de/en/How_to_guides/Register_default_Experiment,datastore,"How_to_guides
/
Register_default_Experiment
# Register Collection of type Default Experiment
## Demidova, Caroline
The Default Experiment is a type of
## Collection
to group sequential or non-sequential Experimental Steps. To register an Experimental Step, navigate to the relevant
## Project
, click on
## + New
tab, select Default Experiment from the the window Select Experiment/Collection type. Fill out the Create Default Experiment form, review the entries and
## Save
.
## Select Project
Click on + New tab
## Select Default Experiment
Fill out Create Default Experiment form
Review the entries and Save.",Register Collection of type Default Experiment,0,en_How_to_guides_Register_default_Experiment_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_default_Experiment.txt,2025-09-30T12:09:13.952537Z,1
docs:datastore:en_How_to_guides_Register_non-seq_Experimental_Step:0,Register non-sequential Experimental Steps,https://datastore.bam.de/en/How_to_guides/Register_non-seq_Experimental_Step,datastore,"How_to_guides
/
Register_non-seq_Experimental_Step
# Register non-sequential Experimental Steps
## Demidova, Caroline
To register non-sequential Experimental steps, register a new
## Object
at the Collection level (Default Experiment).
Navigate to relevant - Default Experiment, click the
## + New
tab, select
## Experimental Step
in the Select an object type drop-down menu. Fill out the New Experimental Step form, display Identification Info (if hidden, open the More drop-down menu and select Show Identification Info).  The Code is automatically generated for Objects and can only be changed during registration. Give the Experimental Step a meaningful
## Name
, as this will be displayed to the users, review the entries and
## Save
.
Note that the new Experimental Step is organized at the same hierarchical level of Objects (
) in the Lab Notebook left-hand  menu.
To fill out the Experimental Step form with a
## Template
predefined for the group by the Data Store Steward, click on the
## Templates
tab in the New Experimental Step form, select
## Template
, add information to the form as required, review the entries and
## Save
. Reload the web page to see the changes.
## Select Default Experiment
Click the + New tab
Select an Object Type - Experimental Step
Fill out the Experimental Step form
Alternatively, select relevant Template
click on Templates tab, select Template and complete relevant information
Review the entries and Save.",Register non-sequential Experimental Steps,0,en_How_to_guides_Register_non-seq_Experimental_Step_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_non-seq_Experimental_Step.txt,2025-09-30T12:09:14.019178Z,1
docs:datastore:en_How_to_guides_Register_objects_Inventory:0,Register Objects in the Inventory,https://datastore.bam.de/en/How_to_guides/Register_objects_Inventory,datastore,"How_to_guides
/
Register_objects_Inventory
# Register Objects in the Inventory
Ariza de Schellenberger, Angela
To register
## Objects
in the Inventory, navigate to the relevant
## Collection
, click on the
## More
drop-down menu, select
## New Object
and
## Object Type
from drop-down menu.  Fill out the Object
## Identification Info
(if hidden, open the
## More
drop-down menu and select Show Identification Info). The
## Code
is generated automatically for Objects and can
only
be changed during registration. Give the Object a meaningful
## Name
, as this will be displayed to the users, review the entries and
## Save
.
Note that you can register an object of type ‘Entry’ in the object form to quickly visualise information. A preview of text, images or tables is displayed in the
## Document
icon of the Collection form. To see the icon, select relevant Collection from the left-hand menu.
## Select Collection
Click on More drop-down menu
## Select New Object
Select an Object Type
Fill out the Object form
Review the entries and Save.",Register Objects in the Inventory,0,en_How_to_guides_Register_objects_Inventory_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_objects_Inventory.txt,2025-09-30T12:09:14.127368Z,1
docs:datastore:en_How_to_guides_Register_project:0,Register a Project,https://datastore.bam.de/en/How_to_guides/Register_project,datastore,"How_to_guides
/
Register_project
# Register a Project
Ariza de Schellenberger, Angela
To register a
## Project
, navigate to the Lab Notebook in the left-hand menu, open the drop-down menu, select My Space and click on
## + New Project
. The Project form will open. Fill out the Identification Info (if hidden, open the
## More
drop-down menu and select Show Identification Info).  Enter the Code and Description, review the entries and
## Save
.
## Code*
### Requirements
## :
Mandatory for openBIS (*)
Allowed characters: A-Z (uppercase), 0-9, '_' (underscore), '-' (hyphen), and '.' (dot)
Separate words with underscores
Should be meaningful, in English, and between 3-30 characters
Cannot be modified or reused.
## Description
### Requirements
## :
Mandatory for BAM Data Store
Should contain enough detail to be understandable to people outside of the group
Should be in the following format: ""English//German""
Should contain 2-50 words.
## Select My Space
Click on + New Project
Enter Code and Description
Review the entries and Save.",Register a Project,0,en_How_to_guides_Register_project_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_project.txt,2025-09-30T12:09:14.195294Z,1
docs:datastore:en_How_to_guides_Register_project_Inventory:0,Register Projects in the Inventory,https://datastore.bam.de/en/How_to_guides/Register_project_Inventory,datastore,"How_to_guides
/
Register_project_Inventory
# Register Projects in the Inventory
## Demidova, Caroline
New Projects can be registered by Data Store Stewards (DSSt(s)) with Group Admin
roles
in division’s
private
## Inventory
## Spaces
. Projects in
public
## Inventory
## Projects
need to be registered by the Data Store team (Instance Admins), please contact at
datastore@bam.de
.",Register Projects in the Inventory,0,en_How_to_guides_Register_project_Inventory_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_project_Inventory.txt,2025-09-30T12:09:14.249930Z,1
docs:datastore:en_How_to_guides_Register_seq_Experimental_Step:0,Register sequential Experimental Steps,https://datastore.bam.de/en/How_to_guides/Register_seq_Experimental_Step,datastore,"How_to_guides
/
Register_seq_Experimental_Step
# Register sequential Experimental Steps
## Demidova, Caroline
To register sequential Experimental Steps, register a new
## Object
at the Object level. To do this, select relevant Experimental Step - Object, click on the
## + New
tab, fill out relevant information, review and
## Save
.
Note that the new Experimental Step is organized under a Children drop-down in the left-hand main menu.
## Select Experimental Step
Click on + New tab
Select an Object Type - Experimental Step
Fill out the Experimental Step form
Alternatively, select relevant Template
Click on Templates tab, select Template,
modify Object name and complete relevant information
Review the entries and Save.",Register sequential Experimental Steps,0,en_How_to_guides_Register_seq_Experimental_Step_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_seq_Experimental_Step.txt,2025-09-30T12:09:14.316808Z,1
docs:datastore:en_How_to_guides_Register_storage_position:0,Allocate storage position to a single Object,https://datastore.bam.de/en/How_to_guides/Register_storage_position,datastore,"How_to_guides
/
Register_storage_position
# Allocate storage position to a single Object
Ariza de Schellenberger, Angela
The digital representation of laboratory storage must be configured by the Data Store Stewards (DSSt(s)) in your division. To add storage information to an
## Object
during registration, navigate to the
## Storage
section in the
## Object
form.  If the Object is already registered, navigate to the relevant Object, click
## Edit
and scroll down to the
## Storage
section.  Click on the
## + New Storage Position
## Tab. The
## Physical Storage
form opens. Select
## Storage
from the drop-down menu, specify the
position
of the Object (e.g., Rack, Box name to display Box position) mark the position of the Object within the Box, click on the
## Accept
tab and then on the
## Save
tab.
## Select Object
Click on Edit tab
Navigate to Storage sections
## Click + New Storage Position
Specify Object Position (e.g., Rack, Box)
Mark the Object(s) position(s) within the Box
Click on Accept tab
Save Object form.",Allocate storage position to a single Object,0,en_How_to_guides_Register_storage_position_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Register_storage_position.txt,2025-09-30T12:09:14.378568Z,1
docs:datastore:en_How_to_guides_Represent_research_data:0,Represent research data,https://datastore.bam.de/en/How_to_guides/Represent_research_data,datastore,"How_to_guides
/
Represent_research_data
# Represent research data
## Page Contents
Conceptual data Model
1. Draw the research data workflow
2. Identify Entities and Entity types
3. Map Entities in the Data Store-openBIS data structure
## 4. Connect Entities
## Demidova, Caroline
¶
Conceptual data Model
To map research data in the Data Store – openBIS Research Data Management system, data are stored together with all experimental steps, tangible and intangible objects (things you do, generate and have) that are relevant to generate traceable and reusable data.
It is advisable that divisions create a conceptual data model to have a visual tool that supports the discussions and development of the data model.  In BAM, the DSSt leads the design of the data model and coordinates the feedback rounds with the division members.  An understandable data model for the division supports effective data management, analysis, and collaboration. It is recommended to understand the concepts from the beginning to improve and expand the data model according to the needs of the department.
The following steps serve as a guide for creating a conceptual data model. Any visualization tool can be used, the
template
in
draw.io
can be downloaded and reused to implement the following steps.
¶
1. Draw the research data workflow
Identify the data (of any format, single or multiple files, datasets) to be stored in the Data Store.
Add the
things you do
(e.g., synthesis, measurements, analysis, etc.) to generate the data and specify them as Experimental Steps.  Attach data to Experimental Steps.
Connect the Experimental Steps with unidirectional arrows to indicate the logically occurrence and dependency.
Add the things you generate,
tangible
or
intangible
(Samples, Materials, etc.).
Add the
things you have
or
need
to complete all Experimental Steps and that are relevant to generate traceable and reusable data. Use general categories such as Chemicals to simplify visualization rather than listing individual chemicals.
To simplify the flowchart, make sure all Experimental Steps have at least one dataset attached (otherwise, check whether the Experimental step is part of another).
¶
2. Identify Entities and Entity types
Identify all
## Entities
used (Chemical 1, Chemical 2) or generated (Samples) in each Experimental Step.
Group similar Entities under common
## Entity Types
(chemicals 1, 2 in Chemicals and Nanoparticles in Samples). Several Entity types have already been defined by BAM users, try to reuse them if possible. Use the
MASTERDATA CHECKER
to identify existing Entity Types and their properties in the Data Store. To add Properties to an Entity Type or to define new Entity Types, the DSSt(s) can contact the Data Store team at
datastore@bam.de
.
If an Entity cannot be grouped with others, list it in the table and leave the Entity Type name blank. Contact the Data Store team to find out how to represent this Entity in the Data Store.
Note that generated things in an Experimental Step such as Code and Data, can be uploaded to the system as datasets.  These datasets can be uniquely described by defining an Entity Type or be uploaded with minimal metadata as generic Datasets with default properties (e.g., dataset name) defined by the system. If no Entity Type is defined for Code or Data, all relevant Information should be stored within the Experimental Step used linked to these items.
¶
3. Map Entities in the Data Store-openBIS data structure
To map Entities in the openBIS organize Entities at Object level within the hierarchical
data structure
of openBIS: Space → Project → Collection → Object → Dataset. Consider the pre-defined organization of the BAM (public) and FB (private) -Inventory and Lab Notebook Spaces.
Map Entities in the openBIS data structure.
List all Objects at the Object level to represent the things you do and the things you generate in the ELN. Organize all the things you have in the Inventory.
Assign meaningful names to Collections. Collections can contain one or more Objects of one or more Types (Entity Types).
¶
## 4. Connect Entities
To connect research data in openBIS, define unidirectional
parent–child relationships
. These connections can be defined Object to Object or Dataset to Dataset across Spaces, regardless of whether they are in the Inventory or in the Lab Notebook. For the Data Store, it is recommended to connect Objects to Objects:  openBIS provides an automatic visualization of Objects. To visuaize and understand data, all Objects should be connected to each other.
Visualize the connections between Objects
.
To define the connections between Objects, draw unidirectional arrows between Objects. By default, each Object can have an unlimited number of parents and/or children or none (N:N relationships with N being any number from 0 to N). For a specific Object type, the DSSt (group admins) can define a maximum number of children and parents.",Represent research data,0,en_How_to_guides_Represent_research_data_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Represent_research_data.txt,2025-09-30T12:09:14.433163Z,1
docs:datastore:en_How_to_guides_Revert_deletions_ELN:0,"Revert deletions of Collection, Objects and Datasets",https://datastore.bam.de/en/How_to_guides/Revert_deletions_ELN,datastore,"How_to_guides
/
Revert_deletions_ELN
# Revert deletions of Collection, Objects and Datasets
## Demidova, Caroline
To revert deletions, navigate to
## Utilities
in the main menu and select the
## Trashcan
. Open the
## Operations
drop-down menu for the relevant content and click on
## Revert Deletion
, a green confirmation message will appear. Reload the webpage and navigate to relevant folders to see the reverted deletions.
Note that deletions from the Trashcan are
irreversible
.
Navigate to Utilities
## Select Trashcan
Open Operations drop-down menu
Click on Revert Deletion
Reload the webpage","Revert deletions of Collection, Objects and Datasets",0,en_How_to_guides_Revert_deletions_ELN_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Revert_deletions_ELN.txt,2025-09-30T12:09:14.498464Z,1
docs:datastore:en_How_to_guides_Scan_barcodes:0,Scan Barcodes,https://datastore.bam.de/en/How_to_guides/Scan_barcodes,datastore,"How_to_guides
/
Scan_barcodes
# Scan Barcodes
## Demidova, Caroline
In the top
Main menu
next to the
## Global Search
field, click on the
## Barcode
icon.  Use a
barcode scanner
or the
camera of a mobile device
to scan the code.  The associated entry is opened directly in openBIS.  Your scanned selection will be saved for future reference.
Click the Barcode icon
Select scanning device
Scan the barcode.",Scan Barcodes,0,en_How_to_guides_Scan_barcodes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Scan_barcodes.txt,2025-09-30T12:09:14.565977Z,1
docs:datastore:en_How_to_guides_Search_Dataset_Lab_Notebook:0,Search - Datasets,https://datastore.bam.de/en/How_to_guides/Search_Dataset_Lab_Notebook,datastore,"How_to_guides
/
## Search_Dataset_Lab_Notebook
# Search - Datasets
## Demidova, Caroline
To search through Datasets, navigate to main menu on the left side, open the
## Utilities
drop-down menu and select
## Advanced Search
. In the
## Search For
drop-down menu, select the
## Dataset
option and select the
## AND
operator to add additional search parameters. For example,  to search for a Dataset by the name of the Registrator, select the option
## Property
in the drop-down menu under
## Field Type
; the option
Registrator [ATTR.REGISTRATOR]
under
## Field Name
, and the option
thatEqualsUserId (UserId)
from the
Comparator Operator
. To define the search value, start typing the Name of the Registrator in the
## Field Value
and click on the Search icon, next to the operator field Using (e.g., AND) to activate the search.
## Open Utilities
## Select Advanced Search
Select Dataset in the Search For drop-down menu
Select AND operator in the Using drop-down menu
Select Property in the drop-down menu under Field Type
Select  Registrator [ATTR.REGISTRATOR] under Field Name
Select thatEqualsUserId (UserId) from the Comparator Operator
Start typing the Name of the Registrator in the Field Value
Click on Search icon.",Search_Dataset_Lab_Notebook,0,en_How_to_guides_Search_Dataset_Lab_Notebook_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Search_Dataset_Lab_Notebook.txt,2025-09-30T12:09:14.629633Z,1
docs:datastore:en_How_to_guides_Search_Experimental_Steps:0,Search - Objects - Experimental Steps in the ELN & save search queries,https://datastore.bam.de/en/How_to_guides/Search_Experimental_Steps,datastore,"How_to_guides
/
## Search_Experimental_Steps
# Search - Objects - Experimental Steps in the ELN & save search queries
Ariza de Schellenberger, Angela
Open the
## Utilities
drop-down menu and select
## Advanced Search
. Click on
## Search For
drop-down menu and select Experiment/Collection, select an operator from the
## Using
drop-down menu (e.g., AND), select
## Field Type
option (e.g., All), enter
## Field Value
(e.g., your BAM username to search for all Collections you have registered). Click the
+
icon to narrow the search further, select
## Field Type
option (e.g., Property) and
## Field Name
option (e.g., Modification Date. A list of all available properties becomes available), enter option for a
Comparator Operator
(e.g., thatISLaterThan (Date)) and select the
date
in
calendar
icon. Click on the
## Search
icon to activate the search. Search values can also be excluded from the search by selection the NOT checkbox.
To save a search query in your own
Lab Notebook's
Space. Click on the
## Save
icon displayed in the upper part of the
## Advance Search
form, the
Save Search query
window will open, fill out search
## Name
and start typing in the
search entity to store query
to find the name of a
## Collection
. To save the search, click Save. Saved searchers are available in the drop-down menu displayed at the top of the Advanced Search page.
## Open Utilities
## Select Advanced Search
Click on Search For and select Experiment/Collection
Select operator from Using drop-down menu (e.g., AND)
Select Field Type option (e.g., All)
Enter Field Value (e.g., your BAM username)
Click on the + icon; select Field Type option (e.g., Property; Field Name option (i.e., Modification Date);
enter option for a Comparator Operator (e.g., thatISLaterThan (Date)) and select the date in calendar icon
Click on search icon
Click on the Save icon
Enter search Name and entity to store query (Collection Name)",Search_Experimental_Steps,0,en_How_to_guides_Search_Experimental_Steps_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Search_Experimental_Steps.txt,2025-09-30T12:09:14.692408Z,1
docs:datastore:en_How_to_guides_Search_Inventory:0,Search - Objects in the Inventory,https://datastore.bam.de/en/How_to_guides/Search_Inventory,datastore,"How_to_guides
/
## Search_Inventory
# Search - Objects in the Inventory
Ariza de Schellenberger, Angela
To search for Objects, navigate to the left menu, open the
## Utilities
drop-down menu and select
## Advanced Search
. Click on
## Search For
drop-down menu and select Experiment/Collection; select an operator (e.g., AND) from the
## Using
drop-down menu. Select the
## Field Type
option (e.g., All), enter a
## Field Value
(e.g., Instrument).  Click the
+
icon to narrow the search further, select
## Field Type
option (e.g., Property). Select one of the displayed values in the
## Field Name
and enter a value in the
## Field Value
. Click on the Search icon to activate the search.  Search values can also be excluded by selecting the NOT checkbox.
## Open Utilities
## Select Advanced Search
Click on Search For
Select operator from Using drop-down menu (e.g., AND)
Select Field Value option (e.g., Instrument)
Click on the + icon
Select Field Type option (e.g., Property)
Enter Field Name and Field Value if available
Click on the search icon.",Search_Inventory,0,en_How_to_guides_Search_Inventory_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Search_Inventory.txt,2025-09-30T12:09:14.758546Z,1
docs:datastore:en_How_to_guides_Share_code:0,Share Code in BAM research GitHub,https://datastore.bam.de/en/How_to_guides/Share_code,datastore,"How_to_guides
/
Share_code
# Share Code in BAM research GitHub
## Page Contents
## ✅ Prerequisites
Two options to Share Your Code
Option A: Create a New Repository
Option B: Copy an Existing Repository
## 🧩 Need Help?
## Demidova, Caroline
This guide walks you through the steps to share your code (scripts, parsers, tools, etc.) in the
BAM research GitHub organization
, following
open-source best practices
.
¶
## ✅ Prerequisites
You have a GitHub account.
You are a member of the
BAM research GitHub organization
.
For access please contact
jose.pizarro-blanco@bam.de
via Microsoft Teams with your GitHub username to request an invitation.
¶
Two options to Share Your Code
¶
Option A: Create a New Repository
Go to the
BAM research GitHub organization
.
## Click
""New repository""
.
Fill in the repository details:
## Name
: Choose a clear, descriptive name.
## Description
: Briefly explain what the code does.
## Visibility
## : Choose
## Public
(preferred for OSS) or
## Private
.
## Click
Create repository
.
¶
Option B: Copy an Existing Repository
## If your code already exists elsewhere:
Fork it
## :
If the original repository is public and has a compatible license:
Go to the original repo and click
## Fork
## → Choose
BAM research
as the destination.
Duplicate it
## :
## If forking isn’t suitable:
Clone the original repo locally.
Create a new repo under BAM research.
Push the code to the new repo.
¶
## 🧩 Need Help?
If you’re unsure about licensing, repository setup, or GitHub workflows, reach out to Data Store Team at
datastore@bam.de
.",Share Code in BAM research GitHub,0,en_How_to_guides_Share_code_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Share_code.txt,2025-09-30T12:09:14.826923Z,1
docs:datastore:en_How_to_guides_Storage_position:0,Enable Storage Widget on Object Form,https://datastore.bam.de/en/How_to_guides/Storage_position,datastore,"How_to_guides
/
Storage_position
# Enable Storage Widget on Object Form
Ariza de Schellenberger, Angela
The Storage Widget is disabled by default. To track storage positions for a particular Object Type, the storage must be enabled by a group Admin (Data Store Steward). To do this, navigate to the left main menu, under
## Utilities
select
## Settings
and your
division number
drop-down menu. Click on the
## Edit
tab and scroll down to the
Object Type definitions Extension
section, choose the Object type, and check the
## Enable Storage
checkbox. Review the entries and
## Save
.
## Under Utilities
## Select Settings
Select division number
Click on Edit tab
Scroll down to Object Type definitions Extension
Select an Object type
## Enable Storage
Review the entries and Save.",Enable Storage Widget on Object Form,0,en_How_to_guides_Storage_position_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Storage_position.txt,2025-09-30T12:09:14.890195Z,1
docs:datastore:en_How_to_guides_Templates:0,Create Templates for Experimental Steps and other Objects,https://datastore.bam.de/en/How_to_guides/Templates,datastore,"How_to_guides
/
## Templates
# Create Templates for Experimental Steps and other Objects
## Demidova, Caroline
To create a Template for an Object type, navigate to left main menu and select
## Utilities
, and then
## Settings
. A Select Group Settings drop-down menu will appear, select your
division number
to open your group settings. Click on the
## Edit
tab, open the Templates section, click on the
## + New Template
tab. Select an Object Type for which the template is intended, e.g. Experimental Step. Fill out the predefined values as required, review the entries and
## Save
.  All available templates to your group are displayed in the Templates section.
Note, using a Template will overwrite the existing parent(s) and child(ren) defined in the registration form of the
## Object
. When registering
sequential Experimental Steps
using a Template, you must
explicitly specify the parent(s) and/or child(ren) during the registration
process. If the parent(s) and child(ren) are fixed for a particular Template, they can be defined at the time of the Template's creation. In such cases, they will be automatically applied whenever the Template is used.
Note that to quickly visualise information and preview text, images or tables in the Object form, you can embed this content when creating a Template for Experimental Steps and other Objects. To display the preview, select the corresponding Project or Collection from the menu on the left and click on the icon in the Document column in the Collection form.
## Under Utilities
## Select Settings
Select division number
Click on Edit tab
Open Templates section
Click on + New Template tab
Select an Object type- Experimental Step
Fill out the form with the pre-defined values
Review the entries and Save.",Templates,0,en_How_to_guides_Templates_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Templates.txt,2025-09-30T12:09:14.953908Z,1
docs:datastore:en_How_to_guides_Upload_data:0,Upload data,https://datastore.bam.de/en/How_to_guides/Upload_data,datastore,"How_to_guides
/
Upload_data
# Upload data
## Demidova, Caroline
The data uploaded in openBIS can be of any type and format, individual files, or data sets. Data uploaded is
immutable
, it cannot be changed, if necessary, different versions of the data must be uploaded. In the Data Store, it is suggested to upload data to the Experimental Steps - Objects. To upload data, navigate to relevant Experimental Step and click on the
## Upload
tab. In the
## Create Dataset
form, enter the Identification Info (if hidden, open the More drop-down menu and select Show Identification Info).  Select
## Dataset Type
if not defined, e.g. Attachment. Fill out the relevant information and click on select
files to upload
, drag and drop or browse files or upload a zip file (select Uncompress-checkbox before import), review the files and
## Save
.  Datasets are displayed in the left-lower corner of the main menu.
Uploaded Datasets are displayed in the left-hand menu. To view the contents of Datasets, click on the Dataset and download it. Please note that there is no preview function for Datasets in openBIS.
Alternatively, you can preview text, images, or tables by registering an Object of type
## Entry
or registering an Object using
## Templates
.
## Select Experimental Step
Click Upload tab
Select Dataset Type (*), e.g. Attachment
Select files to upload
(drag and drop, browse or upload zip file
and select Uncompress-checkbox  before import)
Review the files and Save.
## Select Experimental Step
Click Upload button
Select Entry or Dataset Type (*), e.g. Attachment
Select files to upload
(drag and drop, browse or upload zip file
and select Uncompress-checkbox  before import)
Review the files and Save.",Upload data,0,en_How_to_guides_Upload_data_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Upload_data.txt,2025-09-30T12:09:15.018101Z,1
docs:datastore:en_How_to_guides_Use_barcodes_qr_codes:0,Use Barcodes and QR Codes,https://datastore.bam.de/en/How_to_guides/Use_barcodes_qr_codes,datastore,"How_to_guides
/
Use_barcodes_qr_codes
# Use Barcodes and QR Codes
## Page Contents
🔧 Use Barcodes and QR Codes
## ✅ Prerequisites
🪪 Step 1: Choose the Code Content
🖨️ Step 2: Generate the Codes
## 🧾 Step 3: Print Code
📥 Step 4: Print and Apply Stickers
Hardware to print Barcodes/QR codes
📡 Step 5: Scan and Use in openBIS
Alternative Barcode/QR code batch label generator
Ariza de Schellenberger, Angela
¶
🔧 Use Barcodes and QR Codes
To enable fast and secure referencing of physical objects (e.g., samples, instruments) in openBIS using barcodes or QR codes.
¶
## ✅ Prerequisites
Access to an openBIS instance
Registered Objects in openBIS - Inventory (e.g., Samples, Instruments)
Barcode/QR code reader (USB or Bluetooth)
Sticker printer or external code generator (optional)
¶
🪪 Step 1: Choose the Code Content
When an Object is registered, a Default Barcode is automatically generated by openBIS.
## Default Barcode
is found under
## Identification Info
and can be displayed to print under
## More
drop-down menu,
Barcode/QR Code Print
.
It is also possible to use the PermId to generate a Barcode/QR code.
¶
## Option A:
Default Barcode = PermId
Default Barcode is vissible under Identification Info of a registered Object and displayed under the More drop-down menu within the Object Form.
## Pros
## :
Always available and unique
Compact (suitable for Micro-QR)
## Cons
## :
Tied to one openBIS instance
Changes on export/import
Only available after object registration
¶
## Option B:
$BARCODE Property
$BARCODE is an internal openBIS Property.
## Pros
## :
Can be pre-assigned and batch imported
Compatible across openBIS intances
## Cons
## :
Uniqueness not enforced
Requires additional data management to keep uniqueness and consistency
¶
🖨️ Step 2: Generate the Codes
¶
## Option A:
In openBIS ELN
Direct integration
Limited formatting
¶
## Option B:
with external Tools
## Linux
## :
qrencode
(CLI, scriptable)
## Windows
## :
## Zint
(GUI, flexible)
## Label Printer Software
## :
Often supports Excel import
Good formatting and printer integration
¶
## 🧾 Step 3: Print Code
¶
Select the Code Format
Choose based on your hardware and space constraints:
## Code Type
## Format
## Pros
## Notes
## Code 128
## 1D
Widely supported
Minimum standard
QR Code
## 2D
Compact, robust
## Recommended
Micro-QR Code
## 2D
Very small (5x5mm)
Ideal for PermIds
💡
## Tip
: Always prefer 2D codes unless you have a specific reason to use 1D.
¶
📥 Step 4: Print and Apply Stickers
Use durable stickers compatible with your printer
Include optional metadata (e.g., contact, organizational unit)
Apply to physical objects clearly and accessibly
¶
Hardware to print Barcodes/QR codes
Alternative to workflow and printers suggested in openBIS documentation of the ETHZ
## Printers
## . FB 9.3 tested following hardware:
## Printers:
Option 1: Brother QL-820NWB Label Printer.
Option 2: Brother QL-700, KdB 21464-01=60000544023
Use of P-Touch Editor Software to customize and desing  templates.
## Scanner:
Inateck BCST-91 2D Barcode Scanner.
¶
📡 Step 5: Scan and Use in openBIS
Use barcode/QR readers (USB-HID or Bluetooth)
## Scanned codes will:
Display object info
Link samples/devices in experiments
¶
Alternative Barcode/QR code batch label generator
A Data Store advanced Label App is being developed by FB 6.5 to simplify the interface between physical Samples generated at the Laboratory and the Data Store
https://github.com/BAMresearch/dalapp",Use Barcodes and QR Codes,0,en_How_to_guides_Use_barcodes_qr_codes_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Use_barcodes_qr_codes.txt,2025-09-30T12:09:15.080152Z,1
docs:datastore:en_How_to_guides_Use_saved_search:0,Search queries - Use saved,https://datastore.bam.de/en/How_to_guides/Use_saved_search,datastore,"How_to_guides
/
Use_saved_search
# Search queries - Use saved
## Demidova, Caroline
To use a saved search query in the ELN, navigate to the left main menu, open the
## Utilities
drop-down menu and select
## Advanced Search
. Open the
top drop-down menu
of the
## Advanced Search
form, select stored search and click on the
## Search
icon to activate the search.
## Open Utilities
## Select Advanced Search
Open the top drop-down menu of the Advanced Search form
Select stored search
Click on Search icon.",Search queries - Use saved,0,en_How_to_guides_Use_saved_search_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Use_saved_search.txt,2025-09-30T12:09:15.141866Z,1
docs:datastore:en_How_to_guides_Verify_storage_position:0,Verify storage position,https://datastore.bam.de/en/How_to_guides/Verify_storage_position,datastore,"How_to_guides
/
Verify_storage_position
# Verify storage position
Ariza de Schellenberger, Angela
To verify whether the storage position was registered correctly, select
## Storage Manager
from the
## Utilities
drop-down menu on the left. Select the Storage where the Object was registered from the
## Storage
drop-down menu. The contents of the Storage (e.g.,rack and  boxes) are displayed. Double click on the box to display the registered Objects. Place the mouse over the Object of interest to display the corresponding metadata fields and click on the Object name displayed in blue to open the Object form.
## Select Utilities
## Select Storage Manager
From the Storage drop-down menu select the Storage
Double click on the box
Place mouse cursor on the Object
Click on the Object's name",Verify storage position,0,en_How_to_guides_Verify_storage_position_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_How_to_guides_Verify_storage_position.txt,2025-09-30T12:09:15.203077Z,1
docs:datastore:en_masterdata_definition:0,Masterdata definition,https://datastore.bam.de/en/masterdata_definition,datastore,"masterdata_definition
# Masterdata definition
## Demidova, Caroline
Last Friday at 1:24 PM
Masterdata defintion in the BAM Data Store:
The Process of Masterdata Definition
Best Practices for Masterdata Definition
Masterdata checker
How to use Masterdata checker
The Process of Masterdata Definition for phase 4",Masterdata definition,0,en_masterdata_definition_0,concept,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_masterdata_definition.txt,2025-09-30T12:09:15.273136Z,1
docs:datastore:en_masterdata_definition_best_practices:0,Best Practices for Masterdata Definition,https://datastore.bam.de/en/masterdata_definition/best_practices,datastore,"masterdata_definition
/
best_practices
# Best Practices for Masterdata Definition
## Page Contents
# Best Practices for Masterdata Definition
Naming Spaces and Projects
## Defining Entity Types
""Inheritance"" of Entity Types
Making Changes to existing Entity Types
Ariza de Schellenberger, Angela
¶
# Best Practices for Masterdata Definition
To ensure a consistent set of
## Masterdata
in the BAM Data Store, we are introducing some rules, naming conventions, and recommendations when creating
## Spaces
,
## Projects
, entity types (
## Collection
,
## Object
,
## Dataset
, and
## Property
types) and controlled vocabularies.
In general, Masterdata should be
as generic as possible and as specific as necessary
. It is encouraged to re-use existing entity types and vocabularies from other groups. For this reason, the use of division-specific information in the codes and labels of entity types (e.g., ""BAM_FBX.Y_ROOM_TEMPERATURE"") should be avoided.
¶
## Naming
## Spaces
and
## Projects
## Spaces
and
## Projects
have a
code/PermID
and a
description
, but no additional label or metadata properties. Therefore, the
## Space
/
## Project
code and description should contain enough information to make it clear what it is for (both to people outside the group and to people who join the group later).
Once created, the code of a
## Space
/
## Project
cannot
be changed.
¶
## Space
/
## Project
## Code:
Can only contain A-Z (uppercase letters), 0-9, '_' (underscore), '-' (hyphen), and '.' (dot).
Words that would normally be separated by a whitespace, should instead be separated by an underscore (""_"").
Should be meaningful.
Should be in English.
Should be between 3-30 characters.
Despite being written in capital letters only, the code of a
## Space
/
## Project
will be displayed in the main menu with uppercase letters for every first letter of a new word (as signified by the use of an underscore) followed by lowercase letters.
¶
## Example:
## A
## Space
or
## Project
with the code ""TEST.THING_UNDERSCORE-HYPHEN"" will be displayed as ""Test.thing  Underscore-hyphen"" in the main menu.
¶
## Space
/
## Project
## Description:
Not mandatory in openBIS but mandatory for the Data Store.
Should contain enough detail to be understandable to people outside of the group.
Should contain 2-50 words.
Should be in English followed by a double slash (//) and a German translation in the following format: ""English description//Deutsche Beschreibung""
¶
## Defining Entity Types
When defining a new entity type of class
## Collection
,
## Object
or
## Dataset
, please check whether a similar entity type (of the same class) already exists that could be reused.
If the new entity type is a specification of an existing, more generic entity type then the new, more specific entity type must include all the Sections and
## Property
types of the existing entity type. For more information on the concept of inheritance of entity types, see
here
.
Collection and Dataset types will be deprecated in future openBIS versions. Therefore only Object Types and Controlled Vocabularies are curently defined for the Data Store.
¶
## Defining
## Object
## Types
The creation of new
## Object
types is one of the main tasks of the
Masterdata definition process
.
¶
## Object
## Code:
Only capital letters allowed.
Can only contain A-Z, 0-9, _ and . (dot).
Words should be separated by an underscore (""_"").
Should be meaningful.
Should be in English.
Should be between 3-20 characters long.
¶
## Object
## Description:
Not mandatory in openBIS but mandatory for BAM Data Store.
Should contain enough details to be understandable to people outside of the group.
Should be up to 250 characters long.
Should be in English followed by a German translation in the following format: ""English description//Deutsche Beschreibung""
¶
## Object
## Generated Code Prefix:
Should be meaningful.
As a convention, we recommend to use the
first 3 letters of the
## Object
type code
.
## Example:
For the
## Object
type ""Instrument"", the code prefix should be ""INS"". Registered
## Objects
of the type ""INSTRUMENT"" will have the code ""INS1"", ""INS2"", etc.
If an identical code prefix already exists:
Choose the first 4-5 letters of the
## Object
type code OR
If the
## Object
type is a specification of a an already existing
## Object
type (see above), take the existing generated code prefix, add a dot/period (.) and then another 2-4 character code to make it unique.
## Example:
For a new
## Object
type ""EXPERIMENTAL_STEP.MICROSCOPY"" that is a specification of the existing
## Object
type ""EXPERIMENTAL_STEP"" with the code prefix ""EXP"", the code prefix of the new
## Object
type could be ""EXP.MIC"".
¶
## Defining
## Property
## Types
## A
## Property
is a metadata field used to describe an entity, i.e.
## Collection
, an
## Object
or a
## Dataset
.
## Property
types have a
code
, a
label
, a
data type
, and a
description
. One and the same
## Property
type can be used for many entity types. When assigning a
## Property
type to an entity type, it can further be defined whether the Property is
mandatory
and whether it should be
editable
by the user in the ELN-LIMS UI. Additionally, a
## Dynamic Property Script
can be added to the
## Propert
y type assignment.
## All
## Property
types are
global
in openBIS which means that they can be assigned to many entity types at the same time. Changes made to a
## Property
type (e.g., changing the label or the description) will thus affect all entity types that the
## Property
type is assigned to and should therefore be considered carefully. The only non-global attributes of a
## Property
type are ""Mandatory"" and ""Editable"" and the
## Dynamic Property Script
which have to be defined individually for each
## Property
type assignment. More information on changing existing entity types can be found
here.
¶
## Internal
## Property
## Types:
There are several internal
## Property
types in the Data Store (pre-defined by openBIS). Their code begins with a ""$"" sign. Some of them should be reused when creating a new entity Type (e.g., $NAME, $XMLCOMMENTS), others are used internally only (e.g., $ELN_SETTINGS). No changes can be made to these internal
## Property
types which is why they don't necessarily follow the naming conventions defined below.
A new
## Object
type should always contain the predefined
## Property
types $NAME, $XMLCOMMENTS and $ANNOTATIONS_STATE. The latter is not visible in any of the forms, but it is necessary to establish parent-child relationships between
## Objects
.
¶
## Property
## Code:
Only capital letters allowed.
Can only contain A-Z, 0-9 and _, -,.
Words should be separated by an underscore (""_"").
Should be meaningful.
Should be in English.
Should be between 3-20 characters long.
¶
## Property
## Label:
Should be meaningful.
Should be in English.
¶
## Property
Data Type:
A Property type can use one of 11 possible data types:
Data type
## Description
## BOOLEAN
yes or no (checkbox)
## INTEGER
integer number
## REAL
decimal number
## DATE
date field
## TIMESTAMP
date with timestamp
## VARCHAR
one-line text
## MULTILINE_VARCHAR
long text (it is possible to enable a Rich Text Editor for this type of property)
## HYPERLINK
## URL
## CONTROLLED_VOCABULARY
list of predefined (alpha-numeric) values
## XML
to be used by Managed Properties and for Spreadsheet components
## OBJECT
1-1 connection to a specific Object type
¶
## Property Description:
Not mandatory in openBIS but mandatory for the Data Store.
Should contain enough details to be understandable to people outside of the group.
Should be up to 250 characters long.
Should be in English followed by a German translation in the following format: ""English description//Deutsche Beschreibung""
¶
Representation of Values and Units of Measurement of Physical Quantities:
Units of measurement (e.g., SI base units) of a physical quantity should be represented as part of the label/description of a
## Property
type and not as an individual
## Property
type. The unit should be specified in square brackets in the
## Property
label. Where relevant, use the English notation to indicate the decimal place of numbers (period instead of comma). Additionally, the unit can also be part of the code to avoid multiple similarly named
## Property
types representing different entities.
## Example:
To specify the room temperature measured in °C, create a single
## Property
type called ROOM_TEMP_IN_CELCIUS with the data type REAL, instead of creating two types ROOM_TEMP_VALUE (data type REAL) and ROOM_TEMP_UNIT (data type CONTROLLED_VOCABULARY).
## Code
## Mandatory
Show in edit views
## Section
Property label
Data type
## Description
## Metadata
Dynamic script
## ROOM_TEMP_IN_CELSIUS
## False
## TRUE
Further information
Room temperature [°C]
## REAL
Room temperature in °C//Raumtemperatur in °C
¶
Assignment of existing
## Property
## Types
It is encouraged to reuse/assign already existing
## Property
types for the definition of new entity types instead of creating new
## Property
types that are synonymous or similar in meaning to those already in use.
## Example:
Instead of creating a new
## Property
type called INSTRUMENT_NAME for an
## Object
type INSTRUMENT, it is recommended to assign the existing
## Property
type $NAME.
¶
## Defining Controlled Vocabularies
A controlled vocabulary is an established list of terms to ensure consistency and uniqueness in the description of a given domain, e.g., a list of room labels, SI units or purity grades. In openBIS controlled vocabularies are one possible data type for metadata
## Properties
. The vocabulary itself has a
code
and a
description
and each term in the vocabulary has a
code
, a
label
, and a
description
.
When defining a new controlled vocabulary, please check whether a similar vocabulary already exists.
All existing controlled vocabularies and their terms are listed in the Vocabulary Browser under the Utilities main menu of the ELN-LIMS UI.
Your vocabulary should contain at least three different terms. If your controlled vocabulary consists only of two terms, consider using Boolean values (TRUE/FALSE) instead.
It is not possible to choose several terms from the same vocabulary in one metadata
## Property
.
¶
## Vocabulary Code:
Only capital letters allowed.
Can only contain A-Z, 0-9 and _, -,.
Words should be separated by an underscore (""_"").
Should be meaningful.
Should be in English.
Should be between 3-20 characters long.
¶
## Vocabulary Description:
Not mandatory in openBIS but mandatory for the Data Store.
Should contain enough details to be understandable to people outside of the group.
Should be up to 250 characters long.
Should be in English followed by a German translation in the following format: ""English description//Deutsche Beschreibung""
¶
## Vocabulary URL Template:
Some controlled vocabularies are documented in the web and have unique and persistent identifiers for each term (e.g., a persistent URL or a DOI). In openBIS, it is possible to define a URL template for this type of vocabulary, which represents the vocabulary-specific part of the URL followed by
${term}
. The term-specific part of the URL must be identical to the
term code
defined in openBIS.
If vocabulary terms with a URL/DOI are used in the ELN-LIMS UI, these are displayed as hyperlinks.
## Example:
For a controlled vocabulary ""UNITS_OF_MEASURE"" which includes terms from the IUPAC Gold Book, the URL template is
https://doi.org/10.1351/goldbook.${term}
. The DOI for the term with the label ""degree Celsius"" is
https://doi.org/10.1351/goldbook.D01561
. Accordingly, the openBIS term code for ""degree Celsius"" must be ""D01561"".
¶
## Term Code:
Only capital letters allowed.
Can only contain A-Z, 0-9 and _, -,.
Words should be separated by an underscore (""_"").
Should be meaningful.
Should be in English.
Should be between 3-20 characters long.
When using a prefix at the beginning of the code, it recommended to use the same prefix for the codes of all terms of a vocabulary.
## Example:
In the vocabulary ""DFG_DEVICE_CODE"" vocabulary (for DFG Gerätegruppenschlüssel), the prefix could be ""DFG_"" for all terms, e.g., ""DFG_0000"", ""DFG_0005"", ""DFG_0010"" etc.
Should not contain only numbers, unless the number is part of an external URL/DOI for the term (see below).
## Example:
In the vocabulary “PURITY_GRADE”, for a term with the label “2.0” and the description “gas purity level 2.0 (99.0%)//Gasreinheit 2.0 (99,0 %), the code should not be “2.0” but something more meaningful like “GAS_PUR_2.0”.
If the vocabulary makes use of a
URL template
to link to definitions of the terms, the term code has to be identical with the term-specific part of the URL/DOI. In this case, it may be necessary to include codes that consist only of numbers.
¶
## Term Label:
Should be meaningful.
Should be in English.
Can be up to 128 characters long.
¶
## Term Description:
Not mandatory in openBIS but mandatory for the Data Store.
Should contain enough details to be understandable to people outside of the group.
Should be up to 250 characters long.
Should be in English followed by a German translation in the following format: ""English description//Deutsche Beschreibung""
¶
""Inheritance"" of Entity Types
When defining a new entity type that is similar to an existing one but requires further specification (i.e., additional
## Property
types), we make use of the principle of
inheritance
. This concept is borrowed from object-oriented programming:
""Most object-oriented programming languages have another feature that differentiates them from other data abstraction languages; class inheritance. Each class has a superclass from which it inherits operations and internal structure. A class can add to the operations it inherits or can redefine inherited operations. However, classes cannot delete inherited operations."" -
Designing reusable classes. RE Johnson, B Foote. Journal of object-oriented programming. 1988.
In case of the BAM Data Store, entity types (
## Collection
,
## Object
and
## Dataset
types) inherit attributes from an entity type of the same class (
## Collection
types inherit from
## Collection
types,
## Object
types inherit from
## Object
## types etc.):
The new, more specific entity type includes all the Sections,
## Property
types, and
Dynamic Property and Entity Validation Scripts
of the existing, more generic entity type.
The code of the new entity type contains the name of the original entity type as a prefix followed by a dot/period (.) followed by the specification term as a suffix: GENERIC.SPECIFIC
Likewise, the generated code prefix of the new entity type contains the code prefix of the original entity type followed by a dot/period (.) followed by the new code prefix: GEN.SPE
The attributes of the
## Property
type assignments (""Mandatory"", ""Editable"") of the existing entity type may be changed but only from FALSE to TRUE (and not the other way round from TRUE to FALSE).
The inheritance concept can also be applied across multiple levels of entity types (e.g., GENERIC.SPECIFIC_LEVEL_1.SPECIFIC_LEVEL_2). The same rules apply.
The principle of inheritance is not native to openBIS but has been developed for the Data Store which is why the above described rules have to be implemented
manually
when defining new entity types.
## Example:
The existing Object type INSTRUMENT (generated code prefix: INS) does not contain the
## Property
types needed to adequately describe the metadata of a camera. Therefore, a new
## Object
type with the name INSTRUMENT.CAMERA (generated code prefix: INS.CAM) is created.
The more specific
## Object
type INSTRUMENT.CAMERA inherits all original Sections (""General information"", ""BAM information"", ""Details"", ""Comments"") and
## Property
types (e.g., ""$NAME"", ""DEVICE_MODEL_NAME"", ""BAM_OE"" etc.) from the more generic
## Object
type INSTRUMENT. The attributes (""Mandatory"", ""Editable"") of the the
## Property
type assignments can be adjusted from FALSE to TRUE (e.g., DEVICE_MODEL_NAME was set as non-mandatory in INSTRUMENT but is defined as mandatory in INSRUMENT.CAMERA). Dynamic Property Scripts stay the same.
In addition, the
## Object
type INSTRUMENT.CAMERA contains two new Sections (""Camera Information"", ""Software information"") with additional
## Property
types (e.g., ""IMAGE_SENSOR_FRAMERATE, ""LENS_MOUNT_TYPE"", ""FIRMWARE_VERSION"") that are
not
included in the
## Object
type INSTRUMENT.
¶
Making Changes to existing Entity Types
It is possible to edit entity types after they have been initially created. These changes affect all entities of the type in question that have already been created in the Data Store. For this reason, changes to existing entity types in the Data Store always have to be discussed with the Data Store team and, if applicable, the person who originally created the entity type to ensure metadata consistency.
If you want to suggest changes to existing Masterdata please follow the same procedure described for
The Process of Masterdata Definition
.
¶
Editing an existing
## Property
## Type:
This should only be done in case of minor corrections (e.g., correction of typos, additions or clarifications) in the label or the description of the
## Property
type. The meaning of
## Property
types should never be changed retrospectively when
## Properties
of the type are already assigned to entity types that are in use in the Data Store instance.
The code of a
## Property
type cannot be changed after its creation nor can a
## Dynamic Property Script
be assigned retrospectively.
¶
Assignment of a new
## Property
Type to an existing Entity Type:
The assignment of a
non-mandatory
## Property
type
(either new or existing) is possible but has to be discussed with the Data Store team and, if applicable, the person who originally created the entity type.
The assignment of a
mandatory
## Property
type
(either new or existing) is only possible if a default value is defined for all entities to which the
## Property
type is assigned. The necessity of assigning a mandatory
## Property
type and the default value have to be discussed with the Data Store team and, if applicable, the person who originally created the entity type.
¶
Deletion of a
## Property
Type Assignment from an existing Entity Type:
This is possible only in exceptional cases because removing the assignment will also remove all existing
## Property
values of this type in the Data Store instance -
data will be lost
! The necessity of the deletion has to be discussed with the Data Store team and, if applicable, the person who originally created the entity type, as well as all users who already registered entities of the type in question.
¶
Deletion of an existing
## Collection
/
## Object
/
## Dataset
## Type:
This is possible only in exceptional cases because removing the entity type will also remove all existing entities of this type in the Data Store instance -
data will be lost
! The necessity of the deletion has to be discussed with the Data Store team and, if applicable, the person who originally created the entity type, as well as all users who already registered entities of the type in question.",Best Practices for Masterdata Definition,0,en_masterdata_definition_best_practices_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_masterdata_definition_best_practices.txt,2025-09-30T12:09:15.338881Z,1
docs:datastore:en_masterdata_definition_definition_of_masterdata:0,The Process of Masterdata Definition,https://datastore.bam.de/en/masterdata_definition/definition_of_masterdata,datastore,"masterdata_definition
/
definition_of_masterdata
# The Process of Masterdata Definition
## Page Contents
# The Process of Masterdata Definition
Masterdata Excel Template examples
Naming Masterdata Excel Files
Color-Coding of Masterdata Excel Files
Organising Masterdata Definition Files
Ariza de Schellenberger, Angela
¶
# The Process of Masterdata Definition
Only instance admins can register
## Masterdata
in openBIS. Since Data Store Stewards (DSSt) have group admin but not instance admin rights in the
main
Data Store instance, they cannot register domain/division-specific Masterdata on their own.
They can use one of the Masterdata Excel templates. To create an Object Type e.g.,
## Instrument
or a Controlled Vocabulary e.g.,
## DFG_DEVICE_CODE
.
Please ensure that you follow the
rules and best practices for Masterdata definition
and check the Excel files with the
Masterdata checker
before you make them available to the Data Store team via GitHub repository or via email at
datastore@bam.de
. Information about how to create a new repository in the Github can be found
here
.  You can also create your repository or fork it from
GitHub BAM research
.  To Share your GitHub repository with the Data Store team, go to your repository in GitHub, copy the URL from the browser address bar and sent it via email to
datastore@bam.de
.
The instance admins of the Data Store team check the Masterdata defined by the divisions and contact the DSSt in case of questions. As soon as the Masterdata for the division is finalised, the instance admins import it into the
main
Data Store instance.
If you want to propose changes to an existing Object Type or Controlled Vocabulary. You can use one of following options:
Create an ""Issue"" in the repository
bam-masterdata repository
. Current Object Types and Controlled Vocabularies can be seen within this folder under
data model
.
Download the Excel format of the
## Object Type
or
## Controlled Vocabulary
you want to modify from the current openBIS Masterdata. Modify it accoring to
Color-Coding of Masterdata Excel files
and
Naming Masterdata Excel Files
.
¶
Masterdata Excel Template examples
We provide examples of the Masterdata Excel file to create an Object Type i.e.,
## Instrument
and a controlled vocabulary i.e.,
## DFG_DEVICE_CODE
.
The openBIS Masterdata Excel template uses terms from earlier openBIS versions (which should not be changed):
""
## Object
type"" is called ""SAMPLE_TYPE""
""
## Collection
type"" is called ""EXPERIMENT_TYPE""
If you use a German Excel version,  note that the terms TRUE/FALSE (shown in the columns ""Mandatory"" and ""Show in edit views"") are automatically renamed to WAHR/FALSCH and must be changed to English.
¶
Naming Masterdata Excel Files
Please create a separate Excel file for each entity type/controlled vocabulary.
The name of each Masterdata Excel file should include:
the entity type:
object_type
or
vocabulary
the code of the entity type/vocabulary should be included while maintaining the exact format (upper-case letters, inlcuding all underscores and dots).
the number of the division
The different parts of the file name have to be separated by underscores.
¶
## Example:
For the
## Object
type INSTRUMENT, the Masterdata Excel file should be named
object_type_INSTRUMENT_FBX.Y.xlsx
.
¶
Color-Coding of Masterdata Excel Files
A new entity type typically contains a combination of pre-existing
## Property
types and newly defined
## Property
types.
In order to easily distinguish between them in the Masterdata Excel file, they should be color-coded to facilitate the Masterdata review process by the Data Store team. This is particularily important when defining an entity type that is a specification of an existing, more generic entity type, and thus ""inherits"" the set of
## Property
types from the generic one, as described
here
.
## Property
type assignments should be color-coded according to the following rules:
## New
## Property
types are marked in
orange
.
## Existing
## Property
types that are already assigned to the more generic entity type are marked in
blue
.
## Existing
## Property
types that are not assigned to the more generic entity type are marked in
green
.
¶
## Example:
A user wants to create a new
## Object
type INSTRUMENT.CAMERA that is a specification of the existing generic
## Object
type INSTRUMENT. INSTRUMENT.CAMERA inherits the complete set of
## Property
types from INSTRUMENT, color-coded in
blue
(e.g., MANUFACTURER). The user also defines several new
## Property
types to describe attributes that are specific to a camera; these are color-coded in
orange
(e.g., IMAGE_SENSOR_FRAMERATE). In addition, the user assigns an existing
## Property
type (e.g., FIRMWARE_VERSION) that has already been assigned to other
## Object
types but not to INSTRUMENT. This should be color-coded in
green
.
¶
Organising Masterdata Definition Files
Each entity type (except
## Property
types)/controlled vocabulary must be represented in a separate Excel file for easier organisation and findability.
Please make sure that the Excel file also includes all (newly created) dependencies such as new controlled vocabularies, new Dynamic Property and Entity Validation Scripts. Ensure that the individual Excel files and the vocabulary code listed in the Excel file of the object type are consistent.
If the Masterdata you generate uses existing vocabulary types (e.g., BAM_OE) or scripts (e.g. date_range_validation.py), they don't need to be sent.",The Process of Masterdata Definition,0,en_masterdata_definition_definition_of_masterdata_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_masterdata_definition_definition_of_masterdata.txt,2025-09-30T12:09:15.418527Z,1
docs:datastore:en_masterdata_definition_masterdata_checker:0,How to use Masterdata checker,https://datastore.bam.de/en/masterdata_definition/masterdata_checker,datastore,"masterdata_definition
/
masterdata_checker
# How to use Masterdata checker
## Demidova, Caroline
¶
1. Prepare Masterdata Excel file
Generate the Excel file (.xlsx) of the Object Type or Controlled Vocabulary according to the
definition of Masterdara
and
best practices
.
¶
2. Open the Masterdata checker and upload the Masterdata Excel file
The Masterdata checker can be accessed
here
. Click on the
Datei auswählen
and select the Masterdata Excel file, click on the
## Check Masterdata
tab to activate the file checker.  Info, warnings, and errors will be listed below under
## Checker Logs
¶
3. Interpretation of the info, warning, and error messages
ERRORs must be
corrected by DSSts before the Masterdata Excel file is shared with the Data Store team via GitHub.
## Info
and
warning
messages provide an additional information to users. and do not required any action of the DSSts.
## ERROR
messages indicate mistakes in the Masterdata Excel file, which will prevent functioning of the system,
mandatory
to fix before proceeding further. In the error drop-down there will be information displayed about the objective of the mistake, e.g.
## Code
is incorect,
## Property
is misslabeled etc. In some cases, the specific cell number will be indicated to where mistake is to be found.
Correct the error in the Masterdata Excel file and re-upload the file to check if the problem has been fixed.
## INFO
provides general information without requiring any action on the part of the user. It informs the user about the data model used to execute the check; the type of the file uploaded, etc. The last drop-down list shows the name of the files that has just been checked.
## WARNING
provides information about the possible issues or  changes in the system but no action is required. Information about the planned future changes in
Property types
can be found in the drop-down list of warnings.
In case you encounter any bugs or would like to give feedback on using the Masterdata Checker, please send a message to the developers at
GitHub
.",How to use Masterdata checker,0,en_masterdata_definition_masterdata_checker_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_masterdata_definition_masterdata_checker.txt,2025-09-30T12:09:15.480931Z,1
docs:datastore:en_Previous_version_of_Wiki_datastore_stewards_properties-handled-by-scripts:0,Properties handled by Scripts,https://datastore.bam.de/en/Previous_version_of_Wiki/datastore/stewards/properties-handled-by-scripts,datastore,"Previous_version_of_Wiki
/
datastore
/
stewards
/
properties-handled-by-scripts
# Properties handled by Scripts
Dynamic Property and Entity Validation Scripts
## Page Contents
Dynamic Property and Entity Validation Scripts
## Dynamic Property Scripts
## Entity Validation Scripts
## Demidova, Caroline
¶
Dynamic Property and Entity Validation Scripts
openBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:
## Dynamic Property Scripts
are a mechanism to automatically compute values of
## Properties
that should not/cannot be manually modified by users.
## Entity Validation Scripts
are a mechanism to ensure metadata consistency of an entity type (
## Collection
,
## Object
, or
## Dataset
type).
Both types of scripts are defined in the openBIS Admin User Interface (UI) under ""Tools"" -> ""Dynamic Property Plugin"" or ""Entity Validation Plugin"", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.
¶
## Dynamic Property Scripts
In most cases, values of
## Properties
are defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic
## Properties
that should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic
## Property
, usually based on the values of one or more
## Properties
of the same entity.
Dynamic Property Scripts are part of the
## Property
type assignments of entity types. This means that the script is not always used for a certain
## Property
type. Instead, it is one of the optional characteristics of a
## Property
type that is assigned to a specific entity type: The same
## Property
type can be a dynamic
## Property
(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal
## Property
that has to be filled by the user.
¶
Example for Dynamic Property Scripts
## The
## Object
type RECTANGLE includes the following three
## Property
## types:
## RECTANGLE_LENGTH_IN_M
## [REAL]
## RECTANGLE_WIDTH_IN_M
## [REAL]
## RECTANGLE_AREA_IN_QM
## [REAL]
Only the first two
## Properties
can be edited by users in the ELN-LIMS UI. Once the
## Object
is saved, the value of the
## Property
## RECTANGLE_AREA_IN_QM
is automatically computed as the product of the values of
## RECTANGLE_LENGTH_IN_M
and
## RECTANGLE_WIDTH_IN_M
as defined in the Dynamic Property Script ""rectangle_area"":
def
calculate
(
)
## :
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
length
=
entity
.
propertyValue
(
## ""RECTANGLE_LENGTH_IN_M""
)
width
=
entity
.
propertyValue
(
## ""RECTANGLE_WIDTH_IN_M""
)
area
=
length
*
width
return
area
## Copy
When assigning the
## Property
type
## RECTANGLE_AREA_IN_QM
to the
## Object
type RECTANGLE, the name of the script is entered in the field ""Dynamic Property Plugin"" in the Admin UI:
When using the Excel import option, the name of the script is entered as ""rectangle_area.py"" in the column ""Dynamic script"" in the Masterdata Excel file.
The script itself is defined (and can be tested) in the openBIS Admin UI under ""Tools"" -> ""Dynamic Property Plugin"" (see screenshot for Entity Validation Scripts
above
). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).
¶
Rules & Best Practices for Dynamic Property Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The computation of the value of the dynamic
## Property
must be based on the values of one or more
## Properties
of the same entity.
## Properties
of other entities must not be accessed.
All information needed for the calculation must be included in the script. External resources must not be accessed.
Dynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.
The script must be added when first creating the
## Property
type or, in the case of an existing
## Property
type, when assigning it to an entity type. It cannot be added retrospectively after the
## Property
type assignment already exists.
¶
## Entity Validation Scripts
Entity Validation Scripts are defined at the entity type (
## Collection
,
## Object
, or
## Dataset
type) level.
Once an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.
¶
Example for Entity Validation Scripts
An example of an Entity Validation Script is the
## Date Range Validation
(""EXPERIMENTAL_STEP.date_range_validation"") which checks for an
## Object
of the type EXPERIMENTAL_STEP whether the date entered for the
## Property
## END.DATE
is later than the date entered for the
## Property
## START.DATE
.
If not, the error message ""End date cannot be before start date!"" is returned and the EXPERIMENTAL_STEP cannot be saved.
#date_range_validation.py
def
getRenderedProperty
(
entity
,
property
)
## :
value
=
entity
.
property
(
property
)
if
value
is
not
## None
## :
return
value
.
renderedValue
(
)
def
validate
(
entity
,
isNew
)
## :
start_date
=
getRenderedProperty
(
entity
,
## ""START_DATE""
)
end_date
=
getRenderedProperty
(
entity
,
## ""END_DATE""
)
if
start_date
is
not
## None
and
end_date
is
not
## None
and
start_date
>
end_date
## :
return
""End date cannot be before start date!""
## Copy
¶
Rules & Best Practices for Entity Validation Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The validation must be based on the values of one or more
## Properties
of the entity being validated.
## Properties
of other entities must not be accessed for validation.
All information required for validation must be included in the script. No external resources may be accessed for the validation.
Entity Validation Scripts must be read-only. No
## Properties
of the entity may be added or edited.
Entity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.",Properties handled by Scripts,0,en_Previous_version_of_Wiki_datastore_stewards_properties-handled-by-scripts_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_Previous_version_of_Wiki_datastore_stewards_properties-handled-by-scripts.txt,2025-09-30T12:09:15.542569Z,1
docs:datastore:en_use_cases:0,List of use cases,https://datastore.bam.de/en/use_cases,datastore,"use_cases
# List of use cases
## Page Contents
openBIS Data Store Use Cases:
Sharing Use Cases in Wiki
Publication of BAM Use cases
## Demidova, Caroline
¶
openBIS Data Store Use Cases:
¶
Sharing Use Cases in Wiki
Data Store stewards are welcome to share an Use case of your division and make your work visible.
We provide a simplified example
EuVSOP
as inspiration for how you can map an use case in the Data Store.  In addition, the benefits of the Data Store implementation for the group, project, etc. can be described.
Simply send us your text in any format (.docx, .txt, etc.) together with screenshots (.jpg, .png) by email (
datastore@bam.de
).  We will implement your example in the wiki.
It is not about waiting for the perfect use case, but about exchanging ideas on how research MSE workflows can be mapped in openBIS Data Store. However, if you need your use case reviewed, please us at
datastore@bam.de
.
EuVSOP
¶
Publication of BAM Use cases
Alternatively, you can also publish an openBIS use case in Zenodo and make it citable:
QI-Digital/publication Zenodo",List of use cases,0,en_use_cases_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_use_cases.txt,2025-09-30T12:09:15.604312Z,1
docs:datastore:en_use_cases_EuVSOP:0,Use case: EuVSOP,https://datastore.bam.de/en/use_cases/EuVSOP,datastore,"use_cases
/
EuVSOP
# Use case: EuVSOP
## Page Contents
Use Case 1: Synthesis of europium‑doped VSOP, customized enhancer solution and improved microscopy fluorescence methodology for unambiguous histological detection
Conceptual Data Model
Map conceptual data model in the Data Store-openBIS data structure
## Demidova, Caroline
¶
Use Case 1: Synthesis of europium‑doped VSOP, customized enhancer solution and improved microscopy fluorescence methodology for unambiguous histological detection
The Eu-VSOP project investigates the unambiguous identification of iron oxide nanoparticles -VSOP doped with Europium (III) for flourescence detection in biological samples such as histological tissue sections.
## Background
: VSOP are very small iron oxide nanoparticles used in magnetic resonance imaging (MRI). These nanoparticles are studied as an alternative to Gadolium based MRI-contrast agents due to their potentially lower toxicity. The clear detection of EuVSOP in tissue sections enables biodistribution studies.
Note that the content of this Demo Project is inspired by some scientific open access publications.
[1]
Some modifications might be included for illustration of openBIS functions.
¶
Conceptual Data Model
## The
conceptual data model
describes the steps required to map research data in the Data Store and includes a guideline template to detail the following steps for a generic example.
¶
Draw the research data Workflow
¶
Identify Entities and Entity Types
¶
Map Entities in the openBIS data structure and Connect Entities
¶
Map conceptual data model in the Data Store-openBIS data structure
¶
Register data in the Lab Notebook:
Register a Project - EuVSOP
Register a Collection – Experimental Steps of the type Default Experimental Step
At the Collection level, register Objects of the Type -Experimental Step for EuVSOP- Synthesis, HEE Treatment.
At the Object level – EuVSOP HEE Treatment, register new Objects of the Type – Experimental Step for: Nanoparticle Iron quantification, Magnetic Characterization and Nanoparticle Size
Upload datasets to all Experimental Steps
Connect Experimental Steps: Edit EuVSOP Synthesis to define as Children the Experimental Step - EuVSOP HEE Treatment.
¶
Register items in the Inventory:
Use inventory spaces defined per default in division´s inventory space (e.g., X.1 Equipment and X.1 Materials).
Register a Project-folder - Instruments in the folder (Space) X.1 Equipment
## Register Instruments
Register a Project folder - Consumables in the folder (Space) X.1 Materials
Register a Collection – Chemicals
Register Objects Chemicals with Batch update registration
¶
Connect Inventory and ELN -Experimental Steps:
Edit Experimental Steps (e.g., EuVSOP Synthesis) to define Objects such as Chemicals and Instruments as Parents.
Visualize Parent-child connections in the hierarchical tree of each Object (e.g., Experimental Step: EuVSOP Nanoparticle Size).  To improve the visibility of the levels in the hierarchical tree, the displayed types for Chemicals and Instruments are excluded from the hierarchical tree in openBIS.
## doi:
10.1186/s12951-017-0301-6
.
↩︎",Use case: EuVSOP,0,en_use_cases_EuVSOP_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_use_cases_EuVSOP.txt,2025-09-30T12:09:15.673089Z,1
docs:datastore:en_use_cases_QI_digital_additive_manufacturing:0,QI-Digital: openBIS Data Model for an Additive Manufacturing,https://datastore.bam.de/en/use_cases/QI_digital_additive_manufacturing,datastore,"use_cases
/
QI_digital_additive_manufacturing
# QI-Digital: openBIS Data Model for an Additive Manufacturing
## Page Contents
## Header
## Demidova, Caroline
Last Thursday at 1:15 PM
¶
## Header
Your content here",QI-Digital: openBIS Data Model for an Additive Manufacturing,0,en_use_cases_QI_digital_additive_manufacturing_0,general,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\en_use_cases_QI_digital_additive_manufacturing.txt,2025-09-30T12:09:15.738170Z,1
docs:datastore:index:0,Welcome to BAM Data Store,https://datastore.bam.de/,datastore,"# Welcome to BAM Data Store
## Page Contents
Welcome to the Data Store Wiki
## Wiki Structure:
## 💡 Concepts
📖 How-to guides
## ❓ FAQ
👥 Use cases
What is the Data Store?
What is openBIS?
What is the Data Store Project?
What is the Data Store rollout process?
## Demidova, Caroline
¶
Welcome to the Data Store Wiki
This Wiki provides information on the BAM Data Store - the central system for research data management at the Bundesanstalt für Materialforschung und -prüfung (BAM).
The Wiki is not intended to replace the openBIS documentation by the ETHZ (
User docs
,
Admin docs
). It provides conscise guidance and should serve as an additional source of openBIS and Data Store documentation for BAM employees.
Some articles of this Wiki are currently under construction. If you have further questions that are not yet answered here, please contact
datastore@bam.de
.
¶
## Wiki Structure:
¶
## 💡 Concepts
Explanation about terms and concepts.
## Explore Concepts
¶
📖 How-to guides
Step-by-step instructions for openBIS functions.
Go to Guides
¶
## ❓ FAQ
Frequently asked questions about Data Store and openBIS.
View FAQ
¶
👥 Use cases
Discover Use cases of the Data Store.
Discover Use cases
¶
What is the Data Store?
The Data Store is the central system for research data management (RDM) at BAM.
The Data Store is the central system for research data management (RDM) at BAM.
It enables divisions to digitally organize and describe laboratory inventory -such as instruments, samples, standard operating procedures (SOPs), using customizable metadata and linked documentation..
Integrated with electronic lab notebook (ELN), the Data Store allows experimental steps to be connected with inventory items, ensuring centralized and traceable documentation of research processes.  This structure linkage supports the FAIR principles
[1]
(Findable, Accessible, Interoperable, Reusable), facilitating both internal and external reuse of research data in line with funding bodies.
By storing data and metadata in a unified system, the Data Store enhances interdisciplinary collaboration and lays the foundation for advanced data analysis, including big data and Artificial Intelligence (AI) applications.
¶
What is openBIS?
openBIS (open Biology Information System) is the underlying platform of the Data Store.
It is an open-source software solution for Research Data Management (RDM) and Electronic Lab Notebook (ELN).
Developed and maintained by the Scientific IT Services (SIS) at ETHZ Zurich (ETHZ), openBIS was originally designed for life sciences
[2]
[3]
, it is now increasingly used materials science and other research domains.
openBIS provides a browser-based graphical user interface (GUI) for the managing digital laboratory inventories and documenting experiments in a standardized way.  Data files can be imported into via the GUI or through programming interfaces and linked to inventory items and experimental steps.
openBIS also supports integration with external tools and services, such as exporting data to
## Zenodo
repository and analyzing scientific data in
## Jupyter Notebook
.
For more information on openBIS visit the official website (
https://openbis.ch/
).
¶
What is the Data Store Project?
The introduction of a RDM system does not happen overnight.
To evaluate openBIS suitability, a pilot phase was conducted from 01.12.2020 to 28.02.2022. During this period, five BAM research groups from diverse domains successfully implemented openBIS, confirming its effectiveness for managing data in various materials science domains.
Following the pilot’s success, the Data Store project was approved to establish the system based on openBIS as a central RDM system across all BAM divisions. The project, led by VP.1 eScience and VP.2 Information Technology, began in October 2022 and is scheduled to run for 3.5 years.
The initial phase focused on developing the necessary IT infrastructure and preparing for the software rollout, included an analysis of RDM needs across BAM. The rollout of the Data Store began in 2023.
For more information about the Data Store project visit the BAM infoportal (
About the Project
)
¶
What is the Data Store rollout process?
Throughout all phases, project management and communication play a crucial role in supporting a smooth and efficient implementation.
The actual Data Store rollout began in May 2023 and is being carried out in successive phases, with several divisions being trained at the same time. Lessons learned are collected at the end of each rollout phase to implement improvements in subsequent rollout phases. The order of the rollout is determined based on the interest expressed by division heads in surveys done in December 2022 and February 2025.
The current onboarding concept lasts 2 to 3 months for assigned Data Store Stewards (DSSt), including 1 day for division heads and all employees working with research data.
The Data Store Stewards are one or two members of the division appointed by the division head. They are ideally permanently employed for sustainability and are familiar with inventory, experimental processes and the (digital) workflows of the division. DSSts are trained to use main functions and customize the group settings.
All other division members who handle research data receive introductory training in the use of the system. The head of division takes part in information events and coordinates the completion of the rollout phase together with the DSSts.
After the rollout, the heads of the division and the DSSts, together with the users, are responsible for storing newly generated research data and the continuous implementation of the Data Store within their division.
Mark D. Wilkinson et al. (2016). ""The FAIR Guiding Principles for scientific data management and stewardship"". Scientific Data. 3 (1): 160018. doi:
## 10.1038/SDATA.2016.18
.
↩︎
Angela Bauch et al. openBIS: a flexible framework for managing and analyzing complex data in biology research. 12, 468 (2011). doi:
10.1186/1471-2105-12-468
.
↩︎
Caterina Barillari et al. openBIS ELN-LIMS: an open-source database for academic laboratories. Bioinformatics. 32 (4), Feb 2016, 638–640. doi:[10.1093/bioinformatics/btv606].(
https://doi.org/10.1093/bioinformatics/btv606
).
↩︎",Welcome to BAM Data Store,0,index_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\index.txt,2025-09-30T12:09:15.802458Z,1
docs:datastore:masterdata_definition_files_20250730_main_instance_object_types.xlsx:0,Properties handled by Scripts,https://datastore.bam.de/masterdata_definition_files/20250730_main_instance_object_types.xlsx,datastore,"Previous_version_of_Wiki
/
datastore
/
stewards
/
properties-handled-by-scripts
# Properties handled by Scripts
Dynamic Property and Entity Validation Scripts
## Page Contents
Dynamic Property and Entity Validation Scripts
## Dynamic Property Scripts
## Entity Validation Scripts
## Demidova, Caroline
¶
Dynamic Property and Entity Validation Scripts
openBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:
## Dynamic Property Scripts
are a mechanism to automatically compute values of
## Properties
that should not/cannot be manually modified by users.
## Entity Validation Scripts
are a mechanism to ensure metadata consistency of an entity type (
## Collection
,
## Object
, or
## Dataset
type).
Both types of scripts are defined in the openBIS Admin User Interface (UI) under ""Tools"" -> ""Dynamic Property Plugin"" or ""Entity Validation Plugin"", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.
¶
## Dynamic Property Scripts
In most cases, values of
## Properties
are defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic
## Properties
that should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic
## Property
, usually based on the values of one or more
## Properties
of the same entity.
Dynamic Property Scripts are part of the
## Property
type assignments of entity types. This means that the script is not always used for a certain
## Property
type. Instead, it is one of the optional characteristics of a
## Property
type that is assigned to a specific entity type: The same
## Property
type can be a dynamic
## Property
(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal
## Property
that has to be filled by the user.
¶
Example for Dynamic Property Scripts
## The
## Object
type RECTANGLE includes the following three
## Property
## types:
## RECTANGLE_LENGTH_IN_M
## [REAL]
## RECTANGLE_WIDTH_IN_M
## [REAL]
## RECTANGLE_AREA_IN_QM
## [REAL]
Only the first two
## Properties
can be edited by users in the ELN-LIMS UI. Once the
## Object
is saved, the value of the
## Property
## RECTANGLE_AREA_IN_QM
is automatically computed as the product of the values of
## RECTANGLE_LENGTH_IN_M
and
## RECTANGLE_WIDTH_IN_M
as defined in the Dynamic Property Script ""rectangle_area"":
def
calculate
(
)
## :
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
length
=
entity
.
propertyValue
(
## ""RECTANGLE_LENGTH_IN_M""
)
width
=
entity
.
propertyValue
(
## ""RECTANGLE_WIDTH_IN_M""
)
area
=
length
*
width
return
area
## Copy
When assigning the
## Property
type
## RECTANGLE_AREA_IN_QM
to the
## Object
type RECTANGLE, the name of the script is entered in the field ""Dynamic Property Plugin"" in the Admin UI:
When using the Excel import option, the name of the script is entered as ""rectangle_area.py"" in the column ""Dynamic script"" in the Masterdata Excel file.
The script itself is defined (and can be tested) in the openBIS Admin UI under ""Tools"" -> ""Dynamic Property Plugin"" (see screenshot for Entity Validation Scripts
above
). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).
¶
Rules & Best Practices for Dynamic Property Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The computation of the value of the dynamic
## Property
must be based on the values of one or more
## Properties
of the same entity.
## Properties
of other entities must not be accessed.
All information needed for the calculation must be included in the script. External resources must not be accessed.
Dynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.
The script must be added when first creating the
## Property
type or, in the case of an existing
## Property
type, when assigning it to an entity type. It cannot be added retrospectively after the
## Property
type assignment already exists.
¶
## Entity Validation Scripts
Entity Validation Scripts are defined at the entity type (
## Collection
,
## Object
, or
## Dataset
type) level.
Once an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.
¶
Example for Entity Validation Scripts
An example of an Entity Validation Script is the
## Date Range Validation
(""EXPERIMENTAL_STEP.date_range_validation"") which checks for an
## Object
of the type EXPERIMENTAL_STEP whether the date entered for the
## Property
## END.DATE
is later than the date entered for the
## Property
## START.DATE
.
If not, the error message ""End date cannot be before start date!"" is returned and the EXPERIMENTAL_STEP cannot be saved.
#date_range_validation.py
def
getRenderedProperty
(
entity
,
property
)
## :
value
=
entity
.
property
(
property
)
if
value
is
not
## None
## :
return
value
.
renderedValue
(
)
def
validate
(
entity
,
isNew
)
## :
start_date
=
getRenderedProperty
(
entity
,
## ""START_DATE""
)
end_date
=
getRenderedProperty
(
entity
,
## ""END_DATE""
)
if
start_date
is
not
## None
and
end_date
is
not
## None
and
start_date
>
end_date
## :
return
""End date cannot be before start date!""
## Copy
¶
Rules & Best Practices for Entity Validation Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The validation must be based on the values of one or more
## Properties
of the entity being validated.
## Properties
of other entities must not be accessed for validation.
All information required for validation must be included in the script. No external resources may be accessed for the validation.
Entity Validation Scripts must be read-only. No
## Properties
of the entity may be added or edited.
Entity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.",Properties handled by Scripts,0,masterdata_definition_files_20250730_main_instance_object_types.xlsx_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\masterdata_definition_files_20250730_main_instance_object_types.xlsx.txt,2025-09-30T12:09:15.867051Z,1
docs:datastore:masterdata_definition_files_20250730_main_instance_vocabulary_types.xlsx:0,Properties handled by Scripts,https://datastore.bam.de/masterdata_definition_files/20250730_main_instance_vocabulary_types.xlsx,datastore,"Previous_version_of_Wiki
/
datastore
/
stewards
/
properties-handled-by-scripts
# Properties handled by Scripts
Dynamic Property and Entity Validation Scripts
## Page Contents
Dynamic Property and Entity Validation Scripts
## Dynamic Property Scripts
## Entity Validation Scripts
## Demidova, Caroline
¶
Dynamic Property and Entity Validation Scripts
openBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:
## Dynamic Property Scripts
are a mechanism to automatically compute values of
## Properties
that should not/cannot be manually modified by users.
## Entity Validation Scripts
are a mechanism to ensure metadata consistency of an entity type (
## Collection
,
## Object
, or
## Dataset
type).
Both types of scripts are defined in the openBIS Admin User Interface (UI) under ""Tools"" -> ""Dynamic Property Plugin"" or ""Entity Validation Plugin"", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.
¶
## Dynamic Property Scripts
In most cases, values of
## Properties
are defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic
## Properties
that should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic
## Property
, usually based on the values of one or more
## Properties
of the same entity.
Dynamic Property Scripts are part of the
## Property
type assignments of entity types. This means that the script is not always used for a certain
## Property
type. Instead, it is one of the optional characteristics of a
## Property
type that is assigned to a specific entity type: The same
## Property
type can be a dynamic
## Property
(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal
## Property
that has to be filled by the user.
¶
Example for Dynamic Property Scripts
## The
## Object
type RECTANGLE includes the following three
## Property
## types:
## RECTANGLE_LENGTH_IN_M
## [REAL]
## RECTANGLE_WIDTH_IN_M
## [REAL]
## RECTANGLE_AREA_IN_QM
## [REAL]
Only the first two
## Properties
can be edited by users in the ELN-LIMS UI. Once the
## Object
is saved, the value of the
## Property
## RECTANGLE_AREA_IN_QM
is automatically computed as the product of the values of
## RECTANGLE_LENGTH_IN_M
and
## RECTANGLE_WIDTH_IN_M
as defined in the Dynamic Property Script ""rectangle_area"":
def
calculate
(
)
## :
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
length
=
entity
.
propertyValue
(
## ""RECTANGLE_LENGTH_IN_M""
)
width
=
entity
.
propertyValue
(
## ""RECTANGLE_WIDTH_IN_M""
)
area
=
length
*
width
return
area
## Copy
When assigning the
## Property
type
## RECTANGLE_AREA_IN_QM
to the
## Object
type RECTANGLE, the name of the script is entered in the field ""Dynamic Property Plugin"" in the Admin UI:
When using the Excel import option, the name of the script is entered as ""rectangle_area.py"" in the column ""Dynamic script"" in the Masterdata Excel file.
The script itself is defined (and can be tested) in the openBIS Admin UI under ""Tools"" -> ""Dynamic Property Plugin"" (see screenshot for Entity Validation Scripts
above
). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).
¶
Rules & Best Practices for Dynamic Property Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The computation of the value of the dynamic
## Property
must be based on the values of one or more
## Properties
of the same entity.
## Properties
of other entities must not be accessed.
All information needed for the calculation must be included in the script. External resources must not be accessed.
Dynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.
The script must be added when first creating the
## Property
type or, in the case of an existing
## Property
type, when assigning it to an entity type. It cannot be added retrospectively after the
## Property
type assignment already exists.
¶
## Entity Validation Scripts
Entity Validation Scripts are defined at the entity type (
## Collection
,
## Object
, or
## Dataset
type) level.
Once an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.
¶
Example for Entity Validation Scripts
An example of an Entity Validation Script is the
## Date Range Validation
(""EXPERIMENTAL_STEP.date_range_validation"") which checks for an
## Object
of the type EXPERIMENTAL_STEP whether the date entered for the
## Property
## END.DATE
is later than the date entered for the
## Property
## START.DATE
.
If not, the error message ""End date cannot be before start date!"" is returned and the EXPERIMENTAL_STEP cannot be saved.
#date_range_validation.py
def
getRenderedProperty
(
entity
,
property
)
## :
value
=
entity
.
property
(
property
)
if
value
is
not
## None
## :
return
value
.
renderedValue
(
)
def
validate
(
entity
,
isNew
)
## :
start_date
=
getRenderedProperty
(
entity
,
## ""START_DATE""
)
end_date
=
getRenderedProperty
(
entity
,
## ""END_DATE""
)
if
start_date
is
not
## None
and
end_date
is
not
## None
and
start_date
>
end_date
## :
return
""End date cannot be before start date!""
## Copy
¶
Rules & Best Practices for Entity Validation Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The validation must be based on the values of one or more
## Properties
of the entity being validated.
## Properties
of other entities must not be accessed for validation.
All information required for validation must be included in the script. No external resources may be accessed for the validation.
Entity Validation Scripts must be read-only. No
## Properties
of the entity may be added or edited.
Entity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.",Properties handled by Scripts,0,masterdata_definition_files_20250730_main_instance_vocabulary_types.xlsx_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\masterdata_definition_files_20250730_main_instance_vocabulary_types.xlsx.txt,2025-09-30T12:09:15.933067Z,1
docs:datastore:masterdata_definition_files_20250730_masterdata_of_controlled_vocabulary_dfg_device_code.xlsx:0,Properties handled by Scripts,https://datastore.bam.de/masterdata_definition_files/20250730_masterdata_of_controlled_vocabulary_dfg_device_code.xlsx,datastore,"Previous_version_of_Wiki
/
datastore
/
stewards
/
properties-handled-by-scripts
# Properties handled by Scripts
Dynamic Property and Entity Validation Scripts
## Page Contents
Dynamic Property and Entity Validation Scripts
## Dynamic Property Scripts
## Entity Validation Scripts
## Demidova, Caroline
¶
Dynamic Property and Entity Validation Scripts
openBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:
## Dynamic Property Scripts
are a mechanism to automatically compute values of
## Properties
that should not/cannot be manually modified by users.
## Entity Validation Scripts
are a mechanism to ensure metadata consistency of an entity type (
## Collection
,
## Object
, or
## Dataset
type).
Both types of scripts are defined in the openBIS Admin User Interface (UI) under ""Tools"" -> ""Dynamic Property Plugin"" or ""Entity Validation Plugin"", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.
¶
## Dynamic Property Scripts
In most cases, values of
## Properties
are defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic
## Properties
that should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic
## Property
, usually based on the values of one or more
## Properties
of the same entity.
Dynamic Property Scripts are part of the
## Property
type assignments of entity types. This means that the script is not always used for a certain
## Property
type. Instead, it is one of the optional characteristics of a
## Property
type that is assigned to a specific entity type: The same
## Property
type can be a dynamic
## Property
(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal
## Property
that has to be filled by the user.
¶
Example for Dynamic Property Scripts
## The
## Object
type RECTANGLE includes the following three
## Property
## types:
## RECTANGLE_LENGTH_IN_M
## [REAL]
## RECTANGLE_WIDTH_IN_M
## [REAL]
## RECTANGLE_AREA_IN_QM
## [REAL]
Only the first two
## Properties
can be edited by users in the ELN-LIMS UI. Once the
## Object
is saved, the value of the
## Property
## RECTANGLE_AREA_IN_QM
is automatically computed as the product of the values of
## RECTANGLE_LENGTH_IN_M
and
## RECTANGLE_WIDTH_IN_M
as defined in the Dynamic Property Script ""rectangle_area"":
def
calculate
(
)
## :
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
length
=
entity
.
propertyValue
(
## ""RECTANGLE_LENGTH_IN_M""
)
width
=
entity
.
propertyValue
(
## ""RECTANGLE_WIDTH_IN_M""
)
area
=
length
*
width
return
area
## Copy
When assigning the
## Property
type
## RECTANGLE_AREA_IN_QM
to the
## Object
type RECTANGLE, the name of the script is entered in the field ""Dynamic Property Plugin"" in the Admin UI:
When using the Excel import option, the name of the script is entered as ""rectangle_area.py"" in the column ""Dynamic script"" in the Masterdata Excel file.
The script itself is defined (and can be tested) in the openBIS Admin UI under ""Tools"" -> ""Dynamic Property Plugin"" (see screenshot for Entity Validation Scripts
above
). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).
¶
Rules & Best Practices for Dynamic Property Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The computation of the value of the dynamic
## Property
must be based on the values of one or more
## Properties
of the same entity.
## Properties
of other entities must not be accessed.
All information needed for the calculation must be included in the script. External resources must not be accessed.
Dynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.
The script must be added when first creating the
## Property
type or, in the case of an existing
## Property
type, when assigning it to an entity type. It cannot be added retrospectively after the
## Property
type assignment already exists.
¶
## Entity Validation Scripts
Entity Validation Scripts are defined at the entity type (
## Collection
,
## Object
, or
## Dataset
type) level.
Once an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.
¶
Example for Entity Validation Scripts
An example of an Entity Validation Script is the
## Date Range Validation
(""EXPERIMENTAL_STEP.date_range_validation"") which checks for an
## Object
of the type EXPERIMENTAL_STEP whether the date entered for the
## Property
## END.DATE
is later than the date entered for the
## Property
## START.DATE
.
If not, the error message ""End date cannot be before start date!"" is returned and the EXPERIMENTAL_STEP cannot be saved.
#date_range_validation.py
def
getRenderedProperty
(
entity
,
property
)
## :
value
=
entity
.
property
(
property
)
if
value
is
not
## None
## :
return
value
.
renderedValue
(
)
def
validate
(
entity
,
isNew
)
## :
start_date
=
getRenderedProperty
(
entity
,
## ""START_DATE""
)
end_date
=
getRenderedProperty
(
entity
,
## ""END_DATE""
)
if
start_date
is
not
## None
and
end_date
is
not
## None
and
start_date
>
end_date
## :
return
""End date cannot be before start date!""
## Copy
¶
Rules & Best Practices for Entity Validation Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The validation must be based on the values of one or more
## Properties
of the entity being validated.
## Properties
of other entities must not be accessed for validation.
All information required for validation must be included in the script. No external resources may be accessed for the validation.
Entity Validation Scripts must be read-only. No
## Properties
of the entity may be added or edited.
Entity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.",Properties handled by Scripts,0,masterdata_definition_files_20250730_masterdata_of_controlled_vocabulary_dfg_device_code.xlsx_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\masterdata_definition_files_20250730_masterdata_of_controlled_vocabulary_dfg_device_code.xlsx.txt,2025-09-30T12:09:15.995715Z,1
docs:datastore:masterdata_definition_files_20250730_masterdata_of_object_type_instrument.xlsx:0,Properties handled by Scripts,https://datastore.bam.de/masterdata_definition_files/20250730_masterdata_of_object_type_instrument.xlsx,datastore,"Previous_version_of_Wiki
/
datastore
/
stewards
/
properties-handled-by-scripts
# Properties handled by Scripts
Dynamic Property and Entity Validation Scripts
## Page Contents
Dynamic Property and Entity Validation Scripts
## Dynamic Property Scripts
## Entity Validation Scripts
## Demidova, Caroline
¶
Dynamic Property and Entity Validation Scripts
openBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:
## Dynamic Property Scripts
are a mechanism to automatically compute values of
## Properties
that should not/cannot be manually modified by users.
## Entity Validation Scripts
are a mechanism to ensure metadata consistency of an entity type (
## Collection
,
## Object
, or
## Dataset
type).
Both types of scripts are defined in the openBIS Admin User Interface (UI) under ""Tools"" -> ""Dynamic Property Plugin"" or ""Entity Validation Plugin"", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.
¶
## Dynamic Property Scripts
In most cases, values of
## Properties
are defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic
## Properties
that should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic
## Property
, usually based on the values of one or more
## Properties
of the same entity.
Dynamic Property Scripts are part of the
## Property
type assignments of entity types. This means that the script is not always used for a certain
## Property
type. Instead, it is one of the optional characteristics of a
## Property
type that is assigned to a specific entity type: The same
## Property
type can be a dynamic
## Property
(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal
## Property
that has to be filled by the user.
¶
Example for Dynamic Property Scripts
## The
## Object
type RECTANGLE includes the following three
## Property
## types:
## RECTANGLE_LENGTH_IN_M
## [REAL]
## RECTANGLE_WIDTH_IN_M
## [REAL]
## RECTANGLE_AREA_IN_QM
## [REAL]
Only the first two
## Properties
can be edited by users in the ELN-LIMS UI. Once the
## Object
is saved, the value of the
## Property
## RECTANGLE_AREA_IN_QM
is automatically computed as the product of the values of
## RECTANGLE_LENGTH_IN_M
and
## RECTANGLE_WIDTH_IN_M
as defined in the Dynamic Property Script ""rectangle_area"":
def
calculate
(
)
## :
""""""Main script function. The result will be used as the value of appropriate dynamic property.""""""
length
=
entity
.
propertyValue
(
## ""RECTANGLE_LENGTH_IN_M""
)
width
=
entity
.
propertyValue
(
## ""RECTANGLE_WIDTH_IN_M""
)
area
=
length
*
width
return
area
## Copy
When assigning the
## Property
type
## RECTANGLE_AREA_IN_QM
to the
## Object
type RECTANGLE, the name of the script is entered in the field ""Dynamic Property Plugin"" in the Admin UI:
When using the Excel import option, the name of the script is entered as ""rectangle_area.py"" in the column ""Dynamic script"" in the Masterdata Excel file.
The script itself is defined (and can be tested) in the openBIS Admin UI under ""Tools"" -> ""Dynamic Property Plugin"" (see screenshot for Entity Validation Scripts
above
). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).
¶
Rules & Best Practices for Dynamic Property Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The computation of the value of the dynamic
## Property
must be based on the values of one or more
## Properties
of the same entity.
## Properties
of other entities must not be accessed.
All information needed for the calculation must be included in the script. External resources must not be accessed.
Dynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.
The script must be added when first creating the
## Property
type or, in the case of an existing
## Property
type, when assigning it to an entity type. It cannot be added retrospectively after the
## Property
type assignment already exists.
¶
## Entity Validation Scripts
Entity Validation Scripts are defined at the entity type (
## Collection
,
## Object
, or
## Dataset
type) level.
Once an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.
¶
Example for Entity Validation Scripts
An example of an Entity Validation Script is the
## Date Range Validation
(""EXPERIMENTAL_STEP.date_range_validation"") which checks for an
## Object
of the type EXPERIMENTAL_STEP whether the date entered for the
## Property
## END.DATE
is later than the date entered for the
## Property
## START.DATE
.
If not, the error message ""End date cannot be before start date!"" is returned and the EXPERIMENTAL_STEP cannot be saved.
#date_range_validation.py
def
getRenderedProperty
(
entity
,
property
)
## :
value
=
entity
.
property
(
property
)
if
value
is
not
## None
## :
return
value
.
renderedValue
(
)
def
validate
(
entity
,
isNew
)
## :
start_date
=
getRenderedProperty
(
entity
,
## ""START_DATE""
)
end_date
=
getRenderedProperty
(
entity
,
## ""END_DATE""
)
if
start_date
is
not
## None
and
end_date
is
not
## None
and
start_date
>
end_date
## :
return
""End date cannot be before start date!""
## Copy
¶
Rules & Best Practices for Entity Validation Scripts
The script must be written in Jython/Python 2.7. No additional modules may be used, only the
## Python Standard Library
.
The validation must be based on the values of one or more
## Properties
of the entity being validated.
## Properties
of other entities must not be accessed for validation.
All information required for validation must be included in the script. No external resources may be accessed for the validation.
Entity Validation Scripts must be read-only. No
## Properties
of the entity may be added or edited.
Entity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.",Properties handled by Scripts,0,masterdata_definition_files_20250730_masterdata_of_object_type_instrument.xlsx_0,code,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\masterdata_definition_files_20250730_masterdata_of_object_type_instrument.xlsx.txt,2025-09-30T12:09:16.058910Z,1
docs:datastore:t:0,Tags,https://datastore.bam.de/t,datastore,"Select one or more tags
Search within results...
## Locale
localeAny
## Order By
## Title
Select one or more tags on the left.",Locale,0,t_0,procedure,C:\Users\cmadaria\Documents\Projects\DeSi\data\raw\wikijs\t.txt,2025-09-30T12:09:16.120898Z,1
