{"id": "docs:openbis:en_20.10.0-11_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/index.html", "repo": "openbis", "title": "OpenBIS Documentation", "section": "OpenBIS Documentation", "text": "## OpenBIS Documentation\n\nThe complete solution for managing your research data.\n## User Documentation\n\n## General Users\n## General Admin Users\n## Advance Features\n## Legacy Advance Features\n## Software Developer Documentation\n\n## Development Environment\n## APIS\n## Server-Side Extensions\n## Client-Side Extensions\n## Legacy Server-Side Extensions\n## System Documentation\n\n## Standalone\n## Docker\nAdvanced configuration\n## Change Log", "timestamp": "2025-09-18T09:38:29.654826Z", "source_priority": 2, "content_type": "reference"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/index.html", "repo": "openbis", "title": "APIS", "section": "APIS", "text": "## APIS\n\nJava / Javascript (V3 API) - openBIS V3 API\n## I. Architecture\nOne AS, one or more DSS\nThe Java API\nThe Javascript API\nII. API Features\nCurrent Features - AS\nCurrent Features - DSS\n## Missing/Planned Features\nIII. Accessing the API\nConnecting in Java\nConnecting in Javascript\nAMD / RequireJS\nAMD / RequireJS bundle\nVAR bundle\nESM bundle\nSynchronous Java vs Asynchronous Javascript\nIV. AS Methods\n## Login\n## Example\n## Personal Access Tokens\n## Session Information\n## Example\nCreating entities\n## Example\nProperties example\nDifferent ids example\nParent child example\nUpdating entities\n## Example\nProperties example\nParents example\nGetting authorization rights for entities\nFreezing entities\n## Space\n## Project\n## Experiment\n## Sample\nData Set\nSearching entities\n## Example\nExample with pagination and sorting\nExample with OR operator\nExample with nested logical operators\nExample with recursive fetch options\nGlobal search\nGetting entities\n## Example\nDeleting entities\n## Example\nSearching entity types\n## Modifications\nCustom AS Services\nSearch for custom services\nExecute a custom service\nArchiving / unarchiving data sets\nArchiving data sets\nUnarchiving data sets\n## Executing Operations\nMethod executeOperations\nMethod getOperationExecutions / searchOperationExecutions\nMethod updateOperationExecutions / deleteOperationExecutions\n### Configuration\n## Semantic Annotations\n## Web App Settings\n## Imports\nV. DSS Methods\nSearch files\n## Example\nDownloading files, folders, and datasets\n## Simple Downloading\nDownload a single file located inside a dataset\nDownload a folder located inside a dataset\nSearch for a dataset and download all its contents, file by file\nDownload a whole dataset recursively\nSearch and list all the files inside a data store\n## Fast Downloading\nWhat happens under the hood?\n## Customizing Fast Dowloading\nRegister Data Sets\nVI. Web application context\nPython (V3 API) - pyBIS!\nDependencies and Requirements\n### Installation\n### General Usage\nTAB completition and other hints in Jupyter / IPython\nChecking input\n## Glossary\nconnect to OpenBIS\nlogin\nVerify certificate\nCheck session token, logout()\nAuthentication without user/password\nPersonal access token (PAT)\n## Caching\nMount openBIS dataStore server\nPrerequisites: FUSE / SSHFS\nMount dataStore server with pyBIS\n## Masterdata\nbrowse masterdata\ncreate property types\ncreate sample types / object types\nassign and revoke properties to sample type / object type\ncreate a dataset type\ncreate an experiment type / collection type\ncreate material types\ncreate plugins\nUsers, Groups and RoleAssignments\n## Spaces\n## Projects\n## Experiments / Collections\ncreate a new experiment\nsearch for experiments\nExperiment attributes\nExperiment properties\n## Samples / Objects\ncreate/update/delete many samples in a transaction\nparents, children, components and container\nsample tags\nSample attributes and properties\nsearch for samples / objects\nfreezing samples\n## Datasets\nworking with existing dataSets\ndownload dataSets\nlink dataSets\ndataSet attributes and properties\nsearch for dataSets\nfreeze dataSets\ncreate a new dataSet\ncreate dataSet with zipfile\ncreate dataSet with mixed content\ncreate dataSet container\nget, set, add and remove parent datasets\nget, set, add and remove child datasets\ndataSet containers\n## Semantic Annotations\n## Tags\nVocabulary and VocabularyTerms\nChange ELN Settings via pyBIS\n## Main Menu\n## Storages\n## Templates\n## Custom Widgets\nThings object\nJSON response\nDataFrame\n## Objects\nBest practices\n## Logout\nIteration over tree structure\nIteration over raw data\nMatlab (V3 API) - How to access openBIS from MATLAB\n## Preamble\n## Setup\nmacOS\n## Windows 10\n### Usage\n## Notes\n## Personal Access Tokens\n## Background\nWhat are “Personal access tokens” ?\nWho can create a “Personal access token” ?\nWhere can I use “Personal access tokens” ?\nWhere “Personal access tokens” are stored ?\nHow long should my “Personal Access Tokens” be valid ?\n### Configuration\n## Typical Application Workflow\n## V3 API", "timestamp": "2025-09-18T09:38:29.658823Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_java-javascript-v3-api:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/java-javascript-v3-api.html", "repo": "openbis", "title": "Java / Javascript (V3 API) - openBIS V3 API", "section": "I. Architecture", "text": "Java / Javascript (V3 API) - openBIS V3 API\n\n## I. Architecture\n\nOpen BIS consists of two main components: an Application Server and one\nor more Data Store Servers. The Application Server manages the system’s\nmeta data, while the Data Store Server(s) manage the file store(s). Each\nData Store Server manages its own file store. Here we will refer to the\nApplication Server as the “AS” and the Data Store Server as the “DSS.”\nOne AS, one or more DSS\n\nWhy is there only one Application Server but multiple Data Store\nServers? It is possible to have only one Data Store Server, but in a\ncomplex project there might be many labs using the same OpenBIS instance\nand therefore sharing the same meta data. Each lab might have its own\nData Store Server to make file management easier and more efficient. The\nData Store Servers are on different Java virtual machines, which enables\nthe files to be processed faster. It is also more efficient when the\nphysical location of the Data Store Server is closer to the lab that is\nusing it. Another reason is that the meta data tends to be relatively\nsmall in size, whereas the files occupy a large amount of space in the\nsystem.\nThe Java API\n\nThe Java V3 API consists of two interfaces:\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerAPI\nch.ethz.sis.openbis.generic.dssapi.v3.IDatastoreServerAPI\nPlease check our JavaDoc for more\n## details:\nhttps://openbis.ch/javadoc/20.10.x/javadoc-api-v3/index.html\nAll V3 API jars are packed in openBIS-API-V3-\n.zip which\nis part of openBIS-clients-and-APIs-\n.zip (the latest version can be downloaded at\nhttps://unlimited.ethz.ch/display/openbis/Production+Releases\n)\nThe Javascript API\n\nThe Javascript V3 API consists of a module hosted at\n<OPENBIS_URL>/resources/api/v3/openbis.js, for instance\nhttp://localhost/openbis\n/ resources/api/v3/openbis.js. Please check\nthe openbis.js file itself for more details.\nII. API Features\n\nCurrent Features - AS\n\nThe current implementation of the V3 openBIS API contains the following\n## features:\nCreation:  Create spaces, projects, experiments and experiment\ntypes, samples and sample types, materials and material types,\nvocabulary terms, tags\nAssociations: Associate spaces, project, experiments, samples,\ndatasets, materials to each other\nTags: Add/Remove/Set tags for experiments, samples, datasets and\nmaterials\nProperties: Set properties for experiments, samples, datasets and\nmaterials\nSearch: Search & get spaces, project, experiments, samples,\ndatasets, materials, vocabulary terms, tags\nUpdate: Update spaces, project, experiments, samples, datasets,\nmaterials, vocabulary terms, tags\nDeletion: Delete spaces, project, experiments, samples, datasets,\nmaterials, vocabulary terms, tags\nAuthentication: Login as user, login as another user, login as an\nanonymous user\nTransactional features: performing multiple operations in one\ntransaction (with executeOperations method)\nQueries: create/update/get/search/delete/execute queries\nGenerating codes/permids\nCurrent Features - DSS\n\nSearch data set files\nDownload data set files\n## Missing/Planned Features\n\nThe current implementation of the V3 openBIS API does not yet include\nthe following features:\nManagement features: Managing data stores\nSearch features: Searching experiments having samples/datasets,\nsearching datasets (oldest, deleted, for archiving etc.)\nUpdate features: Updating datasets share id, size, status, storage\nconfirmation, post registration status\nIII. Accessing the API\n\nIn order to use V3 API you have to know the url of an openBIS instance\nyou want to connect to. Moreover, before calling any of the API methods\nyou have to login to the system to receive a sessionToken. All the login\nmethods are part of the AS API. Once you successfully authenticate in\nopenBIS you can invoke other methods of the API (at both AS and DSS). In\neach call you have to provide your sessionToken. When you have finished\nworking with the API you should call logout method to release all the\nresources related with your session.\nNote: If the openBIS instance you are connecting to uses SSL and does\nnot have a real certificate (it is using the self-signed certificate\nthat comes with openBIS), you need to tell the java client to use the\ntrust store that comes with openBIS. This can be done by setting the\nproperty\njavax.net\n## .ssl.trustStore. Example:\nUsing openBIS trust store in Java clients\njava\n-Djavax.net.ssl.trustStore\n=\n/home/openbis/openbis/servers/openBIS-server/jetty/etc/openBIS.keystore\n-jar\nthe-client.jar\nConnecting in Java\n\nIn order to connect to openBIS V3 API in Java you can:\nuse IApplicationServerApi (AS) and IDataStoreServerApi (DSS) interfaces directly\nuse OpenBIS facade (that talks to IApplicationServerApi and IDataStoreServerApi interfaces internally)\nUsing the OpenBIS facade has some advantages over using the AS and DSS interfaces directly:\nit hides the details of the protocol and the data serialization format used between the client and the server\nit does not require you to know V3 API endpoints for both AS and DSS and their URLs\nit provides additional utility methods (e.g. getManagedPersonalAccessToken)\nBecause of these reasons, OpenBIS facade is the recommended way of connecting to V3 API in Java.\nCode examples for both approaches are presented below.\nV3ConnectionExampleUsingASAndDSSInterfaces.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.Space\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.fetchoptions.SpaceFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3ConnectionExampleUsingASAndDSSInterfaces\n{\nprivate\nstatic\nfinal\n## String\n## URL\n=\n\"http://localhost:8888/openbis/openbis\"\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n;\nprivate\nstatic\nfinal\nint\n## TIMEOUT\n=\n10000\n;\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// get a reference to AS API\nIApplicationServerApi\nv3\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n## URL\n,\n## TIMEOUT\n);\n// login to obtain a session token\n## String\nsessionToken\n=\nv3\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n// invoke other API methods using the session token, for instance search for spaces\nSearchResult\n<\n## Space\n>\nspaces\n=\nv3\n.\nsearchSpaces\n(\nsessionToken\n,\nnew\nSpaceSearchCriteria\n(),\nnew\nSpaceFetchOptions\n());\n## System\n.\nout\n.\nprintln\n(\n\"Number of spaces: \"\n+\nspaces\n.\ngetObjects\n().\nsize\n());\n// logout to release the resources related with the session\nv3\n.\nlogout\n(\nsessionToken\n);\n}\n}\nV3ConnectionExampleUsingOpenBISFacade.java\nimport\nch.ethz.sis.openbis.generic.OpenBIS\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.Space\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.fetchoptions.SpaceFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria\n;\npublic\nclass\nV3ConnectionExampleUsingOpenBISFacade\n{\nprivate\nstatic\nfinal\n## String\n## URL\n=\n\"http://localhost:8888\"\n;\nprivate\nstatic\nfinal\nint\n## TIMEOUT\n=\n10000\n;\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// create OpenBIS facade (it aggregates methods from both IApplicationServerApi and IDataStoreServerApi)\nOpenBIS\nv3\n=\nnew\nOpenBIS\n(\n## URL\n,\n## TIMEOUT\n);\n// login to obtain a session token (the token is stored in the facade and used for subsequent calls)\nv3\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n// invoke other API methods, for instance search for spaces\nSearchResult\n<\n## Space\n>\nspaces\n=\nv3\n.\nsearchSpaces\n(\nnew\nSpaceSearchCriteria\n(),\nnew\nSpaceFetchOptions\n());\n## System\n.\nout\n.\nprintln\n(\n\"Number of spaces: \"\n+\nspaces\n.\ngetObjects\n().\nsize\n());\n// logout to release the resources related with the session\nv3\n.\nlogout\n();\n}\n}\nConnecting in Javascript\n\nWe have put a lot of effort to make the use of the API in Javascript and Java almost identical. The DTOs which are a big part of the API are exactly\nthe same in both languages. The methods you can invoke via the Javascript and Java APIs are also exactly the same. This makes the switch from\nJavascript to Java or the other way round very easy. Because of some major differences between Javascript and Java development still some things had\nto be done a bit differently. But even then we tried to be conceptually consistent.\nBefore we go into details let’s mention that there are actually 4 different ways the Javascript V3 API can be loaded and used. These are:\nAMD / RequireJS\nAMD / RequireJS bundle\nVAR bundle\nESM bundle\nIMPORTANT: VAR and ESM bundles are currently the recommended way of using the Javascript V3 API. AMD / RequireJS approach is still supported but no\nlonger recommended.\nAMD / RequireJS\n\nInitially, the only way to load and use the V3 API in Javascript was based on AMD modules and RequireJS (see code example below). In that approach,\nwhat we had to do first was to load RequireJS library itself and its config. Once that was done, we could start loading all the necessary V3 API\nclasses and the V3 API facade to make our V3 API calls.\nThis approach worked fine, but there were also some drawbacks:\neach V3 API class we wanted to use had to be explicitly “required” and its full class name had to be provided (e.g. as/dto/space/Space)\nclasses were loaded asynchronously making the code using the V3 API more complex\nevery V3 API class was loaded with a separate HTTP request to the server (loading multiple classes resulted in multiple requests to the server)\nit required a third party dependency manager (here RequireJS)\nBecause of these shortcomings this approach is no longer recommended, but still fully supported. Please use VAR or ESM bundles instead\n(depending on your use case).\nV3ConnectionExampleUsingRequireJS.html\n<!DOCTYPE html>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\ncharset\n=\n\"utf-8\"\n>\n<\ntitle\n>\nV3ConnectionExampleUsingRequireJS\n</\ntitle\n>\n<!--\nThese two js files, i.e. config.js and require.js are RequireJS configuration and RequireJS library itself.\nPlease check http://requirejs.org/ for more details on how RequireJS makes loading dependencies in Javascript easier.\n-->\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"http://localhost:8888/openbis/resources/api/v3/config.js\"\n></\nscript\n>\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"http://localhost:8888/openbis/resources/api/v3/require.js\"\n></\nscript\n>\n</\nhead\n>\n<\nbody\n>\n<\nscript\n>\n// With \"require\" call we asynchronously load \"openbis\", \"SpaceSearchCriteria\" and \"SpaceFetchOptions\" classes that we will need for our example.\n// The function that is passed as a second parameter of the require call is a callback that gets executed once requested classes are loaded.\n// In Javascript we work with exactly the same classes as in Java. For instance, \"ch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria\"\n// Java class and \"as/dto/space/search/SpaceSearchCriteria\" Javascript class have exactly the same methods. In order to find a Javascript class name please\n// check our Javadoc (https://openbis.ch/javadoc/20.10.x/javadoc-api-v3/index.html). The Javascript class name is defined in @JsonObject annotation of each V3 API Java DTO.\nrequire\n([\n\"openbis\"\n,\n\"as/dto/space/search/SpaceSearchCriteria\"\n,\n\"as/dto/space/fetchoptions/SpaceFetchOptions\"\n],\nfunction\n(\nopenbis\n,\nSpaceSearchCriteria\n,\nSpaceFetchOptions\n)\n{\n// get a reference to AS API\nvar\nv3\n=\nnew\nopenbis\n();\n// login to obtain a session token (the token it is automatically stored in openbis object and will be used for all subsequent API calls)\nv3\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n).\ndone\n(\nfunction\n()\n{\n// invoke other API methods, for instance search for spaces\nv3\n.\nsearchSpaces\n(\nnew\nSpaceSearchCriteria\n(),\nnew\nSpaceFetchOptions\n()).\ndone\n(\nfunction\n(\nresult\n)\n{\nalert\n(\n\"Number of spaces: \"\n+\nresult\n.\ngetObjects\n().\nlength\n);\n// logout to release the resources related with the session\nv3\n.\nlogout\n();\n});\n});\n});\n</\nscript\n>\n</\nbody\n>\n</\nhtml\n>\nAMD / RequireJS bundle\n\nTo improve the performance of AMD / RequireJS approach, we started to also provide a bundled version of the “config.js” called “config.bundle.js”\n(found in the same folder).\nUsing the bundled version of the config makes RequireJS issue only one request to the server to load all DTOs at once instead of separate requests for\neach DTO. This will significantly reduce the loading times of your webapp. What is also really nice is the fact it is so easy to start using this\nimprovement. Just load “config.bundle.js” instead of “config.js” and that’s it!\nEven though AMD / RequireJS solution is not recommended anymore, if you have a lot of existing code written with AMD / RequireJS approach then it\nmakes perfect sense to use this improvement before migrating to either VAR or ESM.\nVAR bundle\n\nVAR bundle (bundle assigned to window.openbis variable) allows you to overcome the shortcomings of AMD / RequireJS solution. First, the VAR bundle\nconsists of V3 API facade and all V3 API classes. Therefore, once the bundle is loaded, no further calls to the server are needed. Second, the bundle\nexposes the V3 API classes both via their simple names and their full names (see code example below) which makes it far easier for developers to use.\nThird, it does not require any additional library.\nVAR bundle can be loaded at an HTML page using a standard script tag.\nV3ConnectionExampleUsingVARBundle.html\n<!DOCTYPE html>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\ncharset\n=\n\"utf-8\"\n/>\n<\ntitle\n>\nV3ConnectionExampleUsingVARBundle\n</\ntitle\n>\n<!--\nImport VAR openBIS V3 API Javascript bundle as \"openbis\".\nThe bundle contains V3 API Javascript facade and all V3 API Javascript classes.\nThe facade can be accessed via:\n- \"openbis\" name (e.g. var v3 = new openbis.openbis())\nThe classes can be accessed via:\n- simple name (e.g. var space = new openbis.Space()) - works for classes with a unique simple name (see details below)\n- full name (e.g. var space = new opebis.as.dto.space.Space()) - works for all classes\nClasses with a unique simple name (e.g. Space) can be accessed using both their simple name (e.g. openbis.Space)\nand their full name (e.g. openbis.as.dto.space.Space).\nClasses with a non-unique simple name (e.g. ExternalDmsSearchCriteria) can be accessed only using their full name\n(i.e. as.dto.dataset.search.ExternalDmsSearchCriteria and as.dto.externaldms.search.ExternalDmsSearchCriteria).\nList of classes with duplicated simple names (i.e. accessible only via their full names):\n- as.dto.dataset.search.ExternalDmsSearchCriteria\n- as.dto.externaldms.search.ExternalDmsSearchCriteria\n- as.dto.pat.search.PersonalAccessTokenSessionNameSearchCriteria\n- as.dto.session.search.PersonalAccessTokenSessionNameSearchCriteria\n-->\n<!-- Import the bundle as \"openbis\" (the bundle content is assigned to window.openbis field).\nIn case window.openbis field is already used to store something different, then please\ncall openbis.noConflict() function right after the VAR bundle is loaded. It will bring back\nthe original value of window.openbis field and return the loaded VAR bundle for it to be\nassigned to a different field (works similar to jquery.noConflict() function).\n-->\n<\nscript\nsrc\n=\n\"http://localhost:8888/openbis/resources/api/v3/openbis.var.js\"\n></\nscript\n>\n</\nhead\n>\n<\nbody\n>\n<\nscript\n>\n// create an instance of the Javascript facade\nvar\nv3\n=\nnew\nopenbis\n.\nopenbis\n();\n// login to obtain a session token (the token it is automatically stored in openbis object and will be used for all subsequent API calls)\nv3\n.\nlogin\n(\n\"admin\"\n,\n\"admin\"\n).\ndone\n(\nfunction\n()\n{\n// invoke other API methods, for instance search for spaces\nv3\n.\nsearchSpaces\n(\nnew\nopenbis\n.\nSpaceSearchCriteria\n(),\nnew\nopenbis\n.\nSpaceFetchOptions\n()).\ndone\n(\nfunction\n(\nresult\n)\n{\nalert\n(\n\"Number of spaces: \"\n+\nresult\n.\ngetObjects\n().\nlength\n);\n// logout to release the resources related with the session\nv3\n.\nlogout\n();\n});\n});\n</\nscript\n>\n</\nbody\n>\n</\nhtml\n>\nESM bundle\n\nSimilar to VAR bundle, ESM bundle (ECMAScript module) is a bundle that contains the V3 API facade and all V3 API classes. It also exposes the V3 API\nclasses via both their simple names and their full names. The main difference between VAR and ESM is the format of the bundle and how and where it can\nbe imported.\nESM bundle can be loaded at an HTML page using a standard script tag with type=”module”. It is also well suited for webapps that bundle all their\nresources with tools like Webpack.\nV3ConnectionExampleUsingESMBundle.html\n<!DOCTYPE html>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\ncharset\n=\n\"utf-8\"\n/>\n<\ntitle\n>\nV3ConnectionExampleUsingESMBundle\n</\ntitle\n>\n</\nhead\n>\n<\nbody\n>\n<!--\nImport ESM (ECMAScript module) openBIS V3 API Javascript bundle as \"openbis\".\nThe bundle contains V3 API Javascript facade and all V3 API Javascript classes.\nThe facade can be accessed via:\n- \"openbis\" name (e.g. var v3 = new openbis.openbis())\nThe classes can be accessed via:\n- simple name (e.g. var space = new openbis.Space()) - works for classes with a unique simple name (see details below)\n- full name (e.g. var space = new opebis.as.dto.space.Space()) - works for all classes\nClasses with a unique simple name (e.g. Space) can be accessed using both their simple name (e.g. openbis.Space)\nand their full name (e.g. openbis.as.dto.space.Space).\nClasses with a non-unique simple name (e.g. ExternalDmsSearchCriteria) can be accessed only using their full name\n(i.e. as.dto.dataset.search.ExternalDmsSearchCriteria and as.dto.externaldms.search.ExternalDmsSearchCriteria).\nList of classes with duplicated simple names (i.e. accessible only via their full names):\n- as.dto.dataset.search.ExternalDmsSearchCriteria\n- as.dto.externaldms.search.ExternalDmsSearchCriteria\n- as.dto.pat.search.PersonalAccessTokenSessionNameSearchCriteria\n- as.dto.session.search.PersonalAccessTokenSessionNameSearchCriteria\n-->\n<\nscript\ntype\n=\n\"module\"\n>\n// import the bundle as \"openbis\" (the bundle can be imported with a different name)\nimport\nopenbis\nfrom\n\"http://localhost:8888/openbis/resources/api/v3/openbis.esm.js\"\n// create an instance of the Javascript facade\nvar\nv3\n=\nnew\nopenbis\n.\nopenbis\n();\n// login to obtain a session token (the token it is automatically stored in openbis object and will be used for all subsequent API calls)\nv3\n.\nlogin\n(\n\"admin\"\n,\n\"admin\"\n).\ndone\n(\nfunction\n()\n{\n// invoke other API methods, for instance search for spaces\nv3\n.\nsearchSpaces\n(\nnew\nopenbis\n.\nSpaceSearchCriteria\n(),\nnew\nopenbis\n.\nSpaceFetchOptions\n()).\ndone\n(\nfunction\n(\nresult\n)\n{\nalert\n(\n\"Number of spaces: \"\n+\nresult\n.\ngetObjects\n().\nlength\n);\n// logout to release the resources related with the session\nv3\n.\nlogout\n();\n});\n});\n</\nscript\n>\n</\nbody\n>\n</\nhtml\n>\nSynchronous Java vs Asynchronous Javascript\n\nEven though the V3 API code examples in both Java and Javascript look similar there is one major difference between them. All the methods in the Java\nAPI that connect to the openBIS server are synchronous, while all their Javascript counterparts are asynchronous. Let’s compare how that looks.\nV3JavaCallsAreSynchronous.java\npublic\nclass\nV3JavaCallsAreSynchronous\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created (please check \"Accessing the API\" section for more details)\n// this makes a synchronous (blocking) call to the server\nSearchResult\n<\n## Space\n>\nspaces\n=\nv3\n.\nsearchSpaces\n(\nnew\nSpaceSearchCriteria\n(),\nnew\nSpaceFetchOptions\n());\n// this loop will execute only after searchSpaces method call is finished and spaces have been fetched from the server\nfor\n(\n## Space\nspace\n## :\nspaces\n)\n{\n## System\n.\nout\n.\nprintln\n(\nspace\n.\ngetCode\n());\n}\n}\n}\nV3JavascriptCallsAreAsynchronous.java\n<!DOCTYPE html>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\ncharset\n=\n\"utf-8\"\n/>\n<\ntitle\n>\nV3JavascriptCallsAreAsynchronous\n</\ntitle\n>\n</\nhead\n>\n<\nbody\n>\n<\nscript\n>\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// this makes a non-blocking (asynchronous) call to the server\nv3\n.\nsearchSpaces\n(\nnew\nSpaceSearchCriteria\n(),\nnew\nSpaceFetchOptions\n()).\ndone\n(\n// we need to put the loop in \"done\" callback for it to be executed once spaces have been fetched from the server\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nspace\n){\nconsole\n.\nlog\n(\nspace\n.\ngetCode\n());\n});\n}\n);\n</\nscript\n>\n</\nbody\n>\n</\nhtml\n>\nWhat Javascript V3 API asynchronous functions actually return is a jQuery Promise object, which offers methods like: then, done, fail (\nsee: https://api.jquery.com/category/deferred-object/). These methods can be used for registering different callbacks that will be either executed\nwhen a call succeeds or fails (e.g. due to a network problem).\nA more modern and an easier to understand way of working with Promise objects is the async / await syntax (\nsee: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/async_function). Our asynchronous Javascript V3 API methods support\nit as well.\nIV. AS Methods\n\nThe sections below describe how to use different methods of the V3 API. Each section describes a group of similar methods. For instance, we have one\nsection that describes creation of entities. Even though the API provides us methods for creation of spaces, projects, experiments, samples and\nmaterials, vocabulary terms, tags we only concentrate here on creation of samples. Samples are the most complex entity kind. Once you understand how\ncreation of samples works you will also know how to create other kinds of entities as all creation methods follow the same patterns. The same applies\nfor other methods like updating of entities, searching or getting entities. We will introduce them using the sample example.\nEach section will be split into Java and Javascript subsections. We want to keep Java and Javascript code examples close to each other so that you can\neasily see what are the similarities and differences in the API usage between these two languages.\nNOTE: The following code examples assume that we have already got a reference to the V3 API and we have already authenticated to get a session token.\nMoreover in Javascript example we do not include the html page template to make them shorter and more readable. Please check “Accessing the API”\nsection for examples on how to get a reference to V3 API, authenticate or build a simple html page.\n## Login\n\nOpenBIS provides the following login methods:\nlogin(user, password) - login as a given user\nloginAs(user, password, asUser) - login on behalf of a different\nuser (e.g. I am an admin but I would like to see only things user\n“x” would normally see)\nloginAsAnonymousUser() - login as an anonymous user configured in AS\nservice.properties\nAll login methods return a session token if the provided parameters were\ncorrect. In case a given user does not exist or the provided password\nwas incorrect the login methods return null.\n## Example\n\nV3LoginExample.java\npublic\nclass\nV3LoginExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created (please check \"Accessing the API\" section for more details)\n// login as a specific user\n## String\nsessionToken\n=\nv3\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n## System\n.\nout\n.\nprintln\n(\nsessionToken\n);\n// login on behalf of a different user (I am an admin but I would like to see only things that some other user would normally see)\nsessionToken\n=\nv3\n.\nloginAs\n(\n\"admin\"\n,\n\"password\"\n,\n\"someotheruser\"\n);\n## System\n.\nout\n.\nprintln\n(\nsessionToken\n);\n// login as an anonymous user (anonymous user has to be configured in service.properties first)\nsessionToken\n=\nv3\n.\nloginAsAnonymousUser\n();\n## System\n.\nout\n.\nprintln\n(\nsessionToken\n);\n}\n}\nV3LoginExample.html\n<\nscript\n>\n// we assume here that v3 object has been already created (please check \"Accessing the API\" section for more details)\n// login as a specific user\nv3\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n).\ndone\n(\nfunction\n(\nsessionToken\n)\n{\nalert\n(\nsessionToken\n);\n// login on behalf of a different user (I am an admin but I would like to see only things that some other user would normally see)\nv3\n.\nloginAs\n(\n\"admin\"\n,\n\"password\"\n,\n\"someotheruser\"\n).\ndone\n(\nfunction\n(\nsessionToken\n)\n{\nalert\n(\nsessionToken\n);\n// login as an anonymous user (anonymous user has to be configured in service.properties first)\nv3\n.\nloginAsAnonymousUser\n().\ndone\n(\nfunction\n(\nsessionToken\n)\n{\nalert\n(\nsessionToken\n);\n});\n});\n});\n</\nscript\n>\n## Personal Access Tokens\n\nA personal access token (in short: PAT) can be thought of as a longer lived session token which can be used for integrating openBIS with external systems. If you would like to learn more about the idea behind PATs please read:\n## Personal Access Tokens\n).\nExample of how to create and use a PAT:\nimport\njava.util.Arrays\n;\nimport\njava.util.Date\n;\nimport\njava.util.List\n;\nimport\njava.util.Map\n;\nimport\norg.apache.commons.lang.time.DateUtils\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.PersonalAccessToken\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.create.PersonalAccessTokenCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.fetchoptions.PersonalAccessTokenFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.id.IPersonalAccessTokenId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.id.PersonalAccessTokenPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.fetchoptions.SpaceFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.search.SpaceSearchCriteria\n;\npublic\nclass\nV3PersonalAccessTokenExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nPersonalAccessTokenCreation\ncreation\n=\nnew\nPersonalAccessTokenCreation\n();\ncreation\n.\nsetSessionName\n(\n\"test session\"\n);\ncreation\n.\nsetValidFromDate\n(\nnew\n## Date\n(\n## System\n.\ncurrentTimeMillis\n()\n-\nDateUtils\n.\n## MILLIS_PER_DAY\n));\ncreation\n.\nsetValidToDate\n(\nnew\n## Date\n(\n## System\n.\ncurrentTimeMillis\n()\n+\nDateUtils\n.\n## MILLIS_PER_DAY\n));\n// create and get the new PAT\n## List\n<\nPersonalAccessTokenPermId\n>\nids\n=\nv3api\n.\ncreatePersonalAccessTokens\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\ncreation\n));\n## Map\n<\nIPersonalAccessTokenId\n,\nPersonalAccessToken\n>\nmap\n=\nv3api\n.\ngetPersonalAccessTokens\n(\nsessionToken\n,\nids\n,\nnew\nPersonalAccessTokenFetchOptions\n());\nPersonalAccessToken\npat\n=\nmap\n.\nget\n(\nids\n.\nget\n(\n0\n));\n// use the new PAT to list spaces\nv3api\n.\nsearchSpaces\n(\npat\n.\ngetHash\n(),\nnew\nSpaceSearchCriteria\n(),\nnew\nSpaceFetchOptions\n());\n}\n}\n## Session Information\n\nOpenBIS provides a method to obtain the session information for an\n## already log in user:\n## Example\n\nV3CreationExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.session.SessionInformation\n;\npublic\nclass\nV3SessionInformationExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSessionInformation\nsessionInformation\n=\nv3\n.\ngetSessionInformation\n(\nsessionToken\n);\n## System\n.\nout\n.\nprintln\n(\n## \"User Name: \"\n+\nsessionInformation\n.\ngetUserName\n());\n## System\n.\nout\n.\nprintln\n(\n## \"Home Group: \"\n+\nsessionInformation\n.\ngetHomeGroupCode\n());\n## System\n.\nout\n.\nprintln\n(\n## \"Person: \"\n+\nsessionInformation\n.\ngetPerson\n());\n## System\n.\nout\n.\nprintln\n(\n\"Creator Person: \"\n+\nsessionInformation\n.\ngetCreatorPerson\n());\n}\n}\nCreating entities\n\nThe methods for creating entities in V3 API are called: createSpaces,\ncreateProjects, createExperiments, createSamples, createMaterials,\ncreateVocabularyTerms, createTags. They all allow to create one or more\nentities at once by passing one or more entity creation objects (i.e.\nSpaceCreation, ProjectCreation, ExperimentCreation, SampleCreation,\nMaterialCreation, VocabularyTermCreation, TagCreation). All these\nmethods return as a result a list of the new created entity perm ids.\nNOTE: Creating data sets via V3 API is not available yet. The new V3\ndropboxes are planned but not implemented yet. Please use V2 dropboxes\nuntil V3 version is out.\n## Example\n\nV3CreationExample.java\nimport\njava.util.List\n;\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SamplePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3CreationExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\n// you can also pass more than one creation object to create multiple entities at once\n## List\n<\nSamplePermId\n>\npermIds\n=\nv3\n.\ncreateSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nsample\n));\n## System\n.\nout\n.\nprintln\n(\n\"Perm ids: \"\n+\npermIds\n);\n}\n}\nV3CreationExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n],\nfunction\n(\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\n// you can also pass more than one creation object to create multiple entities at once\nv3\n.\ncreateSamples\n([\nsample\n]).\ndone\n(\nfunction\n(\npermIds\n)\n{\nalert\n(\n\"Perm ids: \"\n+\n## JSON\n.\nstringify\n(\npermIds\n));\n});\n});\n</\nscript\n>\nProperties example\n\nV3CreationWithPropertiesExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3CreationWithPropertiesExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\n// examples of value formats that should be used for different types of properties\nsample\n.\nsetProperty\n(\n## \"MY_VARCHAR\"\n,\n\"this is a description\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_INTEGER\"\n,\n\"123\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_REAL\"\n,\n\"123.45\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_BOOLEAN\"\n,\n\"true\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_MATERIAL\"\n,\n## \"MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_VOCABULARY\"\n,\n## \"MY_TERM_CODE\"\n);\nv3\n.\ncreateSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nsample\n));\n}\n}\nV3CreationWithPropertiesExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n],\nfunction\n(\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\n// examples of value formats that should be used for different types of properties\nsample\n.\nsetProperty\n(\n## \"MY_VARCHAR\"\n,\n\"this is a description\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_INTEGER\"\n,\n\"123\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_REAL\"\n,\n\"123.45\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_BOOLEAN\"\n,\n\"true\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_MATERIAL\"\n,\n## \"MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_VOCABULARY\"\n,\n## \"MY_TERM_CODE\"\n);\nv3\n.\ncreateSamples\n([\nsample\n]).\ndone\n(\nfunction\n(\npermIds\n)\n{\nalert\n(\n\"Perm ids: \"\n+\n## JSON\n.\nstringify\n(\npermIds\n));\n});\n});\n});\n</\nscript\n>\nDifferent ids example\n\nV3CreationWithDifferentIdsExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3CreationWithDifferentIdsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\n// as an experiment id we can use any class that implements IExperimentId interface. For instance, experiment identifier:\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\n// or experiment perm id:\nsample\n.\nsetExperimentId\n(\nnew\nExperimentPermId\n(\n\"20160115170718361-98668\"\n));\nv3\n.\ncreateSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nsample\n));\n}\n}\nV3CreationWithDifferentIdsExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/experiment/id/ExperimentPermId\"\n],\nfunction\n(\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nExperimentPermId\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\n// as an experiment id we can use any class that implements IExperimentId interface. For instance, experiment identifier:\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\n// or experiment perm id:\nsample\n.\nsetExperimentId\n(\nnew\nExperimentPermId\n(\n\"20160115170718361-98668\"\n));\nv3\n.\ncreateSamples\n([\nsample\n]).\ndone\n(\nfunction\n(\npermIds\n)\n{\nalert\n(\n\"Perm ids: \"\n+\n## JSON\n.\nstringify\n(\npermIds\n));\n});\n});\n</\nscript\n>\nParent child example\n\nThe following example creates parent and child samples for a sample type\n## which allow automatic code generation:\nV3CreationParentAndChildExample\nimport\njava.util.Arrays\n;\nimport\njava.util.List\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.id.CreationId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SamplePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3CreationParentAndChildExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nparentSample\n=\nnew\nSampleCreation\n();\nparentSample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nparentSample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nparentSample\n.\nsetCreationId\n(\nnew\nCreationId\n(\n\"parent\"\n));\nSampleCreation\nchildSample\n=\nnew\nSampleCreation\n();\nchildSample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nchildSample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nchildSample\n.\nsetParentIds\n(\n## Arrays\n.\nasList\n(\nparentSample\n.\ngetCreationId\n()));\n## List\n<\nSamplePermId\n>\npermIds\n=\nv3\n.\ncreateSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nparentSample\n,\nchildSample\n));\n## System\n.\nout\n.\nprintln\n(\n\"Perm ids: \"\n+\npermIds\n);\n}\n}\nV3CreationParentAndChildExample.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/common/id/CreationId\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nCreationId\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nparentSample\n=\nnew\nSampleCreation\n();\nparentSample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nparentSample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nparentSample\n.\nsetCreationId\n(\nnew\nCreationId\n(\n\"parent\"\n));\nvar\nchildSample\n=\nnew\nSampleCreation\n();\nchildSample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nchildSample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nchildSample\n.\nsetParentIds\n([\nparentSample\n.\ngetCreationId\n()]);\nv3\n.\ncreateSamples\n([\nparentSample\n,\nchildSample\n]).\ndone\n(\nfunction\n(\npermIds\n)\n{\nalert\n(\n\"Perm ids: \"\n+\n## JSON\n.\nstringify\n(\npermIds\n));\n});\n});\n</\nscript\n>\nUpdating entities\n\nThe methods for updating entities in V3 API are called: updateSpaces,\nupdateProjects, updateExperiments, updateSamples, updateDataSets,\nupdateMaterials, updateVocabularyTerms, updateTags. They all allow to\nupdate one or more entities at once by passing one or more entity update\nobjects (i.e. SpaceUpdate, ProjectUpdate, ExperimentUpdate,\nSampleUpdate, MaterialUpdate, VocabularyTermUpdate, TagUpdate). With\nupdate objects you can update entities without fetching their state\nfirst, i.e. the update objects contain only changes - not the full state\nof entities. All update objects require an id of an entity that will be\nupdated. Please note that some of the entity fields cannot be changed\nonce an entity is created, for instance sample code becomes immutable\nafter creation.\n## Example\n\nV3UpdateExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.update.SampleUpdate\n;\npublic\nclass\nV3UpdateExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// here we update a sample and attach it to a different experiment\nSampleUpdate\nsample\n=\nnew\nSampleUpdate\n();\nsample\n.\nsetSampleId\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_OTHER_EXPERIMENT_CODE\"\n));\n// you can also pass more than one update object to update multiple entities at once\nv3\n.\nupdateSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nsample\n));\n## System\n.\nout\n.\nprintln\n(\n## \"Updated\"\n);\n}\n}\nV3UpdateExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/update/SampleUpdate\"\n,\n\"as/dto/sample/id/SampleIdentifier\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n],\nfunction\n(\nSampleUpdate\n,\nSampleIdentifier\n,\nExperimentIdentifier\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// here we update a sample and attach it to a different experiment\nvar\nsample\n=\nnew\nSampleUpdate\n();\nsample\n.\nsetSampleId\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_OTHER_EXPERIMENT_CODE\"\n));\n// you can also pass more than one update object to update multiple entities at once\nv3\n.\nupdateSamples\n([\nsample\n]).\ndone\n(\nfunction\n()\n{\nalert\n(\n## \"Updated\"\n);\n});\n});\n</\nscript\n>\nProperties example\n\nV3UpdateWithPropertiesExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.update.SampleUpdate\n;\npublic\nclass\nV3UpdateWithPropertiesExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleUpdate\nsample\n=\nnew\nSampleUpdate\n();\nsample\n.\nsetSampleId\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n));\n// examples of value formats that should be used for different types of properties\nsample\n.\nsetProperty\n(\n## \"MY_VARCHAR\"\n,\n\"this is a description\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_INTEGER\"\n,\n\"123\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_REAL\"\n,\n\"123.45\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_BOOLEAN\"\n,\n\"true\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_MATERIAL\"\n,\n## \"MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_VOCABULARY\"\n,\n## \"MY_TERM_CODE\"\n);\nv3\n.\nupdateSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nsample\n));\n## System\n.\nout\n.\nprintln\n(\n## \"Updated\"\n);\n}\n}\nV3UpdateWithPropertiesExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/update/SampleUpdate\"\n,\n\"as/dto/sample/id/SampleIdentifier\"\n],\nfunction\n(\nSampleUpdate\n,\nSampleIdentifier\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleUpdate\n();\nsample\n.\nsetSampleId\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n));\n// examples of value formats that should be used for different types of properties\nsample\n.\nsetProperty\n(\n## \"MY_VARCHAR\"\n,\n\"this is a description\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_INTEGER\"\n,\n\"123\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_REAL\"\n,\n\"123.45\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_BOOLEAN\"\n,\n\"true\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_MATERIAL\"\n,\n## \"MY_MATERIAL_CODE (MY_MATERIAL_TYPE_CODE)\"\n);\nsample\n.\nsetProperty\n(\n## \"MY_VOCABULARY\"\n,\n## \"MY_TERM_CODE\"\n);\nv3\n.\nupdateSamples\n([\nsample\n]).\ndone\n(\nfunction\n()\n{\nalert\n(\n## \"Updated\"\n);\n});\n});\n</\nscript\n>\nParents example\n\nV3UpdateWithParentsExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.update.SampleUpdate\n;\npublic\nclass\nV3UpdateWithParentsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// Let's assume the sample we are about to update has the following parents:\n## // - MY_PARENT_CODE_1\n## // - MY_PARENT_CODE_2\nSampleUpdate\nsample\n=\nnew\nSampleUpdate\n();\nsample\n.\nsetSampleId\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n));\n// We can add and remove parents from the existing list. For instance, here we are adding: MY_PARENT_CODE_3 and removing: MY_PARENT_CODE_1.\n// The list of parents after such change would be: [MY_PARENT_CODE_2, MY_PARENT_CODE_3]. Please note that we don't have to fetch the existing\n// list of parents, we are just defining what changes should be made to this list on the server side. Updating lists of children or contained\n// samples works exactly the same.\nsample\n.\ngetParentIds\n().\nadd\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_PARENT_CODE_3\"\n));\nsample\n.\ngetParentIds\n().\nremove\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_PARENT_CODE_1\"\n));\n// Instead of adding and removing parents we can also set the list of parents to a completely new value.\nsample\n.\ngetParentIds\n().\nset\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_PARENT_CODE_2\"\n),\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_PARENT_CODE_3\"\n));\nv3\n.\nupdateSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nsample\n));\n## System\n.\nout\n.\nprintln\n(\n## \"Updated\"\n);\n}\n}\nV3UpdateWithParentsExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/update/SampleUpdate\"\n,\n\"as/dto/sample/id/SampleIdentifier\"\n],\nfunction\n(\nSampleUpdate\n,\nSampleIdentifier\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// Let's assume the sample we are about to update has the following parents:\n## // - MY_PARENT_CODE_1\n## // - MY_PARENT_CODE_2\nvar\nsample\n=\nnew\nSampleUpdate\n();\nsample\n.\nsetSampleId\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n));\n// We can add and remove parents from the existing list. For instance, here we are adding: MY_PARENT_CODE_3 and removing: MY_PARENT_CODE_1.\n// The list of parents after such change would be: [MY_PARENT_CODE_2, MY_PARENT_CODE_3]. Please note that we don't have to fetch the existing\n// list of parents, we are just defining what changes should be made to this list on the server side. Updating lists of children or contained\n// samples works exactly the same.\nsample\n.\ngetParentIds\n().\nadd\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE/MY_PARENT_CODE_3\"\n));\nsample\n.\ngetParentIds\n().\nremove\n(\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE/MY_PARENT_CODE_1\"\n));\n// Instead of adding and removing parents we can also set the list of parents to a completely new value.\nsample\n.\ngetParentIds\n().\nset\n(\nnew\nSampleIdentifier\n(\n## \"MY_SPACE/MY_PARENT_CODE_2\"\n),\nnew\nSampleIdentifier\n(\n## \"MY_SPACE/MY_PARENT_CODE_3\"\n));\nv3\n.\nupdateSamples\n([\nsample\n]).\ndone\n(\nfunction\n()\n{\nalert\n(\n## \"Updated\"\n);\n});\n});\n</\nscript\n>\nGetting authorization rights for entities\n\nIf the user isn’t allowed to create or update an entity an exception is\nthrown. But often a client application wants to know in advance whether\nsuch operations are allowed or not. With the API method\ngetRights()\nauthorizations rights for specified entities can be requested. Currently\nonly creation and update authorization rights for projects, experiments,\nsamples and data sets (only update right) are returned.\nIn order to check whether an entity can be created or not a dummy\nidentifier has to be provided when calling\ngetRights()\n## . This\nidentifier should be a wellformed identifier which specifies the entity\nto which such a new entity belongs. For example, calling\ngetRights()\nwith\nnew\nExperimentIdentifier(\"/MY-SPACE/PROJECT1/DUMMY\")\nwould return\nrights containing\n## CREATE\nif the user is allowed to create an\nexperiment in the project\n## /MY-SPACE/PROJECT1\n.\nFreezing entities\n\nAn entity (Space, Project, Experiment, Sample, Data Set) can be frozen.\n## There are two types of frozen:\n## Core\nand\nsurface\n. A frozen core means\nthat certain attributes of the entity can not be changed but still\nconnections between entities can be added or removed. A frozen surface\nimplies a frozen core and frozen connections of particular types. To\nfreeze an entity it has to be updated by invoking at least one freeze\nmethod on the update object. Example:\nSampleUpdate sample = new SampleUpdate();\nsample.setSampleId(new SampleIdentifier(\"/MY_SPACE_CODE/MY_SAMPLE_CODE\"));\nsample.freezeForChildren();\nv3.updateSamples(sessionToken, Arrays.asList(sample));\n## Warning\nFreezing can not be reverted.\nThe timestamp of freezing, the types of freezing, the user and the\nidentifier of the frozen entity will be stored in the database as a\nfreezing event.\nThe following tables show all freezing possibilities and what is actual\nfrozen.\n## Space\n\nFreezing method\n## Description\nfreeze\nThe specified space can not be deleted.\nThe description can not be set or changed.\nfreezeForProjects\nSame as freeze() plus no projects can be added to or removed from the specified space.\nfreezeForSamples\nSame as freeze() plus no samples can be added to or removed from the specified space.\n## Project\n\nFreezing method\n## Description\nfreeze\nThe specified project can not be deleted.\nThe description can not be set or changed.\nNo attachments can be added or removed.\nfreezeForExperiments\nSame as freeze() plus no experiments can be added to or removed from the specified project.\nfreezeForSamples\nSame as freeze() plus no samples can be added to or removed from the specified project.\n## Experiment\n\nFreezing method\n## Description\nfreeze\nThe specified experiment can not be deleted.\nNo properties can be added, removed or modified.\nNo attachments can be added or removed.\nfreezeForSamples\nSame as freeze() plus no samples can be added to or removed from the specified experiment.\nfreezeForDataSets\nSame as freeze() plus no data sets can be added to or removed from the specified experiment.\n## Sample\n\nFreezing method\n## Description\nfreeze\nThe specified sample can not be deleted.\nNo properties can be added, removed or modified.\nNo attachments can be added or removed.\nfreezeForComponents\nSame as freeze() plus no component samples can be added to or removed from the specified sample.\nfreezeForChildren\nSame as freeze() plus no child samples can be added to or removed from the specified sample.\nfreezeForParents\nSame as freeze() plus no parent samples can be added to or removed from the specified sample.\nfreezeForDataSets\nSame as freeze() plus no data sets can be added to or removed from the specified sample.\nData Set\n\nFreezing method\n## Description\nfreeze\nThe specified data set can not be deleted.\nNo properties can be added, removed or modified.\nContent copies can be still added or removed for frozen link data sets.\nfreezeForChildren\nSame as freeze() plus no child data sets can be added to or removed from the specified data set.\nfreezeForParents\nSame as freeze() plus no parent data sets can be added to or removed from the specified data set.\nfreezeForComponents\nSame as freeze() plus no component data sets can be added to or removed from the specified data set.\nfreezeForContainers\nSame as freeze() plus no container data sets can be added to or removed from the specified data set.\nSearching entities\n\nThe methods for searching entities in V3 API are called:\nsearchSpaces\n,\nsearchProjects\n,\nsearchExperiments\n,\nsearchSamples\n,\nsearchDataSets\n,\nsearchMaterials\n,\nsearchVocabularyTerms,\nsearchTags\n,\nsearchGlobally\n.\nThey all take criteria and fetch options objects as an input. The\ncriteria object allows you to specify what entities you are looking for.\nFor instance, only entities from a given space, entities of a given\ntype, entities with a property X that equals Y and much much more.\nThe fetch options object allows you to tell the API which parts of the\nentities found should be fetched and returned as a result of the method\ncall. For instance, you can tell the API to return the results only with\nproperties because this is all what you will need for your processing.\nThis gives you a very fine grained control over how much data you\nactually fetch from the server. The less you ask for via fetch options\nthe less data the API has to load from the database and the less data it\nwill have to transfer over the network. Therefore by default, the fetch\noptions object is empty, i.e. it tells the API only to fetch the basic\ninformation about a given entity, i.e. its id, attributes and creation\nand registration dates. If you want to fetch anything more then you have\nto let the API know via fetch options which parts you are also\ninterested in.\nAnother functionality that the fetch options object provides is\npagination (see FetchOptions.from(Integer) and\nFetchOptions.count(Integer) methods). With pagination a user can control\nif a search method shall return all found results or just a given\nsubrange. This is especially useful for handling very large numbers of\nresults e.g. when we want to build a UI to present them. In such a\nsituation, we can perform the search that returns only the first batch\nof results (e.g. the first 100) for the UI to be responsive and ask for\nanother batch only if a user requests that (e.g. via clicking on a next\npage button in the UI). The pagination is available in all the search\nmethods including the global search (i.e. searchGlobally method). A code\nexample on how to use the pagination methods is presented below.\nApart from the pagination the fetch options also provides the means to\nsort the results (see FetchOptions.sortBy() method). What fields can be\nused for sorting depends on the search method and the returned objects.\nResults can be sorted ascending or descending. Sorting by multiple\nfields is also possible (e.g. first sort by type and then by\nidentifier). A code example on how to use sorting is presented below.\n## Example\n\nV3SearchExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria\n;\npublic\nclass\nV3SearchExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// search for samples that are in space with code MY_SPACE_CODE and are of sample type with code MY_SAMPLE_TYPE_CODE\nSampleSearchCriteria\ncriteria\n=\nnew\nSampleSearchCriteria\n();\ncriteria\n.\nwithSpace\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SPACE_CODE\"\n);\ncriteria\n.\nwithType\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n);\n// tell the API to fetch properties for each returned sample\nSampleFetchOptions\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithProperties\n();\nSearchResult\n<\n## Sample\n>\nresult\n=\nv3\n.\nsearchSamples\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\nfor\n(\n## Sample\nsample\n## :\nresult\n.\ngetObjects\n())\n{\n// because we asked for properties via fetch options we can access them here, otherwise NotFetchedException would be thrown by getProperties method\n## System\n.\nout\n.\nprintln\n(\n## \"Sample \"\n+\nsample\n.\ngetIdentifier\n()\n+\n\" has properties: \"\n+\nsample\n.\ngetProperties\n());\n}\n}\n}\nV3SearchExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/search/SampleSearchCriteria\"\n,\n\"as/dto/sample/fetchoptions/SampleFetchOptions\"\n],\nfunction\n(\nSampleSearchCriteria\n,\nSampleFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// search for samples that are in space with code MY_SPACE_CODE and are of sample type with code MY_SAMPLE_TYPE_CODE\nvar\ncriteria\n=\nnew\nSampleSearchCriteria\n();\ncriteria\n.\nwithSpace\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SPACE_CODE\"\n);\ncriteria\n.\nwithType\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n);\n// tell the API to fetch properties for each returned sample\nvar\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithProperties\n();\nv3\n.\nsearchSamples\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nsample\n)\n{\n// because we asked for properties via fetch options we can access them here, otherwise NotFetchedException would be thrown by getProperties method\nalert\n(\n## \"Sample \"\n+\nsample\n.\ngetIdentifier\n()\n+\n\" has properties: \"\n+\n## JSON\n.\nstringify\n(\nsample\n.\ngetProperties\n()));\n});\n});\n});\n</\nscript\n>\nExample with pagination and sorting\n\nV3SearchWithPaginationAndSortingExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria\n;\npublic\nclass\nV3SearchWithPaginationAndSortingExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleSearchCriteria\ncriteria\n=\nnew\nSampleSearchCriteria\n();\nSampleFetchOptions\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\n// get the first 100 results\nfetchOptions\n.\nfrom\n(\n0\n);\nfetchOptions\n.\ncount\n(\n100\n);\n// sort the results first by a type (ascending) and then by an identifier (descending)\nfetchOptions\n.\nsortBy\n().\ntype\n().\nasc\n();\nfetchOptions\n.\nsortBy\n().\nidentifier\n().\ndesc\n();\nSearchResult\n<\n## Sample\n>\nresult\n=\nv3\n.\nsearchSamples\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\n// because of the pagination the list contains only the first 100 objects (or even less if there are fewer results found)\n## System\n.\nout\n.\nprintln\n(\nresult\n.\ngetObjects\n());\n// returns the number of all found results (i.e. potentially more than 100)\n## System\n.\nout\n.\nprintln\n(\nresult\n.\ngetTotalCount\n());\n}\n}\nV3SearchWithPaginationAndSortingExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/search/SampleSearchCriteria\"\n,\n\"as/dto/sample/fetchoptions/SampleFetchOptions\"\n],\nfunction\n(\nSampleSearchCriteria\n,\nSampleFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\ncriteria\n=\nnew\nSampleSearchCriteria\n();\nvar\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\n// get the first 100 results\nfetchOptions\n.\nfrom\n(\n0\n);\nfetchOptions\n.\ncount\n(\n100\n);\n// sort the results first by a type (ascending) and then by an identifier (descending)\nfetchOptions\n.\nsortBy\n().\ntype\n().\nasc\n();\nfetchOptions\n.\nsortBy\n().\nidentifier\n().\ndesc\n();\nv3\n.\nsearchSamples\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\n// because of pagination the list contains only the first 100 objects (or even less if there are fewer results found)\nconsole\n.\nlog\n(\nresult\n.\ngetObjects\n());\n// returns the number of all found results (i.e. potentially more than 100)\nconsole\n.\nlog\n(\nresult\n.\ngetTotalCount\n());\n});\n});\n</\nscript\n>\nExample with OR operator\n\nBy default all specified search criteria have to be fulfilled. If only\none criteria needs to be fulfilled use\ncriteria.withOrOperator()\nas in\nthe following example:\nV3SearchWithOrOperatorExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria\n;\npublic\nclass\nV3SearchWithOrOperatorExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// search for samples that are either in space with code MY_SPACE_CODE or of sample type with code MY_SAMPLE_TYPE_CODE\nSampleSearchCriteria\ncriteria\n=\nnew\nSampleSearchCriteria\n();\ncriteria\n.\nwithOrOperator\n();\ncriteria\n.\nwithSpace\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SPACE_CODE\"\n);\ncriteria\n.\nwithType\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n);\n// tell the API to fetch the type for each returned sample\nSampleFetchOptions\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithType\n();\nSearchResult\n<\n## Sample\n>\nresult\n=\nv3\n.\nsearchSamples\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\nfor\n(\n## Sample\nsample\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\n## \"Sample \"\n+\nsample\n.\ngetIdentifier\n()\n+\n\" [\"\n+\nsample\n.\ngetType\n().\ngetCode\n()\n+\n\"]\"\n);\n}\n}\n}\nV3SearchWithOrOperatorExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/search/SampleSearchCriteria\"\n,\n\"as/dto/sample/fetchoptions/SampleFetchOptions\"\n],\nfunction\n(\nSampleSearchCriteria\n,\nSampleFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// search for samples that are in space with code MY_SPACE_CODE and are of sample type with code MY_SAMPLE_TYPE_CODE\nvar\ncriteria\n=\nnew\nSampleSearchCriteria\n();\ncriteria\n.\nwithOrOperator\n();\ncriteria\n.\nwithSpace\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SPACE_CODE\"\n);\ncriteria\n.\nwithType\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n);\n// tell the API to fetch type for each returned sample\nvar\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithType\n();\nv3\n.\nsearchSamples\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nsample\n)\n{\nalert\n(\n## \"Sample \"\n+\nsample\n.\ngetIdentifier\n()\n+\n\" [\"\n+\nsample\n.\ngetType\n().\ngetCode\n()\n+\n\"]\"\n);\n});\n});\n});\n</\nscript\n>\nExample with nested logical operators\n\nThe following code finds samples with perm ID that ends with “6” AND\n(with code that contains “-” OR that starts with “C”) AND (with\nexperiment OR of type whose code starts with “MASTER”).\nV3SearchWithNestedLogicalOperatorsExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria\n;\npublic\nclass\nV3SearchWithRecursiveFetchOptionsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleSearchCriteria\ncriteria\n=\nnew\nSampleSearchCriteria\n().\nwithAndOperator\n();\ncriteria\n.\nwithPermId\n().\nthatEndsWith\n(\n\"6\"\n);\nSampleSearchCriteria\nsubcriteria1\n=\ncriteria\n.\nwithSubcriteria\n().\nwithOrOperator\n();\nsubcriteria1\n.\nwithCode\n().\nthatContains\n(\n\"-\"\n);\nsubcriteria1\n.\nwithCode\n().\nthatStartsWith\n(\n## \"C\"\n);\nSampleSearchCriteria\nsubcriteria2\n=\ncriteria\n.\nwithSubcriteria\n().\nwithOrOperator\n();\nsubcriteria2\n.\nwithExperiment\n();\nsubcriteria2\n.\nwithType\n().\nwithCode\n().\nthatStartsWith\n(\n## \"MASTER\"\n);\n// tell the API to fetch all descendents for each returned sample\nSampleFetchOptions\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nSearchResult\n<\n## Sample\n>\nresult\n=\nv3\n.\nsearchSamples\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\nfor\n(\n## Sample\nsample\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\n## \"Sample \"\n+\nsample\n.\ngetIdentifier\n()\n+\n\" [\"\n+\nsample\n.\ngetType\n().\ngetCode\n()\n+\n\"]\"\n);\n}\n}\n}\nV3SearchWithNestedLogicalOperatorsExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/search/SampleSearchCriteria\"\n,\n\"as/dto/sample/fetchoptions/SampleFetchOptions\"\n],\nfunction\n(\nSampleSearchCriteria\n,\nSampleFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\ncriteria\n=\nnew\nSampleSearchCriteria\n().\nwithAndOperator\n();\ncriteria\n.\nwithPermId\n().\nthatEndsWith\n(\n\"6\"\n);\nvar\nsubcriteria1\n=\ncriteria\n.\nwithSubcriteria\n().\nwithOrOperator\n();\nsubcriteria1\n.\nwithCode\n().\nthatContains\n(\n\"-\"\n);\nsubcriteria1\n.\nwithCode\n().\nthatStartsWith\n(\n## \"C\"\n);\nvar\nsubcriteria2\n=\ncriteria\n.\nwithSubcriteria\n().\nwithOrOperator\n();\nsubcriteria2\n.\nwithExperiment\n();\nsubcriteria2\n.\nwithType\n().\nwithCode\n().\nthatStartsWith\n(\n## \"MASTER\"\n);\n// tell the API to fetch type for each returned sample\nvar\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nv3\n.\nsearchSamples\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nsample\n)\n{\nalert\n(\n## \"Sample \"\n+\nsample\n.\ngetIdentifier\n()\n+\n\" [\"\n+\nsample\n.\ngetType\n().\ngetCode\n()\n+\n\"]\"\n);\n});\n});\n});\n</\nscript\n>\nExample with recursive fetch options\n\nIn order to get all descendent/acsendents of a sample fetch options can\nbe used recursively by\nusing\nfetchOptions.withChildrenUsing(fetchOptions)\nas in the following\n## example:\nV3SearchWithRecursiveFetchOptionsExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleSearchCriteria\n;\npublic\nclass\nV3SearchWithRecursiveFetchOptionsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleSearchCriteria\ncriteria\n=\nnew\nSampleSearchCriteria\n();\ncriteria\n.\nwithType\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n);\n// tell the API to fetch all descendents for each returned sample\nSampleFetchOptions\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithChildrenUsing\n(\nfetchOptions\n);\nSearchResult\n<\n## Sample\n>\nresult\n=\nv3\n.\nsearchSamples\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\nfor\n(\n## Sample\nsample\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\n## \"Sample \"\n+\nrenderWithDescendants\n(\nsample\n));\n}\n}\nprivate\nstatic\n## String\nrenderWithDescendants\n(\n## Sample\nsample\n)\n{\nStringBuilder\nbuilder\n=\nnew\nStringBuilder\n();\nfor\n(\n## Sample\nchild\n## :\nsample\n.\ngetChildren\n())\n{\nif\n(\nbuilder\n.\nlength\n()\n>\n0\n)\n{\nbuilder\n.\nappend\n(\n\", \"\n);\n}\nbuilder\n.\nappend\n(\nrenderWithDescendants\n(\nchild\n));\n}\nif\n(\nbuilder\n.\nlength\n()\n==\n0\n)\n{\nreturn\nsample\n.\ngetCode\n();\n}\nreturn\nsample\n.\ngetCode\n()\n+\n\" -> (\"\n+\nbuilder\n.\ntoString\n()\n+\n\")\"\n;\n}\n}\nV3SearchWithRecursiveFetchOptionsExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/search/SampleSearchCriteria\"\n,\n\"as/dto/sample/fetchoptions/SampleFetchOptions\"\n],\nfunction\n(\nSampleSearchCriteria\n,\nSampleFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\ncriteria\n=\nnew\nSampleSearchCriteria\n();\ncriteria\n.\nwithType\n().\nwithCode\n().\nthatEquals\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n);\n// tell the API to fetch all descendents for each returned sample\nvar\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithChildrenUsing\n(\nfetchOptions\n);\nv3\n.\nsearchSamples\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nsample\n)\n{\nalert\n(\n## \"Sample \"\n+\nrenderWithDescendants\n(\nsample\n));\n});\n});\nfunction\nrenderWithDescendants\n(\nsample\n)\n{\nvar\nchildren\n=\nsample\n.\ngetChildren\n();\nvar\nlist\n=\n\"\"\n;\nfor\n(\nvar\ni\n=\n0\n;\ni\n<\nchildren\n.\nlength\n;\ni\n++\n)\n{\nif\n(\nlist\n.\nlength\n>\n0\n)\n{\nlist\n+=\n\", \"\n;\n}\nlist\n+=\nrenderWithDescendants\n(\nchildren\n[\ni\n]);\n}\nif\n(\nchildren\n.\nlength\n==\n0\n)\n{\nreturn\nsample\n.\ngetCode\n();\n}\nreturn\nsample\n.\ngetCode\n()\n+\n\" -> (\"\n+\nlist\n+\n\")\"\n}\n});\n</\nscript\n>\nGlobal search\n\nThere are two kinds or global search:\nUsing thatContains() and thatContainsExactly() methods of\nGlobalSearchTextCriteria. This type of search performs the substring\nsearch in any field of any entity.\nUsing thatMatches() method of GlobalSearchTextCriteria. This type of\nsearch performs lexical match using English dictionaly. If a\nmatching string is not a word it is matched as a whole (i.e. code\nwill match code only if a whole code string is provided).\nGlobal search searches for experiments, samples, data sets and materials\nby specifying a text snippet (or complete words) to be found in any type\nof meta data (entity attribute or property). Example:\nV3GlobalSearchExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.global.GlobalSearchObject\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.global.fetchoptions.GlobalSearchObjectFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.global.search.GlobalSearchCriteria\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.global.search.GlobalSearchObjectKind\n;\npublic\nclass\nV3GlobalSearchExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// search for any text matching 'default' but only among samples\nGlobalSearchCriteria\ncriteria\n=\nnew\nGlobalSearchCriteria\n();\ncriteria\n.\nwithObjectKind\n().\nthatIn\n(\nGlobalSearchObjectKind\n.\n## SAMPLE\n);\ncriteria\n.\nwithText\n().\nthatMatches\n(\n\"default\"\n);\n// Fetch also the sample type\nGlobalSearchObjectFetchOptions\nfetchOptions\n=\nnew\nGlobalSearchObjectFetchOptions\n();\nfetchOptions\n.\nwithSample\n().\nwithType\n();\nSearchResult\n<\nGlobalSearchObject\n>\nresult\n=\nv3\n.\nsearchGlobally\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\nfor\n(\nGlobalSearchObject\nobject\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\nobject\n.\ngetObjectKind\n()\n+\n\": \"\n+\nobject\n.\ngetObjectIdentifier\n()\n+\n\" [\"\n+\nobject\n.\ngetSample\n().\ngetType\n().\ngetCode\n()\n+\n\"], score:\"\n+\nobject\n.\ngetScore\n()\n+\n\", match:\"\n+\nobject\n.\ngetMatch\n());\n}\n}\n}\nV3GlobalSearchExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/global/search/GlobalSearchCriteria\"\n,\n\"as/dto/global/search/GlobalSearchObjectKind\"\n,\n\"as/dto/global/fetchoptions/GlobalSearchObjectFetchOptions\"\n],\nfunction\n(\nGlobalSearchCriteria\n,\nGlobalSearchObjectKind\n,\nGlobalSearchObjectFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// search for any text matching 'default' but only among samples\nvar\ncriteria\n=\nnew\nGlobalSearchCriteria\n();\ncriteria\n.\nwithObjectKind\n().\nthatIn\n([\nGlobalSearchObjectKind\n.\n## SAMPLE\n]);\ncriteria\n.\nwithText\n().\nthatMatches\n(\n\"default\"\n);\n// Fetch also the sample type\nvar\nfetchOptions\n=\nnew\nGlobalSearchObjectFetchOptions\n();\nfetchOptions\n.\nwithSample\n().\nwithType\n();\nv3\n.\nsearchGlobally\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nobject\n)\n{\nalert\n(\nobject\n.\ngetObjectKind\n()\n+\n\": \"\n+\nobject\n.\ngetObjectIdentifier\n()\n+\n\" [\"\n+\nobject\n.\ngetSample\n().\ngetType\n().\ngetCode\n()\n+\n\"], score:\"\n+\nobject\n.\ngetScore\n()\n+\n\", match:\"\n+\nobject\n.\ngetMatch\n());\n});\n});\n});\n</\nscript\n>\nGetting entities\n\nThe methods for getting entities in V3 API are called: getSpaces,\ngetProjects, getExperiments, getSamples, getDataSets, getMaterials,\ngetVocabularyTerms, getTags. They all take a list of entity ids and\nfetch options as an input (please check “Searching entities” section for\nmore details on the fetch options). They return a map where the passed\nentity ids become the keys and values are the entities found for these\nids. If no entity was found for a given id or entity exists but you\ndon’t have access to it then there is no entry for such an id in the\nreturned map.\n## Example\n\nV3GetExample.java\nimport\njava.util.Arrays\n;\nimport\njava.util.Map\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.Sample\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.ISampleId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SamplePermId\n;\npublic\nclass\nV3GetExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nISampleId\nid1\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n);\nISampleId\nid2\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE_2\"\n);\nISampleId\nid3\n=\nnew\nSamplePermId\n(\n\"20160115170726679-98669\"\n);\n// perm id of sample /MY_SPACE_CODE/MY_SAMPLE_CODE\nISampleId\nid4\n=\nnew\nSamplePermId\n(\n\"20160118115737079-98672\"\n);\n// perm id of sample /MY_SPACE_CODE/MY_SAMPLE_CODE_3\nISampleId\nid5\n=\nnew\nSamplePermId\n(\n## \"I_DONT_EXIST\"\n);\nSampleFetchOptions\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithProperties\n();\n## Map\n<\nISampleId\n,\n## Sample\n>\nmap\n=\nv3\n.\ngetSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nid1\n,\nid2\n,\nid3\n,\nid4\n,\nid5\n),\nfetchOptions\n);\nmap\n.\nget\n(\nid1\n);\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE\nmap\n.\nget\n(\nid2\n);\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_2\nmap\n.\nget\n(\nid3\n);\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE\nmap\n.\nget\n(\nid4\n);\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_3\nmap\n.\nget\n(\nid5\n);\n// returns null\n}\n}\nV3GetExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/id/SampleIdentifier\"\n,\n\"as/dto/sample/id/SamplePermId\"\n,\n\"as/dto/sample/fetchoptions/SampleFetchOptions\"\n],\nfunction\n(\nSampleIdentifier\n,\nSamplePermId\n,\nSampleFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nid1\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n);\nvar\nid2\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE_2\"\n);\nvar\nid3\n=\nnew\nSamplePermId\n(\n\"20160115170726679-98669\"\n);\n// perm id of sample /MY_SPACE_CODE/MY_SAMPLE_CODE\nvar\nid4\n=\nnew\nSamplePermId\n(\n\"20160118115737079-98672\"\n);\n// perm id of sample   /MY_SPACE_CODE/MY_SAMPLE_CODE_3\nvar\nid5\n=\nnew\nSamplePermId\n(\n## \"I_DONT_EXIST\"\n);\nvar\nfetchOptions\n=\nnew\nSampleFetchOptions\n();\nfetchOptions\n.\nwithProperties\n();\nv3\n.\ngetSamples\n([\nid1\n,\nid2\n,\nid3\n,\nid4\n,\nid5\n],\nfetchOptions\n).\ndone\n(\nfunction\n(\nmap\n)\n{\nmap\n[\nid1\n];\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE\nmap\n[\nid2\n];\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_2\nmap\n[\nid3\n];\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE\nmap\n[\nid4\n];\n// returns sample /MY_SPACE_CODE/MY_SAMPLE_CODE_3\nmap\n[\nid5\n];\n// returns null\n});\n});\n</\nscript\n>\nDeleting entities\n\nThe methods for deleting entities in V3 API are called: deleteSpaces,\ndeleteProjects, deleteExperiments, deleteSamples, deleteDataSets,\ndeleteMaterials, deleteVocabularyTerms, deleteTags. The delete methods\nfor spaces, projects, materials, vocabulary terms, tags perform a\npermanent deletion (there is no trash can for these entities - deletion\ncannot be reverted). The delete methods for experiments, samples and\ndata sets perform a logical deletion (move entities to the trash can)\nand return a deletion id. This deletion id can be used for either\nconfirming the logical deletion to remove the entities permanently or\nreverting the logical deletion to take the entities out from the trash\ncan.\n## Example\n\nV3DeleteExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.deletion.id.IDeletionId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.delete.SampleDeletionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.ISampleId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier\n;\npublic\nclass\nV3DeleteExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nISampleId\nid1\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n);\nISampleId\nid2\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE_2\"\n);\nSampleDeletionOptions\ndeletionOptions\n=\nnew\nSampleDeletionOptions\n();\ndeletionOptions\n.\nsetReason\n(\n\"Testing logical deletion\"\n);\n// logical deletion (move objects to the trash can)\nIDeletionId\ndeletionId\n=\nv3\n.\ndeleteSamples\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nid1\n,\nid2\n),\ndeletionOptions\n);\n// you can use the deletion id to confirm the deletion (permanently delete objects)\nv3\n.\nconfirmDeletions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\ndeletionId\n));\n// you can use the deletion id to revert the deletion (get the objects out from the trash can)\nv3\n.\nrevertDeletions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\ndeletionId\n));\n}\n}\nV3DeleteExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/id/SampleIdentifier\"\n,\n\"as/dto/sample/delete/SampleDeletionOptions\"\n],\nfunction\n(\nSampleIdentifier\n,\nSampleDeletionOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nid1\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE\"\n);\nvar\nid2\n=\nnew\nSampleIdentifier\n(\n## \"/MY_SPACE_CODE/MY_SAMPLE_CODE_2\"\n);\nvar\ndeletionOptions\n=\nnew\nSampleDeletionOptions\n();\ndeletionOptions\n.\nsetReason\n(\n\"Testing logical deletion\"\n);\n// logical deletion (move objects to the trash can)\nv3\n.\ndeleteSamples\n([\nid1\n,\nid2\n],\ndeletionOptions\n).\ndone\n(\nfunction\n(\ndeletionId\n)\n{\n// you can use the deletion id to confirm the deletion (permanently delete objects)\nv3\n.\nconfirmDeletions\n([\ndeletionId\n]);\n// you can use the deletion id to revert the deletion (get the objects out from the trash can)\nv3\n.\nrevertDeletions\n([\ndeletionId\n]);\n});\n});\n</\nscript\n>\nSearching entity types\n\nThe following search methods allows to search for entity types including\nall assigned property\n## types:\nsearchDataSetTypes\n,\nsearchExperimentTypes\n,\nsearchMaterialTypes\nand\nsearchSampleTypes\n. Here is an example which will search for all\nsample types and assigned property types:\nV3SearchTypesExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.property.PropertyAssignment\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.SampleType\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.fetchoptions.SampleTypeFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.search.SampleTypeSearchCriteria\n;\npublic\nclass\nV3SearchTypesExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleTypeSearchCriteria\nsearchCriteria\n=\nnew\nSampleTypeSearchCriteria\n();\nSampleTypeFetchOptions\nfetchOptions\n=\nnew\nSampleTypeFetchOptions\n();\nfetchOptions\n.\nwithPropertyAssignments\n().\nwithPropertyType\n();\nSearchResult\n<\nSampleType\n>\nresult\n=\nv3\n.\nsearchSampleTypes\n(\nsessionToken\n,\nsearchCriteria\n,\nfetchOptions\n);\nfor\n(\nSampleType\nsampleType\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\nsampleType\n.\ngetCode\n());\nfor\n(\nPropertyAssignment\nassignment\n## :\nsampleType\n.\ngetPropertyAssignments\n())\n{\n## System\n.\nout\n.\nprintln\n(\n\"  \"\n+\nassignment\n.\ngetPropertyType\n().\ngetCode\n()\n+\n(\nassignment\n.\nisMandatory\n()\n?\n\"*\"\n## :\n\"\"\n));\n}\n}\n}\n}\nV3SearchTypesExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/sample/search/SampleTypeSearchCriteria\"\n,\n\"as/dto/sample/fetchoptions/SampleTypeFetchOptions\"\n],\nfunction\n(\nSampleTypeSearchCriteria\n,\nSampleTypeFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// here we are interested only in the last updates of samples and projects\nvar\ncriteria\n=\nnew\nSampleTypeSearchCriteria\n();\nvar\nfetchOptions\n=\nnew\nSampleTypeFetchOptions\n();\nfetchOptions\n.\nwithPropertyAssignments\n().\nwithPropertyType\n();\nv3\n.\nsearchSampleTypes\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nsampleType\n)\n{\nvar\nmsg\n=\nsampleType\n.\ngetCode\n();\nvar\nassignments\n=\nsampleType\n.\ngetPropertyAssignments\n();\nfor\n(\nvar\ni\n=\n0\n;\ni\n<\nassignments\n.\nlength\n;\ni\n++\n)\n{\nmsg\n+=\n\"\\n  \"\n+\nassignments\n[\ni\n].\ngetPropertyType\n().\ngetCode\n();\n}\nalert\n(\nmsg\n);\n});\n});\n});\n</\nscript\n>\n## Modifications\n\nThe API allows to ask for the latest modification (UPDATE or\nCREATE_OR_DELETE) for groups of objects of various kinds (see\nclass\nch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.ObjectKind\nfor\na complete list). This feature of the openBIS API helps GUI clients to\nupdate views automatically. Here is an example which asks for the latest\nproject and sample update:\nV3SearchObjectKindModificationsExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.ObjectKind\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.ObjectKindModification\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.OperationKind\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.fetchoptions.ObjectKindModificationFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.objectkindmodification.search.ObjectKindModificationSearchCriteria\n;\npublic\nclass\nV3SearchObjectKindModificationsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// here we are interested only in the last updates of samples and projects\nObjectKindModificationSearchCriteria\ncriteria\n=\nnew\nObjectKindModificationSearchCriteria\n();\ncriteria\n.\nwithObjectKind\n().\nthatIn\n(\nObjectKind\n.\n## PROJECT\n,\nObjectKind\n.\n## SAMPLE\n);\ncriteria\n.\nwithOperationKind\n().\nthatIn\n(\nOperationKind\n.\n## UPDATE\n);\nObjectKindModificationFetchOptions\nfetchOptions\n=\nnew\nObjectKindModificationFetchOptions\n();\nSearchResult\n<\nObjectKindModification\n>\nresult\n=\nv3\n.\nsearchObjectKindModifications\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\nfor\n(\nObjectKindModification\nmodification\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\n\"The last \"\n+\nmodification\n.\ngetOperationKind\n()\n+\n\" of an entity of kind \"\n+\nmodification\n.\ngetObjectKind\n()\n+\n\" occured at \"\n+\nmodification\n.\ngetLastModificationTimeStamp\n());\n}\n}\n}\nV3SearchObjectKindModificationsExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/objectkindmodification/search/ObjectKindModificationSearchCriteria\"\n,\n\"as/dto/objectkindmodification/ObjectKind\"\n,\n\"as/dto/objectkindmodification/OperationKind\"\n,\n\"as/dto/objectkindmodification/fetchoptions/ObjectKindModificationFetchOptions\"\n],\nfunction\n(\nObjectKindModificationSearchCriteria\n,\nObjectKind\n,\nOperationKind\n,\nObjectKindModificationFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n// here we are interested only in the last updates of samples and projects\nvar\ncriteria\n=\nnew\nObjectKindModificationSearchCriteria\n();\ncriteria\n.\nwithObjectKind\n().\nthatIn\n([\nObjectKind\n.\n## PROJECT\n,\nObjectKind\n.\n## SAMPLE\n]);\ncriteria\n.\nwithOperationKind\n().\nthatIn\n([\nOperationKind\n.\n## UPDATE\n]);\nvar\nfetchOptions\n=\nnew\nObjectKindModificationFetchOptions\n();\nv3\n.\nsearchObjectKindModifications\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nmodification\n)\n{\nalert\n(\n\"The last \"\n+\nmodification\n.\ngetOperationKind\n()\n+\n\" of an entity of kind \"\n+\nmodification\n.\ngetObjectKind\n()\n+\n\" occured at \"\n+\nmodification\n.\ngetLastModificationTimeStamp\n());\n});\n});\n});\n</\nscript\n>\nCustom AS Services\n\nIn order to extend openBIS API new custom services can be established by core plugins of type\nservices\n(see\n## Custom Application Server Services\n). The API offers a method to search for a service and to execute a service.\nSearch for custom services\n\nAs with any other search method\nsearchCustomASServices()\nneeds a search criteria\nCustomASServiceSearchCriteria\nand fetch options\nCustomASServiceFetchOptions\n. The following example returns all available custom AS services.\n## Example\n\nV3SearchCustomASServicesExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.service.CustomASService\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.service.fetchoptions.CustomASServiceFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.service.search.CustomASServiceSearchCriteria\n;\npublic\nclass\nV3SearchCustomASServicesExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nCustomASServiceSearchCriteria\ncriteria\n=\nnew\nCustomASServiceSearchCriteria\n();\nCustomASServiceFetchOptions\nfetchOptions\n=\nnew\nCustomASServiceFetchOptions\n();\nSearchResult\n<\nCustomASService\n>\nresult\n=\nv3\n.\nsearchCustomASServices\n(\nsessionToken\n,\ncriteria\n,\nfetchOptions\n);\nfor\n(\nCustomASService\nservice\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\nservice\n.\ngetCode\n()\n+\n\": \"\n+\nservice\n.\ngetLabel\n()\n+\n\" (\"\n+\nservice\n.\ngetDescription\n()\n+\n\")\"\n);\n}\n}\n}\nV3SearchCustomASServicesExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/service/search/CustomASServiceSearchCriteria\"\n,\n\"as/dto/service/fetchoptions/CustomASServiceFetchOptions\"\n],\nfunction\n(\nCustomASServiceSearchCriteria\n,\nCustomASServiceFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\ncriteria\n=\nnew\nCustomASServiceSearchCriteria\n();\nvar\nfetchOptions\n=\nnew\nCustomASServiceFetchOptions\n();\nv3\n.\nsearchCustomASServices\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nservice\n)\n{\nalert\n(\nservice\n.\ngetCode\n()\n+\n\": \"\n+\nservice\n.\ngetLabel\n()\n+\n\" (\"\n+\nservice\n.\ngetDescription\n()\n+\n\")\"\n);\n});\n});\n});\n</\nscript\n>\nExecute a custom service\n\nIn order to execute a custom AS service its code is needed. In addition\na set of key-value pairs can be provided. The key has to be a string\nwhereas the value can be any object. Note, that in case of Java the\nobject has be an instance of class which Java serializable. The\nkey-value pairs are added to\nCustomASServiceExecutionOptions\nobject by\ninvoking\nwithParameter()\nfor each pair.\nThe result can be any object (again it has to be Java serializable in\nthe Java case). In a Java client the result will usually be casted for\nfurther processing.\n## Example\n\nV3ExecuteCustomASServiceExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.service.CustomASServiceExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.service.id.CustomASServiceCode\n;\npublic\nclass\nV3ExecuteCustomASServiceExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nCustomASServiceCode\nid\n=\nnew\nCustomASServiceCode\n(\n\"example-service\"\n);\nCustomASServiceExecutionOptions\noptions\n=\nnew\nCustomASServiceExecutionOptions\n().\nwithParameter\n(\n\"space-code\"\n,\n## \"TEST\"\n);\n## Object\nresult\n=\nv3\n.\nexecuteCustomASService\n(\nsessionToken\n,\nid\n,\noptions\n);\n## System\n.\nout\n.\nprintln\n(\n## \"Result: \"\n+\nresult\n);\n}\n}\nV3ExecuteCustomASServiceExample.html\n<\nscript\n>\nrequire\n([\n\"as/dto/service/id/CustomASServiceCode\"\n,\n\"as/dto/service/CustomASServiceExecutionOptions\"\n],\nfunction\n(\nCustomASServiceCode\n,\nCustomASServiceExecutionOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nid\n=\nnew\nCustomASServiceCode\n(\n\"example-service\"\n);\nvar\noptions\n=\nnew\nCustomASServiceExecutionOptions\n().\nwithParameter\n(\n\"space-code\"\n,\n## \"TEST\"\n);\nv3\n.\nexecuteCustomASService\n(\nid\n,\noptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nalert\n(\nresult\n);\n});\n});\n</\nscript\n>\nArchiving / unarchiving data sets\n\nThe API provides the following methods for handling the data set\narchiving: archiveDataSets and unarchiveDataSets. Both methods schedule\nthe operation to be executed asynchronously, i.e. once\narchiveDataSets/unarchiveDataSets method call finishes the requested\ndata sets are only scheduled for the archiving/unarchiving but are not\nin the archive/store yet.\nArchiving data sets\n\n## Example\n\nV3ArchiveDataSetsExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.archive.DataSetArchiveOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.IDataSetId\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3ArchiveDataSetsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nIDataSetId\nid1\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2266\"\n);\nIDataSetId\nid2\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2267\"\n);\nDataSetArchiveOptions\noptions\n=\nnew\nDataSetArchiveOptions\n();\n// With removeFromDataStore flag set to true data sets are moved to the archive.\n// With removeFromDataStore flag set to false data sets are copied to the archive.\n// Default value is true (move to the archive).\noptions\n.\nsetRemoveFromDataStore\n(\nfalse\n);\n// Schedules archiving of the specified data sets. Archiving itself is executed asynchronously.\nv3\n.\narchiveDataSets\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nid1\n,\nid2\n),\noptions\n);\n## System\n.\nout\n.\nprintln\n(\n\"Archiving scheduled\"\n);\n}\n}\nV3ArchiveDataSetsExample.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/dataset/id/DataSetPermId\"\n,\n\"as/dto/dataset/archive/DataSetArchiveOptions\"\n],\nfunction\n(\nopenbis\n,\nDataSetPermId\n,\nDataSetArchiveOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nid1\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2266\"\n)\nvar\nid2\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2267\"\n)\nvar\noptions\n=\nnew\nDataSetArchiveOptions\n();\n// With removeFromDataStore flag set to true data sets are moved to the archive.\n// With removeFromDataStore flag set to false data sets are copied to the archive.\n// Default value is true (move to the archive).\noptions\n.\nsetRemoveFromDataStore\n(\nfalse\n);\n// Schedules archiving of the specified data sets. Archiving itself is executed asynchronously.\nv3\n.\narchiveDataSets\n([\nid1\n,\nid2\n],\noptions\n).\ndone\n(\nfunction\n()\n{\nalert\n(\n\"Archiving scheduled\"\n);\n});\n});\n});\n</\nscript\n>\nUnarchiving data sets\n\n## Example\n\nV3UnarchiveDataSetsExample.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.IDataSetId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.unarchive.DataSetUnarchiveOptions\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3UnarchiveDataSetsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nIDataSetId\nid1\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2266\"\n);\nIDataSetId\nid2\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2267\"\n);\nDataSetUnarchiveOptions\noptions\n=\nnew\nDataSetUnarchiveOptions\n();\n// Schedules unarchiving of the specified data sets. Unarchiving itself is executed asynchronously.\nv3\n.\nunarchiveDataSets\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nid1\n,\nid2\n),\noptions\n);\n## System\n.\nout\n.\nprintln\n(\n\"Unarchiving scheduled\"\n);\n}\n}\nV3UnarchiveDataSetsExample.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/dataset/id/DataSetPermId\"\n,\n\"as/dto/dataset/unarchive/DataSetUnarchiveOptions\"\n],\nfunction\n(\nopenbis\n,\nDataSetPermId\n,\nDataSetUnarchiveOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nid1\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2266\"\n)\nvar\nid2\n=\nnew\nDataSetPermId\n(\n\"20160524154020607-2267\"\n)\nvar\noptions\n=\nnew\nDataSetUnarchiveOptions\n();\n// Schedules unarchiving of the specified data sets. Unarchiving itself is executed asynchronously.\nv3\n.\nunarchiveDataSets\n([\nid1\n,\nid2\n],\noptions\n).\ndone\n(\nfunction\n()\n{\nalert\n(\n\"Unarchiving scheduled\"\n);\n});\n});\n});\n</\nscript\n>\n## Executing Operations\n\nThe V3 API provides you with methods that allow you to create, update,\nget, search and delete entities, archive and unarchive datasets, execute\ncustom services and much more. With these methods you can\nprogrammatically access most of the openBIS features to build your own\nwebapps, dropboxes or services. Even though these methods are quite\ndifferent, there are some things that they all have in common:\neach method is executed in its own separate transaction\neach method is executed synchronously\nLet’s think about what it really means. Separate transactions make two\n(even subsequent) method calls completely unrelated. For instance, when\nyou make a call to create experiments and then another call to create\nsamples, then even if the sample creation fails the experiments, that\nhad been already created, would remain in the system. Most of the time\nthis is exactly what we want but not always. There are times when we\nwould like to create either both experiments and samples or nothing if\nsomething is wrong. A good example would be an import of some file that\ncontains both experiments and samples. We would like to be able to\nimport the file, fail if it is wrong, correct the file and import it\nagain. With separate transactions we would end up with some things\nalready created after the first failed import and we wouldn’t be able to\nreimport the corrected file again as some things would be already in the\nsystem.\nSynchronous method execution is also something what we expect most of\nthe time. You call a method and it returns once all the work is done.\nFor instance, when we call a method to create samples we know that once\nthe method finishes all the samples have been created in the\nsystem. This makes perfect sense when we need to execute operations that\ndepend on each other, e.g. we can create data sets and attach them to\nsamples only after the samples had been created. Just as with the\nseparate transactions, there are cases when synchronous method execution\nis limiting. Let’s use the file import example again. What would happen\nif a file we wanted to import contained hundreds of thousands of\nentities? The import would probably take a very long time. Our\nsynchronous method call would not return until all the entities have\nbeen created which means we would also block a script/program that makes\nthis method call for a very long time. We could of course create a\nseparate thread in our script/program to overcome this problem but that\nwould add up more complexity. It would be also nice to notify a user\nonce such an operation finishes or fails, e.g. by sending an email.\nUnfortunately that would mean we have to keep our script/program running\nuntil the operation finishes or fails to send such an email. What about\na progress information for running executions or a history of previous\noperations and their results? That would be nice but it would increase\nthe complexity of our script/program even more.\n## Therefore, if you want to:\nexecute multiple operations in a single transaction\nexecute operations asynchronously\nmonitor progress of operations\nreceive notifications about finished/failed operations\nkeep history of operations and their results\n## you should use:\nexecuteOperations method to execute your operations\ngetOperationExecutions and searchOperationExecutions methods to\nretrieve information about operation executions (e.g. progress,\nresults or errors)\nupdateOperationExecutions and deleteOperationExecutions methods to\ncontrol what information should be still kept for a given operation\nexecution and what information can be already removed\nMore details on each of these methods in presented in the sections\nbelow. Please note that all of the described methods are available in\nboth Javascript and Java.\nMethod executeOperations\n\nThis method can be used to execute one or many operations either\nsynchronously or asynchronously. Operations are always executed in a\nsingle transaction (a failure of a single operation triggers a rollback\nof all the operations). The executeOperations method can be used to\nexecute any of the IApplicationServerApi methods (except for\nlogin/logout and executeOperations itself), i.e. for each\nIApplicationServerApi method there is a corresponding operation class\n(class that implements IOperation interface). For instance,\nIApplicationServerApi.createSpaces method is represented by\nCreateSpacesOperation class, IApplicationServerApi.updateSpaces method\nby UpdateSpacesOperation class etc.\nAsynchronous operation execution\n\nAn asynchronous executeOperations invocation only schedules operations\nfor the execution and then immediately returns. Results of the scheduled\noperations can be retrieved later with getOperationExecutions or\nsearchOperationExecutions methods.\nBecause the operations are scheduled to be executed later (in a separate\nthread) a regular try/catch block around executeOperations method will\nonly catch exceptions related with scheduling the operations for the\nexecution, but NOT the exceptions thrown by the operations during the\nexecution. To check for errors that occurred during the execution please\nuse getOperationExecutions and searchOperationExecutions methods once\nthe execution finishes.\nIn order to execute operations asynchronously, executeOperations has to\nbe used with AsynchronousOperationExecutionOptions. With such options,\nthe method returns AsynchronousOperationExecutionResults object.\nAsynchronousOperationExecutionResults object contains automatically\ngenerated executionId that can be used for retrieving additional\ninformation about the execution, fetching the results or errors.\nDuring its life an asynchronous execution goes through the following\n## states:\nNEW - execution has been just created with executeOperations method\nSCHEDULED - execution has been added to a thread pool queue and is\nwaiting for a free thread\nRUNNING - execution has been picked from a thread pool queue by a\nfree thread and is currently executing\nFINISHED/FAILED - if execution finishes successfully then execution\nstate is changed to FINISHED, if anything goes wrong it is changed\nto FAILED\nV3ExecuteOperationsAsynchronous.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3ExecuteOperationsAsynchronous\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nCreateSamplesOperation\noperation\n=\nnew\nCreateSamplesOperation\n(\nsample\n);\nAsynchronousOperationExecutionResults\nresults\n=\n(\nAsynchronousOperationExecutionResults\n)\nv3\n.\nexecuteOperations\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noperation\n),\nnew\nAsynchronousOperationExecutionOptions\n());\n## System\n.\nout\n.\nprintln\n(\n\"Execution id: \"\n+\nresults\n.\ngetExecutionId\n());\n}\n}\nV3ExecuteOperationsAsynchronous.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/sample/create/CreateSamplesOperation\"\n,\n\"as/dto/operation/AsynchronousOperationExecutionOptions\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nCreateSamplesOperation\n,\nAsynchronousOperationExecutionOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nvar\noperation\n=\nnew\nCreateSamplesOperation\n([\nsample\n]);\nv3\n.\nexecuteOperations\n([\noperation\n],\nnew\nAsynchronousOperationExecutionOptions\n()).\ndone\n(\nfunction\n(\nresults\n)\n{\nconsole\n.\nlog\n(\n\"Execution id: \"\n+\nresults\n.\ngetExecutionId\n());\n});\n});\n</\nscript\n>\nSynchronous operation execution\n\nA synchronous executeOperations invocation immediately executes all the\noperations. Any exceptions thrown by the executed operations can be\ncaught with a regular try/catch block around executeOperations method.\nIn order to execute operations synchronously, executeOperations has to\nbe used with SynchronousOperationExecutionOptions. With such options,\nthe method returns SynchronousOperationExecutionResults object.\nSynchronousOperationExecutionResults object contains the results for all\nthe executed operations.\nIn contrast to the asynchronous version, the synchronous call requires\nexecutionId to be explicitly set in SynchronousOperationExecutionOptions\nfor the additional information to be gathered about the execution.\nDuring its life a synchronous execution goes through the following\n## states:\nNEW - execution has been just created with executeOperations method\nRUNNING - execution is being executed by the same thread as\nexecuteOperations method\nFINISHED/FAILED - if execution finishes successfully then execution\nstate is changed to FINISHED, if anything goes wrong it is changed\nto FAILED\nV3ExecuteOperationsSynchronous.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.SynchronousOperationExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.SynchronousOperationExecutionResults\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperationResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3ExecuteOperationsSynchronous\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nCreateSamplesOperation\noperation\n=\nnew\nCreateSamplesOperation\n(\nsample\n);\nSynchronousOperationExecutionResults\nresults\n=\n(\nSynchronousOperationExecutionResults\n)\nv3\n.\nexecuteOperations\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noperation\n),\nnew\nSynchronousOperationExecutionOptions\n());\nCreateSamplesOperationResult\nresult\n=\n(\nCreateSamplesOperationResult\n)\nresults\n.\ngetResults\n().\nget\n(\n0\n);\n## System\n.\nout\n.\nprintln\n(\n\"Sample id: \"\n+\nresult\n.\ngetObjectIds\n());\n}\n}\nV3ExecuteOperationsSynchronous.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/sample/create/CreateSamplesOperation\"\n,\n\"as/dto/operation/SynchronousOperationExecutionOptions\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nCreateSamplesOperation\n,\nSynchronousOperationExecutionOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nvar\noperation\n=\nnew\nCreateSamplesOperation\n([\nsample\n]);\nv3\n.\nexecuteOperations\n([\noperation\n],\nnew\nSynchronousOperationExecutionOptions\n()).\ndone\n(\nfunction\n(\nresults\n)\n{\nvar\nresult\n=\nresults\n.\ngetResults\n()[\n0\n];\nconsole\n.\nlog\n(\n\"Sample id: \"\n+\nresult\n.\ngetObjectIds\n());\n});\n});\n</\nscript\n>\n## Notifications\n\nThe executeOperations method can notify about finished or failed\noperation executions. At the moment the only supported notification\nmethod is email (OperationExecutionEmailNotification).\nFor successfully finished executions an email contains:\nexecution id\nexecution description\nlist of operation summaries and operation results\nFor failed executions an email contains:\nexecution id\nexecution description\nlist of operation summaries\nerror\nV3ExecuteOperationsEmailNotification.java\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecutionEmailNotification\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3ExecuteOperationsEmailNotification\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nCreateSamplesOperation\noperation\n=\nnew\nCreateSamplesOperation\n(\nsample\n);\nAsynchronousOperationExecutionOptions\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\noptions\n.\nsetNotification\n(\nnew\nOperationExecutionEmailNotification\n(\n\"my@email1.com\"\n,\n\"my@email2.com\"\n));\nAsynchronousOperationExecutionResults\nresults\n=\n(\nAsynchronousOperationExecutionResults\n)\nv3\n.\nexecuteOperations\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noperation\n),\noptions\n);\n## System\n.\nout\n.\nprintln\n(\n\"Execution id: \"\n+\nresults\n.\ngetExecutionId\n());\n}\n}\nV3ExecuteOperationsEmailNotification.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/sample/create/CreateSamplesOperation\"\n,\n\"as/dto/operation/AsynchronousOperationExecutionOptions\"\n,\n\"as/dto/operation/OperationExecutionEmailNotification\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nCreateSamplesOperation\n,\nAsynchronousOperationExecutionOptions\n,\nOperationExecutionEmailNotification\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nvar\noperation\n=\nnew\nCreateSamplesOperation\n([\nsample\n]);\nvar\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\noptions\n.\nsetNotification\n(\nnew\nOperationExecutionEmailNotification\n([\n\"my@email1.com\"\n,\n\"my@email2.com\"\n]));\nv3\n.\nexecuteOperations\n([\noperation\n],\noptions\n).\ndone\n(\nfunction\n(\nresults\n)\n{\nconsole\n.\nlog\n(\n\"Execution id: \"\n+\nresults\n.\ngetExecutionId\n());\n});\n});\n</\nscript\n>\nMethod getOperationExecutions / searchOperationExecutions\n\nOperation execution information can be fetched by an owner of an\nexecution (i.e. a person that called executeOperations method) or an\nadmin. Both getOperationExecutions and searchOperationExecutions methods\nwork similar to the other get/search methods in the V3 API.\nThe operation execution information that both methods return can be\ndivided into 3 categories:\nbasic information (code, state, owner, description, creationDate,\nstartDate, finishDate etc.)\nsummary information (summary of operations, progress, error,\nresults)\ndetailed information (details of operations, progress, error,\nresults)\nEach category can have a different availability time (i.e. time for how\nlong a given information is stored in the system). The availability\ntimes can be set via the executeOperations method options (both\nSynchronousOperationExecutionOptions and\n## AsynchronousOperationExecutionOptions):\nbasic information (setAvailabilityTime)\nsummary information (setSummaryAvailabilityTime)\ndetailed information (setDetailsAvailabilityTime)\nIf the times are not explicitly set, then the following defaults are\n## used:\nbasic information (1 year)\nsummary information (1 month)\ndetailed information (1 day)\nThe current availability of each category can be checked with\ngetAvailability, getSummaryAvailability, getDetailsAvailability methods\nof OperationExecution class. The availability can have one of the\n## following values:\nAVAILABLE - an information is available and can be fetched\nDELETE_PENDING - an explicit request to delete the information has\nbeen made with updateOperationExecutions or\ndeleteOperationExecutions method\nDELETED - an explicit request to delete the information has been\nprocessed and the information has been deleted\nTIME_OUT_PENDING - an availability time has expired, the\ninformation has been scheduled to be removed\nTIMED_OUT - an availability time has expired, the information has\nbeen removed\nUpdate of availability values and deletion of operation execution\nrelated information are done with two separate V3 maintenance tasks\n(please check service.properties for their configuration).\nV3GetOperationExecutionsAsynchronous.java\nimport\njava.util.Arrays\n;\nimport\njava.util.Map\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3GetOperationExecutionsAsynchronous\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nCreateSamplesOperation\noperation\n=\nnew\nCreateSamplesOperation\n(\nsample\n);\n// Asynchronous execution: information about an asynchronous operation execution is always gathered, the executionId\n// is also always automatically generated and returned with AsynchronousOperationExecutionResults.\nAsynchronousOperationExecutionOptions\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\n// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.\n// Availability times should be specified in seconds.\noptions\n.\nsetAvailabilityTime\n(\n30\n*\n24\n*\n60\n*\n60\n);\n// one month\noptions\n.\nsetSummaryAvailabilityTime\n(\n24\n*\n60\n*\n60\n);\n// one day\noptions\n.\nsetDetailsAvailabilityTime\n(\n60\n*\n60\n);\n// one hour\n// Execute operation\nAsynchronousOperationExecutionResults\nresults\n=\n(\nAsynchronousOperationExecutionResults\n)\nv3\n.\nexecuteOperations\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noperation\n),\noptions\n);\n// It is an asynchronous execution. It might be still waiting for a free thread,\n// it may be already executing or it may have already finished. It does not matter.\n// We can already fetch the information about it.\n// Specify what information to fetch about the execution\nOperationExecutionFetchOptions\nfo\n=\nnew\nOperationExecutionFetchOptions\n();\nfo\n.\nwithSummary\n();\nfo\n.\nwithSummary\n().\nwithOperations\n();\nfo\n.\nwithSummary\n().\nwithProgress\n();\nfo\n.\nwithSummary\n().\nwithResults\n();\nfo\n.\nwithSummary\n().\nwithError\n();\nfo\n.\nwithDetails\n();\nfo\n.\nwithDetails\n().\nwithOperations\n();\nfo\n.\nwithDetails\n().\nwithProgress\n();\nfo\n.\nwithDetails\n().\nwithResults\n();\nfo\n.\nwithDetails\n().\nwithError\n();\n// Get information about the execution\n## Map\n<\nIOperationExecutionId\n,\nOperationExecution\n>\nexecutions\n=\nv3\n.\ngetOperationExecutions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nresults\n.\ngetExecutionId\n()),\nfo\n);\nOperationExecution\nexecution\n=\nexecutions\n.\nget\n(\nresults\n.\ngetExecutionId\n());\n// Summary contains String representation of operations, progress, results and error\n## String\nsummaryOperation\n=\nexecution\n.\ngetSummary\n().\ngetOperations\n().\nget\n(\n0\n);\n## System\n.\nout\n.\nprintln\n(\n\"Summary.operation: \"\n+\nsummaryOperation\n);\n## System\n.\nout\n.\nprintln\n(\n\"Summary.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\n## System\n.\nout\n.\nprintln\n(\n\"Summary.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\n## System\n.\nout\n.\nprintln\n(\n\"Summary.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n// Details contain object representation of operations, progress, results and error\nCreateSamplesOperation\ndetailsOperation\n=\n(\nCreateSamplesOperation\n)\nexecution\n.\ngetDetails\n().\ngetOperations\n().\nget\n(\n0\n);\n## System\n.\nout\n.\nprintln\n(\n\"Details.operation: \"\n+\ndetailsOperation\n);\n## System\n.\nout\n.\nprintln\n(\n\"Details.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\n## System\n.\nout\n.\nprintln\n(\n\"Details.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\n## System\n.\nout\n.\nprintln\n(\n\"Details.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n}\n}\nV3GetOperationExecutionsAsynchronous.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/sample/create/CreateSamplesOperation\"\n,\n\"as/dto/operation/AsynchronousOperationExecutionOptions\"\n,\n\"as/dto/operation/fetchoptions/OperationExecutionFetchOptions\"\n,\n\"as/dto/operation/id/OperationExecutionPermId\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nCreateSamplesOperation\n,\nAsynchronousOperationExecutionOptions\n,\nOperationExecutionFetchOptions\n,\nOperationExecutionPermId\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nvar\noperation\n=\nnew\nCreateSamplesOperation\n([\nsample\n]);\n// Asynchronous execution: information about an asynchronous operation execution is always gathered, the executionId\n// is also always automatically generated and returned with AsynchronousOperationExecutionResults.\nvar\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\n// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.\n// Availability times should be specified in seconds.\noptions\n.\nsetAvailabilityTime\n(\n30\n*\n24\n*\n60\n*\n60\n);\n// one month\noptions\n.\nsetSummaryAvailabilityTime\n(\n24\n*\n60\n*\n60\n);\n// one day\noptions\n.\nsetDetailsAvailabilityTime\n(\n60\n*\n60\n);\n// one hour\n// Execute operation\nv3\n.\nexecuteOperations\n([\noperation\n],\noptions\n).\ndone\n(\nfunction\n(\nresults\n)\n{\n// It is an asynchronous execution. It might be still waiting for a free thread,\n// it may be already executing or it may have already finished. It does not matter.\n// We can already fetch the information about it.\n// Specify what information to fetch about the execution\nvar\nfo\n=\nnew\nOperationExecutionFetchOptions\n();\nfo\n.\nwithSummary\n();\nfo\n.\nwithSummary\n().\nwithOperations\n();\nfo\n.\nwithSummary\n().\nwithProgress\n();\nfo\n.\nwithSummary\n().\nwithResults\n();\nfo\n.\nwithSummary\n().\nwithError\n();\nfo\n.\nwithDetails\n();\nfo\n.\nwithDetails\n().\nwithOperations\n();\nfo\n.\nwithDetails\n().\nwithProgress\n();\nfo\n.\nwithDetails\n().\nwithResults\n();\nfo\n.\nwithDetails\n().\nwithError\n();\n// Get information about the execution\nv3\n.\ngetOperationExecutions\n([\nresults\n.\ngetExecutionId\n()\n],\nfo\n).\ndone\n(\nfunction\n(\nexecutions\n)\n{\nvar\nexecution\n=\nexecutions\n[\nresults\n.\ngetExecutionId\n()];\n// Summary contains String representation of operations, progress, results and error\nvar\nsummaryOperation\n=\nexecution\n.\ngetSummary\n().\ngetOperations\n()[\n0\n];\nconsole\n.\nlog\n(\n\"Summary.operation: \"\n+\nsummaryOperation\n);\nconsole\n.\nlog\n(\n\"Summary.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\nconsole\n.\nlog\n(\n\"Summary.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\nconsole\n.\nlog\n(\n\"Summary.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n// Details contain object representation of operations, progress, results and error\nvar\ndetailsOperation\n=\nexecution\n.\ngetDetails\n().\ngetOperations\n()[\n0\n];\nconsole\n.\nlog\n(\n\"Details.operation: \"\n+\ndetailsOperation\n);\nconsole\n.\nlog\n(\n\"Details.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\nconsole\n.\nlog\n(\n\"Details.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\nconsole\n.\nlog\n(\n\"Details.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n});\n});\n});\n</\nscript\n>\nV3GetOperationExecutionsSynchronous.java\nimport\njava.util.Arrays\n;\nimport\njava.util.Map\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.SynchronousOperationExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.OperationExecutionPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3GetOperationExecutionsSynchronous\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE_7\"\n);\nCreateSamplesOperation\noperation\n=\nnew\nCreateSamplesOperation\n(\nsample\n);\n// Synchronous execution: to gather information about a synchronous operation execution, the executionId has to\n// be explicitly set in the options object. OperationExecutionPermId created with no-argument constructor automatically\n// generates a random permId value.\nSynchronousOperationExecutionOptions\noptions\n=\nnew\nSynchronousOperationExecutionOptions\n();\noptions\n.\nsetExecutionId\n(\nnew\nOperationExecutionPermId\n());\n// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.\n// Availability times should be specified in seconds.\noptions\n.\nsetAvailabilityTime\n(\n30\n*\n24\n*\n60\n*\n60\n);\n// one month\noptions\n.\nsetSummaryAvailabilityTime\n(\n24\n*\n60\n*\n60\n);\n// one day\noptions\n.\nsetDetailsAvailabilityTime\n(\n60\n*\n60\n);\n// one hour\n// Execute operation\nv3\n.\nexecuteOperations\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noperation\n),\noptions\n);\n// Specify what information to fetch about the execution\nOperationExecutionFetchOptions\nfo\n=\nnew\nOperationExecutionFetchOptions\n();\nfo\n.\nwithSummary\n();\nfo\n.\nwithSummary\n().\nwithOperations\n();\nfo\n.\nwithSummary\n().\nwithProgress\n();\nfo\n.\nwithSummary\n().\nwithResults\n();\nfo\n.\nwithSummary\n().\nwithError\n();\nfo\n.\nwithDetails\n();\nfo\n.\nwithDetails\n().\nwithOperations\n();\nfo\n.\nwithDetails\n().\nwithProgress\n();\nfo\n.\nwithDetails\n().\nwithResults\n();\nfo\n.\nwithDetails\n().\nwithError\n();\n// Get information about the execution\n## Map\n<\nIOperationExecutionId\n,\nOperationExecution\n>\nexecutions\n=\nv3\n.\ngetOperationExecutions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noptions\n.\ngetExecutionId\n()),\nfo\n);\nOperationExecution\nexecution\n=\nexecutions\n.\nget\n(\noptions\n.\ngetExecutionId\n());\n// Summary contains String representation of operations, progress, results and error\n## String\nsummaryOperation\n=\nexecution\n.\ngetSummary\n().\ngetOperations\n().\nget\n(\n0\n);\n## System\n.\nout\n.\nprintln\n(\n\"Summary.operation: \"\n+\nsummaryOperation\n);\n## System\n.\nout\n.\nprintln\n(\n\"Summary.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\n## System\n.\nout\n.\nprintln\n(\n\"Summary.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\n## System\n.\nout\n.\nprintln\n(\n\"Summary.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n// Details contain object representation of operations, progress, results and error\nCreateSamplesOperation\ndetailsOperation\n=\n(\nCreateSamplesOperation\n)\nexecution\n.\ngetDetails\n().\ngetOperations\n().\nget\n(\n0\n);\n## System\n.\nout\n.\nprintln\n(\n\"Details.operation: \"\n+\ndetailsOperation\n);\n## System\n.\nout\n.\nprintln\n(\n\"Details.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\n## System\n.\nout\n.\nprintln\n(\n\"Details.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\n## System\n.\nout\n.\nprintln\n(\n\"Details.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n}\n}\nV3GetOperationExecutionsSynchronous.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/sample/create/CreateSamplesOperation\"\n,\n\"as/dto/operation/SynchronousOperationExecutionOptions\"\n,\n\"as/dto/operation/fetchoptions/OperationExecutionFetchOptions\"\n,\n\"as/dto/operation/id/OperationExecutionPermId\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nCreateSamplesOperation\n,\nSynchronousOperationExecutionOptions\n,\nOperationExecutionFetchOptions\n,\nOperationExecutionPermId\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nvar\noperation\n=\nnew\nCreateSamplesOperation\n([\nsample\n]);\n// Synchronous execution: to gather information about a synchronous operation execution, the executionId has to\n// be explicitly set in the options object. OperationExecutionPermId created with no-argument constructor automatically\n// generates a random permId value.\nvar\noptions\n=\nnew\nSynchronousOperationExecutionOptions\n();\noptions\n.\nsetExecutionId\n(\nnew\nOperationExecutionPermId\n());\n// Both synchronous and asynchronous executions: default availability times can be overwritten using the options object.\n// Availability times should be specified in seconds.\noptions\n.\nsetAvailabilityTime\n(\n30\n*\n24\n*\n60\n*\n60\n);\n// one month\noptions\n.\nsetSummaryAvailabilityTime\n(\n24\n*\n60\n*\n60\n);\n// one day\noptions\n.\nsetDetailsAvailabilityTime\n(\n60\n*\n60\n);\n// one hour\n// Execute operation\nv3\n.\nexecuteOperations\n([\noperation\n],\noptions\n).\ndone\n(\nfunction\n()\n{\n// Specify what information to fetch about the execution\nvar\nfo\n=\nnew\nOperationExecutionFetchOptions\n();\nfo\n.\nwithSummary\n();\nfo\n.\nwithSummary\n().\nwithOperations\n();\nfo\n.\nwithSummary\n().\nwithProgress\n();\nfo\n.\nwithSummary\n().\nwithResults\n();\nfo\n.\nwithSummary\n().\nwithError\n();\nfo\n.\nwithDetails\n();\nfo\n.\nwithDetails\n().\nwithOperations\n();\nfo\n.\nwithDetails\n().\nwithProgress\n();\nfo\n.\nwithDetails\n().\nwithResults\n();\nfo\n.\nwithDetails\n().\nwithError\n();\n// Get information about the execution\nv3\n.\ngetOperationExecutions\n([\noptions\n.\ngetExecutionId\n()\n],\nfo\n).\ndone\n(\nfunction\n(\nexecutions\n)\n{\nvar\nexecution\n=\nexecutions\n[\noptions\n.\ngetExecutionId\n()];\n// Summary contains String representation of operations, progress, results and error\nvar\nsummaryOperation\n=\nexecution\n.\ngetSummary\n().\ngetOperations\n()[\n0\n];\nconsole\n.\nlog\n(\n\"Summary.operation: \"\n+\nsummaryOperation\n);\nconsole\n.\nlog\n(\n\"Summary.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\nconsole\n.\nlog\n(\n\"Summary.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\nconsole\n.\nlog\n(\n\"Summary.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n// Details contain object representation of operations, progress, results and error\nvar\ndetailsOperation\n=\nexecution\n.\ngetDetails\n().\ngetOperations\n()[\n0\n];\nconsole\n.\nlog\n(\n\"Details.operation: \"\n+\ndetailsOperation\n);\nconsole\n.\nlog\n(\n\"Details.progress: \"\n+\nexecution\n.\ngetSummary\n().\ngetProgress\n());\nconsole\n.\nlog\n(\n\"Details.results: \"\n+\nexecution\n.\ngetSummary\n().\ngetResults\n());\nconsole\n.\nlog\n(\n\"Details.error: \"\n+\nexecution\n.\ngetSummary\n().\ngetError\n());\n});\n});\n});\n</\nscript\n>\nMethod updateOperationExecutions / deleteOperationExecutions\n\nThe updateOperationExecutions and deleteOperationExecutions methods can\nbe used to explicitly delete some part of information or delete all the\ninformation about a given operation execution before a corresponding\navailability time expires.\nV3UpdateOperationExecutions.java\nimport\njava.util.Arrays\n;\nimport\njava.util.Map\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.update.OperationExecutionUpdate\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3UpdateOperationExecutions\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nCreateSamplesOperation\noperation\n=\nnew\nCreateSamplesOperation\n(\nsample\n);\nAsynchronousOperationExecutionOptions\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\n// Execute operation\nAsynchronousOperationExecutionResults\nresults\n=\n(\nAsynchronousOperationExecutionResults\n)\nv3\n.\nexecuteOperations\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noperation\n),\noptions\n);\n// You can explicitly request a deletion of summary or details. Here we want to delete details.\nOperationExecutionUpdate\nupdate\n=\nnew\nOperationExecutionUpdate\n();\nupdate\n.\nsetExecutionId\n(\nresults\n.\ngetExecutionId\n());\nupdate\n.\ndeleteDetails\n();\nv3\n.\nupdateOperationExecutions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nupdate\n));\n// Let's check the execution information\nOperationExecutionFetchOptions\nfo\n=\nnew\nOperationExecutionFetchOptions\n();\nfo\n.\nwithSummary\n();\nfo\n.\nwithDetails\n();\n## Map\n<\nIOperationExecutionId\n,\nOperationExecution\n>\nexecutions\n=\nv3\n.\ngetOperationExecutions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nresults\n.\ngetExecutionId\n()),\nfo\n);\nOperationExecution\nexecution\n=\nexecutions\n.\nget\n(\nresults\n.\ngetExecutionId\n());\n// Summary availability is AVAILABLE. Details availability is either DELETE_PENDING or DELETED\n// depending on whether a maintenance task has already processed the deletion request.\n## System\n.\nout\n.\nprintln\n(\n## \"Summary: \"\n+\nexecution\n.\ngetSummary\n());\n## System\n.\nout\n.\nprintln\n(\n\"Summary.availability: \"\n+\nexecution\n.\ngetSummaryAvailability\n());\n## System\n.\nout\n.\nprintln\n(\n## \"Details: \"\n+\nexecution\n.\ngetDetails\n());\n## System\n.\nout\n.\nprintln\n(\n\"Details.availability: \"\n+\nexecution\n.\ngetDetailsAvailability\n());\n}\n}\nV3UpdateOperationExecutions.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/sample/create/CreateSamplesOperation\"\n,\n\"as/dto/operation/AsynchronousOperationExecutionOptions\"\n,\n\"as/dto/operation/update/OperationExecutionUpdate\"\n,\n\"as/dto/operation/fetchoptions/OperationExecutionFetchOptions\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nCreateSamplesOperation\n,\nAsynchronousOperationExecutionOptions\n,\nOperationExecutionUpdate\n,\nOperationExecutionFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nvar\noperation\n=\nnew\nCreateSamplesOperation\n([\nsample\n]);\nvar\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\n// Execute operation\nv3\n.\nexecuteOperations\n([\noperation\n],\noptions\n).\ndone\n(\nfunction\n(\nresults\n)\n{\n// You can explicitly request a deletion of summary or details. Here we want to delete details.\nvar\nupdate\n=\nnew\nOperationExecutionUpdate\n();\nupdate\n.\nsetExecutionId\n(\nresults\n.\ngetExecutionId\n());\nupdate\n.\ndeleteDetails\n();\nv3\n.\nupdateOperationExecutions\n([\nupdate\n]).\ndone\n(\nfunction\n()\n{\n// Let's check the execution information\nvar\nfo\n=\nnew\nOperationExecutionFetchOptions\n();\nfo\n.\nwithSummary\n();\nfo\n.\nwithDetails\n();\nv3\n.\ngetOperationExecutions\n([\nresults\n.\ngetExecutionId\n()\n],\nfo\n).\ndone\n(\nfunction\n(\nexecutions\n)\n{\nvar\nexecution\n=\nexecutions\n[\nresults\n.\ngetExecutionId\n()];\n// Summary availability is AVAILABLE. Details availability is either DELETE_PENDING or DELETED\n// depending on whether a maintenance task has already processed the deletion request.\nconsole\n.\nlog\n(\n## \"Summary: \"\n+\nexecution\n.\ngetSummary\n());\nconsole\n.\nlog\n(\n\"Summary.availability: \"\n+\nexecution\n.\ngetSummaryAvailability\n());\nconsole\n.\nlog\n(\n## \"Details: \"\n+\nexecution\n.\ngetDetails\n());\nconsole\n.\nlog\n(\n\"Details.availability: \"\n+\nexecution\n.\ngetDetailsAvailability\n());\n});\n});\n});\n});\n</\nscript\n>\nV3DeleteOperationExecutions.java\nimport\njava.util.Arrays\n;\nimport\njava.util.Map\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.experiment.id.ExperimentIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.AsynchronousOperationExecutionResults\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.OperationExecution\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.delete.OperationExecutionDeletionOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.fetchoptions.OperationExecutionFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.operation.id.IOperationExecutionId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.CreateSamplesOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.create.SampleCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.id.SpacePermId\n;\npublic\nclass\nV3DeleteOperationExecutions\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nSampleCreation\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nCreateSamplesOperation\noperation\n=\nnew\nCreateSamplesOperation\n(\nsample\n);\nAsynchronousOperationExecutionOptions\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\n// Execute operation\nAsynchronousOperationExecutionResults\nresults\n=\n(\nAsynchronousOperationExecutionResults\n)\nv3\n.\nexecuteOperations\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\noperation\n),\noptions\n);\n// Explicitly request a deletion of all the information about the execution\nOperationExecutionDeletionOptions\ndeletionOptions\n=\nnew\nOperationExecutionDeletionOptions\n();\ndeletionOptions\n.\nsetReason\n(\n\"test reason\"\n);\nv3\n.\ndeleteOperationExecutions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nresults\n.\ngetExecutionId\n()),\ndeletionOptions\n);\n// Let's check whether the execution information is still available\n## Map\n<\nIOperationExecutionId\n,\nOperationExecution\n>\nexecutions\n=\nv3\n.\ngetOperationExecutions\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nresults\n.\ngetExecutionId\n()),\nnew\nOperationExecutionFetchOptions\n());\nOperationExecution\nexecution\n=\nexecutions\n.\nget\n(\nresults\n.\ngetExecutionId\n());\n// Depending on whether a maintenance task has already processed the deletion request\n// the execution will be either null or the returned execution availability will be DELETE_PENDING.\n## System\n.\nout\n.\nprintln\n(\n## \"Availability: \"\n+\n(\nexecution\n!=\nnull\n?\nexecution\n.\ngetAvailability\n()\n## :\nnull\n));\n}\n}\nV3DeleteOperationExecutions.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"as/dto/sample/create/SampleCreation\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/space/id/SpacePermId\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/sample/create/CreateSamplesOperation\"\n,\n\"as/dto/operation/AsynchronousOperationExecutionOptions\"\n,\n\"as/dto/operation/delete/OperationExecutionDeletionOptions\"\n,\n\"as/dto/operation/fetchoptions/OperationExecutionFetchOptions\"\n],\nfunction\n(\nopenbis\n,\nSampleCreation\n,\nEntityTypePermId\n,\nSpacePermId\n,\nExperimentIdentifier\n,\nCreateSamplesOperation\n,\nAsynchronousOperationExecutionOptions\n,\nOperationExecutionDeletionOptions\n,\nOperationExecutionFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nsample\n=\nnew\nSampleCreation\n();\nsample\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"MY_SAMPLE_TYPE_CODE\"\n));\nsample\n.\nsetSpaceId\n(\nnew\nSpacePermId\n(\n## \"MY_SPACE_CODE\"\n));\nsample\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n\"/MY_SPACE_CODE/MY_PROJECT_CODE/MY_EXPERIMENT_CODE\"\n));\nsample\n.\nsetCode\n(\n## \"MY_SAMPLE_CODE\"\n);\nvar\noperation\n=\nnew\nCreateSamplesOperation\n([\nsample\n]);\nvar\noptions\n=\nnew\nAsynchronousOperationExecutionOptions\n();\n// Execute operation\nv3\n.\nexecuteOperations\n([\noperation\n],\noptions\n).\ndone\n(\nfunction\n(\nresults\n)\n{\n// Explicitly request a deletion of all the information about the execution\nvar\ndeletionOptions\n=\nnew\nOperationExecutionDeletionOptions\n();\ndeletionOptions\n.\nsetReason\n(\n\"test reason\"\n);\nv3\n.\ndeleteOperationExecutions\n([\nresults\n.\ngetExecutionId\n()\n],\ndeletionOptions\n).\ndone\n(\nfunction\n()\n{\n// Let's check whether the execution information is still available\nv3\n.\ngetOperationExecutions\n([\nresults\n.\ngetExecutionId\n()\n],\nnew\nOperationExecutionFetchOptions\n()).\ndone\n(\nfunction\n(\nexecutions\n)\n{\nvar\nexecution\n=\nexecutions\n[\nresults\n.\ngetExecutionId\n()];\n// Depending on whether a maintenance task has already processed the deletion request\n// the execution will be either null or the returned execution availability will be DELETE_PENDING.\nconsole\n.\nlog\n(\n## \"Availability: \"\n+\n(\nexecution\n!=\nnull\n?\nexecution\n.\ngetAvailability\n()\n## :\nnull\n));\n});\n});\n});\n});\n</\nscript\n>\n### Configuration\n\nMany aspects of the operation execution behavior can be configured via\nservice.properties file.\nMore details on what exactly can be configured can be found in the file\nitself.\n## Semantic Annotations\n\nIf terms like: semantic web, RDF, OWL are new to you, then it is highly\nrecommended to read the following tutorial first:\nhttp://www.linkeddatatools.com/semantic-web-basics\n.\nIn short: semantic annotations allow you to define a meaning for openBIS\nsample types, property types and sample property assignments by the\nmeans of ontology terms. This, together with standards like “Dublin\n## Core” (\nhttp://dublincore.org/\n) can help you integrate openBIS with\nother systems and exchange data between them with a well defined meaning\neasily.\nTo describe a meaning of a single sample type, property type or sample\nproperty assignment a collection of semantic annotations can be used.\nTherefore, for instance, you can use one annotation to describe a\ngeneral meaning of a property and another one to describe a unit that is\nused for its values.\nIn order to make the openBIS configuration easier to maintain sample\nproperty assignments inherit semantic annotations from a corresponding\nproperty type. This inheritance works only for sample property\nassignments without any semantic annotations, i.e. if there is at least\none semantic annotation defined at a sample property assignment level\nthen nothing gets inherited from the property type level anymore. The\ninheritance makes it possible to define a meaning of a property once, at\nthe property type level, and override it, only if needed, at sample\nproperty assignment level.\nV3 API provides the following methods to manipulate the semantic\n## annotations:\ncreateSemanticAnnotations\nupdateSemanticAnnotations\ndeleteSemanticAnnotations\ngetSemanticAnnotations\nsearchSemanticAnnotations\nThese methods work similar to the other create/update/delete/get/search\nV3 API counterparts.\nMoreover, once semantic annotations are defined, it is possible to\nsearch for samples and sample types that have a given semantic\nannotation. To do it, one has to use searchSamples and searchSampleTypes\nmethods and specify appropriate withType().withSemanticAnnotations()\ncondition in SampleSearchCriteria or withSemanticAnnotations() condition\nin SampleTypeSearchCriteria.\n## Web App Settings\n\nThe web app settings functionality is a user specific key-value map\nwhere a user specific configuration can be stored. The settings are\npersistent, i.e. they can live longer than a user session that created\nthem. Web app settings of a given user can be read/updated only by that\nuser or by an instance admin.\nWebAppSettingsExample.java\nimport\njava.util.Arrays\n;\nimport\njava.util.Map\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.person.Person\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.person.fetchoptions.PersonFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.person.id.IPersonId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.person.id.Me\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.person.update.PersonUpdate\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.WebAppSetting\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.create.WebAppSettingCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.fetchoptions.WebAppSettingsFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.webapp.update.WebAppSettingsUpdateValue\n;\npublic\nclass\nWebAppSettingsExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nPersonUpdate\nupdate\n=\nnew\nPersonUpdate\n();\n// update the currently logged in user\nupdate\n.\nsetUserId\n(\nnew\n## Me\n());\n// add \"setting1a\" and \"setting1b\" to \"app1\" (other settings for \"app1\" will remain unchanged)\nWebAppSettingsUpdateValue\napp1\n=\nupdate\n.\ngetWebAppSettings\n(\n\"app1\"\n);\napp1\n.\nadd\n(\nnew\nWebAppSettingCreation\n(\n\"setting1a\"\n,\n\"value1a\"\n));\napp1\n.\nadd\n(\nnew\nWebAppSettingCreation\n(\n\"setting1b\"\n,\n\"value1b\"\n));\n// set \"setting2a\", \"setting2b\" and \"setting2c\" for \"app2\" (other settings for \"app2\" will be removed)\nWebAppSettingsUpdateValue\napp2\n=\nupdate\n.\ngetWebAppSettings\n(\n\"app2\"\n);\napp2\n.\nset\n(\nnew\nWebAppSettingCreation\n(\n\"setting2a\"\n,\n\"value2a\"\n),\nnew\nWebAppSettingCreation\n(\n\"setting2b\"\n,\n\"value2b\"\n),\nnew\nWebAppSettingCreation\n(\n\"setting2c\"\n,\n\"value2c\"\n));\n// remove \"setting3a\" from \"app3\" (other settings for \"app3\" will remain unchanged)\nWebAppSettingsUpdateValue\napp3\n=\nupdate\n.\ngetWebAppSettings\n(\n\"app3\"\n);\napp3\n.\nremove\n(\n\"setting3a\"\n);\nv3\n.\nupdatePersons\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nupdate\n));\n// option 1 : fetch a person with all settings of all web apps\nPersonFetchOptions\npersonFo1\n=\nnew\nPersonFetchOptions\n();\npersonFo1\n.\nwithAllWebAppSettings\n();\n// option 2 : fetch a person with either all or chosen settings of chosen web apps\nPersonFetchOptions\npersonFo2\n=\nnew\nPersonFetchOptions\n();\n// option 2a : fetch \"app1\" with all settings\nWebAppSettingsFetchOptions\napp1Fo\n=\npersonFo2\n.\nwithWebAppSettings\n(\n\"app1\"\n);\napp1Fo\n.\nwithAllSettings\n();\n// option 2b : fetch \"app2\" with chosen settings\nWebAppSettingsFetchOptions\napp2Fo\n=\npersonFo2\n.\nwithWebAppSettings\n(\n\"app2\"\n);\napp2Fo\n.\nwithSetting\n(\n\"setting2a\"\n);\napp2Fo\n.\nwithSetting\n(\n\"setting2b\"\n);\n## Map\n<\nIPersonId\n,\n## Person\n>\npersons\n=\nv3\n.\ngetPersons\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nnew\n## Me\n()),\npersonFo2\n);\n## Person\nperson\n=\npersons\n.\nvalues\n().\niterator\n().\nnext\n();\n// get \"setting1a\" for \"app1\"\nWebAppSetting\nsetting1a\n=\nperson\n.\ngetWebAppSettings\n(\n\"app1\"\n).\ngetSetting\n(\n\"setting1a\"\n);\n## System\n.\nout\n.\nprintln\n(\nsetting1a\n.\ngetValue\n());\n// get all fetched settings for \"app2\"\n## Map\n<\n## String\n,\nWebAppSetting\n>\nsettings2\n=\nperson\n.\ngetWebAppSettings\n(\n\"app2\"\n).\ngetSettings\n();\n## System\n.\nout\n.\nprintln\n(\nsettings2\n);\n}\n}\nWebAppSettingsExample.html\n<\nscript\n>\nrequire\n([\n\"jquery\"\n,\n\"openbis\"\n,\n\"as/dto/person/update/PersonUpdate\"\n,\n\"as/dto/person/id/Me\"\n,\n\"as/dto/webapp/create/WebAppSettingCreation\"\n,\n\"as/dto/person/fetchoptions/PersonFetchOptions\"\n],\nfunction\n(\n$\n,\nopenbis\n,\nPersonUpdate\n,\n## Me\n,\nWebAppSettingCreation\n,\nPersonFetchOptions\n)\n{\n$\n(\ndocument\n).\nready\n(\nfunction\n()\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nupdate\n=\nnew\nPersonUpdate\n();\n// update the currently logged in user\nupdate\n.\nsetUserId\n(\nnew\n## Me\n());\n// add \"setting1a\" and \"setting1b\" to \"app1\" (other settings for \"app1\" will remain unchanged)\nvar\napp1\n=\nupdate\n.\ngetWebAppSettings\n(\n\"app1\"\n);\napp1\n.\nadd\n(\nnew\nWebAppSettingCreation\n(\n\"setting1a\"\n,\n\"value1a\"\n));\napp1\n.\nadd\n(\nnew\nWebAppSettingCreation\n(\n\"setting1b\"\n,\n\"value1b\"\n));\n// set \"setting2a\", \"setting2b\" and \"setting2c\" for \"app2\" (other settings for \"app2\" will be removed)\nvar\napp2\n=\nupdate\n.\ngetWebAppSettings\n(\n\"app2\"\n);\napp2\n.\nset\n([\nnew\nWebAppSettingCreation\n(\n\"setting2a\"\n,\n\"value2a\"\n),\nnew\nWebAppSettingCreation\n(\n\"setting2b\"\n,\n\"value2b\"\n),\nnew\nWebAppSettingCreation\n(\n\"setting2c\"\n,\n\"value2c\"\n)\n]);\n// remove \"setting3a\" from \"app3\" (other settings for \"app3\" will remain unchanged)\nvar\napp3\n=\nupdate\n.\ngetWebAppSettings\n(\n\"app3\"\n);\napp3\n.\nremove\n(\n\"setting3a\"\n);\nv3\n.\nupdatePersons\n([\nupdate\n]).\ndone\n(\nfunction\n()\n{\n// option 1 : fetch a person with all settings of all web apps\nvar\npersonFo1\n=\nnew\nPersonFetchOptions\n();\npersonFo1\n.\nwithAllWebAppSettings\n();\n// option 2 : fetch a person with either all or chosen settings of chosen web apps\nvar\npersonFo2\n=\nnew\nPersonFetchOptions\n();\n// option 2a : fetch \"app1\" with all settings\nvar\napp1Fo\n=\npersonFo2\n.\nwithWebAppSettings\n(\n\"app1\"\n);\napp1Fo\n.\nwithAllSettings\n();\n// option 2b : fetch \"app2\" with chosen settings\nvar\napp2Fo\n=\npersonFo2\n.\nwithWebAppSettings\n(\n\"app2\"\n);\napp2Fo\n.\nwithSetting\n(\n\"setting2a\"\n);\napp2Fo\n.\nwithSetting\n(\n\"setting2b\"\n);\nv3\n.\ngetPersons\n([\nnew\n## Me\n()\n],\npersonFo2\n).\ndone\n(\nfunction\n(\npersons\n)\n{\nvar\nperson\n=\npersons\n[\nnew\n## Me\n()];\n// get \"setting1a\" for \"app1\"\nvar\nsetting1a\n=\nperson\n.\ngetWebAppSettings\n(\n\"app1\"\n).\ngetSetting\n(\n\"setting1a\"\n);\nconsole\n.\nlog\n(\nsetting1a\n.\ngetValue\n());\n// get all fetched settings for \"app2\"\nvar\nsettings2\n=\nperson\n.\ngetWebAppSettings\n(\n\"app2\"\n).\ngetSettings\n();\nconsole\n.\nlog\n(\nsettings2\n);\n});\n});\n});\n});\n</\nscript\n>\n## Imports\n\nThe imports that are normally accesible via “Import” menu in the generic\nopenBIS UI can be also used programatically from within a V3 custom AS\nservice. Such an import process consists of two steps:\nuploading a file to /openbis/upload servlet to be temporarily stored\nunder a specific user session key (more information on the upload\nservlet can be found\nhere\n)\nimporting the uploaded file using one\nof ch.ethz.sis.openbis.generic.asapi.v3.plugin.service.IImportService\nmethods accessible from within a V3 custom AS service\n## Currently available import methods:\nString createExperiments(String sessionToken, String uploadKey,\nString experimentTypeCode, boolean async, String userEmail)\nString updateExperiments(String sessionToken, String uploadKey,\nString experimentTypeCode, boolean async, String userEmail)\nString createSamples(String sessionToken, String uploadKey, String\nsampleTypeCode, String defaultSpaceIdentifier, String\nspaceIdentifierOverride, String experimentIdentifierOverride,\nboolean updateExisting, boolean async, String userEmail)\nString updateSamples(String sessionToken, String uploadKey, String\nsampleTypeCode, String defaultSpaceIdentifier, String\nspaceIdentifierOverride, String experimentIdentifierOverride,\nboolean async, String userEmail)\nString updateDataSets(String sessionToken, String uploadKey, String\ndataSetTypeCode, boolean async, String userEmail)\nString createMaterials(String sessionToken, String uploadKey, String\nmaterialTypeCode, boolean updateExisting, boolean async, String\nuserEmail)\nString updateMaterials(String sessionToken, String uploadKey, String\nmaterialTypeCode, boolean ignoreUnregistered, boolean async, String\nuserEmail)\nString generalImport(String sessionToken, String uploadKey, String\ndefaultSpaceIdentifier, boolean updateExisting,\nboolean async, String userEmail) - import of samples and materials\nfrom an Excel file\nString customImport(String sessionToken, String uploadKey, String\ncustomImportCode, boolean async, String userEmail) - import\ndelegated to a dropbox\n## Parameters:\n## Parameter\n## Type\n## Methods\n## Description\nsessionToken\n## String\n## ALL\nopenBIS session token; to get a session token of a currently logged in user inside a custom AS service context.getSessionToken() method shall be used.\nuploadKey\n## String\n## ALL\nA key the file to be imported has been uploaded to (see the 1st step of the import process described above).\nasync\nboolean\n## ALL\nA flag that controls whether the import should be performed synchronously (i.e. in the current thread) or asynchronously (i.e. in a separate thread). For asynchronous imports an email with either an execution result or error is sent to the specified email address (see userEmail parameter).\nuserEmail\n## String\n## ALL\nAn email address where an execution result or error should be sent to (only for asynchronous imports - see async parameter).\nexperimentTypeCode\n## String\ncreateExperiments, updateExperiments\nA type of experiments to be created/updated.\nsampleTypeCode\n## String\ncreateSamples, updateSamples\nA type of samples to be created/updated.\ndataSetTypeCode\n## String\nupdateDataSets\nA type of data sets to be updated.\nmaterialTypeCode\n## String\ncreateMaterials, updateMaterials\nA type of materials to be created/updated.\ncustomImportCode\n## String\ncustomImport\nA code of a custom import the import process should be delegated to. A custom import sends the uploaded file to a dropbox. Inside a dropbox the uploaded file can be accessed via transaction.getIncoming() method.\ndefaultSpaceIdentifier\n## String\ncreateSamples, updateSamples, generalImport\nA default space identifier. If null then identifiers of samples to be created/updated are expected to be specified in the uploaded file. If not null then:\ncodes of samples to be created are automatically generated and the samples are created in the requested default space\nidentifiers of samples to be updated can omit the space part (the requested default space will be automatically added)\nspaceIdentifierOverride\n## String\ncreateSamples, updateSamples\nA space identifier to be used instead of the ones defined in the uploaded file.\nexperimentIdentifierOverride\n## String\ncreateSamples, updateSamples\nAn experiment identifier to be used instead of the ones defined in the uploaded file.\nupdateExisting\nboolean\ncreateSamples, createMaterials, generalImport\nA flag that controlls whether in case of an attempt to create an already existing entity an update should be performed or such a creation should fail.\nignoreUnregistered\nboolean\nupdateMaterials\nA flag that controlls whether in case of an attempt to update a nonexistent entity such update should be silently ignored or it should fail.\n## File formats:\nThe TSV examples below assume experiment/sample/dataset/material type\nused contains exactly one property called “DESCRIPTION”.\n## Method\n## Template\ncreateExperiments\ncreate-experiments-import-template.tsv\nupdateExperiments\nupdate-experiments-import-template.tsv\ncreateSamples\ncreate-samples-import-template.tsv\nupdateSamples\nupdate-samples-import-template.tsv\nupdateDataSets\nupdate-data-sets-import-template.tsv\ncreateMaterials\ncreate-materials-import-template.tsv\nupdateMaterials\nupdate-materials-import-template.tsv\ngeneralImport\ncustomImport\nany kind of file\n## Return values:\nAll methods return a message with a short summary of the performed\noperation, e.g. a synchronous createSamples method call could return a\nmessage like “Registration of 1 sample(s) is complete.” while the\nasynchronous version could return a message like “When the import is\ncomplete the confirmation or failure report will be sent by email.”.\nAn example webapp to upload a file with samples and a custom AS service\nto import that file is presented below.\nImportSamplesWebAppExample.html\n<!DOCTYPE html>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\ncharset\n=\n\"utf-8\"\n>\n<\ntitle\n>\nSamples import\n</\ntitle\n>\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"/openbis-test/resources/api/v3/config.js\"\n></\nscript\n>\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"/openbis-test/resources/api/v3/require.js\"\n></\nscript\n>\n</\nhead\n>\n<\nbody\n>\n<\nscript\n>\nrequire\n([\n\"jquery\"\n,\n\"openbis\"\n,\n\"as/dto/service/id/CustomASServiceCode\"\n,\n\"as/dto/service/CustomASServiceExecutionOptions\"\n],\nfunction\n(\n$\n,\nopenbis\n,\nCustomASServiceCode\n,\nCustomASServiceExecutionOptions\n)\n{\n$\n(\ndocument\n).\nready\n(\nfunction\n()\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\nuploadFrame\n=\n$\n(\n\"#uploadFrame\"\n);\nuploadFrame\n.\nload\n(\nfunction\n()\n{\nalert\n(\n\"Upload finished\"\n)\n});\nvar\nuploadForm\n=\n$\n(\n\"#uploadForm\"\n);\nuploadForm\n.\nfind\n(\n\"input[name=sessionID]\"\n).\nval\n(\nsessionToken\n);\nvar\nimportForm\n=\n$\n(\n\"#importForm\"\n);\nimportForm\n.\nsubmit\n(\nfunction\n(\ne\n)\n{\ne\n.\npreventDefault\n();\nvar\nsampleType\n=\nimportForm\n.\nfind\n(\n\"input[name=sampleType]\"\n).\nval\n();\nvar\nserviceId\n=\nnew\nCustomASServiceCode\n(\n\"import-service\"\n);\nvar\nserviceOptions\n=\nnew\nCustomASServiceExecutionOptions\n();\nserviceOptions\n.\nwithParameter\n(\n\"sampleType\"\n,\nsampleType\n);\nfacade\n.\nexecuteCustomASService\n(\nserviceId\n,\nserviceOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nalert\n(\n\"Import successful: \"\n+\nresult\n);\n}).\nfail\n(\nfunction\n(\nerror\n)\n{\nalert\n(\n\"Import failed: \"\n+\nerror\n.\nmessage\n);\n});\nreturn\nfalse\n;\n});\n});\n});\n</\nscript\n>\n<\niframe\nid\n=\n\"uploadFrame\"\nname\n=\n\"uploadFrame\"\nstyle\n=\n\"display: none\"\n></\niframe\n>\n<\nh1\n>\nStep 1 : upload samples file\n</\nh1\n>\n<\nform\nid\n=\n\"uploadForm\"\nmethod\n=\n\"post\"\naction\n=\n\"/openbis/upload\"\nenctype\n=\n\"multipart/form-data\"\ntarget\n=\n\"uploadFrame\"\n>\n<\ninput\ntype\n=\n\"file\"\nname\n=\n\"importWebappUploadKey\"\nmultiple\n=\n\"multiple\"\n>\n<\ninput\ntype\n=\n\"hidden\"\nname\n=\n\"sessionID\"\n>\n<\ninput\ntype\n=\n\"hidden\"\nname\n=\n\"sessionKeysNumber\"\nvalue\n=\n\"1\"\n>\n<\ninput\ntype\n=\n\"hidden\"\nname\n=\n\"sessionKey_0\"\nvalue\n=\n\"importWebappUploadKey\"\n>\n<\ninput\ntype\n=\n\"submit\"\n>\n</\nform\n>\n<\nh1\n>\nStep 2 : import samples file\n</\nh1\n>\n<\nform\nid\n=\n\"importForm\"\n>\n<\nlabel\n>\n## Sample Type\n</\nlabel\n>\n<\ninput\ntype\n=\n\"text\"\nname\n=\n\"sampleType\"\n>\n<\ninput\ntype\n=\n\"submit\"\n>\n</\nform\n>\n</\nbody\n>\n</\nhtml\n>\nImportSamplesServiceExample.py\ndef\nprocess\n(\ncontext\n,\nparameters\n## ):\nsampleType\n=\nparameters\n.\nget\n(\n\"sampleType\"\n)\nreturn\ncontext\n.\ngetImportService\n()\n.\ncreateSamples\n(\ncontext\n.\ngetSessionToken\n(),\n\"importWebappUploadKey\"\n,\nsampleType\n,\n## None\n,\n## None\n,\n## None\n,\n## False\n,\n## False\n,\n## None\n);\n### Generate identifiers\nV3 API provides 2 methods for generating unique identifiers:\ncreatePermIdStrings - generates globally unique identifiers that\nconsist of a timestamp and a sequence generated number (e.g.\n“20180531170854641-944”); this method uses one global sequence.\ncreateCodes - generates identifiers that are unique for a given\nentity kind and consist of a prefix and a sequence generated number\n(e.g. “MY-PREFIX-147”); this method uses a dedicated sequence for\neach entity kind.\nGenerateIdentifiersExample.java\nimport\njava.util.List\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.EntityKind\n;\npublic\nclass\nGenerateIdentifiersExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\n## List\n<\n## String\n>\npermIds\n=\nv3\n.\ncreatePermIdStrings\n(\nsessionToken\n,\n2\n);\n## List\n<\n## String\n>\ncodes\n=\nv3\n.\ncreateCodes\n(\nsessionToken\n,\n## \"MY-PREFIX-\"\n,\nEntityKind\n.\n## SAMPLE\n,\n3\n);\n## System\n.\nout\n.\nprintln\n(\npermIds\n);\n// example output: [20180531170854641-944, 20180531170854641-945]\n## System\n.\nout\n.\nprintln\n(\ncodes\n);\n// example output: [MY-PREFIX-782, MY-PREFIX-783, MY-PREFIX-784]\n}\n}\nGenerateIdentifiersExample.html\n<\nscript\n>\nrequire\n([\n\"jquery\"\n,\n\"openbis\"\n,\n\"as/dto/entitytype/EntityKind\"\n],\nfunction\n(\n$\n,\nopenbis\n,\nEntityKind\n)\n{\n$\n(\ndocument\n).\nready\n(\nfunction\n()\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nv3\n.\ncreatePermIdStrings\n(\n2\n).\nthen\n(\nfunction\n(\npermIds\n)\n{\nconsole\n.\nlog\n(\npermIds\n);\n// example output: [20180531170854641-944, 20180531170854641-945]\n});\nv3\n.\ncreateCodes\n(\n## \"MY-PREFIX-\"\n,\nEntityKind\n.\n## SAMPLE\n,\n3\n).\nthen\n(\nfunction\n(\ncodes\n)\n{\nconsole\n.\nlog\n(\ncodes\n);\n// example output: [MY-PREFIX-782, MY-PREFIX-783, MY-PREFIX-784]\n});\n});\n});\n</\nscript\n>\nV. DSS Methods\n\nSearch files\n\nThe searchFiles method can be used to search for data set files at a\nsingle data store (Java version) or at multiple data stores at the same\ntime (Javascript version).\nSimilar to the other V3 search methods it takes as parameters a\nsessionToken, search criteria and fetch options and returns a search\nresult object.\nWhen searching across multiple data stores the results from each data\nstore are combined together and returned back as a single regular search\nresult object as if it was returned by only one data store.\n## Example\n\nV3SearchDataSetFilesExample.java\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.search.DataSetSearchCriteria\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.DataSetFile\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fetchoptions.DataSetFileFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.search.DataSetFileSearchCriteria\n;\npublic\nclass\nV3SearchDataSetFilesExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n// we assume here that v3 objects for both AS and DSS have been already created and we have already called login on AS to get the sessionToken (please check \"Accessing the API\" section for more details)\nDataSetFileSearchCriteria\ncriteria\n=\nnew\nDataSetFileSearchCriteria\n();\nDataSetSearchCriteria\ndataSetCriteria\n=\ncriteria\n.\nwithDataSet\n().\nwithOrOperator\n();\ndataSetCriteria\n.\nwithCode\n().\nthatEquals\n(\n## \"MY_DATA_SET_CODE_1\"\n);\ndataSetCriteria\n.\nwithCode\n().\nthatEquals\n(\n## \"MY_DATA_SET_CODE_2\"\n);\n// Searches for files at at a single data store\nSearchResult\n<\nDataSetFile\n>\nresult\n=\ndssV3\n.\nsearchFiles\n(\nsessionToken\n,\ncriteria\n,\nnew\nDataSetFileFetchOptions\n());\nfor\n(\nDataSetFile\nfile\n## :\nresult\n.\ngetObjects\n())\n{\n## System\n.\nout\n.\nprintln\n(\n\"DataSet: \"\n+\nfile\n.\ngetDataSetPermId\n()\n+\n\" has file: \"\n+\nfile\n.\ngetPath\n());\n}\n}\n}\nV3SearchDataSetFilesAtAllDataStoresExample.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"dss/dto/datasetfile/search/DataSetFileSearchCriteria\"\n,\n\"dss/dto/datasetfile/fetchoptions/DataSetFileFetchOptions\"\n],\nfunction\n(\nDataSetFileSearchCriteria\n,\nDataSetFileFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\ncriteria\n=\nnew\nDataSetFileSearchCriteria\n();\nvar\ndataSetCriteria\n=\ncriteria\n.\nwithDataSet\n().\nwithOrOperator\n();\ndataSetCriteria\n.\nwithCode\n().\nthatEquals\n(\n## \"MY_DATA_SET_CODE_1\"\n);\ndataSetCriteria\n.\nwithCode\n().\nthatEquals\n(\n## \"MY_DATA_SET_CODE_2\"\n);\nvar\nfetchOptions\n=\nnew\nDataSetFileFetchOptions\n();\n// getDataStoreFacade() call (without any parameters) returns a facade object that uses all available data stores,\n// e.g. calling searchFiles on such a facade searches for files at all available data stores\nv3\n.\ngetDataStoreFacade\n().\nsearchFiles\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nfile\n)\n{\nconsole\n.\nlog\n(\n\"DataSet: \"\n+\nfile\n.\ngetDataSetPermId\n()\n+\n\" has file: \"\n+\nfile\n.\ngetPath\n());\n});\n});\n});\n</\nscript\n>\nV3SearchDataSetFilesAtChosenDataStoresExample.html\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"dss/dto/datasetfile/search/DataSetFileSearchCriteria\"\n,\n\"dss/dto/datasetfile/fetchoptions/DataSetFileFetchOptions\"\n],\nfunction\n(\nDataSetFileSearchCriteria\n,\nDataSetFileFetchOptions\n)\n{\n// we assume here that v3 object has been already created and we have already called login (please check \"Accessing the API\" section for more details)\nvar\ncriteria\n=\nnew\nDataSetFileSearchCriteria\n();\nvar\ndataSetCriteria\n=\ncriteria\n.\nwithDataSet\n().\nwithOrOperator\n();\ndataSetCriteria\n.\nwithCode\n().\nthatEquals\n(\n## \"MY_DATA_SET_CODE_1\"\n);\ndataSetCriteria\n.\nwithCode\n().\nthatEquals\n(\n## \"MY_DATA_SET_CODE_2\"\n);\nvar\nfetchOptions\n=\nnew\nDataSetFileFetchOptions\n();\n// getDataStoreFacade(\"DSS1\",\"DSS2\") returns a facade object that uses only \"DSS1\" and \"DSS2\" data stores,\n// e.g. calling searchFiles on such a facade searches for files only at these two data stores even if there\n// are more datastores available\nv3\n.\ngetDataStoreFacade\n(\n## \"DSS1\"\n,\n## \"DSS2\"\n).\nsearchFiles\n(\ncriteria\n,\nfetchOptions\n).\ndone\n(\nfunction\n(\nresult\n)\n{\nresult\n.\ngetObjects\n().\nforEach\n(\nfunction\n(\nfile\n)\n{\nconsole\n.\nlog\n(\n\"DataSet: \"\n+\nfile\n.\ngetDataSetPermId\n()\n+\n\" has file: \"\n+\nfile\n.\ngetPath\n());\n});\n});\n});\n</\nscript\n>\nDownloading files, folders, and datasets\n\nDatasets that are created in Open BIS can be accessed by V3 API in a\nnumber of different ways. It’s possible to download individual files,\nfolders, and entire datasets as illustrated in the following examples.\nTo get started, it is necessary to reference both the AS API\n(IApplicationServerApi) and the DSS API (IDataStoreServerAPI), and login\nand get a session token object.\nThe API provides two methods for downloading:\nSimple downloading: A single InputStream is returned which contains\nall files and file meta data.\nFast downloading: A FastDownloadSession object is returned which is\nused by a helper class to download files in parallel streams in\nchunks. It is based on the\nSIS File Transfer Protocol\n.\n## Simple Downloading\n\nBy setting the DataSetFileDownloadOptions it’s possible to change how\ndata is downloaded - data can be downloaded file by file, or by folder,\nby an entire dataset in a recursive manner. It is also possible to\nsearch for datasets by defining the appropriate search criteria\n(DataSetFileSearchCriteria).\nIn order to download content via the V3 DSS API, the dataset needs to\nalready be inside Open BIS. It is necessary to know the dataset code at\nthe very minimum. It is helpful to also know the file path to the file\ndesired to download.\nDownload a single file located inside a dataset\n\nHere is how to download a single file and print out the contents, when\nthe dataset code and the file path are known. Here a search is not\nnecessary since the file path and dataset code are known.\nA note about recursion\n\nNote that when only downloading one file, it is better to set the\nrecursive flag to false in DataSetFileDownloadOptions, although it makes\nno difference in the results returned. The recursive flag really only\nmatters when downloading entire datasets or directories - if it is true,\nthen the entire tree of contents will be downloaded, if false, then the\nsingle path requested will be downloaded. If that path is just a\ndirectory then the returned result will consist of just meta data about\nthe directory.\nDownload a single file\nimport\njava.io.InputStream\n;\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3DSSExample1\n{\n## // DATASET EXAMPLE STRUCTURE\n// The dataset consists of a root folder with 2 files and a subfolder with 1 file\n## // root:\n//   - file1.txt\n//   - file2.txt\n## //   - subfolder:\n//      - file3.txt\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n## String\n## AS_URL\n=\n\"https://localhost:8443/openbis/openbis\"\n;\n## String\n## DSS_URL\n=\n\"https://localhost:8444/datastore_server\"\n;\n// Reference the DSS\nIDataStoreServerApi\ndss\n=\nHttpInvokerUtils\n.\ncreateStreamSupportingServiceStub\n(\nIDataStoreServerApi\n.\nclass\n,\n## DSS_URL\n+\nIDataStoreServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n// Reference the AS and login & get a session token\nIApplicationServerApi\nas\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n## AS_URL\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n## String\nsessionToken\n=\nas\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n// Download a single file with a path and a dataset code\nDataSetFileDownloadOptions\noptions\n=\nnew\nDataSetFileDownloadOptions\n();\noptions\n.\nsetRecursive\n(\nfalse\n);\nIDataSetFileId\nfileToDownload\n=\nnew\nDataSetFilePermId\n(\nnew\nDataSetPermId\n(\n\"20161205154857065-25\"\n),\n\"root/subfolder/file3.txt\"\n);\n// Download the files into a stream and read them with the file reader\n// Here there is only one file, but we need to put it in an array anyway\nInputStream\nstream\n=\ndss\n.\ndownloadFiles\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nfileToDownload\n),\noptions\n);\nDataSetFileDownloadReader\nreader\n=\nnew\nDataSetFileDownloadReader\n(\nstream\n);\nDataSetFileDownload\nfile\n=\nnull\n;\n// Print out the contents\nwhile\n((\nfile\n=\nreader\n.\nread\n())\n!=\nnull\n)\n{\n## System\n.\nout\n.\nprintln\n(\n## \"Downloaded \"\n+\nfile\n.\ngetDataSetFile\n().\ngetPath\n()\n+\n\" \"\n+\nfile\n.\ngetDataSetFile\n().\ngetFileLength\n());\n## System\n.\nout\n.\nprintln\n(\n## \"-----FILE CONTENTS-----\"\n);\n## System\n.\nout\n.\nprintln\n(\nfile\n.\ngetInputStream\n());\n}\n}\n}\nDownload a folder located inside a dataset\n\nThe example below demonstrates how to download a folder and all its\ncontents, when the dataset code and the folder path are known. The goal\nhere is to download the directory called “subfolder” and the file\n“file3.txt” which will return two objects, one representing the metadata\nof the directory, and the other representing both the meta data of\nfile3.txt and the file contents. Note that setting recursive flag to\ntrue will return both the subfolder directory object AND file3.txt,\nwhile setting recursive flag to false will return just the meta data of\nthe directory object.\nDownload a folder\nimport\njava.io.InputStream\n;\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3DSSExample2\n{\n## // DATASET EXAMPLE STRUCTURE\n// The dataset consists of a root folder with 2 files and a subfolder with 1 file\n## // root:\n//   - file1.txt\n//   - file2.txt\n## //   - subfolder:\n//      - file3.txt\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n## String\n## AS_URL\n=\n\"https://localhost:8443/openbis/openbis\"\n;\n## String\n## DSS_URL\n=\n\"https://localhost:8444/datastore_server\"\n;\n// Reference the DSS\nIDataStoreServerApi\ndss\n=\nHttpInvokerUtils\n.\ncreateStreamSupportingServiceStub\n(\nIDataStoreServerApi\n.\nclass\n,\n## DSS_URL\n+\nIDataStoreServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n// Reference the AS and login & get a session token\nIApplicationServerApi\nas\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n## AS_URL\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n## String\nsessionToken\n=\nas\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n// Download a single folder (containing a file inside) with a path and a data set code\nDataSetFileDownloadOptions\noptions\n=\nnew\nDataSetFileDownloadOptions\n();\nIDataSetFileId\nfileToDownload\n=\nnew\nDataSetFilePermId\n(\nnew\nDataSetPermId\n(\n\"20161205154857065-25\"\n),\n\"root/subfolder\"\n);\n// Setting recursive flag to true will return both the subfolder directory object AND file3.txt\noptions\n.\nsetRecursive\n(\ntrue\n);\n// Setting recursive flag to false will return just the meta data of the directory object\n//options.setRecursive(false);\n// Read the contents and print them out\nInputStream\nstream\n=\ndss\n.\ndownloadFiles\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nfileToDownload\n),\noptions\n);\nDataSetFileDownloadReader\nreader\n=\nnew\nDataSetFileDownloadReader\n(\nstream\n);\nDataSetFileDownload\nfile\n=\nnull\n;\nwhile\n((\nfile\n=\nreader\n.\nread\n())\n!=\nnull\n)\n{\n## System\n.\nout\n.\nprintln\n(\n## \"Downloaded \"\n+\nfile\n.\ngetDataSetFile\n().\ngetPath\n()\n+\n\" \"\n+\nfile\n.\ngetDataSetFile\n().\ngetFileLength\n());\n## System\n.\nout\n.\nprintln\n(\n## \"-----FILE CONTENTS-----\"\n);\n## System\n.\nout\n.\nprintln\n(\nfile\n.\ngetInputStream\n());\n}\n}\n}\nSearch for a dataset and download all its contents, file by file\n\nHere is an example that demonstrates how to search for datasets and\ndownload the contents file by file. Here recursion is not used - see\nexample 4 for a recursive example. To search for datasets, it is\nnecessary to assign the appropriate criteria in the\nDataSetFileSearchCriteria object. It is also possible to search for\ndatasets that contain certain files, as demonstrated below. Searching\nfor files via the searchFiles method returns a list of DataSetFile\nobjects that contain meta data about the files and also the file\ncontents. The meta data includes the file perm ids, the dataset perm ids\n(the perm ids are objects, not simple codes!), the file path, the file\nlength, and whether or not the file is a directory. With this list of\nfiles, it is possible to iterate and access the contents as shown in\nthis example.\nSearch & download a whole dataset, file by file\nimport\njava.io.InputStream\n;\nimport\njava.util.LinkedList\n;\nimport\njava.util.List\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.DataSetFile\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fetchoptions.DataSetFileFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.search.DataSetFileSearchCriteria\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3DSSExample3\n{\n## // DATASET EXAMPLE STRUCTURE\n// The dataset consists of a root folder with 2 files and a subfolder with 1 file\n## // root:\n//   - file1.txt\n//   - file2.txt\n## //   - subfolder:\n//      - file3.txt\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n## String\n## AS_URL\n=\n\"https://localhost:8443/openbis/openbis\"\n;\n## String\n## DSS_URL\n=\n\"https://localhost:8444/datastore_server\"\n;\n// Reference the DSS\nIDataStoreServerApi\ndss\n=\nHttpInvokerUtils\n.\ncreateStreamSupportingServiceStub\n(\nIDataStoreServerApi\n.\nclass\n,\n## DSS_URL\n+\nIDataStoreServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n// Reference the AS and login & get a session token\nIApplicationServerApi\nas\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n## AS_URL\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n## String\nsessionToken\n=\nas\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n// Create search criteria\nDataSetFileSearchCriteria\ncriteria\n=\nnew\nDataSetFileSearchCriteria\n();\ncriteria\n.\nwithDataSet\n().\nwithCode\n().\nthatEquals\n(\n\"20161205154857065-25\"\n);\n// Search for a dataset with a certain file inside like this:\n//criteria.withDataSet().withChildren().withPermId(mypermid);\n// Search for the files & put the file perm ids in a list for easy access\n// (file perm ids are objects containing meta data describing the file)\nSearchResult\n<\nDataSetFile\n>\nresult\n=\ndss\n.\nsearchFiles\n(\nsessionToken\n,\ncriteria\n,\nnew\nDataSetFileFetchOptions\n());\n## List\n<\nDataSetFile\n>\nfiles\n=\nresult\n.\ngetObjects\n();\n// This returns the following list of objects:\n// DataSetFile(\"root\", isDirectory = true)\n// DataSetFile(\"root/file1.txt\", isDirectory = false)\n// DataSetFile(\"root/file2.txt\", isDirectory = false)\n// DataSetFile(\"root/subfolder\", isDirectory = true)\n// DataSetFile(\"root/subfolder/file3.txt\", isDirectory = false)\n## List\n<\nIDataSetFileId\n>\nfileIds\n=\nnew\nLinkedList\n<\nIDataSetFileId\n>\n();\nfor\n(\nDataSetFile\nfile\n## :\nfiles\n)\n{\n## System\n.\nout\n.\nprintln\n(\nfile\n.\ngetPath\n()\n+\n\" \"\n+\nfile\n.\ngetFileLength\n());\nfileIds\n.\nadd\n(\nfile\n.\ngetPermId\n());\n}\n// Download the files & print the contents\nDataSetFileDownloadOptions\noptions\n=\nnew\nDataSetFileDownloadOptions\n();\noptions\n.\nsetRecursive\n(\nfalse\n);\nInputStream\nstream\n=\ndss\n.\ndownloadFiles\n(\nsessionToken\n,\nfileIds\n,\noptions\n);\nDataSetFileDownloadReader\nreader\n=\nnew\nDataSetFileDownloadReader\n(\nstream\n);\nDataSetFileDownload\nfile\n=\nnull\n;\nwhile\n((\nfile\n=\nreader\n.\nread\n())\n!=\nnull\n)\n{\n## System\n.\nout\n.\nprintln\n(\n## \"Downloaded \"\n+\nfile\n.\ngetDataSetFile\n().\ngetPath\n()\n+\n\" \"\n+\nfile\n.\ngetDataSetFile\n().\ngetFileLength\n());\n## System\n.\nout\n.\nprintln\n(\nfile\n.\ngetInputStream\n());\n}\n}\n}\nDownload a whole dataset recursively\n\nHere is a simplified way to download a dataset. Instead of downloading\nfiles one by one, it is possible to download the entire dataset\nrecursively by simply setting the recursive file to true in the\nDataSetFileDownloadOptions object.\nDownload a whole dataset recursively\nimport\njava.io.InputStream\n;\nimport\njava.util.Arrays\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3DSSExample4\n{\n## // DATASET EXAMPLE STRUCTURE\n// The dataset consists of a root folder with 2 files and a subfolder with 1 file\n## // root:\n//   - file1.txt\n//   - file2.txt\n## //   - subfolder:\n//      - file3.txt\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n## String\n## AS_URL\n=\n\"https://localhost:8443/openbis/openbis\"\n;\n## String\n## DSS_URL\n=\n\"https://localhost:8444/datastore_server\"\n;\n// Reference the DSS\nIDataStoreServerApi\ndss\n=\nHttpInvokerUtils\n.\ncreateStreamSupportingServiceStub\n(\nIDataStoreServerApi\n.\nclass\n,\n## DSS_URL\n+\nIDataStoreServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n// Reference the AS and login & get a session token\nIApplicationServerApi\nas\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n## AS_URL\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n## String\nsessionToken\n=\nas\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n// Download the files and print the contents\nDataSetFileDownloadOptions\noptions\n=\nnew\nDataSetFileDownloadOptions\n();\nIDataSetFileId\nfileId\n=\nnew\nDataSetFilePermId\n(\nnew\nDataSetPermId\n(\n\"20161205154857065-25\"\n));\noptions\n.\nsetRecursive\n(\ntrue\n);\nInputStream\nstream\n=\ndss\n.\ndownloadFiles\n(\nsessionToken\n,\n## Arrays\n.\nasList\n(\nfileId\n),\noptions\n);\nDataSetFileDownloadReader\nreader\n=\nnew\nDataSetFileDownloadReader\n(\nstream\n);\nDataSetFileDownload\nfile\n=\nnull\n;\nwhile\n((\nfile\n=\nreader\n.\nread\n())\n!=\nnull\n)\n{\nfile\n.\ngetInputStream\n();\n## System\n.\nout\n.\nprintln\n(\n## \"Downloaded \"\n+\nfile\n.\ngetDataSetFile\n().\ngetPath\n()\n+\n\" \"\n+\nfile\n.\ngetDataSetFile\n().\ngetFileLength\n());\n}\n}\n}\nSearch and list all the files inside a data store\n\nHere is an example that demonstrates how to list all the files in a data\nstore. By simply leaving the following line as is:\nDataSetFileSearchCriteria criteria = new DataSetFileSearchCriteria();\nit will automatically return every object in the data store. This is\nuseful when it is desired to list an entire directory or iterate over\nthe whole data store.\nSearch and list all files inside a data store\nimport\njava.io.InputStream\n;\nimport\njava.util.LinkedList\n;\nimport\njava.util.List\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.DataSetFile\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownload\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.download.DataSetFileDownloadReader\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fetchoptions.DataSetFileFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.search.DataSetFileSearchCriteria\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nV3DSSExample5\n{\n## // DATASET EXAMPLE STRUCTURE\n// The dataset consists of a root folder with 2 files and a subfolder with 1 file\n## // root:\n//   - file1.txt\n//   - file2.txt\n## //   - subfolder:\n//      - file3.txt\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\n## String\n## AS_URL\n=\n\"https://localhost:8443/openbis/openbis\"\n;\n## String\n## DSS_URL\n=\n\"https://localhost:8444/datastore_server\"\n;\n// Reference the DSS\nIDataStoreServerApi\ndss\n=\nHttpInvokerUtils\n.\ncreateStreamSupportingServiceStub\n(\nIDataStoreServerApi\n.\nclass\n,\n## DSS_URL\n+\nIDataStoreServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n// Reference the AS and login & get a session token\nIApplicationServerApi\nas\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n## AS_URL\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n## String\nsessionToken\n=\nas\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\n// Create search criteria\nDataSetFileSearchCriteria\ncriteria\n=\nnew\nDataSetFileSearchCriteria\n();\ncriteria\n.\nwithDataSet\n();\n//comment out this line below, and just leave the criteria empty - and it will return everything.\n//criteria.withDataSet().withCode().thatEquals(\"20151201115639682-98322\");\n// Search for the files & put the file perm ids (objects containing meta data) in a list for easy access\nSearchResult\n<\nDataSetFile\n>\nresult\n=\ndss\n.\nsearchFiles\n(\nsessionToken\n,\ncriteria\n,\nnew\nDataSetFileFetchOptions\n());\n## List\n<\nDataSetFile\n>\nfiles\n=\nresult\n.\ngetObjects\n();\n## List\n<\nIDataSetFileId\n>\nfileIds\n=\nnew\nLinkedList\n<\nIDataSetFileId\n>\n();\nfor\n(\nDataSetFile\nfile\n## :\nfiles\n)\n{\n## System\n.\nout\n.\nprintln\n(\nfile\n.\ngetPath\n()\n+\n\" \"\n+\nfile\n.\ngetFileLength\n());\nfileIds\n.\nadd\n(\nfile\n.\ngetPermId\n());\n}\n// Download the files and print the contents\nDataSetFileDownloadOptions\noptions\n=\nnew\nDataSetFileDownloadOptions\n();\noptions\n.\nsetRecursive\n(\nfalse\n);\nInputStream\nstream\n=\ndss\n.\ndownloadFiles\n(\nsessionToken\n,\nfileIds\n,\noptions\n);\nDataSetFileDownloadReader\nreader\n=\nnew\nDataSetFileDownloadReader\n(\nstream\n);\nDataSetFileDownload\nfile\n=\nnull\n;\nwhile\n((\nfile\n=\nreader\n.\nread\n())\n!=\nnull\n)\n{\n## System\n.\nout\n.\nprintln\n(\n## \"Downloaded \"\n+\nfile\n.\ngetDataSetFile\n().\ngetPath\n()\n+\n\" \"\n+\nfile\n.\ngetDataSetFile\n().\ngetFileLength\n());\n## System\n.\nout\n.\nprintln\n(\n## \"-----FILE CONTENTS-----\"\n);\n## System\n.\nout\n.\nprintln\n(\nfile\n.\ngetInputStream\n());\n}\n}\n}\n## Fast Downloading\n\nFast downloading is based on the\nSIS File Transfer Protocol\nand\n## library. Downloading is done in two steps:\nCreate a fast download session with the\nmethod\ncreateFastDownloadSession()\non  V3 DSS API. One parameter\nis a list of data set file ids. Such an id contains the data set\ncode and the path to the file inside the data set. If a file id\npoints to a folder the whole folder will be downloaded. The last\nparameter specifies download preferences. Currently only the wished\nnumber of parallel download streams can be specified. The API call\nreturns a\nFastDownloadSession\nobject.\nDownload the files with the helper class\nFastDownloader\n## . The\nsimplest usage is just do ``\nSearch and list all files inside a data store\nnew FastDownloader(downloadSession).downloadTo(destinationFolder);\nThe files are stored in the destination folder in\n/\n.\nHere is a complete example:\nSearch and list all files inside a data store\nimport\njava.io.File\n;\nimport\njava.nio.file.Path\n;\nimport\njava.util.ArrayList\n;\nimport\njava.util.Collection\n;\nimport\njava.util.List\n;\nimport\njava.util.Map\n;\nimport\njava.util.Map.Entry\n;\nimport\norg.apache.commons.lang3.time.StopWatch\n;\nimport\nch.ethz.sis.filetransfer.DownloadListenerAdapter\n;\nimport\nch.ethz.sis.filetransfer.IDownloadItemId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.DataSet\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.fetchoptions.DataSetFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.search.DataSetSearchCriteria\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fastdownload.FastDownloadSession\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.fastdownload.FastDownloadSessionOptions\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.DataSetFilePermId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.datasetfile.id.IDataSetFileId\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.fastdownload.FastDownloadResult\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.fastdownload.FastDownloader\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\nimport\nch.systemsx.cisd.openbis.common.api.client.ServiceFinder\n;\npublic\nclass\nV3FastDownloadExample\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\nIApplicationServerApi\nv3\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n\"http://localhost:8888/openbis/openbis\"\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n,\n10000\n);\n## String\nsessionToken\n=\nv3\n.\nlogin\n(\n\"test\"\n,\n\"password\"\n);\n// Search for some data sets\nDataSetSearchCriteria\nsearchCriteria\n=\nnew\nDataSetSearchCriteria\n();\nsearchCriteria\n.\nwithCode\n().\nthatStartsWith\n(\n\"201902\"\n);\nDataSetFetchOptions\nfetchOptions\n=\nnew\nDataSetFetchOptions\n();\nfetchOptions\n.\nwithDataStore\n();\nfetchOptions\n.\nwithPhysicalData\n();\n## List\n<\nDataSet\n>\ndataSets\n=\nv3\n.\nsearchDataSets\n(\nsessionToken\n,\nsearchCriteria\n,\nfetchOptions\n).\ngetObjects\n();\n// Get the DSS URL from the first data set assuming that all data sets from the same data store\n## String\ndssUrl\n=\ndataSets\n.\nget\n(\n0\n).\ngetDataStore\n().\ngetDownloadUrl\n();\n## System\n.\nout\n.\nprintln\n(\n\"url:\"\n+\ndssUrl\n);\n// Create DSS server\nIDataStoreServerApi\ndssServer\n=\nnew\nServiceFinder\n(\n\"datastore_server\"\n,\nIDataStoreServerApi\n.\n## SERVICE_URL\n)\n.\ncreateService\n(\nIDataStoreServerApi\n.\nclass\n,\ndssUrl\n);\n// We download all files of the all found data sets.\n## List\n<\nDataSetFilePermId\n>\nfileIds\n=\nnew\nArrayList\n<>\n();\nfor\n(\nDataSet\ndataSet\n## :\ndataSets\n)\n{\nfileIds\n.\nadd\n(\nnew\nDataSetFilePermId\n(\nnew\nDataSetPermId\n(\ndataSet\n.\ngetCode\n())));\n}\n// Create the download session for 2 streams in parallel (if possible)\nFastDownloadSession\ndownloadSession\n=\ndssServer\n.\ncreateFastDownloadSession\n(\nsessionToken\n,\nfileIds\n,\nnew\nFastDownloadSessionOptions\n().\nwithWishedNumberOfStreams\n(\n2\n));\n// Do the actual download into 'targets/fast-download' and print the time needed by using a download listener\nFastDownloadResult\nresult\n=\nnew\nFastDownloader\n(\ndownloadSession\n).\nwithListener\n(\nnew\nDownloadListenerAdapter\n()\n{\nprivate\nStopWatch\nstopWatch\n=\nnew\nStopWatch\n();\n## @Override\npublic\nvoid\nonDownloadStarted\n()\n{\nstopWatch\n.\nstart\n();\n}\n## @Override\npublic\nvoid\nonDownloadFinished\n(\n## Map\n<\nIDownloadItemId\n,\n## Path\n>\nitemPaths\n)\n{\n## System\n.\nout\n.\nprintln\n(\n\"Successfully finished after \"\n+\nstopWatch\n);\n}\n## @Override\npublic\nvoid\nonDownloadFailed\n(\n## Collection\n<\n## Exception\n>\ne\n)\n{\n## System\n.\nout\n.\nprintln\n(\n\"Downloading failed after \"\n+\nstopWatch\n);\n}\n})\n.\ndownloadTo\n(\nnew\n## File\n(\n\"targets/fast-download\"\n));\n// Print the mapping of data set file id to the actual path\nfor\n(\n## Entry\n<\nIDataSetFileId\n,\n## Path\n>\nentry\n## :\nresult\n.\ngetPathsById\n().\nentrySet\n())\n{\n## System\n.\nout\n.\nprintln\n(\nentry\n);\n}\nv3\n.\nlogout\n(\nsessionToken\n);\n}\n}\nWhat happens under the hood?\n\nThe files to be downloaded are chunked into chunks of maximum size 1 MB.\nOn the DSS a special web service (\nFileTransferServerServlet\n) provides\nthese chunks. On the client side these chunks are requested and stored\nin the file system. This is done in parallel if possible and requested\n(withWishedNumberOfStreams). The server tells the client the actual\nnumber of streams available for parallel downloading without slowing\ndown DSS. The actual number of streams depends on\nthe wished number of streams\nthe number of streams currently used by other download sessions\nthe maximum number of allowed streams as specified by the\nproperty\napi.v3.fast-download.maximum-number-of-allowed-streams\nin\n## DSS\nservice.properties\n. Default value is 10.\nThe actual number of streams is half of the number of free streams or\nthe wished number of streams, if it is less. The number of free streams\nis given by the difference between the maximum number of allowed streams\nand the total number of used streams.\nIt is possible that the actual number of streams is zero if the server\nis currently too busy with downloading (that is, there is no free\ndowload stream available). The FastDownloader will retry it later.\n## Customizing Fast Dowloading\n\nThere are three ways to customizing the FastDownloader:\nwithListener(): Adds a listener which will be notified when\nthe download session has been started/finished/failed,\nthe download of a file/folder has been started/finished and\na chunk has been downloaded.\nThere can be several listeners. By default there are no\nlisteners. Note, that listeners are notified in a separated\nthread associated with the download session.\nwithLogger(): Sets a logger. By default nothing is logged.\nwithRetryProviderFactory(): Sets the factory which creates a retry\nprovider. A retry provider knows when and how often a failed action\n(e.g. sever request) should be retried. By default it is retried\nthree times. The first retry is a second later. For each following\nretry the waiting time is increases by the factor two.\nRegister Data Sets\n\nTo register datasets using the Java or JavaScript API use one of the\nfollowing examples as a template.\n## Example (Java)\nRegister Data Set\nimport\njava.util.UUID\n;\nimport\norg.eclipse.jetty.client.HttpClient\n;\nimport\norg.eclipse.jetty.client.api.Request\n;\nimport\norg.eclipse.jetty.client.util.MultiPartContentProvider\n;\nimport\norg.eclipse.jetty.client.util.StringContentProvider\n;\nimport\norg.eclipse.jetty.http.HttpMethod\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.dataset.id.DataSetPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.entitytype.id.EntityTypePermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.sample.id.SampleIdentifier\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.IDataStoreServerApi\n;\nimport\nch.ethz.sis.openbis.generic.dssapi.v3.dto.dataset.create.UploadedDataSetCreation\n;\nimport\nch.systemsx.cisd.common.http.JettyHttpClientFactory\n;\nimport\nch.systemsx.cisd.common.spring.HttpInvokerUtils\n;\npublic\nclass\nRegisterDataSet\n{\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\nthrows\n## Exception\n{\nfinal\n## String\n## AS_URL\n=\n\"http://localhost:8888/openbis/openbis\"\n;\nfinal\n## String\n## DSS_URL\n=\n\"http://localhost:8889/datastore_server\"\n;\nfinal\nOpenBIS\nopenbisV3\n=\nnew\nOpenBIS\n(\n## AS_URL\n,\n## DSS_URL\n);\nopenbisV3\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n);\nfinal\n## Path\npath\n=\n## Path\n.\nof\n(\n\"/uploadPath\"\n);\nfinal\n## String\nuploadId\n=\nopenbisV3\n.\nuploadFileWorkspaceDSS\n(\npath\n);\nfinal\nUploadedDataSetCreation\ncreation\n=\nnew\nUploadedDataSetCreation\n();\ncreation\n.\nsetUploadId\n(\nuploadId\n);\ncreation\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n## \"/DEFAULT/DEFAULT/DEFAULT\"\n));\ncreation\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"ATTACHMENT\"\n,\nEntityKind\n.\n## DATA_SET\n));\ntry\n{\nfinal\nDataSetPermId\ndataSetPermId\n=\nopenbisV3\n.\ncreateUploadedDataSet\n(\ncreation\n);\n// A data set assigned to the experiment \"/DEFAULT/DEFAULT/DEFAULT\" with the folder \"uploadPath\" is created\n## System\n.\nout\n.\nprintln\n(\n\"dataSetPermId=\"\n+\ndataSetPermId\n);\n}\ncatch\n(\nfinal\n## Exception\ne\n)\n{\ne\n.\nprintStackTrace\n();\n}\nopenbisV3\n.\nlogout\n();\n}\n}\n## Example (Javascript)\nRegister Data Set\n<!DOCTYPE html>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\ncharset\n=\n\"utf-8\"\n>\n<\ntitle\n>\nDataset upload\n</\ntitle\n>\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"/openbis-test/resources/api/v3/config.js\"\n></\nscript\n>\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"/openbis-test/resources/api/v3/require.js\"\n></\nscript\n>\n</\nhead\n>\n<\nbody\n>\n<\nlabel\nfor\n=\n\"myfile\"\n>\nSelect a file:\n</\nlabel\n>\n<\ninput\ntype\n=\n\"file\"\nid\n=\n\"myFile\"\n/>\n<\nscript\n>\nrequire\n([\n\"openbis\"\n,\n\"dss/dto/dataset/create/UploadedDataSetCreation\"\n,\n\"as/dto/experiment/id/ExperimentIdentifier\"\n,\n\"as/dto/entitytype/id/EntityTypePermId\"\n,\n\"as/dto/entitytype/EntityKind\"\n],\nfunction\n(\nopenbis\n,\nUploadedDataSetCreation\n,\nExperimentIdentifier\n,\nEntityTypePermId\n,\nEntityKind\n)\n{\nvar\ntestProtocol\n=\nwindow\n.\nlocation\n.\nprotocol\n;\nvar\ntestHost\n=\nwindow\n.\nlocation\n.\nhostname\n;\nvar\ntestPort\n=\nwindow\n.\nlocation\n.\nport\n;\nvar\ntestUrl\n=\ntestProtocol\n+\n\"//\"\n+\ntestHost\n+\n\":\"\n+\ntestPort\n;\nvar\ntestApiUrl\n=\ntestUrl\n+\n\"/openbis/openbis/rmi-application-server-v3.json\"\n;\nvar\nopenbisV3\n=\nnew\nopenbis\n(\ntestApiUrl\n);\nvar\nfileInput\n=\ndocument\n.\ngetElementById\n(\n\"myFile\"\n);\nfileInput\n.\nonchange\n=\n(\ne\n)\n=>\n{\nvar\nfiles\n=\ne\n.\ntarget\n.\nfiles\n;\nopenbisV3\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n).\ndone\n(\nsessionToken\n=>\n{\nvar\ndataStoreFacade\n=\nopenbisV3\n.\ngetDataStoreFacade\n();\ndataStoreFacade\n.\nuploadFilesWorkspaceDSS\n(\nfiles\n).\ndone\n(\nuploadId\n=>\n{\nvar\ncreation\n=\nnew\nUploadedDataSetCreation\n();\ncreation\n.\nsetUploadId\n(\nuploadId\n);\ncreation\n.\nsetExperimentId\n(\nnew\nExperimentIdentifier\n(\n## \"/DEFAULT/DEFAULT/DEFAULT\"\n));\ncreation\n.\nsetTypeId\n(\nnew\nEntityTypePermId\n(\n## \"ATTACHMENT\"\n,\nEntityKind\n.\n## DATA_SET\n));\ndataStoreFacade\n.\ncreateUploadedDataSet\n(\ncreation\n).\ndone\n(\ndataSetPermId\n=>\n{\n// A data set assigned to the experiment \"/DEFAULT/DEFAULT/DEFAULT\" with the folder \"uploadPath\" is created\nconsole\n.\nlog\n(\n\"dataSetPermId=\"\n+\ndataSetPermId\n);\nopenbisV3\n.\nlogout\n();\n}).\nfail\n(\nerror\n=>\n{\nconsole\n.\nerror\n(\nerror\n);\nopenbisV3\n.\nlogout\n();\n});\n});\n});\n}\n});\n</\nscript\n>\n</\nbody\n>\n</\nhtml\n>\nVI. Web application context\n\nWhen making web applications and embedding them into an openBIS tab on\nthe core UI is often required to have information about the context\nthose applications are being loaded for two particular purposes:\nMaking the application context sensitive and show\ninformation/functionality related to the current context. The\ncontext object provided by\ngetWebAppContext()\ncontains all\ninformation required for this purpose.\nLogin into the facade without presenting the user with another login\nscreen since they have already login into openBIS. For\nthat\nloginFromContext()\ncan be used.\nThis methods only exist on the Javascript facade with the purpose of\nbeing used on embedded web applications, calling them from an external\nweb application will do nothing.\nWebAppContextExample.html\n<\nscript\n>\nrequire\n([\n'openbis'\n],\nfunction\n(\nopenbis\n)\n{\nvar\nopenbisV3\n=\nnew\nopenbis\n();\nvar\nwebappcontext\n=\nopenbisV3\n.\ngetWebAppContext\n();\nconsole\n.\nlog\n(\nwebappcontext\n.\ngetWebappCode\n());\nconsole\n.\nlog\n(\nwebappcontext\n.\ngetSessionId\n());\nconsole\n.\nlog\n(\nwebappcontext\n.\ngetEntityKind\n());\nconsole\n.\nlog\n(\nwebappcontext\n.\ngetEntityType\n());\nconsole\n.\nlog\n(\nwebappcontext\n.\ngetEntityIdentifier\n());\nconsole\n.\nlog\n(\nwebappcontext\n.\ngetEntityPermId\n());\nopenbisV3\n.\nloginFromContext\n();\nopenbisV3\n.\ngetSessionInformation\n().\ndone\n(\nfunction\n(\nsessionInfo\n)\n{\nconsole\n.\nlog\n(\nsessionInfo\n.\ngetUserName\n());\n});\n});\n</\nscript\n>", "timestamp": "2025-09-18T09:38:29.713687Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_matlab-v3-api:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/matlab-v3-api.html", "repo": "openbis", "title": "Matlab (V3 API) - How to access openBIS from MATLAB", "section": "Preamble", "text": "Matlab (V3 API) - How to access openBIS from MATLAB\n\n## Preamble\n\nopenBIS\nis a research data management system developed by\n## ETH SIS\n. Data stored in openBIS can be accessed directly via the web UI or programmatically using APIs. For example,\npyBIS\nis a project that provides a Python 3 module for interacting with openBIS.\n## MATLAB\nis a high-level numerical computing environment that is popular in many areas of science. This repository provides a toolbox to access data in openBIS directly from MATLAB.\n## Setup\n\nThe toolbox interacts with openBIS by calling pyBIS functions directly from MATLAB. Therefore, both Python and MATLAB have to be installed and configured properly. Please consult the\nMATLAB - Python compatibility table\nto choose the correct versions. Also note that Python 2.7 is no longer supported!\nmacOS\n\nOn macOS, the setup has been tested with a Miniconda Python distribution.\nDownload and install\n## Miniconda3\n(use a Python version according to the\nMATLAB - Python compatibility table\n)\nOpen the terminal and install pyBIS with pip:\npip\ninstall\npybis\nFind the path to your Python executable:\nwhich\npython\nOpen MATLAB and set the Python executable. On Matlab R2019b or later, use the command:\npyenv('Version',\n'Path/to/python')\n. Replace with the path found in previous step. On earlier versions of Matlab, the\npyenv\ncommand is called\npyversion\n.\n## Windows 10\n\nOn Windows using the Anaconda or Miniconda approach did not work (for some reason, MATLAB could not find the Python modules). On the other hand, using the standard Python installation seems to work.\nDownload and install Python\nhere\n(use a Python version according to the\nMATLAB - Python compatibility table\n). Make sure to choose the\n64-bit version\n.\nDuring the installation, make sure Python is added to the Path and registered as default Python interpreter. To do this, select the little tick box\n## Add\n## Python\n3.x\nto\n## PATH\nin the installation window:\nOpen Windows PowerShell and install pyBIS with pip:\npip\ninstall\npybis\nFind the path to your Python executable by typing:\n## Get-Command\npython\n. The path is listed in the Source column, i.e.\nC:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python38\\python.exe\n. Copy the path by selecting it and pressing\n## Ctrl-C\nOpen MATLAB and set the Python executable. On Matlab R2019b or later, use the command:\npyenv('Version',\n'C:\\Path\\to\\Programs\\python.exe')\n. Replace with the path found in step 4. On earlier versions of Matlab, the\npyenv\ncommand is called\npyversion\n.\n### Usage\n\n## Download\nthis repository\nand add it to your Matlab Path. If you are running the toolbox for the first time, make sure to carry out the steps described under\n## Setup\nabove. An\nexample script\ndemonstrating some common usage patterns is provided in the repository. The script can be run interactively in the MATLAB Live Editor. Type\ndoc\nOpenBis\nin the Matlab Command Window to access the built-in documentation.\n## Notes\n\nI do not have time to test these instructions and the toolbox with all combinations of Python & Matlab versions on different operating systems. In general, a combination of recent Python and Matlab versions should work on macOS and Windows. If you run into any issues, please feel free to contact the\nSIS Helpdesk\n.", "timestamp": "2025-09-18T09:38:29.732209Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html", "repo": "openbis", "title": "Personal Access Tokens", "section": "Personal Access Tokens", "text": "## Personal Access Tokens\n\n## Background\n\n“Personal access token” (in short: PAT) is an openBIS feature that was\nintroduced to simplify integration of openBIS with other systems. Such\nintegrations are usually done using openBIS V3 API and therefore require\nan external application to authenticate in openBIS to fetch or create\nsome data. Without “Personal access tokens” the only way of\nauthenticating in openBIS V3 API was the V3 API login method. Given a\nuser name and a password the login method would return back an openBIS\nsession token, which could be later used in other V3 API calls as a\nsecret and a proof of who we are.\nUnfortunately, even though this approach worked well it had some\nlimitations. These were mainly caused by the nature of session tokens in\n## openBIS:\nsession tokens are short lived\nsession tokens do not survive openBIS restarts\nobtaining a new session token requires a user name and a password\nBecause of these limitations external applications had to be prepared\nfor a situation where an openBIS session token stops working. They had\nto know how to recover. When one session token expired or was\ninvalidated they had to obtain a new one by calling the login method\nagain and providing a user name and a password. But even then the whole\nstate of the previous session (e.g. files stored in the session\nworkspace) would be gone and not available in the new session.\nDepending on a use case and a type of the integration that could cause\nsmaller or bigger headaches for the developers of the external system.\nFortunately, “Personal access tokens” come to a rescue.\nWhat are “Personal access tokens” ?\n\nA personal access token (in short: PAT) is very similar to a session\ntoken but there are also some important differences.\n## Similarities:\na PAT is bound to a specific user and represents that user’s\nsession. Two users can’t share a session using PAT. Internal PAT\nsessions identifier is the combination of both the userId and the\nsession name.\na PAT is a secret that must not be publicly shared (having a user’s\nPAT one can perform any actions in openBIS that this user could\nnormally perform, except for user and PAT management)\na user can have multiple PATs active at the same time\na PAT can be used in places where a regular session token could be\nnormally used, e.g. to call V3 API methods (a full list of endpoints\nthat support PATs is presented below)\n## Differences:\na PAT is created using a dedicated “createPersonalAccessTokens” V3\nAPI method (not using “login” method as a regular session token)\na PAT can be long lived (its validFrom and validTo dates are defined\nat the moment of creation), still it should be replaced periodically\nfor security reasons\na PAT session survives openBIS restarts, i.e. the same PAT can be\nused before and after a restart (session workspace folder state is\nalso kept)\nmultiple PATs may represent a single PAT session (both PATs must\nhave the same “session name”) - this becomes useful for handling a\ntransition period from one soon to be expired PAT to a new PAT that\nreplaces it without losing the session’s state\nWho can create a “Personal access token” ?\n\nAny openBIS user can manage its own PATs. Instance admin users can\nmanage all PATs in the system.\nWhere can I use “Personal access tokens” ?\n\nEndpoints that support PATs:\n## AS:\n## V3 API\nFile Upload Servlet (class: UploadServiceServlet, path: /upload)\n## File Download Servlet (class: DownloadServiceServlet, path:\n/download)\n## Session Workspace Provider\n## DSS:\n## V3 API\n## File Upload Servlet (class: StoreShareFileUploadServlet, path:\n/store_share_file_upload)\nFile Download Servlet (class: DatasetDownloadServlet, path: /*)\n## Session Workspace Upload Servlet (class:\n## SessionWorkspaceFileUploadServlet, path:\n/session_workspace_file_upload)\n## Session Workspace Download Servlet (class:\n## SessionWorkspaceFileDownloadServlet, path:\n/session_workspace_file_download)\n## Session Workspace Provider\n## SFTP\nWhere “Personal access tokens” are stored ?\n\nPATs are stored in “personal-access-tokens.json” JSON file. By default\nthe file is located in the main openBIS folder where it survives openBIS\nrestarts and upgrades.\nThe location can be changed using “personal-access-tokens-file-path”\nproperty in AS service.properties. The JSON file is read at the openBIS\nstart up.\nHow long should my “Personal Access Tokens” be valid ?\n\nBecause of security reasons PATs should not be valid indefinitely.\nInstead, each PAT should have a well defined validity period after which\nit should be replaced with a new PAT with a different hash. To make this\ntransition as smooth as possible please use the following guide:\ncreate PAT_1 with sessionName = <MY_SESSION> and use it in\nyour integration\nwhen PAT_1 is soon to be expired, create PAT_2 with the same\nsessionName = <MY_SESSION> (both PAT_1 and PAT_2 will work\nat this point and will refer to the same openBIS session)\nreplace PAT_1 with PAT_2 in your integration\nPATs created by the same user and with the same “session name” refer\nunder the hood to the same openBIS session. Therefore, even if one of\nsuch PATs expires the session is kept active and its state is\nmaintained.\n### Configuration\n\n“Personal access tokens” functionality is enabled by default. To\n## configure it please use AS service.properties:\n# personal access tokens feature\npersonal-access-tokens-enabled = true", "timestamp": "2025-09-18T09:38:29.738616Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html", "repo": "openbis", "title": "Personal Access Tokens", "section": "change the default location of the JSON file that stores personal access tokens (default: personal-access-tokens.json file in the main openBIS folder)", "text": "# change the default location of the JSON file that stores personal access tokens (default: personal-access-tokens.json file in the main openBIS folder)\npersonal-access-tokens-file-path = MY_FOLDER/personal-access-tokens.json", "timestamp": "2025-09-18T09:38:29.738616Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:2", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html", "repo": "openbis", "title": "Personal Access Tokens", "section": "set maximum allowed validity period (in seconds) - personal access token with a longer validity period cannot be created (default: 30 days)", "text": "# set maximum allowed validity period (in seconds) - personal access token with a longer validity period cannot be created (default: 30 days)\npersonal-access-tokens-max-validity-period = 2592000", "timestamp": "2025-09-18T09:38:29.738616Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_personal-access-tokens:3", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/personal-access-tokens.html", "repo": "openbis", "title": "Personal Access Tokens", "section": "set validity warning period (in seconds) - owners of personal access tokens that are going to expire within this warning period are going to receive email notifications (default: 5 days)", "text": "# set validity warning period (in seconds) - owners of personal access tokens that are going to expire within this warning period are going to receive email notifications (default: 5 days)\npersonal-access-tokens-validity-warning-period = 259200\n## Typical Application Workflow\n\nMost typical use case for Personal Access Tokens is to run code on a\nthird party service against openBIS.\nOn such services we want to have:\nA long lasting session with openBIS for several days that survives\nrestarts.\nWe don’t want to keep the user and password stored.\nFor such services we recommend to create a PAT on log in and store the\nPAT instead. We provide the example Gradle project with the java class\nPersonalAccessTokensApplicationWorkflows (\nsource downloadable\nhere\n)\nas the recommend way to manage getting the most up to date personal\naccess token for an application and user. Including creation and renewal\nmanagement.\nprivate\nstatic\nfinal\n## String\n## URL\n=\n\"https://openbis-sis-ci-sprint.ethz.ch/openbis/openbis\"\n+\nIApplicationServerApi\n.\n## SERVICE_URL\n;\nprivate\nstatic\nfinal\nint\n## TIMEOUT\n=\n10000\n;\nprivate\nstatic\nfinal\n## String\n## USER\n=\n\"admin\"\n;\nprivate\nstatic\nfinal\n## String\n## PASSWORD\n=\n\"changeit\"\n;\npublic\nstatic\nvoid\nmain\n(\n## String\n[]\nargs\n)\n{\nIApplicationServerApi\nv3\n=\nHttpInvokerUtils\n.\ncreateServiceStub\n(\nIApplicationServerApi\n.\nclass\n,\n## URL\n,\n## TIMEOUT\n);\n## String\nsessionToken\n=\nv3\n.\nlogin\n(\n## USER\n,\n## PASSWORD\n);\n## System\n.\nout\n.\nprintln\n(\n\"sessionToken: \"\n+\nsessionToken\n);\nPersonalAccessTokenPermId\npat\n=\nPersonalAccessTokensApplicationWorkflows\n.\ngetApplicationPersonalAccessTokenOnLogin\n(\nv3\n,\nsessionToken\n,\n## \"MY_APPLICATION\"\n);\n## System\n.\nout\n.\nprintln\n(\n\"pat: \"\n+\npat\n);\nv3\n.\nlogout\n(\nsessionToken\n);\n}\npackage\nch.ethz.sis.pat\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.search.SearchResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.PersonalAccessToken\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.create.PersonalAccessTokenCreation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.fetchoptions.PersonalAccessTokenFetchOptions\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.id.PersonalAccessTokenPermId\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.pat.search.PersonalAccessTokenSearchCriteria\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.session.SessionInformation\n;\nimport\norg.apache.commons.lang3.time.DateUtils\n;\nimport\njava.util.Calendar\n;\nimport\njava.util.Date\n;\nimport\njava.util.List\n;\nimport\njava.util.Map\n;\npublic\nclass\nPersonalAccessTokensApplicationWorkflows\n{\nprivate\nstatic\nfinal\nint\n## DAY_IN_SECONDS\n=\n24\n*\n60\n*\n60\n;\nprivate\nstatic\nfinal\n## String\n## PERSONAL_ACCESS_TOKENS_MAX_VALIDITY_PERIOD\n=\n\"personal-access-tokens-max-validity-period\"\n;\nprivate\nstatic\nfinal\n## String\n## PERSONAL_ACCESS_TOKENS_VALIDITY_WARNING_PERIOD\n=\n\"personal-access-tokens-validity-warning-period\"\n;\nprivate\nPersonalAccessTokensApplicationWorkflows\n()\n{\n}\n/*\n* This utility method returns the current application token, creates one if no one is found and renews it if is close to expiration.\n* Requires are real session token hence requires a form where the user can input its user and password on an application.\n*/\npublic\nstatic\nPersonalAccessTokenPermId\ngetApplicationPersonalAccessTokenOnLogin\n(\nIApplicationServerApi\nv3\n,\n## String\nsessionToken\n,\n## String\napplicationName\n)\n{\n// Obtain servers renewal information\n## Map\n<\n## String\n,\n## String\n>\ninformation\n=\nv3\n.\ngetServerInformation\n(\nsessionToken\n);\nint\npersonalAccessTokensRenewalPeriodInSeconds\n=\n## Integer\n.\nparseInt\n(\ninformation\n.\nget\n(\nPersonalAccessTokensApplicationWorkflows\n.\n## PERSONAL_ACCESS_TOKENS_VALIDITY_WARNING_PERIOD\n));\nint\npersonalAccessTokensRenewalPeriodInDays\n=\npersonalAccessTokensRenewalPeriodInSeconds\n/\n## DAY_IN_SECONDS\n;\nint\npersonalAccessTokensMaxValidityPeriodInSeconds\n=\n## Integer\n.\nparseInt\n(\ninformation\n.\nget\n(\nPersonalAccessTokensApplicationWorkflows\n.\n## PERSONAL_ACCESS_TOKENS_MAX_VALIDITY_PERIOD\n));\nint\npersonalAccessTokensMaxValidityPeriodInDays\n=\npersonalAccessTokensMaxValidityPeriodInSeconds\n/\n## DAY_IN_SECONDS\n;\n// Obtain user id\nSessionInformation\nsessionInformation\n=\nv3\n.\ngetSessionInformation\n(\nsessionToken\n);\n// Search for PAT for this user and application\n// NOTE: Standard users only get their PAT but admins get all, filtering with the user solves this corner case\nPersonalAccessTokenSearchCriteria\npersonalAccessTokenSearchCriteria\n=\nnew\nPersonalAccessTokenSearchCriteria\n();\npersonalAccessTokenSearchCriteria\n.\nwithSessionName\n().\nthatEquals\n(\napplicationName\n);\npersonalAccessTokenSearchCriteria\n.\nwithOwner\n().\nwithUserId\n().\nthatEquals\n(\nsessionInformation\n.\ngetPerson\n().\ngetUserId\n());\nSearchResult\n<\nPersonalAccessToken\n>\npersonalAccessTokenSearchResult\n=\nv3\n.\nsearchPersonalAccessTokens\n(\nsessionToken\n,\npersonalAccessTokenSearchCriteria\n,\nnew\nPersonalAccessTokenFetchOptions\n());\nPersonalAccessToken\nbestTokenFound\n=\nnull\n;\nPersonalAccessTokenPermId\nbestTokenFoundPermId\n=\nnull\n;\n// Obtain longer lasting application token\nfor\n(\nPersonalAccessToken\npersonalAccessToken\n## :\npersonalAccessTokenSearchResult\n.\ngetObjects\n())\n{\nif\n(\npersonalAccessToken\n.\ngetValidToDate\n().\nafter\n(\nnew\n## Date\n()))\n{\nif\n(\nbestTokenFound\n==\nnull\n)\n{\nbestTokenFound\n=\npersonalAccessToken\n;\n}\nelse\nif\n(\npersonalAccessToken\n.\ngetValidToDate\n().\nafter\n(\nbestTokenFound\n.\ngetValidToDate\n()))\n{\nbestTokenFound\n=\npersonalAccessToken\n;\n}\n}\n}\n// If best token doesn't exist, create\nif\n(\nbestTokenFound\n==\nnull\n)\n{\nbestTokenFoundPermId\n=\ncreateApplicationPersonalAccessToken\n(\nv3\n,\nsessionToken\n,\napplicationName\n,\npersonalAccessTokensMaxValidityPeriodInDays\n);\n}\n// If best token is going to expire in less than the warning period, renew\n## Calendar\nrenewalDate\n=\n## Calendar\n.\ngetInstance\n();\nrenewalDate\n.\nadd\n(\n## Calendar\n.\n## DAY_OF_MONTH\n,\npersonalAccessTokensRenewalPeriodInDays\n);\nif\n(\nbestTokenFound\n!=\nnull\n&&\nbestTokenFound\n.\ngetValidToDate\n().\nbefore\n(\nrenewalDate\n.\ngetTime\n()))\n{\nbestTokenFoundPermId\n=\ncreateApplicationPersonalAccessToken\n(\nv3\n,\nsessionToken\n,\napplicationName\n,\npersonalAccessTokensMaxValidityPeriodInDays\n);\n}\n// If we have not created or renewed, return current\nif\n(\nbestTokenFoundPermId\n==\nnull\n)\n{\nbestTokenFoundPermId\n=\nbestTokenFound\n.\ngetPermId\n();\n}\nreturn\nbestTokenFoundPermId\n;\n}\nprivate\nstatic\nPersonalAccessTokenPermId\ncreateApplicationPersonalAccessToken\n(\nIApplicationServerApi\nv3\n,\n## String\nsessionToken\n,\n## String\napplicationName\n,\nint\npersonalAccessTokensMaxValidityPeriodInDays\n)\n{\nPersonalAccessTokenCreation\ncreation\n=\nnew\nPersonalAccessTokenCreation\n();\ncreation\n.\nsetSessionName\n(\napplicationName\n);\ncreation\n.\nsetValidFromDate\n(\nnew\n## Date\n(\n## System\n.\ncurrentTimeMillis\n()\n-\nDateUtils\n.\n## MILLIS_PER_DAY\n));\ncreation\n.\nsetValidToDate\n(\nnew\n## Date\n(\n## System\n.\ncurrentTimeMillis\n()\n+\nDateUtils\n.\n## MILLIS_PER_DAY\n*\npersonalAccessTokensMaxValidityPeriodInDays\n));\n## List\n<\nPersonalAccessTokenPermId\n>\npersonalAccessTokens\n=\nv3\n.\ncreatePersonalAccessTokens\n(\nsessionToken\n,\n## List\n.\nof\n(\ncreation\n));\nreturn\npersonalAccessTokens\n.\nget\n(\n0\n);\n}\n}\n## V3 API\n\nCode examples for personal access tokens can be found in the main V3 API documentation:\nopenBIS V3 API#PersonalAccessTokens", "timestamp": "2025-09-18T09:38:29.738616Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_python-v3-api:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/python-v3-api.html", "repo": "openbis", "title": "Python (V3 API) - pyBIS!", "section": "Jupyter Notebook", "text": "Python (V3 API) - pyBIS!\n\npyBIS is a Python module for interacting with openBIS. pyBIS is designed to be most useful in a\n## Jupyter Notebook\nor IPython environment, especially if you are developing Python scripts for automatisation. Jupyter Notebooks offer some sort of IDE for openBIS, supporting TAB completition and immediate data checks, making the life of a researcher hopefully easier.\nDependencies and Requirements\n\npyBIS relies the openBIS API v3\nopenBIS version 16.05.2 or newer is required\n19.06.5 or later is recommended\npyBIS uses Python 3.6 or newer and the Pandas module\n### Installation\n\npip\ninstall\n--\nupgrade\npybis\nThat command will download install pyBIS and all its dependencies. If pyBIS is already installed, it will be upgraded to the latest version.\nIf you haven’t done yet, install Jupyter and/or Jupyter Lab (the next Generation of Jupyter):\npip\ninstall\njupyter\npip\ninstall\njupyterlab\n### General Usage\n\nTAB completition and other hints in Jupyter / IPython\n\nin a Jupyter Notebook or IPython environment, pybis helps you to enter the commands\nAfter every dot\n.\nyou might hit the\n## TAB\nkey in order to look at the available commands.\nif you are unsure what parameters to add to a , add a question mark right after the method and hit\n## SHIFT+ENTER\nJupyter will then look up the signature of the method and show some helpful docstring\nChecking input\n\nWhen working with properties of entities, they might use a\ncontrolled vocabulary\nor are of a specific\nproperty type\n.\nAdd an underscore\n_\ncharacter right after the property and hit\n## SHIFT+ENTER\nto show the valid values\nWhen a property only acceps a controlled vocabulary, you will be shown the valid terms in a nicely formatted table\nif you try to assign an\ninvalid value\nto a property, you’ll receive an error immediately\n## Glossary\n\n## spaces:\nused for authorisation eg. to separate two working groups. If you have permissions in a space, you can see everything which in that space, but not necessarily in another space (unless you have the permission).\n## projects:\na space consists of many projects.\n## experiments / collections:\na projects contain many experiments. Experiments can have\nproperties\n## samples / objects:\nan experiment contains many samples. Samples can have\nproperties\n## dataSet:\na dataSet which contains the actual\ndata files\n, either pyhiscal (stored in openBIS dataStore) or linked\n## attributes:\nevery entity above contains a number of attributes. They are the same accross all instances of openBIS and independent of their type.\n## properties:\nAdditional specific key-value pairs, available for these entities:\nexperiments\nsamples\ndataSets\nevery single instance of an entity must be of a specific\nentity type\n(see below). The type defines the set of properties.\n## experiment type / collection type:\na type for experiments which specifies its properties\n## sample type / object type:\na type for samples / objects which specifies its properties\n## dataSet type:\na type for dataSets which specifies its properties\n## property type:\na single property, as defined in the entity types above. It can be of a classic data type (e.g. INTEGER, VARCHAR, BOOLEAN) or its values can be controlled (CONTROLLEDVOCABULARY).\n## plugin:\na script written in\n## Jython\nwhich allows to check property values in a even more detailed fashion\nconnect to OpenBIS\n\nlogin\n\nIn an\ninteractive session\ne.g. inside a Jupyter notebook, you can use\ngetpass\nto enter your password safely:\nfrom\npybis\nimport\n## Openbis\no\n=\n## Openbis\n(\n'https://example.com'\n)\no\n=\n## Openbis\n(\n'example.com'\n)\n# https:// is assumed\nimport\ngetpass\npassword\n=\ngetpass\n.\ngetpass\n()\no\n.\nlogin\n(\n'username'\n,\npassword\n,\nsave_token\n=\n## True\n)\n# save the session token in ~/.pybis/example.com.token\nIn a\nscript\nyou would rather use two\nenvironment variables\nto provide username and password:\nfrom\npybis\nimport\n## Openbis\no\n=\n## Openbis\n(\nos\n.\nenviron\n[\n## 'OPENBIS_HOST'\n])\no\n.\nlogin\n(\nos\n.\nenviron\n[\n## 'OPENBIS_USERNAME'\n],\nos\n.\nenviron\n[\n## 'OPENBIS_PASSWORD'\n])\nAs an even better alternative, you should use personal access tokens (PAT) to avoid username/password altogether. See below.\nVerify certificate\n\nBy default, your SSL-Certification is being verified. If you have a test-instance with a self-signed certificate, you’ll need to turn off this verification explicitly:\nfrom\npybis\nimport\n## Openbis\no\n=\n## Openbis\n(\n'https://test-openbis-instance.com'\n,\nverify_certificates\n=\n## False\n)\nCheck session token, logout()\n\nCheck whether your session, i.e. the\nsession token\nis still valid and log out:\nprint\n(\nf\n## \"Session is active:\n{\no\n.\nis_session_active\n()\n}\nand token is\n{\no\n.\ntoken\n}\n\"\n)\no\n.\nlogout\n()\nprint\n(\nf\n## \"Session is active:\n{\no\n.\nis_session_active\n()\n\"}\nAuthentication without user/password\n\nIn some configurations Openbis can be accessible via Single Sign On technology (SSO), in that case users may not have their own user/password.\nUpon login, Openbis generates a unique access token that can be used to allow pybis log into the active user session. You may find this token in cookies of the ELN UI.\nTo log in with a session token, you need to use\nset_token\n## method:\nfrom\npybis\nimport\n## Openbis\no\n=\n## Openbis\n(\n'https://test-openbis-instance.com'\n)\no\n.\nset_token\n(\n\"some_user-220808165456793xA3D0357C5DE66A5BAD647E502355FE2C\"\n)\n# logged into 'some_user' session!\n## Note\nKeep you access tokens safe and don’t share it with others! They are invalidated when one of the following situations happen:\nExplicit logout() call.\nNumber of sessions per user has reached beyond configured limit.\nSession timeout is reached.\nOpenbis instance is restarted.\nPersonal access token (PAT)\n\nAs an (new) alternative to login every time you run a script, you can create tokens which\nonce issued, do\nnot need username or password\nare\nmuch longer valid\nthan session tokens (default is one year)\nsurvive restarts\nof an openBIS instance\nTo create a token, you first need a valid session – either through classic login or by assigning an existing valid session token:\nfrom\npybis\nimport\n## Openbis\no\n=\n## Openbis\n(\n'https://test-openbis-instance.com'\n)\no\n.\nlogin\n(\n\"username\"\n,\n\"password\"\n)\n# or\no\n.\nset_token\n(\n\"your_username-220808165456793xA3D0357C5DE66A5BAD647E502355FE2C\"\n)\nThen you can create a new personal access token (PAT) and use it for all further pyBIS queries:\npat\n=\no\n.\nget_or_create_personal_access_token\n(\nsessionName\n=\n## \"Project A\"\n)\no\n.\nset_token\n(\npat\n,\nsave_token\n=\n## True\n)\n## You may also use permId directly:\npat\n=\no\n.\nget_or_create_personal_access_token\n(\nsessionName\n=\n## \"Project A\"\n)\no\n.\nset_token\n(\npat\n.\npermId\n,\nsave_token\n=\n## True\n)\n## Note\nIf there is an existing PAT with the same\nsessionName\nwhich is still valid and the validity is within the warning period (defined by the server), then this existing PAT is returned instead. However, you can enforce creating a new PAT by passing the argument\nforce=True\n.\n## Note\nMost operations are permitted using the PAT,\nexcept\n## :\nall operations on personal access tokens itself\ni.e. create, list, delete operations on tokens\nFor these operations, you need to use a session token instead.\nTo get a list of all currently available tokens:\no\n.\nget_personal_access_tokens\n()\no\n.\nget_personal_access_tokens\n(\nsessionName\n=\n## \"APPLICATION_1\"\n)\nTo delete the first token shown in the list:\no\n.\nget_personal_access_tokens\n()[\n0\n]\n.\ndelete\n(\n'some reason'\n)\n## Caching\n\n## With\npyBIS\n1.17.0\n, a lot of caching has been introduced to improve the speed of object lookups that do not change often. If you encounter any problems, you can turn it off like this:\no\n=\n## Openbis\n(\n'https://example.com'\n,\nuse_cache\n=\n## False\n)\n# or later in the script\no\n.\nuse_cache\n=\n## False\no\n.\nclear_cache\n()\no\n.\nclear_cache\n(\n'sampleType'\n)\nMount openBIS dataStore server\n\nPrerequisites: FUSE / SSHFS\n\nMounting an openBIS dataStore server requires FUSE / SSHFS to be installed (requires root privileges). The mounting itself requires no root privileges.\nMac OS X\nFollow the installation instructions on\nhttps://osxfuse.github.io\nUnix Cent OS 7\n$\nsudo\nyum\ninstall\nepel-release\n$\nsudo\nyum\n--enablerepo\n=\nepel\n-y\ninstall\nfuse-sshfs\n$\nuser\n=\n\"\n$(\nwhoami\n)\n\"\n$\nusermod\n-a\n## -G\nfuse\n\"\n$user\n\"\nAfter the installation, an\nsshfs\ncommand should be available.\nMount dataStore server with pyBIS\n\nBecause the mount/unmount procedure differs from platform to platform, pyBIS offers two simple methods:\no\n.\nmount\n()\no\n.\nmount\n(\nusername\n,\npassword\n,\nhostname\n,\nmountpoint\n,\nvolname\n)\no\n.\nis_mounted\n()\no\n.\nunmount\n()\no\n.\nget_mountpoint\n()\nCurrently, mounting is supported for Linux and Mac OS X only.\nAll attributes, if not provided, are re-used by a previous login() command, including personal access tokens. If no mountpoint is provided, the default mounpoint will be\n~/hostname\n. If this directory does not exist, it will be created. The directory must be empty before mounting.\n## Masterdata\n\nOpenBIS stores quite a lot of meta-data along with your dataSets. The collection of data that describes this meta-data (i.e. meta-meta-data) is called masterdata. It consists of:\nsample types\ndataSet types\nmaterial types\nexperiment types\nproperty types\nvocabularies\nvocabulary terms\nplugins (jython scripts that allow complex data checks)\ntags\nsemantic annotations\nbrowse masterdata\n\nsample_types\n=\no\n.\nget_sample_types\n()\n# get a list of sample types\nsample_types\n.\ndf\n# DataFrame object\nst\n=\no\n.\nget_sample_types\n()[\n3\n]\n# get 4th element of that list\nst\n=\no\n.\nget_sample_type\n(\n## 'YEAST'\n)\nst\n.\ncode\nst\n.\ngeneratedCodePrefix\nst\n.\nattrs\n.\nall\n()\n# get all attributes as a dict\nst\n.\nget_validationPlugin\n()\n# returns a plugin object\nst\n.\nget_property_assignments\n()\n# show the list of properties\n# for that sample type\no\n.\nget_material_types\n()\no\n.\nget_dataset_types\n()\no\n.\nget_experiment_types\n()\no\n.\nget_collection_types\n()\no\n.\nget_property_types\n()\npt\n=\no\n.\nget_property_type\n(\n## 'BARCODE_COMPLEXITY_CHECKER'\n)\npt\n.\nattrs\n.\nall\n()\no\n.\nget_plugins\n()\npl\n=\no\n.\nget_plugin\n(\n'Diff_time'\n)\npl\n.\nscript\n# the Jython script that processes this property\no\n.\nget_vocabularies\n()\no\n.\nget_vocabulary\n(\n## 'BACTERIAL_ANTIBIOTIC_RESISTANCE'\n)\no\n.\nget_terms\n(\nvocabulary\n=\n## 'STORAGE'\n)\no\n.\nget_tags\n()\ncreate property types\n\n## Samples\n(objects),\nexperiments\n(collections) and\ndataSets\ncontain type-specific\nproperties\n. When you create a new sample, experiment or datasSet of a given type, the set of properties is well defined. Also, the values of these properties are being type-checked.\nThe first step in creating a new entity type is to create a so called\nproperty type\n## :\npt_text\n=\no\n.\nnew_property_type\n(\ncode\n=\n## 'MY_NEW_PROPERTY_TYPE'\n,\nlabel\n=\n'yet another property type'\n,\ndescription\n=\n'my first property'\n,\ndataType\n=\n## 'VARCHAR'\n,\n)\npt_text\n.\nsave\n()\npt_int\n=\no\n.\nnew_property_type\n(\ncode\n=\n## 'MY_NUMBER'\n,\nlabel\n=\n'property contains a number'\n,\ndataType\n=\n## 'INTEGER'\n,\n)\npt_int\n.\nsave\n()\npt_voc\n=\no\n.\nnew_property_type\n(\ncode\n=\n## 'MY_CONTROLLED_VOCABULARY'\n,\nlabel\n=\n'label me'\n,\ndescription\n=\n'give me a description'\n,\ndataType\n=\n## 'CONTROLLEDVOCABULARY'\n,\nvocabulary\n=\n## 'STORAGE'\n,\n)\npt_voc\n.\nsave\n()\npt_richtext\n=\no\n.\nnew_property_type\n(\ncode\n=\n## 'MY_RICHTEXT_PROPERTY'\n,\nlabel\n=\n'richtext data'\n,\ndescription\n=\n'property contains rich text'\n,\ndataType\n=\n## 'MULTILINE_VARCHAR'\n,\nmetaData\n=\n{\n'custom_widget'\n## :\n## 'Word Processor'\n}\n)\npt_richtext\n.\nsave\n()\npt_spread\n=\no\n.\nnew_property_type\n(\ncode\n=\n## 'MY_TABULAR_DATA'\n,\nlabel\n=\n'data in a table'\n,\ndescription\n=\n'property contains a spreadsheet'\n,\ndataType\n=\n## 'XML'\n,\nmetaData\n=\n{\n'custom_widget'\n## :\n## 'Spreadsheet'\n}\n)\npt_spread\n.\nsave\n()\n## The\ndataType\nattribute can contain any of these values:\n## INTEGER\n## VARCHAR\n## MULTILINE_VARCHAR\n## REAL\n## TIMESTAMP\n## DATE\n## BOOLEAN\n## HYPERLINK\n## XML\n## CONTROLLEDVOCABULARY\n## MATERIAL\n## SAMPLE\nWhen choosing\n## CONTROLLEDVOCABULARY\n, you must specify a\nvocabulary\nattribute (see example). Likewise, when choosing\n## MATERIAL\n, a\nmaterialType\nattribute must be provided.\nTo create a\nrichtext property\n, use\n## MULTILINE_VARCHAR\nas\ndataType\nand set\nmetaData\nto\n{'custom_widget'\n## :\n## 'Word\n## Processor'}\nas shown in the example above.\nTo create a\ntabular, spreadsheet-like property\n, use\n## XML\nas\ndataType\nand set\nmetaData\nto\n{'custom_widget'\n## :\n## 'Spreadhseet'}\nas shown in the example above.\n## Note\n: PropertyTypes that start with a $ are by definition\nmanagedInternally\nand therefore this attribute must be set to True.\ncreate sample types / object types\n\nThe second step (after creating a property type, see above) is to create the\nsample type\n. The new name for\nsample\nis\nobject\n. You can use both methods interchangeably:\nnew_sample_type()\n==\nnew_object_type()\nsample_type\n=\no\n.\nnew_sample_type\n(\ncode\n=\n'my_own_sample_type'\n,\n# mandatory\ngeneratedCodePrefix\n=\n## 'S'\n,\n# mandatory\ndescription\n=\n''\n,\nautoGeneratedCode\n=\n## True\n,\nsubcodeUnique\n=\n## False\n,\nlistable\n=\n## True\n,\nshowContainer\n=\n## False\n,\nshowParents\n=\n## True\n,\nshowParentMetadata\n=\n## False\n,\nvalidationPlugin\n=\n## 'Has_Parents'\n# see plugins below\n)\nsample_type\n.\nsave\n()\n## When\nautoGeneratedCode\nattribute is set to\n## True\n, then you don’t need to provide a value for\ncode\nwhen you create a new sample. You can get the next autoGeneratedCode like this:\nsample_type\n.\nget_next_sequence\n()\n# eg. 67\nsample_type\n.\nget_next_code\n()\n# e.g. FLY77\nFrom pyBIS 1.31.0 onwards, you can provide a\ncode\neven for samples where its sample type has\nautoGeneratedCode=True\nto offer the same functionality as ELN-LIMS. In earlier versions of pyBIS, providing a code in this situation caused an error.\nassign and revoke properties to sample type / object type\n\nThe third step, after saving the sample type, is to\nassign or revoke properties\nto the newly created sample type. This assignment procedure applies to all entity types (dataset type, experiment type).\nsample_type\n.\nassign_property\n(\nprop\n=\n'diff_time'\n,\n# mandatory\nsection\n=\n''\n,\nordinal\n=\n5\n,\nmandatory\n=\n## True\n,\ninitialValueForExistingEntities\n=\n'initial value'\nshowInEditView\n=\n## True\n,\nshowRawValueInForms\n=\n## True\n)\nsample_type\n.\nrevoke_property\n(\n'diff_time'\n)\nsample_type\n.\nget_property_assignments\n()\n⚠️ Note: ordinal position\nIf a new property is assigned in a place of an existing property, the old property assignment ordinal value will be increased by 1\ncreate a dataset type\n\nThe second step (after creating a\nproperty type\n, see above) is to create the\ndataset type\n. The third step is to\nassign or revoke the properties\nto the newly created dataset type.\ndataset_type\n=\no\n.\nnew_dataset_type\n(\ncode\n=\n'my_dataset_type'\n,\n# mandatory\ndescription\n=\n## None\n,\nmainDataSetPattern\n=\n## None\n,\nmainDataSetPath\n=\n## None\n,\ndisallowDeletion\n=\n## False\n,\nvalidationPlugin\n=\n## None\n,\n)\ndataset_type\n.\nsave\n()\ndataset_type\n.\nassign_property\n(\n'property_name'\n)\ndataset_type\n.\nrevoke_property\n(\n'property_name'\n)\ndataset_type\n.\nget_property_assignments\n()\ncreate an experiment type / collection type\n\nThe second step (after creating a\nproperty type\n, see above) is to create the\nexperiment type\n.\nThe new name for\nexperiment\nis\ncollection\n. You can use both methods interchangeably:\nnew_experiment_type()\n==\nnew_collection_type()\nexperiment_type\n=\no\n.\nnew_experiment_type\n(\ncode\n,\ndescription\n=\n## None\n,\nvalidationPlugin\n=\n## None\n,\n)\nexperiment_type\n.\nsave\n()\nexperiment_type\n.\nassign_property\n(\n'property_name'\n)\nexperiment_type\n.\nrevoke_property\n(\n'property_name'\n)\nexperiment_type\n.\nget_property_assignments\n()\ncreate material types\n\nMaterials and material types are deprecated in newer versions of openBIS.\nmaterial_type\n=\no\n.\nnew_material_type\n(\ncode\n,\ndescription\n=\n## None\n,\nvalidationPlugin\n=\n## None\n,\n)\nmaterial_type\n.\nsave\n()\nmaterial_type\n.\nassign_property\n(\n'property_name'\n)\nmaterial_type\n.\nrevoke_property\n(\n'property_name'\n)\nmaterial_type\n.\nget_property_assignments\n()\ncreate plugins\n\nPlugins are Jython scripts that can accomplish more complex data-checks than ordinary types and vocabularies can achieve. They are assigned to entity types (dataset type, sample type etc).\nDocumentation and examples can be found here\npl\n=\no\n.\nnew_plugin\n(\nname\n=\n'my_new_entry_validation_plugin'\n,\npluginType\n=\n## 'ENTITY_VALIDATION'\n,\n# or 'DYNAMIC_PROPERTY' or 'MANAGED_PROPERTY',\nentityKind\n=\n## None\n,\n# or 'SAMPLE', 'MATERIAL', 'EXPERIMENT', 'DATA_SET'\nscript\n=\n'def calculate(): pass'\n# a JYTHON script\n)\npl\n.\nsave\n()\nUsers, Groups and RoleAssignments\n\nUsers can only login into the openBIS system when:\nthey are present in the authentication system (e.g. LDAP)\nthe username/password is correct\nthe user’s mail address needs is present\nthe user is already added to the openBIS user list (see below)\nthe user is assigned a role which allows a login, either directly assigned or indirectly assigned via a group membership\no\n.\nget_groups\n()\ngroup\n=\no\n.\nnew_group\n(\ncode\n=\n'group_name'\n,\ndescription\n=\n'...'\n)\ngroup\n=\no\n.\nget_group\n(\n'group_name'\n)\ngroup\n.\nsave\n()\ngroup\n.\nassign_role\n(\nrole\n=\n## 'ADMIN'\n,\nspace\n=\n## 'DEFAULT'\n)\ngroup\n.\nget_roles\n()\ngroup\n.\nrevoke_role\n(\nrole\n=\n## 'ADMIN'\n,\nspace\n=\n## 'DEFAULT'\n)\ngroup\n.\nadd_members\n([\n'admin'\n])\ngroup\n.\nget_members\n()\ngroup\n.\ndel_members\n([\n'admin'\n])\ngroup\n.\ndelete\n()\no\n.\nget_persons\n()\nperson\n=\no\n.\nnew_person\n(\nuserId\n=\n'username'\n)\nperson\n.\nspace\n=\n## 'USER_SPACE'\nperson\n.\nsave\n()\n# person.delete() is currently not possible.\nperson\n.\nassign_role\n(\nrole\n=\n## 'ADMIN'\n,\nspace\n=\n## 'MY_SPACE'\n)\nperson\n.\nassign_role\n(\nrole\n=\n## 'OBSERVER'\n)\nperson\n.\nget_roles\n()\nperson\n.\nrevoke_role\n(\nrole\n=\n## 'ADMIN'\n,\nspace\n=\n## 'MY_SPACE'\n)\nperson\n.\nrevoke_role\n(\nrole\n=\n## 'OBSERVER'\n)\no\n.\nget_role_assignments\n()\no\n.\nget_role_assignments\n(\nspace\n=\n## 'MY_SPACE'\n)\no\n.\nget_role_assignments\n(\ngroup\n=\n## 'MY_GROUP'\n)\nra\n=\no\n.\nget_role_assignment\n(\ntechId\n)\nra\n.\ndelete\n()\n## Spaces\n\nSpaces are fundamental way in openBIS to divide access between groups. Within a space, data can be easily shared. Between spaces, people need to be given specific access rights (see section above). The structure in openBIS is as follows:\nspace\nproject\nexperiment / collection\nsample / object\ndataset\nspace\n=\no\n.\nnew_space\n(\ncode\n=\n'space_name'\n,\ndescription\n=\n''\n)\nspace\n.\nsave\n()\no\n.\nget_spaces\n(\nstart_with\n=\n0\n,\n# start_with and count\ncount\n=\n10\n,\n# enable paging\n)\nspace\n=\no\n.\nget_space\n(\n## 'MY_SPACE'\n)\n# get individual attributes\nspace\n.\ncode\nspace\n.\ndescription\nspace\n.\nregistrator\nspace\n.\nregistrationDate\nspace\n.\nmodifier\nspace\n.\nmodificationDate\n# set individual attribute\n# most of the attributes above are set automatically and cannot be modified.\nspace\n.\ndescription\n=\n'...'\n# get all attributes as a dictionary\nspace\n.\nattrs\n.\nall\n()\nspace\n.\ndelete\n(\n'reason for deletion'\n)\n## Projects\n\nProjects live within spaces and usually contain experiments (aka collections):\nspace\nproject\nexperiment / collection\nsample / object\ndataset\nproject\n=\no\n.\nnew_project\n(\nspace\n=\nspace\n,\ncode\n=\n'project_name'\n,\ndescription\n=\n'some project description'\n)\nproject\n=\nspace\n.\nnew_project\n(\ncode\n=\n'project_code'\n,\ndescription\n=\n'project description'\n)\nproject\n.\nsave\n()\no\n.\nget_projects\n(\nspace\n=\n## 'MY_SPACE'\n,\n# show only projects in MY_SPACE\nstart_with\n=\n0\n,\n# start_with and count\ncount\n=\n10\n,\n# enable paging\n)\no\n.\nget_projects\n(\nspace\n=\n## 'MY_SPACE'\n)\nspace\n.\nget_projects\n()\nproject\n.\nget_experiments\n()\n# see details and limitations in Section 'search for experiments'\nproject\n.\nget_attachments\n()\n# deprecated, as attachments are not compatible with ELN-LIMS.\n# Attachments are an old concept and should not be used anymore.\np\n.\nadd_attachment\n(\n# deprecated, see above\nfileName\n=\n'testfile'\n,\ndescription\n=\n'another file'\n,\ntitle\n=\n'one more attachment'\n)\nproject\n.\ndownload_attachments\n(\n<\npath\nor\ncwd\n>\n)\n# deprecated, see above\n# get individual attributes\nproject\n.\ncode\nproject\n.\ndescription\n# set individual attribute\nproject\n.\ndescription\n=\n'...'\n# get all attributes as a dictionary\nproject\n.\nattrs\n.\nall\n()\nproject\n.\nfreeze\n=\n## True\nproject\n.\nfreezeForExperiments\n=\n## True\nproject\n.\nfreezeForSamples\n=\n## True\n## Experiments / Collections\n\n## Experiments live within projects:\nspace\nproject\nexperiment / collection\nsample / object\ndataset\nThe new name for\nexperiment\nis\ncollection\n. You can use boths names interchangeably:\nget_experiment()\n=\nget_collection()\nnew_experiment()\n=\nnew_collection()\nget_experiments()\n=\nget_collections()\ncreate a new experiment\n\nexp\n=\no\n.\nnew_experiment\ncode\n=\n## 'MY_NEW_EXPERIMENT'\n,\ntype\n=\n## 'DEFAULT_EXPERIMENT'\n,\nproject\n=\n## '/MY_SPACE/YEASTS'\n)\nexp\n.\nsave\n()\nsearch for experiments\n\nexperiments\n=\no\n.\nget_experiments\n(\nproject\n=\n## 'YEASTS'\n,\nspace\n=\n## 'MY_SPACE'\n,\ntype\n=\n## 'DEFAULT_EXPERIMENT'\n,\ntags\n=\n'*'\n,\nfinished_flag\n=\n## False\n,\nprops\n=\n[\n'name'\n,\n'finished_flag'\n]\n)\nexperiments\n=\nproject\n.\nget_experiments\n()\nexperiment\n=\nexperiments\n[\n0\n]\n# get first experiment of result list\nexperiment\n=\nexperiment\nfor\nexperiment\nin\nexperiments\n## :\n# iterate over search results\nprint\n(\nexperiment\n.\nprops\n.\nall\n())\ndataframe\n=\nexperiments\n.\ndf\n# get Pandas DataFrame of result list\nexp\n=\no\n.\nget_experiment\n(\n## '/MY_SPACE/MY_PROJECT/MY_EXPERIMENT'\n)\nNote: Attributes download\n## The\nget_experiments()\nmethod, by default, returns fewer details to make the download process faster.\nHowever, if you want to include specific attributes in the results, you can do so by using the\nattrs\nparameter.\n## The\nget_experiments()\nmethod results include only\nidentifier\n,\npermId\n,\ntype\n,\nregistrator\n,\nregistrationDate\n,\nmodifier\n,\nmodificationDate\nexperiments = o.get_experiments(\nproject       = 'YEASTS',\nspace         = 'MY_SPACE',\ntype          = 'DEFAULT_EXPERIMENT',\nattrs          = [\"parents\", \"children\"]\n)", "timestamp": "2025-09-18T09:38:29.759059Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_python-v3-api:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/python-v3-api.html", "repo": "openbis", "title": "Python (V3 API) - pyBIS!", "section": "⚠️ Clarification", "text": "identifier             permId                type               registrator    registrationDate     modifier    modificationDate     parents                    children\n--  ---------------------  --------------------  -----------------  -------------  -------------------  ----------  -------------------  -------------------------  ----------\n0  /MY_SPACE/YEASTS/EXP1  20230407070122991-46  DEFAULT_EXPERIMENT  admin          2023-04-07 09:01:23  admin       2023-04-07 09:02:22  ['/MY_SPACE/YEASTS/EXP2']  []\n## ⚠️ Clarification\nget_datasets()\nmethod is always downloading object properties\nNot downloaded attributes (e.g\nparents\n,\nchildren\n) will not be removed upon\nsave()\nunless explicitly done by the user.\n## None\nvalues of list\nattributes\nare ignored during saving process\nExperiment attributes\n\nexp\n.\nattrs\n.\nall\n()\n# returns all attributes as a dict\nexp\n.\nattrs\n.\ntags\n=\n[\n'some'\n,\n'tags'\n]\nexp\n.\ntags\n=\n[\n'some'\n,\n'tags'\n]\n# same thing\nexp\n.\nsave\n()\nexp\n.\ncode\nexp\n.\ndescription\nexp\n.\nregistrator\n...\nexp\n.\nproject\n=\n'my_project'\nexp\n.\nspace\n=\n'my_space'\nexp\n.\nfreeze\n=\n## True\nexp\n.\nfreezeForDataSets\n=\n## True\nexp\n.\nfreezeForSamples\n=\n## True\nexp\n.\nsave\n()\n# needed to save/update the changed attributes and properties\nExperiment properties\n\nGetting properties\nexperiment\n.\nprops\n==\nds\n.\np\n# you can use either .props or .p to access the properties\nexperiment\n.\np\n# in Jupyter: show all properties in a nice table\nexperiment\n.\np\n()\n# get all properties as a dict\nexperiment\n.\nprops\n.\nall\n()\n# get all properties as a dict\nexperiment\n.\np\n(\n'prop1'\n,\n'prop2'\n)\n# get some properties as a dict\nexperiment\n.\np\n.\nget\n(\n'$name'\n)\n# get the value of a property\nexperiment\n.\np\n[\n'property'\n]\n# get the value of a property\nSetting properties\nexperiment\n.\nexperiment\n=\n'first_exp'\n# assign sample to an experiment\nexperiment\n.\nproject\n=\n'my_project'\n# assign sample to a project\nexperiment\n.\np\n.\n+\n## TAB\n# in Jupyter/IPython: show list of available properties\nexperiment\n.\np\n.\nmy_property_\n+\n## TAB\n# in Jupyter/IPython: show datatype or controlled vocabulary\nexperiment\n.\np\n[\n'my_property'\n]\n=\n\"value\"\n# set the value of a property\nexperiment\n.\np\n.\nset\n(\n'my_property, '\nvalue\n')   # set the value of a property\nexperiment\n.\np\n.\nmy_property\n=\n\"some value\"\n# set the value of a property\nexperiment\n.\np\n.\nset\n({\n'my_property'\n## :\n'value'\n})\n# set the values of some properties\nexperiment\n.\nset_props\n({\nkey\n## :\nvalue\n})\n# set the values of some properties\nexperiment\n.\nsave\n()\n# needed to save/update the changed attributes and properties\n## Samples / Objects\n\n## Samples usually live within experiments/collections:\nspace\nproject\nexperiment / collection\nsample / object\ndataset\nThe new name for\nsample\nis\nobject\n. You can use boths names interchangeably:\nget_sample()\n=\nget_object()\nnew_sample()\n=\nnew_object()\nget_samples()\n=\nget_objects()\netc.\nsample\n=\no\n.\nnew_sample\n(\ntype\n=\n## 'YEAST'\n,\nspace\n=\n## 'MY_SPACE'\n,\nexperiment\n=\n## '/MY_SPACE/MY_PROJECT/EXPERIMENT_1'\n,\nparents\n=\n[\nparent_sample\n,\n## '/MY_SPACE/YEA66'\n],\n# you can use either permId, identifier\nchildren\n=\n[\nchild_sample\n],\n# or sample object\nprops\n=\n{\n\"name\"\n## :\n\"some name\"\n,\n\"description\"\n## :\n\"something interesting\"\n}\n)\nsample\n=\nspace\n.\nnew_sample\n(\ntype\n=\n## 'YEAST'\n)\nsample\n.\nsave\n()\nsample\n=\no\n.\nget_sample\n(\n## '/MY_SPACE/MY_SAMPLE_CODE'\n)\nsample\n=\no\n.\nget_sample\n(\n'20170518112808649-52'\n)\nsamples\n=\no\n.\nget_samples\n(\ntype\n=\n## 'UNKNOWN'\n)\n# see details and limitations in Section 'search for samples / objects'\n# get individual attributes\nsample\n.\nspace\nsample\n.\ncode\nsample\n.\npermId\nsample\n.\nidentifier\nsample\n.\ntype\n# once the sample type is defined, you cannot modify it\n# set attribute\nsample\n.\nspace\n=\n## 'MY_OTHER_SPACE'\nsample\n.\nexperiment\n# a sample can belong to one experiment only\nsample\n.\nexperiment\n=\n## '/MY_SPACE/MY_PROJECT/MY_EXPERIMENT'\nsample\n.\nproject\nsample\n.\nproject\n=\n## '/MY_SPACE/MY_PROJECT'\n# only works if project samples are\nenabled\nsample\n.\ntags\nsample\n.\ntags\n=\n[\n'guten_tag'\n,\n'zahl_tag'\n]\nsample\n.\nattrs\n.\nall\n()\n# returns all attributes as a dict\nsample\n.\nprops\n.\nall\n()\n# returns all properties as a dict\nsample\n.\nget_attachments\n()\n# deprecated, as attachments are not compatible with ELN-LIMS.\n# Attachments are an old concept and should not be used anymore.\nsample\n.\ndownload_attachments\n(\n<\npath\nor\ncwd\n>\n)\n# deprecated, see above\nsample\n.\nadd_attachment\n(\n'testfile.xls'\n)\n# deprecated, see above\nsample\n.\ndelete\n(\n'deleted for some reason'\n)\ncreate/update/delete many samples in a transaction\n\nCreating a single sample takes some time. If you need to create many samples, you might want to create them in one transaction. This will transfer all your sample data at once. The Upside of this is the\ngain in speed\n. The downside: this is a\nall-or-nothing\noperation, which means, either all samples will be registered or none (if any error occurs).\ncreate many samples in one transaction\ntrans\n=\no\n.\nnew_transaction\n()\nfor\ni\nin\nrange\n(\n0\n,\n100\n## ):\nsample\n=\no\n.\nnew_sample\n(\n...\n)\ntrans\n.\nadd\n(\nsample\n)\ntrans\n.\ncommit\n()\nupdate many samples in one transaction\ntrans\n=\no\n.\nnew_transaction\n()\nfor\nsample\nin\no\n.\nget_samples\n(\ncount\n=\n100\n## ):\nsample\n.\nprop\n.\nsome_property\n=\n'different value'\ntrans\n.\nadd\n(\nsample\n)\ntrans\n.\ncommit\n()\ndelete many samples in one transaction\ntrans\n=\no\n.\nnew_transaction\n()\nfor\nsample\nin\no\n.\nget_samples\n(\ncount\n=\n100\n## ):\nsample\n.\nmark_to_be_deleted\n()\ntrans\n.\nadd\n(\nsample\n)\ntrans\n.\nreason\n(\n'go what has to go'\n)\ntrans\n.\ncommit\n()\n## Note:\nYou can use the\nmark_to_be_deleted()\n,\nunmark_to_be_deleted()\nand\nis_marked_to_be_deleted()\nmethods to set and read the internal flag.\nparents, children, components and container\n\nsample\n.\nget_parents\n()\nsample\n.\nset_parents\n([\n## '/MY_SPACE/PARENT_SAMPLE_NAME'\n)\nsample\n.\nadd_parents\n(\n## '/MY_SPACE/PARENT_SAMPLE_NAME'\n)\nsample\n.\ndel_parents\n(\n## '/MY_SPACE/PARENT_SAMPLE_NAME'\n)\nsample\n.\nget_children\n()\nsample\n.\nset_children\n(\n## '/MY_SPACE/CHILD_SAMPLE_NAME'\n)\nsample\n.\nadd_children\n(\n## '/MY_SPACE/CHILD_SAMPLE_NAME'\n)\nsample\n.\ndel_children\n(\n## '/MY_SPACE/CHILD_SAMPLE_NAME'\n)\n# A Sample may belong to another Sample, which acts as a container.\n# As opposed to DataSets, a Sample may only belong to one container.\nsample\n.\ncontainer\n# returns a sample object\nsample\n.\ncontainer\n=\n## '/MY_SPACE/CONTAINER_SAMPLE_NAME'\n# watch out, this will change the identifier of the sample to:\n# /MY_SPACE/CONTAINER_SAMPLE_NAME:SAMPLE_NAME\nsample\n.\ncontainer\n=\n''\n# this will remove the container.\n# A Sample may contain other Samples, in order to act like a container (see above)\n# caveat: containers are NOT compatible with ELN-LIMS\n# The Sample-objects inside that Sample are called «components» or «contained Samples»\n# You may also use the xxx_contained() functions, which are just aliases.\nsample\n.\nget_components\n()\nsample\n.\nset_components\n(\n## '/MY_SPACE/COMPONENT_NAME'\n)\nsample\n.\nadd_components\n(\n## '/MY_SPACE/COMPONENT_NAME'\n)\nsample\n.\ndel_components\n(\n## '/MY_SPACE/COMPONENT_NAME'\n)\nsample tags\n\nsample\n.\nget_tags\n()\nsample\n.\nset_tags\n(\n'tag1'\n)\nsample\n.\nadd_tags\n([\n'tag2'\n,\n'tag3'\n])\nsample\n.\ndel_tags\n(\n'tag1'\n)\nSample attributes and properties\n\nGetting properties\nsample\n.\nattrs\n.\nall\n()\n# returns all attributes as a dict\nsample\n.\nattribute_name\n# return the attribute value\nsample\n.\nprops\n==\nds\n.\np\n# you can use either .props or .p to access the properties\nsample\n.\np\n# in Jupyter: show all properties in a nice table\nsample\n.\np\n()\n# get all properties as a dict\nsample\n.\nprops\n.\nall\n()\n# get all properties as a dict\nsample\n.\np\n(\n'prop1'\n,\n'prop2'\n)\n# get some properties as a dict\nsample\n.\np\n.\nget\n(\n'$name'\n)\n# get the value of a property\nsample\n.\np\n[\n'property'\n]\n# get the value of a property\nSetting properties\nsample\n.\nexperiment\n=\n'first_exp'\n# assign sample to an experiment\nsample\n.\nproject\n=\n'my_project'\n# assign sample to a project\nsample\n.\np\n.\n+\n## TAB\n# in Jupyter/IPython: show list of available properties\nsample\n.\np\n.\nmy_property_\n+\n## TAB\n# in Jupyter/IPython: show datatype or controlled vocabulary\nsample\n.\np\n[\n'my_property'\n]\n=\n\"value\"\n# set the value of a property\nsample\n.\np\n.\nset\n(\n'my_property, '\nvalue\n')   # set the value of a property\nsample\n.\np\n.\nmy_property\n=\n\"some value\"\n# set the value of a property\nsample\n.\np\n.\nset\n({\n'my_property'\n## :\n'value'\n})\n# set the values of some properties\nsample\n.\nset_props\n({\nkey\n## :\nvalue\n})\n# set the values of some properties\nsample\n.\nsave\n()\n# needed to save/update the attributes and properties\nsearch for samples / objects\n\nThe result of a search is always list, even when no items are found. The\n.df\nattribute returns\nthe Pandas dataFrame of the results.\nsamples\n=\no\n.\nget_samples\n(\nspace\n=\n## 'MY_SPACE'\n,\ntype\n=\n## 'YEAST'\n,\ntags\n=\n[\n'*'\n],\n# only sample with existing tags\nstart_with\n=\n0\n,\n# start_with and count\ncount\n=\n10\n,\n# enable paging\nwhere\n=\n{\n## \"$SOME.WEIRD-PROP\"\n## :\n\"hello\"\n# only receive samples where properties match\n}\nregistrationDate\n=\n\"2020-01-01\"\n,\n# date format: YYYY-MM-DD\nmodificationDate\n=\n\"<2020-12-31\"\n,\n# use > or < to search for specified date and later / earlier\nattrs\n=\n[\n# show these attributes in the dataFrame\n'sample.code'\n,\n'registrator.email'\n,\n'type.generatedCodePrefix'\n],\nparent_property\n=\n'value'\n,\n# search in a parent's property\nchild_property\n=\n'value'\n,\n# search in a child's property\ncontainer_property\n=\n'value'\n# search in a container's property\nparent\n=\n## '/MY_SPACE/PARENT_SAMPLE'\n,\n# sample has this as its parent\nparent\n=\n'*'\n,\n# sample has at least one parent\nchild\n=\n## '/MY_SPACE/CHILD_SAMPLE'\n,\nchild\n=\n'*'\n,\n# sample has at least one child\ncontainer\n=\n## 'MY_SPACE/CONTAINER'\n,\ncontainer\n=\n'*'\n# sample lives in a container\nprops\n=\n[\n## '$NAME'\n,\n## 'MATING_TYPE'\n]\n# show these properties in the result\n)\nsample\n=\nsamples\n[\n9\n]\n# get the 10th sample\n# of the search results\nsample\n=\nsamples\n[\n## '/SPACE/AABC'\n]\n# same, fetched by identifier\nfor\nsample\nin\nsamples\n## :\n# iterate over the\nprint\n(\nsample\n.\ncode\n)\n# search results\nsamples\n.\ndf\n# returns a Pandas DataFrame object\nsamples\n=\no\n.\nget_samples\n(\nprops\n=\n\"*\"\n)\n# retrieve all properties of all samples\nNote: Attributes download\n## The\nget_samples()\nmethod, by default, returns fewer details to make the download process faster.\nHowever, if you want to include specific attributes in the results, you can do so by using the\nattrs\nparameter.\n## The\nget_samples()\nmethod results include only\nidentifier\n,\npermId\n,\ntype\n,\nregistrator\n,\nregistrationDate\n,\nmodifier\n,\nmodificationDate\nsamples\n=\no\n.\nget_samples\n(\nspace\n=\n## 'MY_SPACE'\n,\ntype\n=\n## 'YEAST'\n,\nattrs\n=\n[\n\"parents\"\n,\n\"children\"\n]\n)\nidentifier\npermId\ntype\nregistrator\nregistrationDate\nmodifier\nmodificationDate\nparents\nchildren\n--\n---------------------\n--------------------\n-----------------\n-------------\n-------------------\n----------\n-------------------\n-------------------------\n----------\n0\n/\n## MY_SPACE\n/\n## YEASTS\n/\n## SAMPLE1\n20230407070121337\n-\n47\n## YEAST\nadmin\n2023\n-\n04\n-\n07\n09\n## :\n06\n## :\n23\nadmin\n2023\n-\n04\n-\n07\n09\n## :\n06\n## :\n22\n[\n## '/MY_SPACE/YEASTS/EXP2'\n]\n[]\n## ⚠️ Clarification\nget_samples()\nmethod is always downloading object properties\nNot downloaded attributes (e.g\nparents\n,\nchildren\n) will not be removed upon\nsave()\nunless explicitly done by the user.\n## None\nvalues of list\nattributes\nare ignored during saving process\n## Example:\n# get sample with get_sample() method\nsample\n=\no\n.\nget_sample\n(\n## '/DEFAULT/DEFAULT/EXP2'\n)\nsample\n## Out\n[\n1\n## ]:\nattribute\nvalue\n-------------------\n------------------------------\ncode\n## EXP2\npermId\n20230823205338303\n-\n49\nidentifier\n/\n## DEFAULT\n/\n## DEFAULT\n/\n## EXP2\ntype\n## EXPERIMENTAL_STEP\nproject\n/\n## DEFAULT\n/\n## DEFAULT\nparents\n[]\n# empty list\nchildren\n[\n## '/DEFAULT/DEFAULT/EXP3'\n]\ncomponents\n[]\n# get sample with get_samples() method\nsamples\n=\no\n.\nget_samples\n(\nidentifier\n=\n## '/DEFAULT/DEFAULT/EXP2'\n)\nsamples\n[\n0\n]\n## Out\n[\n1\n## ]:\nattribute\nvalue\n-------------------\n------------------------------\ncode\n## EXP2\npermId\n20230823205338303\n-\n49\nidentifier\n/\n## DEFAULT\n/\n## DEFAULT\n/\n## EXP2\ntype\n## EXPERIMENTAL_STEP\nproject\n/\n## DEFAULT\n/\n## DEFAULT\nparents\n# None value\nchildren\n# None value\ncomponents\n[]\nfreezing samples\n\nsample\n.\nfreeze\n=\n## True\nsample\n.\nfreezeForComponents\n=\n## True\nsample\n.\nfreezeForChildren\n=\n## True\nsample\n.\nfreezeForParents\n=\n## True\nsample\n.\nfreezeForDataSets\n=\n## True\n## Datasets\n\nDatasets are by all means the most important openBIS entity. The actual files are stored as datasets; all other openBIS entities mainly are necessary to annotate and to structure the data:\nspace\nproject\nexperiment / collection\nsample / object\ndataset\nworking with existing dataSets\n\nsearch for datasets\nThis example does the following\nsearch for all datasets of type\n## SCANS\n, retrieve the first 10 entries\nprint out all properties\nprint the list of all files in this dataset\ndownload the dataset\ndatasets\n=\nsample\n.\nget_datasets\n(\ntype\n=\n## 'SCANS'\n,\nstart_with\n=\n0\n,\ncount\n=\n10\n)\nfor\ndataset\nin\ndatasets\n## :\nprint\n(\ndataset\n.\nprops\n())\nprint\n(\ndataset\n.\nfile_list\n)\ndataset\n.\ndownload\n()\ndataset\n=\ndatasets\n[\n0\n]\nNote: Attributes download\n## The\nget_datasets()\nmethod, by default, returns fewer details to make the download process faster.\nHowever, if you want to include specific attributes in the results, you can do so by using the\nattrs\nparameter.\n## The\nget_datasets()\nmethod results include only\npermId\n,\ntype\n,\nexperiment\n,\nsample\n,\nregistrationDate\n,\nmodificationDate\n,\nlocation\n,\nstatus\n,\npresentInArchive\n,\nsize\ndatasets\n=\no\n.\nget_datasets\n(\nspace\n=\n## 'MY_SPACE'\n,\nattrs\n=\n[\n\"parents\"\n,\n\"children\"\n]\n)\npermId\ntype\nexperiment\nsample\nregistrationDate\nmodificationDate\nlocation\nstatus\npresentInArchive\nsize\nparents\nchildren\n--\n--------------------\n--------\n------------------------\n---------------------\n-------------------\n-------------------\n---------------------------------------\n---------\n------------------\n------\n------------------------\n------------------------\n0\n20230526101657295\n-\n48\n## RAW_DATA\n/\n## MY_SPACE\n/\n## DEFAULT\n/\n## DEFAULT\n/\n## MY_SPACE\n/\n## DEFAULT\n/\n## EXP1\n2023\n-\n05\n-\n26\n12\n## :\n16\n## :\n58\n2023\n-\n05\n-\n26\n12\n## :\n17\n## :\n37\n1\n## F60C7DC\n-\n63\n## D8\n-\n4\n## C07\n/\n20230526101657295\n-\n48\n## AVAILABLE\n## False\n469\n[]\n[\n'20230526101737019-49'\n]\n1\n20230526101737019\n-\n49\n## RAW_DATA\n/\n## MY_SPACE\n/\n## DEFAULT\n/\n## DEFAULT\n/\n## MY_SPACE\n/\n## DEFAULT\n/\n## EXP1\n2023\n-\n05\n-\n26\n12\n## :\n17\n## :\n37\n2023\n-\n05\n-\n26\n12\n## :\n17\n## :\n37\n1\n## F60C7DC\n-\n63\n## D8\n-\n4\n## C07\n/\n20230526101737019\n-\n49\n## AVAILABLE\n## False\n127\n[\n'20230526101657295-48'\n]\n[]\n## ⚠️ Clarification\nget_datasets()\nmethod is always downloading object properties\nNot downloaded attributes (e.g\nparents\n,\nchildren\n) will not be removed upon\nsave()\nunless explicitly done by the user.\n## None\nvalues of list\nattributes\nare ignored during saving process\n## More dataset functions:\nds\n=\no\n.\nget_dataset\n(\n'20160719143426517-259'\n)\nds\n.\nget_parents\n()\nds\n.\nget_children\n()\nds\n.\nsample\nds\n.\nexperiment\nds\n.\nphysicalData\nds\n.\nstatus\n# AVAILABLE   LOCKED   ARCHIVED\n# ARCHIVE_PENDING   UNARCHIVE_PENDING\n# BACKUP_PENDING\nds\n.\narchive\n()\n# archives a dataset, i.e. moves it to a slower but cheaper diskspace (tape).\n# archived datasets cannot be downloaded, they need to be unarchived first.\n# This is an asynchronous process,\n# check ds.status regularly until the dataset becomes 'ARCHIVED'\nds\n.\nunarchive\n()\n# this starts an asynchronous process which gets the dataset from the tape.\n# Check ds.status regularly until it becomes 'AVAILABLE'\nds\n.\nattrs\n.\nall\n()\n# returns all attributes as a dict\nds\n.\nprops\n.\nall\n()\n# returns all properties as a dict\nds\n.\nadd_attachment\n()\n# Deprecated. Attachments usually contain meta-data\nds\n.\nget_attachments\n()\n# about the dataSet, not the data itself.\nds\n.\ndownload_attachments\n(\n<\npath\nor\ncwd\n>\n)\n# Deprecated, as attachments are not compatible with ELN-LIMS.\n# Attachments are an old concept and should not be used anymore.\ndownload dataSets\n\no\n.\ndownload_prefix\n# used for download() and symlink() method.\n# Is set to data/hostname by default, but can be changed.\nds\n.\nget_files\n(\nstart_folder\n=\n\"/\"\n)\n# get file list as Pandas dataFrame\nds\n.\nfile_list\n# get file list as array\nds\n.\nfile_links\n# file list as a dict containing direct https links\nds\n.\ndownload\n()\n# simply download all files to data/hostname/permId/\nds\n.\ndownload\n(\ndestination\n=\n'my_data'\n,\n# download files to folder my_data/\ncreate_default_folders\n=\n## False\n,\n# ignore the /original/DEFAULT folders made by openBIS\nwait_until_finished\n=\n## False\n,\n# download in background, continue immediately\nworkers\n=\n10\n# 10 downloads parallel (default)\n)\nds\n.\ndownload_path\n# returns the relative path (destination) of the files after a ds.download()\nds\n.\nis_physical\n()\n# TRUE if dataset is physically\nlink dataSets\n\nInstead of downloading a dataSet, you can create a symbolic link to a dataSet in the openBIS dataStore. To do that, the openBIS dataStore needs to be mounted first (see mount method above).\n## Note:\nSymbolic links and the mount() feature currently do not work with Windows.\no\n.\ndownload_prefix\n# used for download() and symlink() method.\n# Is set to data/hostname by default, but can be changed.\nds\n.\nsymlink\n()\n# creates a symlink for this dataset: data/hostname/permId\n# tries to mount openBIS instance\n# in case it is not mounted yet\nds\n.\nsymlink\n(\ntarget_dir\n=\n'data/dataset_1/'\n,\n# default target_dir is: data/hostname/permId\nreplace_if_symlink_exists\n=\n## True\n)\nds\n.\nis_symlink\n()\ndataSet attributes and properties\n\nGetting properties\nds\n.\nattrs\n.\nall\n()\n# returns all attributes as a dict\nds\n.\nattribute_name\n# return the attribute value\nds\n.\nprops\n==\nds\n.\np\n# you can use either .props or .p to access the properties\nds\n.\np\n# in Jupyter: show all properties in a nice table\nds\n.\np\n()\n# get all properties as a dict\nds\n.\nprops\n.\nall\n()\n# get all properties as a dict\nds\n.\np\n(\n'prop1'\n,\n'prop2'\n)\n# get some properties as a dict\nds\n.\np\n.\nget\n(\n'$name'\n)\n# get the value of a property\nds\n.\np\n[\n'property'\n]\n# get the value of a property\nSetting properties\nds\n.\nexperiment\n=\n'first_exp'\n# assign dataset to an experiment\nds\n.\nsample\n=\n'my_sample'\n# assign dataset to a sample\nds\n.\np\n.\n+\n## TAB\n# in Jupyter/IPython: show list of available properties\nds\n.\np\n.\nmy_property_\n+\n## TAB\n# in Jupyter/IPython: show datatype or controlled vocabulary\nds\n.\np\n[\n'my_property'\n]\n=\n\"value\"\n# set the value of a property\nds\n.\np\n.\nset\n(\n'my_property, '\nvalue\n')   # set the value of a property\nds\n.\np\n.\nmy_property\n=\n\"some value\"\n# set the value of a property\nds\n.\np\n.\nset\n({\n'my_property'\n## :\n'value'\n})\n# set the values of some properties\nds\n.\nset_props\n({\nkey\n## :\nvalue\n})\n# set the values of some properties\nsearch for dataSets\n\nThe result of a search is always list, even when no items are found\n## The\n.df\nattribute returns the Pandas dataFrame of the results\ndatasets\n=\no\n.\nget_datasets\n(\ntype\n=\n## 'MY_DATASET_TYPE'\n,\n**\n{\n## \"SOME.WEIRD:PROP\"\n## :\n\"value\"\n},\n# property name contains a dot or a\n# colon: cannot be passed as an argument\nstart_with\n=\n0\n,\n# start_with and count\ncount\n=\n10\n,\n# enable paging\nregistrationDate\n=\n\"2020-01-01\"\n,\n# date format: YYYY-MM-DD\nmodificationDate\n=\n\"<2020-12-31\"\n,\n# use > or < to search for specified date and later / earlier\nparent_property\n=\n'value'\n,\n# search in a parent's property\nchild_property\n=\n'value'\n,\n# search in a child's property\ncontainer_property\n=\n'value'\n# search in a container's property\nparent\n=\n## '/MY_SPACE/PARENT_DS'\n,\n# has this dataset as its parent\nparent\n=\n'*'\n,\n# has at least one parent dataset\nchild\n=\n## '/MY_SPACE/CHILD_DS'\n,\nchild\n=\n'*'\n,\n# has at least one child dataset\ncontainer\n=\n## 'MY_SPACE/CONTAINER_DS'\n,\ncontainer\n=\n'*'\n,\n# belongs to a container dataset\nattrs\n=\n[\n# show these attributes in the dataFrame\n'sample.code'\n,\n'registrator.email'\n,\n'type.generatedCodePrefix'\n],\nprops\n=\n[\n## '$NAME'\n,\n## 'MATING_TYPE'\n]\n# show these properties in the result\n)\ndatasets\n=\no\n.\nget_datasets\n(\nprops\n=\n\"*\"\n)\n# retrieve all properties of all dataSets\ndataset\n=\ndatasets\n[\n0\n]\n# get the first dataset in the search result\nfor\ndataset\nin\ndatasets\n## :\n# iterate over the datasets\n...\ndf\n=\ndatasets\n.\ndf\n# returns a Pandas dataFrame object of the search results\nIn some cases, you might want to retrieve precisely certain datasets. This can be achieved by\nmethods chaining (but be aware, it might not be very performant):\ndatasets\n=\no\n.\nget_experiments\n(\nproject\n=\n## 'YEASTS'\n)\n\\\n.\nget_samples\n(\ntype\n=\n## 'FLY'\n)\n\\\n.\nget_datasets\n(\ntype\n=\n## 'ANALYZED_DATA'\n,\nprops\n=\n[\n## 'MY_PROPERTY'\n],\n## MY_PROPERTY\n=\n'some analyzed data'\n)\n## another example:\ndatasets\n=\no\n.\nget_experiment\n(\n## '/MY_NEW_SPACE/MY_PROJECT/MY_EXPERIMENT4'\n)\n\\\n.\nget_samples\n(\ntype\n=\n## 'UNKNOWN'\n)\n\\\n.\nget_parents\n()\n\\\n.\nget_datasets\n(\ntype\n=\n## 'RAW_DATA'\n)\nfreeze dataSets\n\nonce a dataSet has been frozen, it cannot be changed by anyone anymore\nso be careful!\nds\n.\nfreeze\n=\n## True\nds\n.\nfreezeForChildren\n=\n## True\nds\n.\nfreezeForParents\n=\n## True\nds\n.\nfreezeForComponents\n=\n## True\nds\n.\nfreezeForContainers\n=\n## True\nds\n.\nsave\n()\ncreate a new dataSet\n\nds_new\n=\no\n.\nnew_dataset\n(\ntype\n=\n## 'ANALYZED_DATA'\n,\nexperiment\n=\n## '/SPACE/PROJECT/EXP1'\n,\nsample\n=\n## '/SPACE/SAMP1'\n,\nfiles\n=\n[\n'my_analyzed_data.dat'\n],\nprops\n=\n{\n'name'\n## :\n'some good name'\n,\n'description'\n## :\n'...'\n}\n)\nds_new\n.\nsave\n()\ncreate dataSet with zipfile\n\n## DataSet containing one zipfile which will be unzipped in openBIS:\nds_new\n=\no\n.\nnew_dataset\n(\ntype\n=\n## 'RAW_DATA'\n,\nsample\n=\n## '/SPACE/SAMP1'\n,\nzipfile\n=\n'my_zipped_folder.zip'\n,\n)\nds_new\n.\nsave\n()\ncreate dataSet with mixed content\n\nmixed content means: folders and files are provided\na relative specified folder (and all its content) will end up in the root, while keeping its structure\n../measurements/\n–>\n/measurements/\nsome/folder/somewhere/\n–>\n/somewhere/\nrelative files will also end up in the root\nmy_file.txt\n–>\n/my_file.txt\n../somwhere/else/my_other_file.txt\n–>\n/my_other_file.txt\nsome/folder/file.txt\n–>\n/file.txt\nuseful if DataSet contains files and folders\nthe content of the folder will be zipped (on-the-fly) and uploaded to openBIS\nopenBIS will keep the folder structure intact\nrelative path will be shortened to its basename. For example:\nlocal\nopenBIS\n../../myData/\nmyData/\nsome/experiment/results/\nresults/\nds_new\n=\no\n.\nnew_dataset\n(\ntype\n=\n## 'RAW_DATA'\n,\nsample\n=\n## '/SPACE/SAMP1'\n,\nfiles\n=\n[\n'../measurements/'\n,\n'my_analyis.ipynb'\n,\n'results/'\n]\n)\nds_new\n.\nsave\n()\ncreate dataSet container\n\nA DataSet of kind=CONTAINER contains other DataSets, but no files:\nds_new\n=\no\n.\nnew_dataset\n(\ntype\n=\n## 'ANALYZED_DATA'\n,\nexperiment\n=\n## '/SPACE/PROJECT/EXP1'\n,\nsample\n=\n## '/SPACE/SAMP1'\n,\nkind\n=\n## 'CONTAINER'\n,\nprops\n=\n{\n'name'\n## :\n'some good name'\n,\n'description'\n## :\n'...'\n}\n)\nds_new\n.\nsave\n()\nget, set, add and remove parent datasets\n\ndataset\n.\nget_parents\n()\ndataset\n.\nset_parents\n([\n'20170115220259155-412'\n])\ndataset\n.\nadd_parents\n([\n'20170115220259155-412'\n])\ndataset\n.\ndel_parents\n([\n'20170115220259155-412'\n])\nget, set, add and remove child datasets\n\ndataset\n.\nget_children\n()\ndataset\n.\nset_children\n([\n'20170115220259155-412'\n])\ndataset\n.\nadd_children\n([\n'20170115220259155-412'\n])\ndataset\n.\ndel_children\n([\n'20170115220259155-412'\n])\ndataSet containers\n\nA DataSet may belong to other DataSets, which must be of kind=CONTAINER\nAs opposed to Samples, DataSets may belong (contained) to more than one DataSet-container\ncaveat: containers are NOT compatible with ELN-LIMS\ndataset\n.\nget_containers\n()\ndataset\n.\nset_containers\n([\n'20170115220259155-412'\n])\ndataset\n.\nadd_containers\n([\n'20170115220259155-412'\n])\ndataset\n.\ndel_containers\n([\n'20170115220259155-412'\n])\na DataSet of kind=CONTAINER may contain other DataSets, to act like a folder (see above)\nthe DataSet-objects inside that DataSet are called components or contained DataSets\nyou may also use the xxx_contained() functions, which are just aliases.\ncaveat: components are NOT compatible with ELN-LIMS\ndataset\n.\nget_components\n()\ndataset\n.\nset_components\n([\n'20170115220259155-412'\n])\ndataset\n.\nadd_components\n([\n'20170115220259155-412'\n])\ndataset\n.\ndel_components\n([\n'20170115220259155-412'\n])\n## Semantic Annotations\n\ncreate semantic annotation for sample type ‘UNKNOWN’:\nsa\n=\no\n.\nnew_semantic_annotation\n(\nentityType\n=\n## 'UNKNOWN'\n,\npredicateOntologyId\n=\n'po_id'\n,\npredicateOntologyVersion\n=\n'po_version'\n,\npredicateAccessionId\n=\n'pa_id'\n,\ndescriptorOntologyId\n=\n'do_id'\n,\ndescriptorOntologyVersion\n=\n'do_version'\n,\ndescriptorAccessionId\n=\n'da_id'\n)\nsa\n.\nsave\n()\nCreate semantic annotation for property type (predicate and descriptor values omitted for brevity)\nsa\n=\no\n.\nnew_semantic_annotation\n(\npropertyType\n=\n## 'DESCRIPTION'\n,\n...\n)\nsa\n.\nsave\n()\n## Create\nsemantic annotation for sample property assignment (predicate and descriptor values omitted for brevity)\nsa\n=\no\n.\nnew_semantic_annotation\n(\nentityType\n=\n## 'UNKNOWN'\n,\npropertyType\n=\n## 'DESCRIPTION'\n,\n...\n)\nsa\n.\nsave\n()\n## Create\na semantic annotation directly from a sample type. Will also create sample property assignment annotations when propertyType is given:\nst\n=\no\n.\nget_sample_type\n(\n## \"ORDER\"\n)\nst\n.\nnew_semantic_annotation\n(\n...\n)\nGet all\nsemantic annotations\no\n.\nget_semantic_annotations\n()\n## Get\nsemantic annotation by perm id\nsa\n=\no\n.\nget_semantic_annotation\n(\n\"20171015135637955-30\"\n)\n## Update\nsemantic annotation\nsa\n.\npredicateOntologyId\n=\n'new_po_id'\nsa\n.\ndescriptorOntologyId\n=\n'new_do_id'\nsa\n.\nsave\n()\n## Delete\nsemantic annotation\nsa\n.\ndelete\n(\n'reason'\n)\n## Tags\n\nnew_tag\n=\no\n.\nnew_tag\n(\ncode\n=\n'my_tag'\n,\ndescription\n=\n'some descriptive text'\n)\nnew_tag\n.\ndescription\n=\n'some new description'\nnew_tag\n.\nsave\n()\no\n.\nget_tags\n()\no\n.\nget_tag\n(\n'/username/TAG_Name'\n)\no\n.\nget_tag\n(\n'TAG_Name'\n)\ntag\n.\nget_experiments\n()\ntag\n.\nget_samples\n()\ntag\n.\nget_owner\n()\n# returns a person object\ntag\n.\ndelete\n(\n'why?'\n)\nVocabulary and VocabularyTerms\n\nAn entity such as Sample (Object), Experiment (Collection), Material or DataSet can be of a specific\nentity type\n## :\n## Sample Type (Object Type)\n## Experiment Type (Collection Type)\nDataSet Type\n## Material Type\nEvery type defines which\n## Properties\nmay be defined. Properties act like\n## Attributes\n, but they are type-specific. Properties can contain all sorts of information, such as free text, XML, Hyperlink, Boolean and also\n## Controlled Vocabulary\n. Such a Controlled Vocabulary consists of many\nVocabularyTerms\n. These terms are used to only allow certain values entered in a Property field.\nSo for example, you want to add a property called\n## Animal\nto a Sample and you want to control which terms are entered in this Property field. For this you need to do a couple of steps:\ncreate a new vocabulary\nAnimalVocabulary\nadd terms to that vocabulary:\n## Cat, Dog, Mouse\ncreate a new PropertyType (e.g.\n## Animal\n) of DataType\n## CONTROLLEDVOCABULARY\nand assign the\nAnimalVocabulary\nto it\ncreate a new SampleType (e.g.\n## Pet\n) and\nassign\nthe created PropertyType to that Sample type.\nIf you now create a new Sample of type\n## Pet\nyou will be able to add a property\n## Animal\nto it which only accepts the terms\n## Cat, Dog\nor\n## Mouse\n.\ncreate new Vocabulary with three VocabularyTerms\nvoc\n=\no\n.\nnew_vocabulary\n(\ncode\n=\n## 'BBB'\n,\ndescription\n=\n'description of vocabulary aaa'\n,\nurlTemplate\n=\n'https://ethz.ch'\n,\nterms\n=\n[\n{\n\"code\"\n## :\n'term_code1'\n,\n\"label\"\n## :\n\"term_label1\"\n,\n\"description\"\n## :\n\"term_description1\"\n},\n{\n\"code\"\n## :\n'term_code2'\n,\n\"label\"\n## :\n\"term_label2\"\n,\n\"description\"\n## :\n\"term_description2\"\n},\n{\n\"code\"\n## :\n'term_code3'\n,\n\"label\"\n## :\n\"term_label3\"\n,\n\"description\"\n## :\n\"term_description3\"\n}\n]\n)\nvoc\n.\nsave\n()\nvoc\n.\nvocabulary\n=\n'description of vocabulary BBB'\nvoc\n.\nchosenFromList\n=\n## True\nvoc\n.\nsave\n()\n# update\ncreate additional VocabularyTerms\nterm\n=\no\n.\nnew_term\n(\ncode\n=\n## 'TERM_CODE_XXX'\n,\nvocabularyCode\n=\n## 'BBB'\n,\nlabel\n=\n'here comes a label'\n,\ndescription\n=\n'here might appear a meaningful description'\n)\nterm\n.\nsave\n()\nupdate VocabularyTerms\nTo change the ordinal of a term, it has to be moved either to the top with the\n.move_to_top()\nmethod or after another term using the\n.move_after_term('TERM_BEFORE')\nmethod.\nvoc\n=\no\n.\nget_vocabulary\n(\n## 'STORAGE'\n)\nterm\n=\nvoc\n.\nget_terms\n()[\n## 'RT'\n]\nterm\n.\nlabel\n=\n## \"Room Temperature\"\nterm\n.\nofficial\n=\n## True\nterm\n.\nmove_to_top\n()\nterm\n.\nmove_after_term\n(\n'-40'\n)\nterm\n.\nsave\n()\nterm\n.\ndelete\n()\nChange ELN Settings via pyBIS\n\n## Main Menu\n\nThe ELN settings are stored as a\nJSON string\nin the\n$eln_settings\nproperty of the\n## GENERAL_ELN_SETTINGS\nsample. You can show the\nMain Menu settings\n## like this:\nimport\njson\nsettings_sample\n=\no\n.\nget_sample\n(\n## \"/ELN_SETTINGS/GENERAL_ELN_SETTINGS\"\n)\nsettings\n=\njson\n.\nloads\n(\nsettings_sample\n.\nprops\n[\n\"$eln_settings\"\n])\nprint\n(\nsettings\n[\n\"mainMenu\"\n])\n{\n'showLabNotebook'\n## :\n## True\n,\n'showInventory'\n## :\n## True\n,\n'showStock'\n## :\n## True\n,\n'showObjectBrowser'\n## :\n## True\n,\n'showExports'\n## :\n## True\n,\n'showStorageManager'\n## :\n## True\n,\n'showAdvancedSearch'\n## :\n## True\n,\n'showUnarchivingHelper'\n## :\n## True\n,\n'showTrashcan'\n## :\n## False\n,\n'showVocabularyViewer'\n## :\n## True\n,\n'showUserManager'\n## :\n## True\n,\n'showUserProfile'\n## :\n## True\n,\n'showZenodoExportBuilder'\n## :\n## False\n,\n'showBarcodes'\n## :\n## False\n,\n'showDatasets'\n## :\n## True\n}\nTo modify the\nMain Menu settings\n, you have to change the settings dictionary, convert it back to json and save the sample:\nsettings\n[\n'mainMenu'\n][\n'showTrashcan'\n]\n=\n## False\nsettings_sample\n.\nprops\n[\n'$eln_settings'\n]\n=\njson\n.\ndumps\n(\nsettings\n)\nsettings_sample\n.\nsave\n()\n## Storages\n\n## The\nELN storages settings\ncan be found in the samples of project\n## /ELN_SETTINGS/STORAGES\no\n.\nget_samples\n(\nproject\n=\n## '/ELN_SETTINGS/STORAGES'\n)\nTo change the settings, just change the sample’s properties and save the sample:\nsto\n=\no\n.\nget_sample\n(\n## '/ELN_SETTINGS/STORAGES/BENCH'\n)\nsto\n.\nprops\n()\n{\n'$name'\n## :\n## 'Bench'\n,\n'$storage.row_num'\n## :\n'1'\n,\n'$storage.column_num'\n## :\n'1'\n,\n'$storage.box_num'\n## :\n'9999'\n,\n'$storage.storage_space_warning'\n## :\n'80'\n,\n'$storage.box_space_warning'\n## :\n'80'\n,\n'$storage.storage_validation_level'\n## :\n## 'BOX_POSITION'\n,\n'$xmlcomments'\n## :\n## None\n,\n'$annotations_state'\n## :\n## None\n}\nsto\n.\nprops\n[\n'$storage.box_space_warning'\n]\n=\n'80'\nsto\n.\nsave\n()\n## Templates\n\n## The\nELN templates settings\ncan be found in the samples of project\n## /ELN_SETTINGS/TEMPLATES\no\n.\nget_samples\n(\nproject\n=\n## '/ELN_SETTINGS/TEMPLATES'\n)\nTo change the settings, use the same technique as shown above with the storages settings.\n## Custom Widgets\n\nTo change the\nCustom Widgets settings\n, get the\nproperty_type\nand set the\nmetaData\n## attribute:\npt\n=\no\n.\nget_property_type\n(\n## 'YEAST.SOURCE'\n)\npt\n.\nmetaData\n=\n{\n'custom_widget'\n## :\n## 'Spreadsheet'\n}\npt\n.\nsave\n()\nCurrently, the value of the\ncustom_widget\nkey can be set to either\n## Spreadsheet\n(for tabular, Excel-like data)\n## Word\n## Processor\n(for rich text data)\nThings object\n\nGeneral flow of data processing in PyBIS consists of:\npreparing a JSON request to OpenBIS\nreceiving a JSON response and validating it\npacking it in user-friendly\nclass\ncontaining some helper methods.\nThere are multiple classes implemented, depending on the initial PyBIS call it may change (e.g. pybis.sample.Sample for\nget_sample()\nor pybis.experiment.Experiment for\nget_experiment()\n).\n## In\n[\n1\n## ]:\nexperiment\n=\no\n.\nget_experiment\n(\n## '/DEFAULT/DEFAULT/DEFAULT'\n)\n## In\n[\n2\n## ]:\ntype\n(\nexperiment\n)\n## Out\n[\n3\n## ]:\npybis\n.\nexperiment\n.\n## Experiment\nFor methods returning multiple results (e.g.\nget_samples()\n,\nget_experiments()\n,\nget_groups()\n), a special class has been designed to hold the response. This class is pybis.things.Things.\n## In\n[\n1\n## ]:\nexperiments\n=\no\n.\nget_experiments\n()\n## In\n[\n2\n## ]:\ntype\n(\nexperiments\n)\n## Out\n[\n3\n## ]:\npybis\n.\nthings\n.\n## Things\n## Things\nclass offers three main ways to access the received data:\nJson response\n## Objects\nDataFrame\nAccessing the Json response (\nthings.response['objects']\n) directly bypasses the need to build additional Python objects; its main use case is for integrations where there are numerous results returned.\nOn the other hand, Objects (\nthings.objects\n) and DataFrame (\nthings.df\n) will build the needed Python objects the first time they are used; they offer a more pretty output, and their main use case is to be used in\nInteractive applications like Jupyter Notebooks.\nJSON response\n\n## All\n## Things\nobjects contain parsed JSON response from the OpenBIS, it may help with advanced searches and validation schemes.\nIt is accessible via\nresponse\nattribute.\n## Example\nexperiments\n=\no\n.\nget_experiments\n()\nfor\nexperiment\nin\nexperiments\n.\nresponse\n[\n'objects'\n## ]:\nprint\n(\nexperiment\n[\n'properties'\n])\n## Would produce following output:\n{}\n{\n## '$NAME'\n## :\n## 'Storages Collection'\n}\n{\n## '$NAME'\n## :\n## 'Template Collection'\n}\n{\n## '$NAME'\n## :\n## 'Storage Positions Collection'\n}\n{\n## '$NAME'\n## :\n## 'General Protocols'\n,\n## '$DEFAULT_OBJECT_TYPE'\n## :\n## 'GENERAL_PROTOCOL'\n}\n{\n## '$NAME'\n## :\n## 'Product Collection'\n,\n## '$DEFAULT_OBJECT_TYPE'\n## :\n## 'PRODUCT'\n}\nDataFrame\n\ndf\nattribute returns\npandas.core.frame.DataFrame\nobject with columns defined adequate to the response it is containing.\n## Note\nDataFrame building can be time-consuming depending on the size of data. Therefore its loading has been deferred to the first access to\ndf\nattribute (i.e. DataFrame is being lazy-loaded)\n## Example\nexperiments\n=\no\n.\nget_experiments\n()\nexperiments\n.\ndf\n## Would produce following output:\npermId\nidentifier\nregistrationDate\nmodificationDate\ntype\nregistrator\n0\n20240209011800684\n-\n1\n/\n## DEFAULT\n/\n## DEFAULT\n/\n## DEFAULT\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n01\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n01\n## UNKNOWN\nsystem\n1\n20240209011808121\n-\n4\n/\n## ELN_SETTINGS\n/\n## STORAGES\n/\n## STORAGES_COLLECTION\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n## COLLECTION\nsystem\n2\n20240209011808121\n-\n5\n/\n## ELN_SETTINGS\n/\n## TEMPLATES\n/\n## TEMPLATES_COLLECTION\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n## COLLECTION\nsystem\n3\n20240209011808121\n-\n6\n/\n## STORAGE\n/\n## STORAGE_POSITIONS\n/\n## STORAGE_POSITIONS_C\n...\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n## COLLECTION\nsystem\n4\n20240209011808121\n-\n17\n/\n## METHODS\n/\n## PROTOCOLS\n/\n## GENERAL_PROTOCOLS\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n## COLLECTION\nsystem\n5\n20240209011808121\n-\n18\n/\n## STOCK_CATALOG\n/\n## PRODUCTS\n/\n## PRODUCT_COLLECTION\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n08\n## COLLECTION\nsystem\n6\n20240209011822486\n-\n24\n/\n## DEFAULT_LAB_NOTEBOOK\n/\n## DEFAULT_PROJECT\n/\n## DEFAULT_\n...\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n22\n2024\n-\n02\n-\n09\n02\n## :\n18\n## :\n22\n## DEFAULT_EXPERIMENT\nsystem\n## Objects\n\nobjects\nattribute, similarly to\ndf\nbuilds a list of objects in a lazy way to easily access underlying data.\n## Note\nNot all PyBIS methods implements objects creation.\n## Example\nst\n=\no\n.\nget_sample_type\n(\n## 'EXPERIMENTAL_STEP'\n)\ntype\n(\nst\n.\nget_property_assignments\n()\n.\nobjects\n[\n0\n])\nst\n.\nget_property_assignments\n()\n.\nobjects\n[\n0\n]\n## Would produce following output:\npybis.entity_type.PropertyAssignment", "timestamp": "2025-09-18T09:38:29.759059Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_apis_python-v3-api:2", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/apis/python-v3-api.html", "repo": "openbis", "title": "Python (V3 API) - pyBIS!", "section": "Logout", "text": "attribute                        value\n-------------------------------  -------------------\npropertyType                     $NAME\ndataType                         VARCHAR\nsection                          General info\nordinal                          1\nmandatory                        False\ninitialValueForExistingEntities\nshowInEditView                   True\nshowRawValueInForms              False\nregistrator\nregistrationDate                 2024-02-09 02:18:24\nplugin\nunique                           False\nBest practices\n\n## Logout\n\nEvery PyBIS\nlogin()\ncall makes OpenBIS create a special session object and allocate resources to keep it alive. These sessions are terminated only when:\n## Explicit\nlogout()\ncall is performed.\nNumber of sessions per user has reached beyond configured limit.\nSession timeout is reached.\nKeeping large number of idle concurrent sessions may influence your OpenBIS instance. Please use\nlogout()\nin your scripts whenever you feel like OpenBIS connection is no longer required.\nIteration over tree structure\n\nOpenBIS data model is constructed in a tree structure, iterating over it ban be done in at least 2 ways:\nBy method chaining (i.e. using the result of the previous call):\nfor\nspace\nin\no\n.\nget_spaces\n## ():\nprint\n(\nspace\n.\ncode\n)\nfor\nproject\nin\nspace\n.\nget_projects\n## ():\nprint\n(\nf\n'\n\\t\n{\nproject\n.\ncode\n}\n'\n)\nfor\nexperiment\nin\nproject\n.\nget_experiments\n## ():\nprint\n(\nf\n'\n\\t\\t\n{\nexperiment\n.\ncode\n}\n'\n)\nfor\nsample\nin\nexperiment\n.\nget_samples\n## ():\nprint\n(\nf\n'\n\\t\\t\\t\n{\nsample\n.\ncode\n}\n'\n)\nfor\ndataset\nin\nsample\n.\nget_datasets\n## ():\nprint\n(\nf\n'\n\\t\\t\\t\\t\n{\ndataset\n.\ncode\n}\n'\n)\n## By individual call of Openbis object:\nfor\nspace\nin\no\n.\nget_spaces\n## ():\nprint\n(\nspace\n.\ncode\n)\nfor\nproject\nin\no\n.\nget_projects\n(\nspace\n=\nspace\n.\ncode\n## ):\nprint\n(\nf\n'\n\\t\n{\nproject\n.\ncode\n}\n'\n)\nfor\nexperiment\nin\no\n.\nget_experiments\n(\nspace\n=\nspace\n.\ncode\n,\nproject\n=\nproject\n.\ncode\n## ):\nprint\n(\nf\n'\n\\t\\t\n{\nexperiment\n.\ncode\n}\n'\n)\nfor\nsample\nin\no\n.\nget_samples\n(\nspace\n=\nspace\n.\ncode\n,\nproject\n=\nproject\n.\ncode\n,\nexperiment\n=\nexperiment\n.\ncode\n## ):\nprint\n(\nf\n'\n\\t\\t\\t\n{\nsample\n.\ncode\n}\n'\n)\nfor\ndataset\nin\no\n.\nget_datasets\n(\nsample\n=\nsample\n.\ncode\n## ):\nprint\n(\nf\n'\n\\t\\t\\t\\t\n{\ndataset\n.\ncode\n}\n'\n)\nFirst solution is faster and cleaner to use, it is a recommended way to iterate over the data structure.\nIteration over raw data\n\nIf performance is of the higher priority, iterating over the raw data would be recommended solution:\nfor\nspace\nin\no\n.\nget_spaces\n()\n.\nresponse\n[\n'objects'\n## ]:\nprint\n(\nspace\n[\n'code'\n])\nfor\nproject\nin\no\n.\nget_projects\n(\nspace\n=\nspace\n[\n'code'\n])\n.\nresponse\n[\n'objects'\n## ]:\nprint\n(\nf\n'\n\\t\n{\nproject\n[\n\"code\"\n]\n}\n'\n)\nfor\nexperiment\nin\no\n.\nget_experiments\n(\nspace\n=\nspace\n[\n'code'\n],\nproject\n=\nproject\n[\n'code'\n])\n.\nresponse\n[\n'objects'\n## ]:\nprint\n(\nf\n'\n\\t\\t\n{\nexperiment\n[\n\"code\"\n]\n}\n'\n)\nfor\nsample\nin\no\n.\nget_samples\n(\nspace\n=\nspace\n[\n'code'\n],\nproject\n=\nproject\n[\n'code'\n],\nexperiment\n=\nexperiment\n[\n'code'\n])\n.\nresponse\n## :\nprint\n(\nf\n'\n\\t\\t\\t\n{\nsample\n[\n\"code\"\n]\n}\n'\n)\nfor\ndataset\nin\no\n.\nget_datasets\n(\nsample\n=\nsample\n[\n'code'\n])\n.\nresponse\n## :\nprint\n(\nf\n'\n\\t\\t\\t\\t\n{\ndataset\n[\n\"code\"\n]\n}\n'\n)", "timestamp": "2025-09-18T09:38:29.759059Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_client-side-extensions_eln-lims-web-ui-extensions:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/client-side-extensions/eln-lims-web-ui-extensions.html", "repo": "openbis", "title": "ELN-LIMS WEB UI extensions", "section": "Introduction", "text": "ELN-LIMS WEB UI extensions\n\n## Introduction\n\nThe current aim of this extensions is to accommodate two groups of\n## modifications:\nPure Configuration, enabling/disabling some features, to clean the\ninterface and make it less confusing for non expert users. Very often\nalso to add type extensions for types specified with another master data\nextension.\nextending the interface to accommodate additional functionality without\nneeding to deal with the internals.\nPlugin structure\n\nplugins folder\n\nEach folder on this folder is a ELN UI extension.\nEach extension currently contains a single file with name “plugin.js”.\nconfig.js file\n\nContains a section called  PLUGINS_CONFIGURATION indicating the plugins\nto be loaded from the plugins folder.\nvar\n## PLUGINS_CONFIGURATION\n=\n{\nextraPlugins\n## :\n[\n\"life-sciences\"\n,\n\"flow\"\n,\n\"microscopy\"\n]\n}\nplugin.js file\n\nContains the actual source of the plugin, we can distinguish three clear\nsections/patterns on the skeleton of the interface:\n## Interface:\nhttps://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/js/config/ELNLIMSPlugin.js\n1. Configuring views through the use of a JSON structure. Part of this\n## structure are:\nforcedDisableRTF (Deprecated in favour of Custom Widgets\nconfigurable from the Instance Settings on the UI)\nforceMonospaceFont (Deprecated in favour of Custom Widgets\nconfigurable from the Instance Settings on the UI)\nexperimentTypeDefinitionsExtension\nsampleTypeDefinitionsExtension\ndataSetTypeDefinitionsExtension\nThese are used extensively since they come at a very low development\neffort. Best examples of how to use these definition extensions can be\nfound in technologies that ship with the ELN:\n## Generic Technology:\nhttps://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/generic/plugin.js\n## Life Sciences Technology:\nhttps://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/life-sciences/plugin.js\n2. Extending views through the use of the\n## Interceptor\n## Pattern\n## Template Methods:\n## ONLY\nallow to add content in certain portions\nof the Interface.\n## ONLY\navailable for Experiment, Sample and\nDataSet form views. These template methods are easy to use, they\nallow to add custom components isolating the programmer from the\nrest of the form.\nexperimentFormTop\nexperimentFormBottom\nsampleFormTop\ndataSetFormBottom\ndataSetFormTop\ndataSetFormBottom\nEvent Listeners: Allow to listen the before/after paint events for\n## ALL\nform views and list views. Allow the programmer to change\nthe model before is displayed and any part of the view after.\nProvide versatility but with added complexity of dealing with the\ncomplete form.\nbeforeViewPaint\nafterViewPaint\nTemplate methods are only needed to add custom components to from\nviews. Best examples of how to use these can be found in\ntechnologies that ship with the ELN:\n## Microscopy Technology:\nhttps://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/microscopy/plugin.js\n## 3. Other Extensions:\nonSampleSave: Reserved for internal use and discouraged to use. It\nis tricky to use properly.\ngetExtraUtilities: Allows to extend the utilities menu. A great\nexample is this template:\nhttps://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/template-extra-utilities/plugin.js\nSource Code Examples (plugin.js)\n\n### Configuration Only Extensions\n\nAn example with only type configurations extensions is show below.\nfunction\nMyTechnology\n()\n{\nthis\n.\ninit\n();\n}\n$\n.\nextend\n(\nMyTechnology\n.\nprototype\n,\nELNLIMSPlugin\n.\nprototype\n,\n{\ninit\n## :\nfunction\n()\n{\n},\nexperimentTypeDefinitionsExtension\n## :\n{\n## \"FOLDER\"\n## :\n{\n## \"TOOLBAR\"\n## :\n{\n## CREATE\n## :\nfalse\n,\n## FREEZE\n## :\nfalse\n,\n## EDIT\n## :\nfalse\n,\n## MOVE\n## :\nfalse\n,\n## DELETE\n## :\nfalse\n,\n## UPLOAD_DATASET\n## :\nfalse\n,\n## UPLOAD_DATASET_HELPER\n## :\nfalse\n,\n## EXPORT_ALL\n## :\nfalse\n,\n## EXPORT_METADATA\n## :\ntrue\n}\n}\n},\nsampleTypeDefinitionsExtension\n## :\n{\n## \"SAMPLE_TYPE\"\n## :\n{\n## \"TOOLBAR\"\n## :\n{\n## CREATE\n## :\ntrue\n,\n## EDIT\n## :\ntrue\n,\n## FREEZE\n## :\ntrue\n,\n## MOVE\n## :\ntrue\n,\n## COPY\n## :\ntrue\n,\n## DELETE\n## :\ntrue\n,\n## PRINT\n## :\ntrue\n,\n## HIERARCHY_GRAPH\n## :\ntrue\n,\n## HIERARCHY_TABLE\n## :\ntrue\n,\n## UPLOAD_DATASET\n## :\ntrue\n,\n## UPLOAD_DATASET_HELPER\n## :\ntrue\n,\n## EXPORT_ALL\n## :\ntrue\n,\n## EXPORT_METADATA\n## :\ntrue\n,\n## TEMPLATES\n## :\ntrue\n,\n## BARCODE\n## :\ntrue\n},\n## \"SHOW\"\n## :\nfalse\n,\n## \"SAMPLE_CHILDREN_DISABLED\"\n## :\nfalse\n,\n## \"SAMPLE_CHILDREN_ANY_TYPE_DISABLED\"\n## :\nfalse\n,\n## \"SAMPLE_PARENTS_DISABLED\"\n## :\nfalse\n,\n## \"SAMPLE_PARENTS_ANY_TYPE_DISABLED\"\n## :\ntrue\n,\n## \"SAMPLE_PARENTS_HINT\"\n## :\n[{\n## \"LABEL\"\n## :\n## \"Parent Label\"\n,\n## \"TYPE\"\n## :\n## \"PARENT_TYPE\"\n,\n## \"ANNOTATION_PROPERTIES\"\n## :\n[]\n}],\n## \"SAMPLE_CHILDREN_HINT\"\n## :\n[{\n## \"LABEL\"\n## :\n## \"Children Label\"\n,\n## \"TYPE\"\n## :\n## \"CHILDREN_TYPE\"\n,\n## \"MIN_COUNT\"\n## :\n0\n,\n## \"ANNOTATION_PROPERTIES\"\n## :\n[{\n## \"TYPE\"\n## :\n## \"ANNOTATION.SYSTEM.COMMENTS\"\n,\n## \"MANDATORY\"\n## :\nfalse\n}]\n}],\n## \"ENABLE_STORAGE\"\n## :\nfalse\n,\n## \"SHOW_ON_NAV\"\n## :\nfalse\n,\n## \"SHOW_ON_NAV_FOR_PARENT_TYPES\"\n## :\nundefined\n,\nextraToolbar\n## :\nundefined\n},\n},\ndataSetTypeDefinitionsExtension\n## :\n{\n## \"DATASET_TYPE\"\n## :\n{\n## \"TOOLBAR\"\n## :\n{\n## EDIT\n## :\ntrue\n,\n## FREEZE\n## :\ntrue\n,\n## MOVE\n## :\ntrue\n,\n## ARCHIVE\n## :\ntrue\n,\n## DELETE\n## :\ntrue\n,\n## HIERARCHY_TABLE\n## :\ntrue\n,\n## EXPORT_ALL\n## :\ntrue\n,\n## EXPORT_METADATA\n## :\ntrue\n},\n## \"DATASET_PARENTS_DISABLED\"\n## :\nfalse\n,\nextraToolbar\n## :\nundefined\n},\n}\n});\nprofile\n.\nplugins\n.\npush\n(\nnew\nMyTechnology\n());\n## Toolbar Extensions\n\nAn example with only toolbar extensions is shown below, variables with a\n## dollar sign ‘$’ indicate they are jquery components:\nfunction\nMyTechnology\n()\n{\nthis\n.\ninit\n();\n}\n$\n.\nextend\n(\nMyTechnology\n.\nprototype\n,\nELNLIMSPlugin\n.\nprototype\n,\n{\ninit\n## :\nfunction\n()\n{\n},\nsampleTypeDefinitionsExtension\n## :\n{\n## \"SAMPLE_TYPE\"\n## :\n{\nextraToolbar\n## :\nfunction\n(\nmode\n,\nsample\n)\n{\nvar\ntoolbarModel\n=\n[];\nif\n(\nmode\n===\nFormMode\n.\n## VIEW\n)\n{\nvar\n$demoButton\n=\nFormUtil\n.\ngetButtonWithIcon\n(\n\"glyphicon-heart\"\n,\nfunction\n()\n{\n//This empty function could be a call to do something in particular\n});\ntoolbarModel\n.\npush\n({\ncomponent\n## :\n$demoButton\n,\ntooltip\n## :\n## \"Demo\"\n});\n}\nreturn\ntoolbarModel\n;\n}\n},\n},\ndataSetTypeDefinitionsExtension\n## :\n{\n## \"DATASET_TYPE\"\n## :\n{\nextraToolbar\n## :\nfunction\n(\nmode\n,\ndataset\n)\n{\nvar\ntoolbarModel\n=\n[];\nif\n(\nmode\n===\nFormMode\n.\n## VIEW\n)\n{\nvar\n$demoButton\n=\nFormUtil\n.\ngetButtonWithIcon\n(\n\"glyphicon-heart\"\n,\nfunction\n()\n{\n//This empty function could be a call to do something in particular\n});\ntoolbarModel\n.\npush\n({\ncomponent\n## :\n$demoButton\n,\ntooltip\n## :\n## \"Demo\"\n});\n}\nreturn\ntoolbarModel\n;\n}\n},\n}\n});\nprofile\n.\nplugins\n.\npush\n(\nnew\nMyTechnology\n());\nExtra Views as Utilities\n\nPlease check the provided example:\nhttps://sissource.ethz.ch/sispub/openbis/-/blob/master/ui-eln-lims/src/core-plugins/eln-lims/1/as/webapps/eln-lims/html/plugins/template-extra-utilities/plugin.js", "timestamp": "2025-09-18T09:38:29.770647Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_client-side-extensions_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/client-side-extensions/index.html", "repo": "openbis", "title": "Client-Side Extensions", "section": "Client-Side Extensions", "text": "## Client-Side Extensions\n\nELN-LIMS WEB UI extensions\n## Introduction\nPlugin structure\nplugins folder\nconfig.js file\nplugin.js file\nSource Code Examples (plugin.js)\n### Configuration Only Extensions\n## Toolbar Extensions\nExtra Views as Utilities\nopenBIS webapps\n## Introduction\n## Example\n## Directory Structure\nplugin.properties\n## URL\n### Server Configuration\n### Jetty Configuration\nEmbedding webapps in the OpenBIS UI\n## Introduction\nConfiguring embedded webapps\nCreating embedded webapps\nLinking to subtabs of other entity detail views\nCross communication openBIS > DSS\n## Background\n### Default Configuration\n### Basic Configuration\n### Advanced Configuration\nEmbedding openBIS Grids in Web Apps\n### Requirements\n## Use\nImage Viewer component", "timestamp": "2025-09-18T09:38:29.772827Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_client-side-extensions_openbis-webapps:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/client-side-extensions/openbis-webapps.html", "repo": "openbis", "title": "openBIS webapps", "section": "Introduction", "text": "openBIS webapps\n\n## Introduction\n\nWebapps are HTML5 apps that interact with openBIS. Webapps can be\ndistributed as core-plugins. To supply a webapp plugin, create a folder\ncalled\nwebapps\nin the\nas\n. Each subfolder of the\nwebapps\nfolder is\ntreated as a webapp plugin. A webapp plugin requires two things, a\nplugin.properties\nfile, as with all plugins, and a folder containing\nthe content of the webapp. This folder can have any name and needs to be\nreferenced in the\nplugin.properties\nfile with the key\nwebapp-folder\n.\nIt is recommended to name the webapp folder\nhtml\nas done in the\nexamples below. This has the advantage that an existing subfolder named\netc\nwill not be changed after an upgrade of the plugin. That is, the\ncontent of the folder\nhtml/etc\nwill be completely untouched by\nupgrades. This feature allows to provide an initial configuration (say\nin\nhtml/etc/config.js\n) with some default settings which can be\noverridden by the customer.\nThe webapp is then served by the same web server (jetty) that serves\nopenBIS. The name of the webapp defines the URL used to access it. See\nthe example below. The file index.html is used as a welcome page if the\nuser does not specifically request a particular page.\n## Warning\nAn openBIS webapp is\nnot\na J2EE webapp. It has more in common with an app for mobile devices.\n## Example\n\nThis is an example of a webapp. In a real webapp, the name of the webapp\ncan be any valid folder name. The same goes for the folder in the webapp\ncontaining the the code. The name of the webapp folder is what is used\nto define the URL. The name of the folder containing the code is neither\nshown nor available to the user.\n## Directory Structure\n\n[module]\n[version]\nas\nwebapps\nexample-webapp\nplugin.properties\nhtml\nindex.html\nfun-viewer\nplugin.properties\nhtml\ncode\nindex.html\nplugin.properties\n\n# The properties file for an example webapps plugin\n# This file has no properties defined because none need to be defined.\nwebapp-folder = html\n## URL\n\nIf openBIS is served at the URL\nhttps://my.domain.com:8443/openbis\n,\nthe above webapps will be available under the following URLs:\nhttps://my.domain.com:8443/openbis/webapp/example-webapp\nhttps://my.domain.com:8443/openbis/webapp/fun-viewer\n### Server Configuration\n\nThere are two things to consider in the server configuration. The\ninjection of webapps is done through Jetty, which is the web server we\nuse for openBIS. If you use the default provided jetty.xml\nconfiguration, then you do not need to do anything extra; if, on the\nother hand, you have a custom jetty.xml configuration, then you will\nneed to update your jetty.xml file to support webapps.\n### Jetty Configuration\n\nIf your openBIS server has a custom jetty.xml file, you will need to\nmodify the file to include support for injecting web apps. To do this,\nyou will need to replace\norg.eclipse.jetty.deploy.providers.WebAppProvider by\nch.systemsx.cisd.openbis.generic.server.util.OpenbisWebAppProvider in\naddAppProvider\ncall to your jetty.xml.\njetty.xml\n## <Call\nname=\n\"addBean\"\n>\n## <Arg>\n## <New\nid=\n\"DeploymentManager\"\nclass=\n\"org.eclipse.jetty.deploy.DeploymentManager\"\n>\n## <Set\nname=\n\"contexts\"\n>\n## <Ref\nid=\n## \"Contexts\"\n/>\n## </Set>\n## <Call\nname=\n\"addAppProvider\"\n>\n## <Arg>\n## <New\nclass=\n\"ch.systemsx.cisd.openbis.generic.server.util.OpenbisWebAppProvider\"\n>\n## <Set\nname=\n\"monitoredDir\"\n## ><Property\nname=\n\"jetty.home\"\ndefault=\n\".\"\n/>\n/webapps\n## </Set>\n## <Set\nname=\n\"scanInterval\"\n>\n0\n## </Set>\n## <Set\nname=\n\"extractWars\"\n>\ntrue\n## </Set>\n## </New>\n## </Arg>\n## </Call>\n## </New>\n## </Arg>\n## </Call>\nEmbedding webapps in the OpenBIS UI\n\n## Introduction\n\nWebapps can be used as both standalone applications as well as can be\nembedded in the OpenBIS web UI. Standalone webapps are built to\ncompletely replace the original OpenBIS web interface with customer\nadjusted layout and functionality. Users of the standalone webapps are\nusually completely unaware of the default OpenBIS look and feel. The\nwebapp itself provides them with all the functionality they need: login\npages, web forms, searches, images, charts etc. The standalone webapp is\na right choice when you want to build a very specific and fully featured\nweb interface from scratch. If you want to use the default OpenBIS UI\nbut extend it with some custom functionality then embedding a webapp in\nthe OpenBIS UI is probably a way to go. To make a webapp visible as a\npart of the default OpenBIS UI you have to define where the webapp\nshould be shown using “openbisui-contexts” property. Moreover some of\nthe contexts also require additional information describing when the\nwebapp should be shown. For instance, to embed a webapp in the\nexperiment details view that will be displayed for experiments with type\n## “MY_EXPERIMENT_TYPE” your plugin.properties file should look like:\nplugin.propeties\nwebapp-folder = html\nopenbisui-contexts = experiment-details-view\nexperiment-entity-types = MY_EXPERIMENT_TYPE\nConfiguring embedded webapps\n\nA full list of supported properties is presented below.\n## Property Key\n## Description\nAllowed values\nopenbisui-contexts\nPlace where the webapp is shown in the OpenBIS UI.\nmodules-menu\nwebapp is an item in the modules top menu\nexperiment-details-view\nwebapp is a tab in the experiment details view\nrequires experiment-entity-types to be defined\nsample-details-view\nwebapp is a tab in the sample details view\nrequires sample-entity-types to be defined\ndata-set-details-view\nwebapp is a tab in the data set details view\nrequires data-set-entity-types to be defined\nmaterial-details-view\nwebapp is a tab in the material details view\nrequires material-entity-types to be defined\nAccepts a comma separated list of values with regular expressions, e.g. “modules-menu, .*-details-view”\nlabel\nThe label. It will be shown in the GUI.\n## String\nsorting\nSorting of the webapp. Webapps are sorted by “sorting” and “folder name” ascending with nulls last (webapps without sorting are presented last).\n## Integer\nexperiment-entity-types\nTypes of experiments the webapp should be displayed for.\nAccepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”\nsample-entity-types\nTypes of samples the webapp should be displayed for.\nAccepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”\ndata-set-entity-types\nTypes of data sets the webapp should be displayed for.\nAccepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”\nmaterial-entity-types\nTypes of materials the webapp should be displayed for.\nAccepts a comma separated list of values with regular expressions, e.g. “TYPE_A_1, TYPE_A_2, TYPE_B_.*”\nCreating embedded webapps\n\nEmbedded webapps similar to the standalone counterparts are HTML5\napplications that interact with OpenBIS. Because embedded webapps are\nshown inside the OpenBIS UI they have access to additional information\nabout the context they are displayed in. For instance, a webapp that is\ndisplayed in the experiment-details-view context knows that it is\ndisplayed for an experiment entity, with a given type, identifier and\npermid. Having this information the webapp can adjust itself and display\nonly data related to the currently chosen entity. Apart from the entity\ndetails, a webapp also receives a current sessionId that can be used for\ncalling OpenBIS JSON RPC services. This way embedded webapps can reuse a\ncurrent session that was created when a user logged in to the OpenBIS\nrather than provide their own login pages for authentication. A sample\nwebapp that makes use of this context information is presented below:\nwebapp.html\n<\nhtml\n>\n<\nhead\n>\n<!-- include jquery library required by the openbis.js -->\n<\nscript\nsrc\n=\n\"/openbis/resources/js/jquery.js\"\n></\nscript\n>\n<!-- include openbis library to gain access to the openbisWebAppContext and openbis objects -->\n<\nscript\nsrc\n=\n\"/openbis/resources/js/openbis.js\"\n></\nscript\n>\n</\nhead\n>\n<\nbody\n>\n<\ndiv\nid\n=\n\"log\"\n></\ndiv\n>\n<\nscript\n>\n$\n(\ndocument\n).\nready\n(\nfunction\n(){\n// create a context object to access the context information\nvar\nc\n=\nnew\nopenbisWebAppContext\n();\n$\n(\n\"#log\"\n).\nappend\n(\n\"SessionId: \"\n+\nc\n.\ngetSessionId\n()\n+\n\"<br/>\"\n);\n$\n(\n\"#log\"\n).\nappend\n(\n\"EntityKind: \"\n+\nc\n.\ngetEntityKind\n()\n+\n\"<br/>\"\n);\n$\n(\n\"#log\"\n).\nappend\n(\n\"EntityType: \"\n+\nc\n.\ngetEntityType\n()\n+\n\"<br/>\"\n);\n$\n(\n\"#log\"\n).\nappend\n(\n\"EntityIdentifier: \"\n+\nc\n.\ngetEntityIdentifier\n()\n+\n\"<br/>\"\n);\n$\n(\n\"#log\"\n).\nappend\n(\n\"EntityPermId: \"\n+\nc\n.\ngetEntityPermId\n()\n+\n\"<br/>\"\n);\n// create an OpenBIS facade to call JSON RPC services\nvar\no\n=\nnew\nopenbis\n();\n// reuse the current sessionId that we received in the context for all the facade calls\no\n.\nuseSession\n(\nc\n.\ngetSessionId\n());\n// call one of the OpenBIS facade methods\no\n.\nlistProjects\n(\nfunction\n(\nresponse\n){\n$\n(\n\"#log\"\n).\nappend\n(\n\"<br/>Projects:<br/>\"\n);\n$\n.\neach\n(\nresponse\n.\nresult\n,\nfunction\n(\nindex\n,\nvalue\n){\n$\n(\n\"#log\"\n).\nappend\n(\nvalue\n.\ncode\n+\n\"<br/>\"\n);\n});\n});\n});\n</\nscript\n>\n</\nbody\n>\n</\nhtml\n>\nLinking to subtabs of other entity detail views\n\nA link from a webapp to an entity subtab looks like this:\n<a href=\"#\" onclick=\"window.top.location.hash='#entity=[ENTITY_KIND]&permId=[PERM_ID]&ui-subtab=[SECTION];return false;\">Link Text</a>\n, for example\n<a href=\"#\" onclick=\"window.top.location.hash='#entity=EXPERIMENT&permId=20140716095938913-1&ui-subtab=webapp-section_test-webapp;return false;\">Experiment webapp</a>\nENTITY_KIND = ‘EXPERIMENT’ / ‘SAMPLE’ / ‘DATA_SET’ / ‘MATERIAL’\nPERM_ID = Entity permid\nSECTION = Subtab identifier.\n## Notes about subtab identifiers:\nThe valid subtab identifiers can be found from\nch.systemsx.cisd.openbis.generic.client.web.client.application.framework.DisplayTypeIDGenerator.java\nManaged property subtab identifiers are of format\n‘managed_property_section_[MANAGED_PROPERTY_TYPE_CODE]’\nWebapp subtab identifiers are of format\n‘webapp-section_[WEBAPP_CODE]’ (webapp code is a name of the\nwebapp core-plugin folder, i.e.\n[technology]/[version]/as/webapps/[WEBAPP_CODE])\nCross communication openBIS > DSS\n\n## Background\n\nSometimes is required for a web app started in openBIS to make a call to\nthe DSS. This happens often to upload files or navigate datasets between\nothers.\nMaking calls to different domains is forbidden by the web security\nsandbox and a common client side issue.\nTo make the clients accept the responses without additional\nconfiguration by the users, the server should set a special header\n“Access-Control-Allow-Origin” on the response when accessing from a\ndifferent domain or port.\n### Default Configuration\n\nThis is done automatically by the DSS for any requests coming from well\nknown openBIS web apps.\nA well known openBIS web app is a web app running using the same URL\nconfigured for openbis on the DSS service.properties.\nDSS service.properties\n# The URL of the openBIS server\nserver-url = https://sprint-openbis.ethz.ch:8446\nEven if the web app is accessible from other URLs, not using the URL\nconfigured on the DSS service.properties will lead to the DSS not\nrecognizing the app.\n## Warning\nAs a consequence the DSS will not set the necessary header and the client will reject the responses.\n### Basic Configuration\n\nThis is required very often in enterprise environments where the\nreachable openBIS URL is not necessarily the one configured on the DSS\nservice.properties.\nIs possible to add additional URLS configuring the AS\nservice.properties.\nAS service.properties\ntrusted-cross-origin-domains= https://195.176.122.56:8446\nThe first time the DSS will need to check the valid URLs after a start\nup will contact the AS to retrieve the additional trusted domain list.\n### Advanced Configuration\n\nA very typical approach is to run both the AS and DSS on the same port\nusing a reverse proxy like Apache or NGNIX. This way the web security\nsandbox is respected. On this case the “Access-Control-Allow-Origin”\nheader is unnecessary and will also work out of the box.\n## Warning\nEven with this configuration, sometimes happens that a web app call the DSS using an auto detected URL given by openBIS. This auto detected URL not necessarily respects your proxy configuration, giving a different port or hostname to the DSS.\nOn this case you will need to solve the problems with one of the methods\nexplained above or modify your web app.\nEmbedding openBIS Grids in Web Apps\n\nUsers of openBIS will have encountered the advanced and powerful table\nviews used in the application. These views allow for sorting and\nfiltering. It is possible to take advantage of these views in custom web\nUIs.\n### Requirements\n\nIt is possible to use openBIS table views in a web UI when the data for\nthe table comes from an aggregation service. The parameters to the\naggregation service are passed as URL query parameters, thus an\nadditional requirement is that all the aggregation service parameters\ncan be passed this way. A final requirement is that the web UI be\nexposed as an embedded webapp (this is necessary because of the way\nopenBIS keeps track of the user of the system). If these requirements\nare met, then it will be possible to embed an openBIS table view display\nthe aggregation service data in a web UI.\n## Use\n\nTo embed a table view, add an iframe to the web UI. The URL of the\niframe should have the following form:\n{openbis url}?viewMode=GRID#action=AGGREGATION_SERVICE&serviceKey={aggregation service key}&dss={data store server code}[& gridSettingsId][& gridHeaderText][& service parameters]\n## Parameters:\n## Parameter\n## Description\n## Required\nserviceKey\nAn aggregation service that will be used for generating the data for the grid.\ntrue\ndss\nA code of a data store that will be used for generating the data for the grid.\ntrue\ngridSettingsId\nAn identifier of the grid that will be used for storing the grid settings (visibility of columns, sorting, filters etc.). If not specified then the serviceKey parameter is used.\nfalse\ngridHeaderText\nA header of the grid. If not specified then the header is not shown.\nfalse\n## Example:\nhttp://localhost:8888/openbis-test/index.html?viewMode=GRID#action=AGGREGATION_SERVICE&serviceKey=sp-233&dss=standard&gridSettingsId=myTestGridSettingsId&gridHeaderText=myTestGridHeaderText&name=hello\n## Full Example\n<!DOCTYPE html>\n<\nhtml\nxmlns\n=\n\"http://www.w3.org/1999/xhtml\"\nxml:lang\n=\n\"en-US\"\nlang\n=\n\"en-US\"\n>\n<\nhead\n>\n<\nmeta\nhttp-equiv\n=\n\"content-type\"\ncontent\n=\n\"text/html; charset=utf-8\"\n/>\n<\ntitle\n>\n## Embedded Grid Example\n</\ntitle\n>\n</\nhead\n>\n<\nbody\n>\n<\niframe\nsrc\n=\n\"http://localhost:8888/openbis-test/index.html?viewMode=GRID#action=AGGREGATION_SERVICE&serviceKey=sp-233&dss=standard&gridSettingsId=myTestGridSettingsId&gridHeaderText=myTestGridHeaderText&name=hello\"\nwidth\n=\n\"100%\"\nheight\n=\n\"95%\"\nstyle\n=\n\"border: none\"\n>\n</\nbody\n>\n</\nhtml\n>\nImage Viewer component\n\n## Image viewer screenshot:\nExample usage of the image viewer component:\n<!DOCTYPE html>\n<\nhtml\n>\n<\nhead\n>\n<\nmeta\ncharset\n=\n\"utf-8\"\n>\n<\ntitle\n>\n## Image Viewer Example\n</\ntitle\n>\n<\nlink\nrel\n=\n\"stylesheet\"\nhref\n=\n\"/openbis/resources/lib/bootstrap/css/bootstrap.min.css\"\n>\n<\nlink\nrel\n=\n\"stylesheet\"\nhref\n=\n\"/openbis/resources/lib/bootstrap-slider/css/bootstrap-slider.min.css\"\n>\n<\nlink\nrel\n=\n\"stylesheet\"\nhref\n=\n\"/openbis/resources/components/imageviewer/css/image-viewer.css\"\n>\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"/openbis/resources/config.js\"\n></\nscript\n>\n<\nscript\ntype\n=\n\"text/javascript\"\nsrc\n=\n\"/openbis/resources/require.js\"\n></\nscript\n>\n</\nhead\n>\n<\nbody\n>\n<\nscript\n>\n// ask for jquery library, openbis-screening facade and the image viewer component\nrequire\n([\n\"jquery\"\n,\n\"openbis-screening\"\n,\n\"components/imageviewer/ImageViewerWidget\"\n],\nfunction\n(\n$\n,\nopenbis\n,\nImageViewerWidget\n)\n{\n$\n(\ndocument\n).\nready\n(\nfunction\n()\n{\nvar\nfacade\n=\nnew\nopenbis\n();\nfacade\n.\nlogin\n(\n\"admin\"\n,\n\"password\"\n,\nfunction\n(\nresponse\n)\n{\n// create the image viewer component for the specific data sets\nvar\nwidget\n=\nnew\nImageViewerWidget\n(\nfacade\n,\n[\n\"20140513145946659-3284\"\n,\n\"20140415140347875-53\"\n,\n\"20140429125231346-56\"\n,\n\"20140429125614418-59\"\n,\n\"20140506132344798-146\"\n]);\n// do the customization once the component is loaded\nwidget\n.\naddLoadListener\n(\nfunction\n()\n{\nvar\nview\n=\nwidget\n.\ngetDataSetChooserWidget\n().\ngetView\n();\n// example of how to customize a widget\nview\n.\ngetDataSetText\n=\nfunction\n(\ndataSetCode\n)\n{\nreturn\n\"My data set: \"\n+\ndataSetCode\n;\n};\n// example of how to add a change listener to a widget\nwidget\n.\ngetDataSetChooserWidget\n().\naddChangeListener\n(\nfunction\n(\nevent\n)\n{\nconsole\n.\nlog\n(\n\"data set changed from: \"\n+\nevent\n.\ngetOldValue\n()\n+\n\" to: \"\n+\nevent\n.\ngetNewValue\n());\n});\n});\n// render the component and add it to the page\n$\n(\n\"#container\"\n).\nappend\n(\nwidget\n.\nrender\n());\n});\n});\n});\n</\nscript\n>\n<\ndiv\nid\n=\n\"container\"\nstyle\n=\n\"padding: 20px\"\n></\ndiv\n>\n</\nbody\n>\n</\nhtml\n>", "timestamp": "2025-09-18T09:38:29.776958Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_development-environment_architectural-overview:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/development-environment/architectural-overview.html", "repo": "openbis", "title": "Architectural Overview", "section": "Architectural Overview", "text": "## Architectural Overview\n\nRepository organization\n\nThe repository contains these kind of modules used to build the openBIS distributable:\napi-*: API Facades\napp-*: Applications\nbuild: Build scripts\ncore-plugins-*: Core plugins distributed with openBIS\nlib-*: Internally maintained libraries used to build openBIS\nserver-*: Server components\ntest-*: Integration tests\nui-*: User interfaces", "timestamp": "2025-09-18T09:38:29.780560Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_development-environment_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/development-environment/index.html", "repo": "openbis", "title": "Development Environment", "section": "Development Environment", "text": "## Development Environment\n\n### System Requirements\n## Architectural Overview\nRepository organization\nInstallation And Configuration Guide\nBuilding openBIS\nWhere the build is found?\nWhy we disable tests to make the build?\nWhy the core UI made using GWT is not build anymore?\nHow to compile the V3 JS bundle used by the new Admin UI in production?\nDevelopment of openBIS\n### Requirements\n## Step By Step\nSource Code Auto Formatting\n## Commit Messages Formatting\n## Source Code Copyright Header\n## Typical Errors\nIntelliJ can’t find package com.sun.*, but I can compile the project using the command line!\nIntelliJ can’t find a particular method\nTest seem to run through Gradle and fail\nTest seem to run through intelliJ but throw a package not open error\nDevelopment of NG UI\nSetting up IntelliJ Idea", "timestamp": "2025-09-18T09:38:29.782630Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_development-environment_installation-and-configuration-guide:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/development-environment/installation-and-configuration-guide.html", "repo": "openbis", "title": "Installation And Configuration Guide", "section": ":", "text": "Installation And Configuration Guide\n\nBuilding openBIS\n\ngit\nclone\nhttps\n## :\n//\nsissource\n.\nethz\n.\nch\n/\nsispub\n/\nopenbis\n.\ngit\ncd\napp\n-\nopenbis\n-\ninstaller\n/\n./\ngradlew\nclean\n./\ngradlew\nbuild\n-\nx\ntest\n\"-Dorg.gradle.jvmargs=--add-opens=java.base/java.text=ALL-UNNAMED --add-opens=java.desktop/java.awt.font=ALL-UNNAMED\"\nWhere the build is found?\n\n./\napp\n-\nopenbis\n-\ninstaller\n/\ntargets\n/\ngradle\n/\ndistributions\n/\nopenBIS\n-\ninstallation\n-\nstandard\n-\ntechnologies\n-\n## SNAPSHOT\n-\nrXXXXXXXXXX\n.\ntar\n.\ngz\nWhy we disable tests to make the build?\n\nThey increase the time to obtain a build plus some tests could have additional environment\nrequirements.\nWhy the core UI made using GWT is not build anymore?\n\nThe core UI is deprecated for removal on next mayor release and requires JDK8.\nFor now it can be build following the next commands and only with JDK8:\ngit\nclone\nhttps\n## :\n//\nsissource\n.\nethz\n.\nch\n/\nsispub\n/\nopenbis\n.\ngit\ncd\ncore\n-\nplugin\n-\nopenbis\n/\n./\ngradlew\nclean\n./\ngradlew\nbuildCoreUIPackageUsingJDK8\n-\nx\ntest\nHow to compile the V3 JS bundle used by the new Admin UI in production?\n\ngit\nclone\nhttps\n## :\n//\nsissource\n.\nethz\n.\nch\n/\nsispub\n/\nopenbis\n.\ngit\ncd\napi\n-\nopenbis\n-\njavascript\n/\n./\ngradlew\nclean\n./\ngradlew\nbundleOpenbisStaticResources\n-\nx\ntest\nThe output can be found at:\nserver-application-server/source/java/ch/systemsx/cisd/openbis/public/resources/api/v3\nconfig.bundle.js\nconfig.bundle.min.js\nopenbis.bundle.js\nopenbis.bundle.min.js\nDevelopment of openBIS\n\n### Requirements\n\n### Software Requirements\nIntelliJ IDEA CE\n## Step By Step\n\n## File\n->\n## New\n->\n## Project\n## From\n## Existing\n## Sources\n## Select\nthe\nbuild\nfolder\nto\nload\nthe\ngradle\nmodel\n## After\nthe\nmodel\nis\nloaded\nexecute\nthe\ntasks\n## :\nopenBISDevelopementEnvironmentASPrepare\nopenBISDevelopementEnvironmentASStart\nopenBISDevelopementEnvironmentDSSStart\nSource Code Auto Formatting\n\nOpenBIS source code uses a particular style preset that guarantees all code is formatted uniformly.\nTo make use of the preset go to File/Settings or IntelliJIDEA/Preferences depending on your OS.\nThen import the XML file under ‘docs/codestyle/SIS_Conventions_IntelliJ_V3.xml’. See images below:\n## Commit Messages Formatting\n\nOpenBIS source code commit messages use a particular formatting.\nThis formatting guarantees that there is a User Story behind it.\nTo ensure commits follow the formatting ‘Git Hooks’ are provided.\nJust copy them from the root folder of this repo run the next command:\n%/>\ncp\n./\ndocs\n/\nhooks\n/*\n./.\ngit\n/\nhooks\n/\n%/>\ngit\nadd\n## README\n.\nmd\n%/>\ngit\ncommit\n-\nm\n\"Test incorrectly formatted message\"\n## Aborting\ncommit\n.\n## Your\ncommit\nmessage\nis\nmissing\nan\nissue\nnumber\n(\n## 'SSDM-XXXXX:'\n)\n## Source Code Copyright Header\n\nOpenBIS source code is licensed under SIS copyright and licensed under ‘Apache 2 License’:\nhttp\n## :\n//\nwww\n.\napache\n.\norg\n/\nlicenses\n/\n## LICENSE\n-\n2.0\nTo guarantee all new source files contain the appropriate license a preset is provided.\nTo make use of the preset go to File/Settings or IntelliJIDEA/Preferences depending on your OS.\nThen import the XML file under ‘docs/copyright/Copyright_IntelliJ.xml’ under the copyright section as the image below indicate.\nLast, set the Copyright Profile under the Copyright section as the image below indicate:\n## Typical Errors\n\nIntelliJ can’t find package com.sun.*, but I can compile the project using the command line!\n\nTurn off “File | Settings | Build, Execution, Deployment | Compiler | Java Compiler | Use –release\noption for cross-compilation”.\nIntelliJ can’t find a particular method\n\nCode compatiblity 1.8 is set by default to work well with our javadoc tools but it can be set to 17 on IntelliJ. See image below.\nTest seem to run through Gradle and fail\n\nThey need to be set to run using IntelliJ.\nTest seem to run through intelliJ but throw a package not open error\n\nThe project does not uses modules yet. Add ‘–add-opens’ statements manually when launching the tests as shown below.\nDevelopment of NG UI\n\nGenerate openBIS JS bundle by running in command line\ncd /<OPENBIS_PROJECT_ROOT>/api-openbis-javascript\n./gradlew :bundleOpenbisStaticResources\nStart openBIS in your chosen IDE (NG UI assumes it will run\n## at: http://localhost:8888/openbis-test/):\nrun openBISDevelopementEnvironmentASPrepare gradle task\nrun openBISDevelopementEnvironmentASStart gradle task\nIn command line do:\ncd /<OPENBIS_PROJECT_ROOT>/ui-admin\nnpm install\nnpm run dev\nOpen in your chosen browser a url, by default: http://localhost:9999/admin\nSetting up IntelliJ Idea\n\nUnder “IntelliJ IDEA” -> “Preferences” -> “Languages and Frameworks” -> Javascript, set the\nlanguage version to ECMAScript 6.", "timestamp": "2025-09-18T09:38:29.785870Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_custom-import:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/custom-import.html", "repo": "openbis", "title": "Custom Import", "section": "Custom Import", "text": "## Custom Import\n\n## Introduction\n\n## Custom\n## Import\nis a feature designed to give web users a chance to\nimport a file via\n## Jython\n## Dropboxes\n.\n### Usage\n\nTo upload a file via\n## Custom\n## Import\n, the user should\nchoose\n## Import\n->\n## Custom\n## Import\nin openBIS top menu. The\n## Custom\n## Import\ntab will be opened, and the user will get the combo box\nfilled with the list of configured imports. After selecting the desired\n## Custom\n## Import,\nthe\nuser will be asked to select a file. After\nselecting a file and clicking\nthe\n## Save\nbutton, the import will start.\nThe user should be aware, that the import is done in a synchronous way,\nsometimes it might take a while to import data (it depends on the\ndropbox code).\nIf a template file has been configured a download link will appear. The\ndownloaded template file can be used to create the file to be imported.\n### Configuration\n\nTo have the possibility to use a\n## Custom\n## Import\nfunctionality, this\nneeds an AS\ncore plugin\nof type\ncustom-imports. The\nplugin.properties\nof each plugin has several\n## parameters:\nparameter name\ndescription\nname\nThe value of this parameter will be used as a name of Custom Import in web UI.\ndss-code\nThis parameter needs to specify the code of the datastore server running the dropbox which should be used by the Custom Import.\ndropbox-name\nThe value is the name of the dropbox that is used by the Custom Import.\ndescription\nSpecifies a description of the Custom Import. The description is shown as a tooltip in the web UI.\ntemplate-entity-kind\nCustom import templates are represented in OpenBIS as entity attachments. To make a given file available as a custom import template create an attachment with this file and refer to this attachment with template-entity-kind, template-entity-permid, template-attachment-name parameters, where: template-entity-kind is the kind of the entity the attachment has been added to (allowed values: PROJECT, EXPERIMENT, SAMPLE), template-entity-permid is the perm id of that entity and template-attachment-name is the file name of the attachment.\ntemplate-entity-permid\ntemplate-attachment-name\nExample configuration\n\nplugin.properties\nname = Example custom import\ndss-code = DSS1\ndropbox-name = jython-dropbox-1\ndescription = This is an example custom import\ntemplate-entity-kind = PROJECT\ntemplate-entity-permid = 20120814111307034-82319\ntemplate-attachment-name = project_custom_import_template.txt\nThe dropbox needs to be defined on\nthe\n## DSS\nside as a\n## RPC\ndropbox\n## :\nservice.properties\ndss-rpc.put.<DATA_SET_TYPE> = jython-dropbox-1", "timestamp": "2025-09-18T09:38:29.791391Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/index.html", "repo": "openbis", "title": "Legacy Server-Side Extensions", "section": "Legacy Server-Side Extensions", "text": "## Legacy Server-Side Extensions\n\n## Custom Import\n## Introduction\n### Usage\n### Configuration\nExample configuration\n## Processing Plugins\n## Introduction\n## Multiple Processing Queues\n## Archiving\n## Generic Processing Plugins\nRevokeLDAPUserAccessMaintenanceTask\nDataSetCopierForUsers\nDataSetCopier\nDataSetCopierForUsers\nJythonBasedProcessingPlugin\nReportingBasedProcessingPlugin\nDataSetAndPathInfoDBConsistencyCheckProcessingPlugin\nScreeningReportingBasedProcessingPlugin\n## Reporting Plugins\n## Introduction\n## Generic Reporting Plugins\nDecoratingTableModelReportingPlugin\n## Transformations\nGenericDssLinkReportingPlugin\nAggregationService\nJythonAggregationService\nIngestionService\nJythonIngestionService\nJythonBasedReportingPlugin\nTSVViewReportingPlugin\n## Screening Reporting Plugins\nScreeningJythonBasedAggregationServiceReportingPlugin\nScreeningJythonBasedDbModifyingAggregationServiceReportingPlugin\nScreeningJythonBasedReportingPlugin\n## Search Domain Services\nConfiguring a Service\nQuerying a Service\n## Service Implementations\nBlastDatabase\n## Optional Query Parameters\n## Search Results", "timestamp": "2025-09-18T09:38:29.795047Z", "source_priority": 2, "content_type": "concept"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_processing-plugins:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/processing-plugins.html", "repo": "openbis", "title": "Processing Plugins", "section": "Processing Plugins", "text": "## Processing Plugins\n\n## Introduction\n\nA processing plugin runs on the DSS. It processes a specified set of data sets. The user can trigger a processing plugin in the openBIS Web application. After processing an e-mail is sent to the user.\nA processing plugin is configured on the DSS best by introducing a\ncore plugin\nof type\nprocessing-plugins\n. All processing plugins have the following properties in common:\n## Property Key\n## Description\nclass\nThe fully-qualified Java class name of the reporting plugin. The class has to implement IProcessingPluginTask.\nlabel\nThe label. It will be shown in the GUI.\ndataset-types\nComma-separated list of regular expressions. The plugin can process only data sets of types matching one of the regular expressions.  If new data set types are registered with openBIS, the DSS will need to be restarted before the new data set types are known to the processing plugins.\nproperties-file\nPath to an optional file with additional properties.\nallowed-api-parameter-classes\nA comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is allowed as a class of a Java parameter object of a remote API call. For more details see API Security.\ndisallowed-api-parameter-classes\nA comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is not allowed as a class of a Java parameter object of a remote API call. For more details see API Security.\n## Multiple Processing Queues\n\nBy default only one processing plugin task is processed. All other\nscheduled tasks have to wait in a queue. This can be inconvenient if\nthere is a mixture of long tasks (taking hours or even days) and short\ntasks (taking only seconds or minutes).\nDSS can be configured two run more than one processing queue. Each queue\n(except the default one) has a name (which also appears in the log\nfile). Also a regular expression is associated with the queue. When a\nprocessing plugin task is scheduled the appropriate queue is selected by\nthe ID of the processing plugin (this is either a name in the\nproperty\nprocessing-plugins\nof\nservice.properties\nof DSS or the name\nof the core-plugin folder). If the ID matches the regular expression the\ntask is added to the corresponding queue. If non of the regular\nexpression matches the default queue is used.\nThe queues have to be specified by the\nproperty\ndata-set-command-queue-mapping\n. It contains a comma-separated\nlist of queue definitions. Each definition has the form\n<queue\nname>:<regular\nexpression>\n## Archiving\n\nIf archiving is enable (i.e.\narchiver.class\nin\nservice.properties\nof\nDSS is defined or a core-plugin of type\nmiscellaneous\nwith\n## ID\narchiver\nis defined) there will be three processing plugins with\nthe following IDs:\n## Archiving\n,\n## Copying\ndata\nsets\nto\narchive\n, and\n## Unarchiving\n## Generic Processing Plugins\n\nRevokeLDAPUserAccessMaintenanceTask\n\n## Note\nThis Maintenance Task should only be used if the server uses\nLDAP only, it will take users from other authentication services as\nmissing.\n## Description\n: Renames, deactivates and delete all roles from users\nthat are no longer available on LDAP following the next algorithm.\nGrabs all active users.\nThe users that follow all the points of the next criteria are\nrenamed to userId-YYYY.MM.DD and deactivated:\nAre not a system user.\nDon’t have the ETL_SERVER role.\nDon’t have a LDAP principal.\n### Configuration\n## :\n## Property Key\n## Description\nserver-url\nLDAP server URL.\nsecurity-principal-distinguished-name\nLDAP principal distinguished name.\nsecurity-principal-password\nLDAP principal password.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.RevokeLDAPUserAccessMaintenanceTask\ninterval = 60 s\nserver-url = ldap://d.ethz.ch/DC=d,DC=ethz,DC=ch\nsecurity-principal-distinguished-name = CN=cisd-helpdesk,OU=EthUsers,DC=d,DC=ethz,DC=ch\nsecurity-principal-password = ******\nDataSetCopierForUsers\n\nDataSetCopier\n\n## Description\n: Copies all files of the specified data sets to another (remote) folder. The actual copying is done by the rsync command.\n### Configuration\n## :\n## Property Key\n## Description\ndestination\nPath to the destination folder. This can be a path to a local/mounted folder or to a remote folder accessible via SSH. In this case the name of the host has to appear as a prefix. General syntax: [\n:][\n:]\nhard-link-copy\nIf true hard links are created for each file of the data sets. This works only if the share which stores the data set is in the same local file system as the destination folder. Default: false.\nrename-to-dataset-code\nIf true the copied data set will be renamed to the data set code. Default: false.\nrsync-executable\nOptional path to the executable command rsync.\nrsync-password-file\nPath to the rsync password file. It is only needed if an rsync module is used.\nssh-executable\nOptional path to the executable command ssh. SSH is only needed for not-mounted folders which are accessible via SSH.\nln-executable\nOptional path to the executable command ln. The ln command is only needed when hard-link-copy = true.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DataSetCopier\nlabel = Copy to analysis incoming folder\ndataset-types = MS_DATA, UNKNOWN\ndestination = analysis-server:analysis-incoming-data\nrename-to-dataset-code = true\nDataSetCopierForUsers\n\n## Description\n: Copies all files of the specified data sets to a\n(remote) user folder. The actual copying is done by the rsync command.\n### Configuration\n## :\n## Property Key\n## Description\ndestination\nPath template to the destination folder. It should contain ${user} as a placeholder for the user ID.\nThe path can point to a local/mounted folder or to a remote folder accessible via SSH. In this case the name of the host has to appear as a prefix. General syntax: [\n:][\n:]\nhard-link-copy\nIf true hard links are created for each file of the data sets. This works only if the share which stores the data set is in the same local file system as the destination folder. Default: false.\nrename-to-dataset-code\nIf true the copied data set will be renamed to the data set code. Default: false.\nrsync-executable\nOptional path to the executable command rsync.\nrsync-password-file\nPath to the rsync password file. It is only needed if an rsync module is used.\nssh-executable\nOptional path to the executable command ssh. SSH is only needed for not-mounted folders which are accessible via SSH.\nln-executable\nOptional path to the executable command ln. The ln command is only needed when hard-link-copy = true.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DataSetCopierForUsers\nlabel = Copy to user playground\ndataset-types = MS_DATA, UNKNOWN\ndestination = tmp/playground/${user}/data-sets\nhard-link-copy = true\nrename-to-dataset-code = true\nJythonBasedProcessingPlugin\n\n## Description\n: Invokes a Jython script to do the processing. For more details see\nJython-based Reporting and Processing Plugins\n.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonBasedProcessingPlugin\nlabel = Calculate some numbers\ndataset-types = MS_DATA, UNKNOWN\nscript-path = script.py\nReportingBasedProcessingPlugin\n\n## Description\n: Runs a Jython-based reporting plugin of type\nTABLE_MODEL and sends the result table as a TSV file to the user.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\nsingle-report\nIf true only one report will be sent. Otherwise a report for each data set will be sent. Default: false\nemail-subject\nSubject of the e-mail to be sent. Default: None\nemail-body\nBody of the e-mail to be sent. Default: None\nattachment-name\nName of the attached TSV file. Default: report.txt\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.ReportingBasedProcessingPlugin\nlabel = Create monthly report\ndataset-types = MS_DATA, UNKNOWN\nscript-path = script.py\nemail-subject = DSS Monthly Report\nDataSetAndPathInfoDBConsistencyCheckProcessingPlugin\n\n## Description\n: The processing task checks the consistency between the\ndata store and the meta information stored in the\nPathInfoDB\n. It will\n## check for:\nexistence (i.e. exists in PathInfoDB but not on file system or\nexists on file system but not in PathInfoDB)\nfile size\nCRC32 checksum\nIf it finds any deviations, it will send out an email which contains all differences found.\n### Configuration\n: Properties common for all processing plugins (see Introduction)\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DataSetAndPathInfoDBConsistencyCheckProcessingPlugin\nlabel = Check consistency between data store and path info database\ndataset-types = .*\ncreening Processing Plugins\nScreeningReportingBasedProcessingPlugin\n\n## Description\n: Runs a Jython-based reporting plugin of type\nTABLE_MODEL and sends the result table as a TSV file to the user. There\nis some extra support for screening.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\nsingle-report\nIf true only one report will be sent. Otherwise a report for each data set will be sent. Default: false\nemail-subject\nSubject of the e-mail to be sent. Default: None\nemail-body\nBody of the e-mail to be sent. Default: None\nattachment-name\nName of the attached TSV file. Default: report.txt\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningReportingBasedProcessingPlugin\nlabel = Create monthly report\ndataset-types = HCS_IMAGE\nscript-path = script.py\nemail-subject = DSS Monthly Report", "timestamp": "2025-09-18T09:38:29.797909Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_legacy-server-side-extensions_reporting-plugins:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/legacy-server-side-extensions/reporting-plugins.html", "repo": "openbis", "title": "Reporting Plugins", "section": "Reporting Plugins", "text": "## Reporting Plugins\n\n## Introduction\n\nA reporting plugin runs on the DSS. It creates a report as a table or an\nURL for a specified set of data sets or key-value pairs. The user can\ninvoke a reporting plugin in the openBIS Web application. The result\nwill be shown as a table or a link.\nA reporting plugin is one of the three following types. The differences\nare the type of input and output:\n## TABLE_MODEL:\n## Input\n: A set of data sets.\n## Output\n: A table\n## DSS_LINK:\n## Input\n: One data set.\n## Output\n: An URL\n## AGGREGATION_TABLE_MODEL:\n## Input\n: A set of key-value pairs.\n## Output\n: A table\nA reporting plugin is configured on the DSS best by introducing a\ncore\nplugin\nof type\nreporting-plugins\n. All reporting plugins have the following properties\n## in common:\n## Property Key\n## Description\nclass\nThe fully-qualified Java class name of the reporting plugin. The class has to implement IReportingPluginTask.\nlabel\nThe label. It will be shown in the GUI.\ndataset-types\nComma-separated list of regular expressions. The plugin can create a report only for the data sets of types matching one of the regular expressions. If new data set types are registered with openBIS, the DSS will need to be restarted before the new data set types are known to the processing plugins. This is a mandatory property for reporting plugins of type TABLE_MODEL and DSS_LINK. It will be ignored if the type is AGGREGATION_TABLE_MODEL.\nproperties-file\nPath to an optional file with additional properties.\nservlet.\nProperties for an optional servlet. It provides resources referred by URLs in the output of the reporting plugin.\nThis should be used if the servlet is only needed by this reporting plugin. If other plugins also need this servlet it should be configured as a core plugin of type services.\nallowed-api-parameter-classes\nA comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is allowed as a class of a Java parameter object of a remote API call. For more details see API Security.\ndisallowed-api-parameter-classes\nA comma-separated list of regular expression for fully-qualified class names. Any classes matching on of the regular expressions is not allowed as a class of a Java parameter object of a remote API call. For more details see API Security.\n## Generic Reporting Plugins\n\nDecoratingTableModelReportingPlugin\n\n## Type\n## : TABLE_MODEL\n## Description\n: Modifies the output of a reporting plugin of type\n## TABLE_MODEL\n### Configuration\n## :\n## Property Key\n## Description\nreporting-plugin.class\nThe fully-qualified Java class name of the wrapped reporting plugin of type TABLE_MODEL\nreporting-plugin.\nProperty of the wrapped reporting plugin.\ntransformation.class\nThe fully-qualified Java class name of the transformation. It has to implement ITableModelTransformation.\ntransformation.\nProperty of the transformation to be applied.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.DecoratingTableModelReportingPlugin\nlabel = Analysis Summary\ndataset-types = HCS_IMAGE_ANALYSIS_DATA\nreporting-plugin.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin\nreporting-plugin.separator = ,\ntransformation.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.EntityLinksDecorator\ntransformation.link-columns = BARCODE, GENE\ntransformation.BARCODE.entity-kind = SAMPLE\ntransformation.BARCODE.default-space = DEMO\ntransformation.GENE.entity-kind = MATERIAL\ntransformation.GENE.material-type = GENE\n## Transformations\n\nEntityLinksDecorator\n\n## Description\n: Changes plain columns into entity links.\n### Configuration\n## :\n## Property Key\n## Description\nlink-columns\nComma-separated list of column keys.\n.entity-kind\nEntity kind of column\n. Possible values are MATERIAL and SAMPLE.\n.default-space\nOptional space code for SAMPLE columns. It will be used if the column value contains only the sample code.\n.material-type\nMandatory type code for MATERIAL columns.\nGenericDssLinkReportingPlugin\n\n## Type\n## : DSS_LINK\n## Description\n: Creates an URL for a file inside the data set.\n### Configuration\n## :\n## Property Key\n## Description\ndownload-url\nBase URL. Contains protocol, domain, and port.\ndata-set-regex\nOptional regular expression which specifies the file.\ndata-set-path\nOptional relative path in the data set to narrow down the search.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.GenericDssLinkReportingPlugin\nlabel = Summary\ndataset-types = MS_DATA\ndownload-url = https://my.domain.org:8443\ndata-set-regex = summary.*\ndata-set-path = report\nAggregationService\n\n## Warning\nImport Note on Authorization\nIn AggregationServices and IngestionServices, the service programmer needs to ensure proper authorization by himself. He can do so by using the methods from\nIAuthorizationService\n. The user id, which is needed when calling these methods, can be obtained from\nDataSetProcessingContext\n(when using Java), or the variable\nuserId\n(when using Jython).\n## Type:\n## AGGREGATION_TABLE_MODEL\n## Description\n: An abstract superclass for aggregation service\nreporting plugins. An aggregation service reporting plugin takes a hash\nmap containing user parameters as an argument and returns tabular data\n(in the form of a TableModel). The\nJythonBasedAggregationServiceReportingPlugin below is a subclass that\nallows for implementation of the logic in Jython.\n### Configuration\n: Dependent on the subclass.\nTo implement an aggregation service in Java, define a subclass\nof\nch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.AggregationService\n.\nThis subclass must implement the method\nTableModel createReport(Map<String, Object>, DataSetProcessingContext).\n## Example\n## :\nExampleAggregationServicePlugin\npackage\nch.systemsx.cisd.openbis.dss.generic.server.plugins.standard\n;\nimport\njava.io.File\n;\nimport\njava.util.Map\n;\nimport\njava.util.Properties\n;\nimport\nch.systemsx.cisd.openbis.dss.generic.shared.DataSetProcessingContext\n;\nimport\nch.systemsx.cisd.openbis.generic.shared.basic.dto.TableModel\n;\nimport\nch.systemsx.cisd.openbis.generic.shared.util.IRowBuilder\n;\nimport\nch.systemsx.cisd.openbis.generic.shared.util.SimpleTableModelBuilder\n;\n/**\n* @author Chandrasekhar Ramakrishnan\n*/\npublic\nclass\nExampleAggregationServicePlugin\nextends\nAggregationService\n{\nprivate\nstatic\nfinal\nlong\nserialVersionUID\n=\n## 1L\n;\n/**\n* Create a new plugin.\n*\n* @param properties\n* @param storeRoot\n*/\npublic\nExampleAggregationServicePlugin\n(\n## Properties\nproperties\n,\n## File\nstoreRoot\n)\n{\nsuper\n(\nproperties\n,\nstoreRoot\n);\n}\n## @Override\npublic\nTableModel\ncreateReport\n(\n## Map\n<\n## String\n,\n## Object\n>\nparameters\n,\nDataSetProcessingContext\ncontext\n)\n{\nSimpleTableModelBuilder\nbuilder\n=\nnew\nSimpleTableModelBuilder\n(\ntrue\n);\nbuilder\n.\naddHeader\n(\n## \"String\"\n);\nbuilder\n.\naddHeader\n(\n## \"Integer\"\n);\nIRowBuilder\nrow\n=\nbuilder\n.\naddRow\n();\nrow\n.\nsetCell\n(\n## \"String\"\n,\n## \"Hello\"\n);\nrow\n.\nsetCell\n(\n## \"Integer\"\n,\n20\n);\nrow\n=\nbuilder\n.\naddRow\n();\nrow\n.\nsetCell\n(\n## \"String\"\n,\nparameters\n.\nget\n(\n\"name\"\n).\ntoString\n());\nrow\n.\nsetCell\n(\n## \"Integer\"\n,\n30\n);\nreturn\nbuilder\n.\ngetTableModel\n();\n}\n}\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.ExampleAggregationServicePlugin\nlabel = My Report\nJythonAggregationService\n\n## Type:\n## AGGREGATION_TABLE_MODEL\n## Description\n: Invokes a Jython script to create an aggregation\nservice report. For more details see\nJython-based Reporting and Processing Plugins\n.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonAggregationService\nlabel = My Report\nscript-path = script.py\nIngestionService\n\n## Type:\n## AGGREGATION_TABLE_MODEL\n## Description\n: An abstract superclass for aggregation service\nreporting plugins that modify entities in the database. A db-modifying\naggregation service reporting plugin takes a hash map containing user\nparameters and a transaction as arguments and returns tabular data (in\nthe form of a TableModel). The transaction is an\nIDataSetRegistrationTransactionV2\n,\nthe same interface that is used by\ndropboxes\nto register and modify entities. The JythonBasedDbModifyingAggregationServiceReportingPlugin below is a subclass that allows for implementation of the logic in Jython.\n### Configuration\n: Dependent on the subclass.\nTo implement an aggregation service in Java, define a subclass\nof\nch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.IngestionService\n.\nThis subclass must implement the method\nTableModel process(IDataSetRegistrationTransactionV2 transaction, Map<String, Object> parameters, DataSetProcessingContext context)\n## Example\n## :\nExampleDbModifyingAggregationService.java\npackage\nch.systemsx.cisd.openbis.dss.generic.server.plugins.standard\n;\nimport\njava.io.File\n;\nimport\njava.util.Map\n;\nimport\njava.util.Properties\n;\nimport\nch.systemsx.cisd.etlserver.registrator.api.v2.IDataSetRegistrationTransactionV2\n;\nimport\nch.systemsx.cisd.openbis.dss.generic.shared.DataSetProcessingContext\n;\nimport\nch.systemsx.cisd.openbis.dss.generic.shared.dto.DataSetInformation\n;\nimport\nch.systemsx.cisd.openbis.generic.shared.basic.dto.TableModel\n;\nimport\nch.systemsx.cisd.openbis.generic.shared.util.IRowBuilder\n;\nimport\nch.systemsx.cisd.openbis.generic.shared.util.SimpleTableModelBuilder\n;\n/**\n* An example aggregation service\n*\n* @author Chandrasekhar Ramakrishnan\n*/\npublic\nclass\nExampleDbModifyingAggregationService\nextends\nIngestionService\n<\nDataSetInformation\n>\n{\nprivate\nstatic\nfinal\nlong\nserialVersionUID\n=\n## 1L\n;\n/**\n* @param properties\n* @param storeRoot\n*/\npublic\nExampleDbModifyingAggregationService\n(\n## Properties\nproperties\n,\n## File\nstoreRoot\n)\n{\nsuper\n(\nproperties\n,\nstoreRoot\n);\n}\n## @Override\npublic\nTableModel\nprocess\n(\nIDataSetRegistrationTransactionV2\ntransaction\n,\n## Map\n<\n## String\n,\n## Object\n>\nparameters\n,\nDataSetProcessingContext\ncontext\n)\n{\ntransaction\n.\ncreateNewSpace\n(\n\"NewDummySpace\"\n,\nnull\n);\nSimpleTableModelBuilder\nbuilder\n=\nnew\nSimpleTableModelBuilder\n(\ntrue\n);\nbuilder\n.\naddHeader\n(\n## \"String\"\n);\nbuilder\n.\naddHeader\n(\n## \"Integer\"\n);\nIRowBuilder\nrow\n=\nbuilder\n.\naddRow\n();\nrow\n.\nsetCell\n(\n## \"String\"\n,\n## \"Hello\"\n);\nrow\n.\nsetCell\n(\n## \"Integer\"\n,\n20\n);\nrow\n=\nbuilder\n.\naddRow\n();\nrow\n.\nsetCell\n(\n## \"String\"\n,\nparameters\n.\nget\n(\n\"name\"\n).\ntoString\n());\nrow\n.\nsetCell\n(\n## \"Integer\"\n,\n30\n);\nreturn\nbuilder\n.\ngetTableModel\n();\n}\n}\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.ExampleDbModifyingAggregationService\nlabel = My Report\nJythonIngestionService\n\n## Type:\n## AGGREGATION_TABLE_MODEL\n## Description\n: Invokes a Jython script to register and modify entitiesand create an aggregation service report. The script receives a transaction as an argument. For more details see\nJython-based Reporting and Processing Plugins\n.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\nshare-id\nOptional, defaults to 1 when not stated otherwise.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonIngestionService\nlabel = My Report\nscript-path = script.py\nJythonBasedReportingPlugin\n\n## Type:\n## TABLE_MODEL\n## Description\n: Invokes a Jython script to create the report. For more\ndetails see\nJython-based Reporting and Processing\n## Plugins\n.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.jython.JythonBasedReportingPlugin\nlabel = My Report\ndataset-types = MS_DATA, UNKNOWN\nscript-path = script.py\nTSVViewReportingPlugin\n\n## Type:\n## TABLE_MODEL\n## Description\n: Presents the main data set file as a table. The main\nfile is specified by the Main Data Set Pattern and the Main Data Set\nPath of the data set type. The file can be a CSV/TSV file or an Excel\nfile. This reporting plugin works only for one data set.\n### Configuration\n## :\n## Property Key\n## Description\nseparator\nSeparator character. This property will be ignored if the file is an Excel file. Default: TAB character\nignore-comments\nIf true all rows starting with ‘#’ will be ignored. Default: true\nignore-trailing-empty-cells\nIf true trailing empty cells will be ignored. Default: false\nexcel-sheet\nName or index of the Excel sheet used. This property will only be used if the file is an Excel file. Default: 0\ntranspose\nIf true transpose the original table, that is exchange rows with columns. Default: false\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.TSVViewReportingPlugin\nlabel = My Report\ndataset-types = MS_DATA, UNKNOWN\nseparator = ;\n## Screening Reporting Plugins\n\nScreeningJythonBasedAggregationServiceReportingPlugin\n\n## Type:\n## AGGREGATION_TABLE_MODEL\n## Description\n: Invokes a Jython script to create an aggregation\nservice report. For more details see\nJython-based Reporting and\n## Processing\n## Plugins\n. There is some extra support for screening.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningJythonBasedReportingPlugin\nlabel = My Report\ndataset-types = HCS_IMAGE\nscript-path = script.py\nScreeningJythonBasedDbModifyingAggregationServiceReportingPlugin\n\n## Type:\n## AGGREGATION_TABLE_MODEL\n## Description\n: Invokes a Jython script to register and modify entities\nand create an aggregation service report. The screening-specific version\nhas access to the screening facade for queries to the imaging database\nand is given a screening transaction that supports registering plate\nimages and feature vectors. For more details see\nJython-based Reporting\nand Processing\n## Plugins\n.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningJythonBasedReportingPlugin\nlabel = My Report\ndataset-types = HCS_IMAGE\nscript-path = script.py\nScreeningJythonBasedReportingPlugin\n\n## Type:\n## TABLE_MODEL\n## Description\n: Invokes a Jython script to create the report. For more details see\nJython-based Reporting and Processing Plugins\n.\nThere is some extra support for screening.\n### Configuration\n## :\n## Property Key\n## Description\nscript-path\nPath to the jython script.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.screening.server.plugins.jython.ScreeningJythonBasedAggregationServiceReportingPlugin\nlabel = My Report\nscript-path = script.py", "timestamp": "2025-09-18T09:38:29.803913Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_as-api-listener:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/as-api-listener.html", "repo": "openbis", "title": "API Listener Core Plugin (V3 API)", "section": "Introduction", "text": "API Listener Core Plugin (V3 API)\n\n## Introduction\n\nThe V3 API listener core plugin is an implementation of the interceptor pattern:\n<https://en.wikipedia.org/wiki/Interceptor_pattern>\nIt actually intercepts twice, right before an operation is executed, and right after.\nIts main focus is to help integrations. It gives an opportunity to integrators to execute additional functionality before or after an api call with the next purposes:\nModify the API call inputs/outputs immediately before/after they reach its executor.\nTrigger additional internal logic.\nNotify third party systems.\n## Core Plugin\n\nTo archive these goals is necessary to provide a core plugin of the type ‘api-listener’ to the AS:\nPlugin.properties\n\nIt is required to provide an ‘operation-listener.class’ indicating the class name of the listener that will be loaded.\nAdditionally any number of properties following the pattern\noperation-listener.<your-custom-name>\ncan be provided. Custom properties are provided to help maintainability, they give an opportunity to the integrator to only need to compile the listener once and configure it differently for different instances.\nplugin.properties\noperation-listener.class = ch.ethz.sis.openbis.generic.server.asapi.v3.executor.operation.OperationListenerExample\noperation-listener.your-config-property = Your Config Message\nlib\n\nThe core plugin should contain a lib folder with a jar containing a class that implements the interface IOperationListener, this interface is provided with the V3 API jar and provides 3 methods:\nsetup: Runs on startup. Gives one opportunity to read the configuration provided to the core plugin\nbeforeOperation: Runs before each operation occurs. In addition to the operation intercepted it also provides access to the api and the session token used for the operation.\nafterOperation: Intercepts after the operation occurs. In addition to the operation intercepted it also provides access to the api, the session token used for the operation, the operation result and any exception that happened during the operation.\n## Warning\n### Implicit Requirements\nRequirement 1:  The Listener should be Thread Safe Code\nA single instance of the Listener is created during the server startup. Since a single instance is used to serve all requests thread safe code is a requirement. We strongly suggest to not to keep any state.\nRequirement 2: The Listener should not throw Exceptions\nIf the listener throw an exception it will make the API call fail.\nRequirement 3: The Listener should use IOperation and IOperationResult as indicated below\nAll API Operations go through every listener so the method signatures should use IOperation and IOperationResult.\nPlease use instanceof for safe casting.\nIOperationListener\npackage\nch.ethz.sis.openbis.generic.asapi.v3.plugin.listener\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperationResult\n;\nimport\njava.util.Properties\n;\npublic\ninterface\nIOperationListener\n<\n## OPERATION\nextends\nIOperation\n,\n## RESULT\nextends\nIOperationResult\n>\n{\npublic\nstatic\nfinal\n## String\n## LISTENER_PROPERTY_KEY\n=\n\"operation-listener\"\n;\npublic\nstatic\nfinal\n## String\n## LISTENER_CLASS_KEY\n=\n## LISTENER_PROPERTY_KEY\n+\n\".class\"\n;\npublic\nabstract\nvoid\nsetup\n(\n## Properties\nproperties\n);\npublic\nabstract\nvoid\nbeforeOperation\n(\nIApplicationServerApi\napi\n,\n## String\nsessionToken\n,\n## OPERATION\noperation\n);\npublic\nabstract\nvoid\nafterOperation\n(\nIApplicationServerApi\napi\n,\n## String\nsessionToken\n,\n## OPERATION\noperation\n,\n## RESULT\nresult\n,\nRuntimeException\nruntimeException\n);\n}\n## Example - Logging\n\nThe next implementation example captures the calls and logs on the standard openbis log the operation name:\nOperationListenerExample\npackage\nch.ethz.sis.openbis.generic.server.asapi.v3.executor.operation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperation\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.dto.common.operation.IOperationResult\n;\nimport\nch.ethz.sis.openbis.generic.asapi.v3.plugin.listener.IOperationListener\n;\nimport\nch.systemsx.cisd.common.logging.LogCategory\n;\nimport\nch.systemsx.cisd.common.logging.LogFactory\n;\nimport\norg.apache.log4j.Logger\n;\nimport\njava.util.Properties\n;\npublic\nclass\nOperationListenerExample\nimplements\nIOperationListener\n<\nIOperation\n,\nIOperationResult\n>\n{\nprivate\nstatic\nfinal\n## Logger\noperationLog\n=\nLogFactory\n.\ngetLogger\n(\nLogCategory\n.\n## OPERATION\n,\nOperationListenerExample\n.\nclass\n);\nprivate\n## String\nyourConfigProperty\n=\nnull\n;\n## @Override\npublic\nvoid\nsetup\n(\n## Properties\nproperties\n)\n{\nyourConfigProperty\n=\nproperties\n.\ngetProperty\n(\n\"operation-listener.your-config-property\"\n);\noperationLog\n.\ninfo\n(\n\"setup: \"\n+\nyourConfigProperty\n);\n}\n## @Override\npublic\nvoid\nbeforeOperation\n(\nIApplicationServerApi\napi\n,\n## String\nsessionToken\n,\nIOperation\noperation\n)\n{\noperationLog\n.\ninfo\n(\n\"beforeOperation: \"\n+\noperation\n.\ngetClass\n().\ngetSimpleName\n());\n}\n## @Override\npublic\nvoid\nafterOperation\n(\nIApplicationServerApi\napi\n,\n## String\nsessionToken\n,\nIOperation\noperation\n,\nIOperationResult\nresult\n,\nRuntimeException\nruntimeException\n)\n{\noperationLog\n.\ninfo\n(\n\"afterOperation: \"\n+\noperation\n.\ngetClass\n().\ngetSimpleName\n());\n}\n}\n## Example - Loggin Sources\n\nYou can download a complete example with sources\nhere\nto use as a template to make your own.", "timestamp": "2025-09-18T09:38:29.808912Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_as-services:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/as-services.html", "repo": "openbis", "title": "Custom Application Server Services", "section": "Custom Application Server Services", "text": "## Custom Application Server Services\n\n## Introduction\n\nOn Data Store Server (DSS) aggregation/ingestion services based on Jython scripts can be used to extend openBIS by custom services. These services have full access on data store and Application Server (AS).\nOften only access on AS is needed. Going over DSS is a detour. For such cases it is better to write an AS core plugin of type\nservices\n.\nHow to write a custom AS service core plugin\n\nHere is the recipe to create an AS core plugin of type\nservices\n## :\nThe folder\n<core\nplugin\nfolder>/<module>/<version>/as/services/<core\nplugin\nname>\nhas to be created.\nIn this folder two files have to be created:\nplugin.properties\nand\nscript.py\n. The properties file should contain:\nplugin.properties\nclass = ch.ethz.sis.openbis.generic.server.asapi.v3.helper.service.JythonBasedCustomASServiceExecutor\nscript-path = script.py\nThe script file should have the function\nprocess\nwith two arguments. The first argument is the context. It contains the methods\ngetSessionToken()\nand\ngetApplicationService()\nwhich returns an instance of\nch.ethz.sis.openbis.generic.asapi.v3.IApplicationServerApi\n. The second argument is a map of key-value pairs. The key is a string and the values is an arbitrary object. Anything returned by the script will be returned to the caller of the service. Here is an example of a script which creates a space:\nscript.py\nfrom\nch.ethz.sis.openbis.generic.asapi.v3.dto.space.create\nimport\nSpaceCreation\ndef\nprocess\n(\ncontext\n,\nparameters\n## ):\nspace_creation\n=\nSpaceCreation\n()\nspace_creation\n.\ncode\n=\nparameters\n.\nget\n(\n'space_code'\n);\nresult\n=\ncontext\n.\napplicationService\n.\ncreateSpaces\n(\ncontext\n.\nsessionToken\n,\n[\nspace_creation\n]);\nreturn\n## \"Space created:\n%s\n\"\n%\nresult\nNote, that all changes on the AS database will be done in one transaction.\nHow to use a custom AS service\n\nThe application API version 3 offers the following method to search for existing services:\nSearchResult\n<\nCustomASService\n>\nsearchCustomASServices\n(\n## String\nsessionToken\n,\nCustomASServiceSearchCriteria\nsearchCriteria\n,\nCustomASServiceFetchOptions\nfetchOptions\n)\nThe following Java code example returns all available services:\nSearchResult\n<\nCustomASService\n>\nservices\n=\nservice\n.\nsearchCustomASServices\n(\nsessionToken\n,\nnew\nCustomASServiceSearchCriteria\n(),\nnew\nCustomASServiceFetchOptions\n());\nWith the following method of the API version 3 a specified service can\n## be executed:\npublic\n## Object\nexecuteCustomASService\n(\n## String\nsessionToken\n,\nICustomASServiceId\nserviceId\n,\nCustomASServiceExecutionOptions\noptions\n);\n## The\nserviceId\ncan be obtained from a\nCustomASService\nobject (as returned by the\nsearchCustomASServices\nmethod) by the getter method\ngetCode()\n. It can also be created as an instance of\nCustomASServiceCode\n. Note, that the service code is just the core plugin name.\nParameter bindings (i.e. key-value pairs) are specified in the\nCustomASServiceExecutionOptions\nobject by invoking for each binding the method\nwithParameter()\n.\nHere is a code example:\nCustomASServiceExecutionOptions\noptions\n=\nnew\nCustomASServiceExecutionOptions\n().\nwithParameter\n(\n\"space_code\"\n,\n\"my-space\"\n);\n## Object\nresult\n=\nservice\n.\nexecuteCustomASService\n(\nsessionToken\n,\nnew\nCustomASServiceCode\n(\n\"space-creator\"\n),\noptions\n);\n## System\n.\nout\n.\nprintln\n(\nresult\n);", "timestamp": "2025-09-18T09:38:29.812243Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_core-plugins:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/core-plugins.html", "repo": "openbis", "title": "Core Plugins", "section": "Core Plugins", "text": "## Core Plugins\n\n## Motivation\n\n## The\nservice.properties\nfile of openBIS Application Server (AS) and Data Store Server (DSS) can be quite big because of all the configuration data for maintenance tasks, drop-boxes, reporting and processing plugins, etc. Making this configuration more modular will improve the structure. It would also allow to have core plugins shipped with distribution and customized plugins separately. This makes maintenance of these plugins more independent. For example, a new maintenance task plugin can be added in an update without any need for an admin to put the configuration data manually into the\nservice.properties\nfile.\n## Core Plugins Folder Structure\n\nAll plugins whether they are a part of the distribution or added and maintained are stored in the folder usually called\ncore-plugins\n. Standard (i.e. core) plugins are part of the distribution. During installation the folder\ncore-plugins\nis unpacked as a sibling folder of\nopenBIS-server\nand\ndatastore_server\n.\nThe folder structure is organized as follows:\nThe file\ncore-plugins.properties\ncontaining the following properties:\nenabled-modules\n: comma-separated list of regular expressions for all enabled modules.\ndisabled-core-plugins\n: comma-separated list of disabled plugins. All plugins are disabled for which the beginning of full plugin ID matches one of the terms of this list. To disable initialization of master data of a module - disable it’s core plugin “initialize-master-data”\nThe children of\ncore-plugins\nare folders denoting modules like the standard technologies,\nproteomics\nand\nscreening\n. For customization, any module can be added.\nEach module folder has children which are numbered folders. The number denotes the version of the plugins of that module. The version with the largest number will be used. Different modules can have different largest version numbers.\nEvery version folder has the subfolder\nas\nand/or\ndss\nwhich have subfolders for the various types of plugins. The types are different for AS and DSS:\n## AS:\nmaintenance-tasks\n: Maintenance tasks triggered by some time schedule. Property\nclass\ndenotes fully-qualified class name of a class implementing\nch.systemsx.cisd.common.maintenance.IMaintenanceTask\n. For more details see\n## Maintenance Tasks\n.\ndss-data-sources\n: Definition of data sources with corresponding data source definitions for DSS. For more details see\nInstallation and Administrator Guide of the openBIS Server\n.\nquery-databases\n: Databases for SQL queries. For more details see\n## Custom Database Queries\n.\ncustom-imports\n: Custom file imports to DSS via Web interface. For more details see\n## Custom Import\n.\nservices\n: Custom services. For more details see\n## Custom Application Server Services\n.\nwebapps\n: HTML5 applications that use the openBIS API. For more details see\nopenBIS webapps\n.\nmiscellaneous\n: Any additional properties.\n## DSS:\ndrop-boxes\n: ETL server threads for registration of data sets.                            `\nreporting-plugins\n: Reports visible in openBIS. Property\nclass\ndenotes fully-qualified class name of a class implementing\nch.systemsx.cisd.openbis.dss.generic.server.plugins.tasks.IReportingPluginTask\n. For more details see\n## Reporting Plugins\n.\nprocessing-plugins\n: Processing tasks triggered by users. Property\nclass\ndenotes fully-qualified class name of a class implementing\nch.systemsx.cisd.openbis.dss.generic.server.plugins.tasks.IProcessingPluginTask\n. For more details see\n## Processing Plugins\n.\nmaintenance-tasks\n: Maintenance tasks triggered by some time schedule. Property\nclass\ndenotes fully-qualified class name of a class implementing\nch.systemsx.cisd.common.maintenance.IMaintenanceTask\n. For more details see\n## Maintenance Tasks\n.\nsearch-domain-services\n: Services for variaous search domains (e.g. search on sequence databases using BLAST). Property\nclass\ndenotes fully-qualified class name of a class implementing\nch.systemsx.cisd.openbis.dss.generic.shared.api.internal.v2.ISearchDomainService\n.\ndata-sources\n: Internal or external database sources.\nservices\n: Services based on servlets. Property\nclass\ndenotes fully-qualified class name of a class implementing\njavax.servlet.Servlet\n.\nimaging-overview-plugins\n: Data set type specific provider of the overview image of a data set. Property\nclass\ndenotes fully-qualified class name of a class implementing\n## ch.systemsx.cisd.openbis.dss.generic.server.IDatasetImageOverviewPlugin\n.\nfile-system-plugins\n: Provider of a custom DSS file system (FTP/SFTP) view hierarchy. Property\nclass\ndenotes fully-qualified class name of a class\nimplementing\nch.systemsx.cisd.openbis.dss.generic.server.fs.IResolverPlugin\nProperty code denotes the name of the top-level directory\nunder which the custom hierarchy will be visible\nmiscellaneous\n: Any additional properties.\nFolders of each of these types can have an arbitrary number of subfolders. But if the type folder is present it should have at least one subfolder. Each defining one plugin. The name of these subfolders define the plugin ID. It has to be unique over all plugins independent of module and plugin type. It should not contain the characters space ‘ ‘, comma ‘\n,\n’, and equal sign ‘\n=\n’.\nEach plugin folder should contain at least the file\nplugin.properties\n. There could be additional files (referred in\nplugin.properties\n) but no subfolders.\nHere is an example of a typical structure of a core plugins folder:\ncore-plugins\ncore-plugins.properties\nproteomics\n1\nas\ninitialize-master-data.py\ndss\ndrop-boxes\nms-injection\nplugin.properties\nmaintenance-tasks\ndata-set-clean-up\nplugin.properties\nscreening\n1\ncore-plugin.properties\nas\ninitialize-master-data.py\nmaintenance-tasks\nmaterial-reporting\nmapping.txt\nplugin.properties\ncustom-imports\nmyCustomImport\nplugin.properties\ndss\ndrop-boxes\nhcs-dropbox\nlib\ncustom-lib.jar\nhcs-dropbox.py\nplugin.properties\nYou might noticed the file\ninitialize-master-data.py\nin AS core plugins sections  in this example. It is a script to register master data in the openBIS core database. For more details see\nInstallation and Administrator Guide of the openBIS Server\n.\nEach plugin can refer to any number of files. These files are part of\nthe plugin folder. In\nplugin.properties\nthey are referred relative to\nthe plugin folder, that is by file name. Example:\nplugin.properties\nincoming-dir = ${incoming-root-dir}/incoming-hcs\nincoming-data-completeness-condition = auto-detection\ntop-level-data-set-handler = ch.systemsx.cisd.openbis.dss.etl.jython.JythonPlateDataSetHandler\nscript-path = hcs-dropbox.py\nstorage-processor = ch.systemsx.cisd.openbis.dss.etl.PlateStorageProcessor\nstorage-processor.data-source = imaging-db\nstorage-processor.define-channels-per-experiment = false\n### Merging Configuration Data\n\nAt start up of AS and DSS merges the content of\nservice.properties\nwith the content of all\nplugin.properties\nof the latest version per enabled module. Plugin properties can be deleted by adding\n<plugin\nID>.<plugin\nproperty\nkey>\n=\n## __DELETED__\nto service.properties. Example:\nsimple-dropbox.incoming-data-completeness-condition\n=\n## __DELETED__\nThis leads to a deletion of the property\nincoming-data-completeness-condition\nspecified in\nplugins.properties\nof the plugin\nsimple-dropbox\n.\nMerging is done by injection the properties of\nplugin.properties\ninto\nservice.properties\nby adding the plugin ID as a prefix to the property key (not for\nmiscellaneous).\nFor example, the property\nscript-path\nof plugin\nhcs-dropbox\nbecomes\nhcs-dropbox.script-path\n. References to files inside the plugin are replaced by a path relative to the working directory. For the various plugin types (except\nmiscellaneous\n) the plugin ID is appended to the related property in\nservice.properties\nfor this plugin type. For example, plugins of type\ndrop-boxes\nare added to the property\ninputs\n.\nEnabling Modules and Disabling Plugins\n\nThere are three methods to control which plugins are available and witch not:\nenabling by property\nenabled-modules\nin\ncore-plugins.properties\n: This enables all plugins of certain modules.\ndisabling by property\ndisabled-core-plugins\nin\ncore-plugins.properties\n: This allows to disable on a fine grade level specific plugins.\ndisabling by marker file: Plugin developers should use this method when developing new plugins.\n## Enabling Modules\n\nThe property\nenabled-modules\nin\ncore-plugins.properties\nis a comma-separated list of regular expressions denoting modules. All plugins in a module folder of\ncore-plugins\nfolder are enabled if the module name matches one of these regular expressions. If this list is empty or the property hasn’t been specified no core-plugin will be used. Note, that this property is manipulated by openBIS Installer for Standard Technologies. Example:\nservice.properties\nenabled-modules\n=\nscreening,\nproteomics,\ndev-module-.*\nDisabling Core Plugins by Property\n\nThe property\ndisabled-core-plugins\nin\ncore-plugins.properties\nallows to disable plugins selectively either by module name, module combined with plugin type or full plugin ID. Example:\nservice.properties\ndisabled-core-plugins\n=\nscreening,\nproteomics:reporting-plugins,\nproteomics:maintenance-tasks:data-set-clean-up\nDisabling Core Plugins by Marker File\n\nThe empty marker file\ndisabled\nin a certain plugin folder disables the particular plugin.\n## Core Plugin Dependency\n\nA core plugin can depend on another core plugin. The dependency is specified in\n<module>/<version>/core-plugin.properties\n. It has a property named\nrequired-plugins\n. Its value is a comma-separated list of core-plugins on which it depends. The dependency can be pecified selectively either by module name, module combined with plugin type or full plugin ID. Example:\ncore-plugin.properties\nrequired-plugins\n=\nmodule-a,\nmodule-b:initialize-master-data,\nmodule-b:reporting-plugins,\nmodule-a:drop-boxes:generic\nRules for Plugin Writers\n\nAs a consequence of the way plugins are merged with\nservice.properties\nwriters of plugins have to obey the following rules:\nPlugin IDs have to be unique among all plugins whether they are defined in\nservice.properties\nor as core plugins. The only exceptions are plugins of type\nmiscellaneous\n.\n## In\nplugin.properties\nother properties can be referred by the usual\n${<property\nkey>\n} notation. The referred property can be in\nservice.properties\nor in any\nplugin.properties\n.\nAs convention use\n${incoming-root-dir\n} when defining the incoming folder for a drop box.\nRefer files in\nplugin.properties\nonly by names and add them as siblings of\nplugin.properties\nto the plugin folder. Note, that different plugins can refer files with the same name. There will be no ambiguity which file is meant.\nIn order to be completely independent from updates of the core plugins which are part of the distribution create your own module, like\nmy-plugins\n, and put all your plugins there. Do not forget to add your module to the property\nenabled-modules\nin\ncore-plugins.properties\n.\nUsing Java libraries in Core Plugins\n\nOpenBIS allows you to include Java libraries in core plugin folders. The *.jar files have to be stored in\n<code\nplugin\nfolder>/lib\nfolder. For instance, in order to use “my-lib.jar” in “my-dropbox” a following file structure is needed:\nservice.properties\nmy\n-\ntechnology\n1\ndss\ndrop\n-\nboxes\nmy\n-\ndropbox\nlib\nmy\n-\nlib\n.\njar\ndropbox\n.\npy\nplugin\n.\nproperties\nHaving this structure, Java classes from “my-lib.jar” can be imported and used in “dropbox.py” script.\n## Note\nCurrently this feature is only supported for DSS core plugins. Under the hood, a symbolic link to a jar file is created in “datastore_server/lib” folder during DSS startup.", "timestamp": "2025-09-18T09:38:29.817428Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "Dropboxes", "text": "## Dropboxes\n\n## Jython Dropboxes\n\n## Introduction\n\nThe jython dropbox feature makes it possible for a script written in the\nPython language to control the data set registration process of the\nopenBIS Data Store Server. A script can modify the files in the dropbox\nand register data sets, samples, and experiments as part of its\nprocessing. The framework provides tools to track file operations and,\nif necessary, revert them, ensuring that the incoming file or directory\nis returned to its original state in the event of an error.\nBy deafult python 2.5 is used, but it’s possible to use python version\n2.7.\n## Dropboxes are dss core plugins:\n## Core Plugins\n## Simple Example\n\nHere is an example that registers files that arrive in the drop box as\ndata sets. They are explicitly attached to the experiment “JYTHON” in\nthe project “TESTPROJ” and space “TESTGROUP”.\ndata-set-handler-basic.py\ndef\nprocess\n(\ntransaction\n## ):\n# Create a data set\ndataSet\n=\ntransaction\n.\ncreateNewDataSet\n()\n# Reference the incoming file that was placed in the dropbox\nincoming\n=\ntransaction\n.\ngetIncoming\n()\n# Add the incoming file into the data set\ntransaction\n.\nmoveFile\n(\nincoming\n.\ngetAbsolutePath\n(),\ndataSet\n)\n# Get an experiment for the data set\nexp\n=\ntransaction\n.\ngetExperiment\n(\n## \"/TESTGROUP/TESTPROJ/JYTHON\"\n)\n# Set the owner of the data set -- the specified experiment\ndataSet\n.\nsetExperiment\n(\nexp\n)\nThis example is is unrealistically simple, but contains all the elements\nnecessary to implement a jython drop box. The main idea is to perform\nseveral operations within the bounds of a transaction on the data and\nmetadata. The transaction is used to track the changes made so they can\nbe executed together or all reverted if a problem occurs.\n## More Realistic Example\n\nThe above example demonstrates the concept, but it is unrealistically\nsimple. In general, we want to be able to determine and specify the\nexperiment/sample for a data set and explicitly set the data set type as\nwell.\nIn this example, we handle a usage scenario where there is one\nexperiment done every day. All data produced on a single day is\nassociated with the experiment for that date. If the experiment for a\ngiven day does not exist, it is created.\ndata-set-handler-experiment-reg.py\nfrom\ndatetime\nimport\ndatetime\ndef\nprocess\n(\ntransaction\n## ):\n# Try to get the experiment for today\nnow_str\n=\ndatetime\n.\ntoday\n()\n.\nstrftime\n(\n'%Y%m\n%d\n'\n)\nexpid\n=\n## \"/TESTGROUP/TESTPROJ/\"\n+\nnow_str\nexp\n=\ntransaction\n.\ngetExperiment\n(\nexpid\n)\n# Create an experiment if necessary\nif\n## None\n==\nexp\n## :\nexp\n=\ntransaction\n.\ncreateNewExperiment\n(\nexpid\n,\n## \"COMPOUND_HCS\"\n)\nexp\n.\nsetPropertyValue\n(\n## \"DESCRIPTION\"\n,\n\"An experiment created on \"\n+\ndatetime\n.\ntoday\n()\n.\nstrftime\n(\n'%Y-%m-\n%d\n'\n))\nexp\n.\nsetPropertyValue\n(\n## \"COMMENT\"\n,\nnow_str\n)\ndataSet\n=\ntransaction\n.\ncreateNewDataSet\n()\nincoming\n=\ntransaction\n.\ngetIncoming\n()\ntransaction\n.\nmoveFile\n(\nincoming\n.\ngetAbsolutePath\n(),\ndataSet\n)\ndataSet\n.\nsetDataSetType\n(\n## \"HCS_IMAGE\"\n)\ndataSet\n.\nsetExperiment\n(\nexp\n)\nMore complex processing is also possible. In the following sections, we\nexplain how to configure a jython dropbox and describe the API in\ngreater detail.\n## Model\n\nThe model underlying dropbox registration is the following: when a new\nfile or folder is found in the dropbox folder, the process function of\nthe script file is invoked with a\ndata set registration transaction\nas an argument.\nThe process function has the responsibility of looking at the incoming\nfile or folder and determining what needs to be registered or modified\nin the metadata database and what data needs to be stored on the file\nsystem. The\nIDataSetRegistrationTransaction\ninterface\ndefines the API for specifying entities to register and update.\nCommitting a transaction is actually a two-part process. The metadata is stored in the openBIS application server’s database; the data is kept on the file system in a sharded directory structure beneath the data store server’s\nstore\ndirectory. All modifications requested as part of a transaction are committed atomically — they either all succeed or all fail.\n## Several\n## Events\noccur in the process of committing a transaction. By defining jython functions, it is possible to be notified and intervene when an event occurs. Because the infrastructure reserves the right to delay or retry actions if resources become unavailable, the process function and event functions cannot use global variables to communicate with each other. Instead, they should use the registration context object to communicate. Anything stored in the registration context must, however, be serializable by Java serialization.\n## Details\n\n### Dropbox Configuration\n\nA jython dropbox is typically distributed as a\ncore plugin\nand configured in its plugin.properties file. A dropbox configured to run a jython script, which is kept in the same directory as plugin.properties. The configuration requires a storage processor and the name of the script (a full path is not necessary if the script is in the same directory as the plugin.properties). Here is an example configuration for a dropbox that uses the jython handler.\nplugin.properties\n#\n# REQUIRED PARAMETERS\n#\n# The directory to watch for new data sets\nincoming-dir = ${root-dir}/incoming-jython", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JythonTopLevelDataSetHandlerV2 or a subclass thereof", "text": "# The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JythonTopLevelDataSetHandlerV2 or a subclass thereof\ntop-level-data-set-handler = ch.systemsx.cisd.etlserver.registrator.api.v2.JythonTopLevelDataSetHandlerV2", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:2", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "The script to execute, reloaded and recompiled each time a file/folder is placed in the dropbox", "text": "# The script to execute, reloaded and recompiled each time a file/folder is placed in the dropbox\nscript-path = ${root-dir}/data-set-handler.py", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:3", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "The appropriate storage processor", "text": "# The appropriate storage processor\nstorage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:4", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "Specify jython version. Default is whatever is specified in datastore server service.properties under property \"jython-version\"", "text": "# Specify jython version. Default is whatever is specified in datastore server service.properties under property \"jython-version\"\nplugin-jython-version=2.5\n#\n# OPTIONAL PARAMETERS\n#", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "reference"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:5", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "False if incoming directory is assumed to exist.", "text": "# False if incoming directory is assumed to exist.\n# Default - true: Incoming directory will be created on start up if it doesn't exist.\nincoming-dir-create = true", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:6", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "Defines how the drop box decides if a folder is ready to process: either by a 'marker-file' or a time out which is called 'auto-detection'", "text": "# Defines how the drop box decides if a folder is ready to process: either by a 'marker-file' or a time out which is called 'auto-detection'\n# The time out is set globally in the service.properties and is called 'quiet-period'. This means when the number of seconds is over and no changes have\n# been made to the incoming folder the drop will start to register. The marker file must have the following naming schema: '.MARKER_is_finished_<incoming_folder_name>'\nincoming-data-completeness-condition = marker-file", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:7", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "Defines whether the dropbox should handle .h5 archives as folders (true) or as files (false). Default is true.", "text": "# Defines whether the dropbox should handle .h5 archives as folders (true) or as files (false). Default is true.\nh5-folders = true", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:8", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "Defines whether the dropbox should handle .h5ar archives as folders (true) or as files (false). Default is true.", "text": "# Defines whether the dropbox should handle .h5ar archives as folders (true) or as files (false). Default is true.\nh5ar-folders = true\nDevelopment mode\n\nSet property\ndevelopment-mode\n=\ntrue\nin your dropbox to enable a quick\nfeedback loop when developing your dropbox. By default dropboxes have\ncomplex auto-recovery mechanism working, which on errors waits and\nretries the registration several times. It can be useful in case of\nshort network problems or other unexpected turbulences. In this case it\ncan take a long time between the dropbox tries to register something,\nand actual error report. During development it is essential to have a\nquick feedback if your dropbox does what it should or not. Thus - set\nthe development mode if you are modifying your script and remember to\nset it back when you are done.\nJython version\n\nSet property\nplugin-jython-version=2.7\nin your dropbox\nplugin.properties to change default jython version for the single\ndropbox. Available are versions 2.5 and 2.7\nJython API\n\nWhen a new file is placed in the dropbox, the framework compiles and\nexecutes the script, checks that the signatures of the\nprocess\nfunction and any defined event-handling functions are correct, and then\ninvokes its\nprocess\nfunction.\nIDataSetRegistrationTransaction\n\nHave a look\nat\nIDataSetRegistrationTransactionV2\nfor the calls available in a transaction. Note that you need to use the\nfile methods in the transaction, like e.g.\nmoveFile()\n,  rather than\nmanipulating the file system directly to get fully transactional\nbehavior.\nTransDatabase queries\n\nThe query object returned\nby\ngetDatabaseQuery(String\ndataSourceName)\nallows to perform any query\nand executing any statement on the given query database in the context\nof a database transaction. Here are the methods available from the query\n## interface:\npublic\ninterface\nDynamicQuery\n{\n/**\n* Performs a SQL query. The returned List is connected to the database and\n* updateable.\n*\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template.\n*\n* @return The result set as List; each row is represented as one Map<String,Object>.\n*/\n## List\n<\n## Map\n<\n## String\n,\n## Object\n>>\nselect\n(\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n/**\n* Performs a SQL query. The returned List is connected and\n* updateable.\n*\n* @param type  The Java type to return one rows in the returned\n*            result set.\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template.\n*\n* @return The result set as List; each row is represented as one Map<String,Object>.\n*/\n<\n## T\n>\n## List\n<\n## T\n>\nselect\n(\nfinal\n## Class\n<\n## T\n>\ntype\n,\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n/**\n* Executes a SQL statement.\n*\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template.\n*\n* @return The number of rows updated by the SQL statement, or -1 if not\n*         applicable. <b>Note:</b> Not all JDBC drivers support this\n*         cleanly.\n*/\nint\nupdate\n(\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n/**\n* Executes a SQL statement as a batch for all parameter values provided.\n*\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template. At least\n*            one of the parameters needs to be an array or\n*            <code>Collection</code>. If multiple parameters are arrays or\n*            <code>Collection</code>, all of them need to have the same\n*            size.\n*\n* @return The number of rows updated by the SQL statement, or -1 if not\n*         applicable. <b>Note:</b> Not all JDBC drivers support this\n*         cleanly.\n*/\nint\nbatchUpdate\n(\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n/**\n* Executes a SQL statement. Supposed to be used for INSERT statements with\n* an automatically generated integer key.\n*\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template.\n*\n* @return The automatically generated key. <b>Note:</b> Not all JDBC\n*         drivers support this cleanly.\n*/\nlong\ninsert\n(\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n/**\n* Executes a SQL statement. Supposed to be used for INSERT statements with\n* one or more automatically generated keys.\n*\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template.\n*\n* @return The automatically generated keys. <b>Note:</b> Not all JDBC\n*         drivers support this cleanly and it is in general driver-dependent\n*         what keys are present in the returned map.\n*/\n## Map\n<\n## String\n,\n## Object\n>\ninsertMultiKeys\n(\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n/**\n* Executes a SQL statement as a batch for all parameter values provided.\n* Supposed to be used for INSERT statements with an automatically generated\n* integer key.\n*\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template. At least\n*            one of the parameters needs to be an array or\n*            <code>Collection</code>. If multiple parameters are arrays or\n*            <code>Collection</code>, all of them need to have the same\n*            size.\n*\n* @return The automatically generated key for each element of the batch.\n*         <b>Note:</b> Not all JDBC drivers support this cleanly.\n*/\nlong\n[]\nbatchInsert\n(\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n/**\n* Executes a SQL statement as a batch for all parameter values provided.\n* Supposed to be used for INSERT statements with one or more automatically\n* generated keys.\n*\n* @param query  The SQL query template.\n* @param parameters  The parameters to fill into the SQL query template. At least\n*            one of the parameters needs to be an array or\n*            <code>Collection</code>. If multiple parameters are arrays or\n*            <code>Collection</code>, all of them need to have the same\n*            size.\n*\n* @return The automatically generated keys for each element of the batch.\n*         <b>Note:</b> Not all JDBC drivers support this cleanly and it is\n*         in general driver-dependent what keys are present in the returned map.\n*/\n## Map\n<\n## String\n,\n## Object\n>[]\nbatchInsertMultiKeys\n(\nfinal\n## String\nquery\n,\nfinal\n## Object\n...\nparameters\n);\n}\n## Events / Registration Process Hooks\n\nThe script can be informed of events that occur during the registration\nprocess. To be informed of an event, define a function in the script\nfile with the name specified in the table. The script can do anything it\nwants within an event function. Typical things to do in event functions\ninclude sending emails or registering data in secondary databases. Some\nof the event functions can be used to control the behavior of the\nregistration.\nThis table summarizes the supported events.\n## Events Table\n\n## Function Name\n## Return Value\n## Description\npre_metadata_registration(DataSetRegistrationContext context)\nvoid\nCalled before the openBIS AS is informed of the metadata modifications. Throwing an exception in this method aborts the transaction.\npost_metadata_registration(DataSetRegistrationContext context)\nvoid\nThe metadata has been successfully stored in the openBIS AS. This can also be a place to register data in a secondary transaction, with the semantics that any errors are ignored.\nrollback_pre_registration(DataSetRegistrationContext context, Exception exception)\nvoid\nCalled if the metadata was not successfully storedin the openBIS AS.\npost_storage(DataSetRegistrationContext context)\nvoid\nCalled once the data has been placed in the appropriate sharded directory of the store. This can only happen if the metadata was successfully registered with the AS.\nshould_retry_processing(DataSetRegistrationContext context, Exception problem)\nboolean\nA problem occurred with the process function, should the operation be retried? A retry happens only if this method returns true.\nNote: the\nrollback_pre_registration\nfunction is intended to handle\ncases when the dropbox code finished properly, but the registration of\ndata in openbis failed. These kinds of problems are impossible to handle\nfrom inside of the\nprocess\nfunction. The exceptions raised during the\ncall to the\nprocess\nfunction should be handled by the function itself\nby catching exceptions.\n### Typical Usage Table\n\n## Function Name\n### Usage\npre_metadata_registration(DataSetRegistrationContext context)\nThis event can be used as a place to register information in a secondary database. If the transaction in the secondary database does not commit, false can be returned to prevent the data from entering openBIS.\npost_metadata_registration(DataSetRegistrationContext context)\nThis event can be used as a place to register information in a secondary database. Errors encountered are ignored.\nrollback_pre_registration(DataSetRegistrationContext context, Exception exception)\nUndoing a commit to a secondary transaction. Sending an email to the admin that the data set could not be stored.\npost_storage(DataSetRegistrationContext context)\nSending an email to tell the user that the data has been successfully registered. Notifying an external system that a data set has been registered.\nshould_retry_processing(DataSetRegistrationContext context, Exception problem)\nInforming openBIS if it should retry processing a data set.\n## Example Scripts\n\nA simple script that registers the incoming file as a data set\nassociated with a particular experiment.\ndata-set-handler-basic.py\ndef\nprocess\n(\ntransaction\n## ):\ndataSet\n=\ntransaction\n.\ncreateNewDataSet\n()\nincoming\n=\ntransaction\n.\ngetIncoming\n()\ntransaction\n.\nmoveFile\n(\nincoming\n.\ngetAbsolutePath\n(),\ndataSet\n)\ndataSet\n.\nsetExperiment\n(\ntransaction\n.\ngetExperiment\n(\n## \"/TESTGROUP/TESTPROJ/JYTHON\"\n))\nA script that registers the incoming file and associates it to a daily\nexperiment, which is created if necessary.\ndata-set-handler-experiment-reg.py\nfrom\ndatetime\nimport\ndatetime\ndef\nprocess\n(\ntransaction\n)\n# Try to get the experiment for today\nnow_str\n=\ndatetime\n.\ntoday\n()\n.\nstrftime\n(\n'%Y%m\n%d\n'\n)\nexpid\n=\n## \"/TESTGROUP/TESTPROJ/\"\n+\nnow_str\nexp\n=\ntransaction\n.\ngetExperiment\n(\nexpid\n)\n# Create an experiment\nif\n## None\n==\nexp\n## :\nexp\n=\ntransaction\n.\ncreateNewExperiment\n(\nexpid\n,\n## \"COMPOUND_HCS\"\n)\nexp\n.\nsetPropertyValue\n(\n## \"DESCRIPTION\"\n,\n\"An experiment created on \"\n+\ndatetime\n.\ntoday\n()\n.\nstrftime\n(\n'%Y-%m-\n%d\n'\n))\nexp\n.\nsetPropertyValue\n(\n## \"COMMENT\"\n,\nnow_str\n)\ndataSet\n=\ntransaction\n.\ncreateNewDataSet\n()\nincoming\n=\ntransaction\n.\ngetIncoming\n()\ntransaction\n.\nmoveFile\n(\nincoming\n.\ngetAbsolutePath\n(),\ndataSet\n)\ndataSet\n.\nsetDataSetType\n(\n## \"HCS_IMAGE\"\n)\ndataSet\n.\nsetExperiment\n(\nexp\n)\nDelete, Move, or Leave Alone on Error\n\nWhen a problem occurs processing a file in the dropbox, the processing\nis retried. This behavior can be controlled (see\n#Errors\n). If openBIS determines that it should not\nretry after an error or that it cannot successfully register the\nentities requested, the registration fails. It it possible to configure\nwhat happens to a file in the dropbox if a registration fails. The\nconfiguration can specify a behavior – delete the file, move it to an\nerror folder, or leave it untouched – for each of several possible\nsources of errors.\nBy default, the file is left untouched in every case. To change this\nbehavior, specify an on-error-decision property on the drop box. This\nhas one required sub-key, “class”; other sub-keys are determined by the\nclass.\n## Summary\n\n## Main Key:\non-error-decision\n## Required Sub Keys:\nclass : The class the implements the decision\n## There is currently one class available :\nch.systemsx.cisd.etlserver.registrator.ConfiguredOnErrorActionDecision\nThis class has the following sub keys:\ninvalid-data-set (a data set that fails validation)\nvalidation-script-error (the validation script did not execute\ncorrectly)\nregistration-error (openBIS failed to register the data set)\nregistration-script-error (the registration script did not\nexecute correctly)\nstorage-processor-error (the storage processor reports an error)\npost-registration-error (an error happened after the data set\nhad been registered and stored)\n## Example\n\nplugin.properties\n#\n# On Error Decision\n#\n# The class that implements the decision\non-error-decision.class = ch.systemsx.cisd.etlserver.registrator.ConfiguredOnErrorActionDecision", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:9", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "What to do if the validation script has problems", "text": "# What to do if the validation script has problems\non-error-decision.validation-script-error = MOVE_TO_ERROR", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:10", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "What to do if the openBIS does not accept the entities", "text": "# What to do if the openBIS does not accept the entities\non-error-decision.registration-error = MOVE_TO_ERROR", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:11", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "What to do if the registration script has problems", "text": "# What to do if the registration script has problems\non-error-decision.registration-script-error = MOVE_TO_ERROR", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:12", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "What to do if the storage processor does not run correctly", "text": "# What to do if the storage processor does not run correctly\non-error-decision.storage-processor-error = MOVE_TO_ERROR", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:13", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "What to do if an error occurs after the entities have been registered in openBIS", "text": "# What to do if an error occurs after the entities have been registered in openBIS\non-error-decision.post-registration-error = MOVE_TO_ERROR\n## Search\n\nThe transaction provides an interface for listing and searching for core\nentities, experiment, sample, and data set.\n## API\n\nTo use the search capability, one must first retrieve the search service\nfrom the transaction. By default the search service returns the entities\nfiltered to only those accessible by the user on behalf of wich, the\nscript is running. It is still possible to search all existing entities\nby using unfiltered search service accessible from the transaction via\nmethod getSearchServiceUnfiltered().\n## Experiment\n\nFor experiment, there is a facility for listing all experiments that\nbelong to a specified project.\nSample and Data Set\n\nFor sample and data set, a more powerful search capability is available.\nThis requires a bit more knowledge of the java classes, but is very\nflexible. For each entity, there is a simplified method that performs a\nsearch for samples or data sets, respectively, with a specified value\nfor a particular property, optionally restricted by entity type (sample\ntype or data set type). This provides an easy-to-use interface for a\ncommon case. More complex searches, however, need to use the more\npowerful API.\n## Authorization Service\n\nThe transaction provides an interface for querying the access privileges\nof a user and for filtering collections of entities down to those\nvisible to a user.\n## API\n\nTo use the authorization service, one must first retrieve the it from\nthe transaction.\n## Example\n\n## Combined Example\n\nIn this example, we create a data set, list experiments belonging to a\nproject, search for samples, search for data sets, and assign the\nexperiment, sample, and parent data sets based on the results of the\nsearches.\ndata-set-handler-with-search.py\ndef\nprocess\n(\ntr\n## ):\ndata_set\n=\ntr\n.\ncreateNewDataSet\n()\nincoming\n=\ntr\n.\ngetIncoming\n()\ntr\n.\nmoveFile\n(\nincoming\n.\ngetAbsolutePath\n(),\ndata_set\n)\n# Get the search service\nsearch_service\n=\ntr\n.\ngetSearchService\n()\n# List all experiments in a project\nexperiments\n=\nsearch_service\n.\nlistExperiments\n(\n\"/cisd/noe\"\n)\n# Search for all samples with a property value determined by the file name; we don't care about the type\nsamplePropValue\n=\nincoming\n.\ngetName\n()\nsamples\n=\nsearch_service\n.\nsearchForSamples\n(\n## \"ORGANISM\"\n,\nsamplePropValue\n,\n## None\n)\n# If possible, set the owner to the first sample, otherwise the first experiment\nif\nsamples\n.\nsize\n()\n>\n0\n## :\ndata_set\n.\nsetSample\n(\nsamples\n[\n0\n])\nelse\n## :\ndata_set\n.\nsetExperiment\n(\nexperiments\n[\n0\n])\n# Search for any potential parent data sets and use them as parents\nparent_data_sets\n=\nsearch_service\n.\nsearchForDataSets\n(\n## \"COMMENT\"\n,\n\"no comment\"\n,\n## \"HCS_IMAGE\"\n)\nparent_data_set_codes\n=\nmap\n(\nlambda\neach\n## :\neach\n.\ngetDataSetCode\n(),\nparent_data_sets\n)\ndata_set\n.\nsetParentDatasets\n(\nparent_data_set_codes\n)\nAn example from the Deep Sequencing environment handling BAM files:\ndata-set-handler-alignment.py\n'''\nThis is handling bowtie-BAM files and extracts some properties from the BAM header and\nthe samtools flagstat command. The results are formatted and attached  as a property\nto the openBIS DataSet.\nPrerequisites are the DataSetType: ALIGNMENT and\nthe following properties assigned to the DataSetType mentioned above:\nALIGNMENT_SOFTWARE, ISSUED_COMMAND, SAMTOOLS_FLAGSTAT,\n## TOTAL_READS, MAPPED_READS\nObviously you need a working samtools binary\n## Note:\nprint statements go to: ~openbis/sprint/datastore_server/log/startup_log.txt\n'''\nimport\nos\nfrom\nch.systemsx.cisd.openbis.generic.shared.api.v1.dto\nimport\nSearchCriteria\n## FOLDER\n=\n'/net/bs-dsu-data/array0/dsu/dss/incoming-jython-alignment/'\n## SAMTOOLS\n=\n'/usr/local/dsu/samtools/samtools'\ndef\nprocess\n(\ntransaction\n## ):\nincoming\n=\ntransaction\n.\ngetIncoming\n()\n# Create a data set and set type\ndataSet\n=\ntransaction\n.\ncreateNewDataSet\n(\n## \"ALIGNMENT\"\n)\ndataSet\n.\nsetMeasuredData\n(\n## False\n)\nincomingPath\n=\nincoming\n.\ngetAbsolutePath\n()\n# Get the incoming name\nname\n=\nincoming\n.\ngetName\n()\n# expected incoming Name, e.g.:ETHZ_BSSE_110429_63558AAXX_1_sorted.bam\nsplit\n=\nname\n.\nsplit\n(\n\"_\"\n)\nsample\n=\nsplit\n[\n2\n]\n+\n'_'\n+\nsplit\n[\n3\n]\n+\n':'\n+\nsplit\n[\n4\n]\n# Extract values from a samtools view and set the results as DataSet properties\n# Command: samtools view -H ETHZ_BSSE_110429_63558AAXX_1_sorted.bam\narguments\n=\n## SAMTOOLS\n+\n' view -H '\n+\n## FOLDER\n+\nname\n#print('Arguments: '+ arguments)\ncmdResult\n=\nos\n.\npopen\n(\narguments\n)\n.\nread\n()\nproperties\n=\ncmdResult\n.\nsplit\n(\n\"\n\\n\n\"\n)[\n-\n2\n]\n.\nsplit\n(\n'\n\\t\n'\n)\naligner\n=\n(\nproperties\n[\n1\n]\n.\nsplit\n(\n':'\n)[\n1\n]\n.\nupper\n()\n+\n'_'\n+\nproperties\n[\n2\n]\n.\nsplit\n(\n':'\n)[\n1\n])\ncommand\n=\nproperties\n[\n3\n]\narguments\n=\n## SAMTOOLS\n+\n' flagstat '\n+\n## FOLDER\n+\nname\ncmdResult\n=\nos\n.\npopen\n(\narguments\n)\n.\nread\n()\ntotalReads\n=\ncmdResult\n.\nsplit\n(\n'\n\\n\n'\n)[\n0\n]\n.\nsplit\n(\n' '\n)[\n0\n]\nmappedReads\n=\ncmdResult\n.\nsplit\n(\n'\n\\n\n'\n)[\n2\n]\n.\nsplit\n(\n' '\n)[\n0\n]\ndataSet\n.\nsetPropertyValue\n(\n## \"ALIGNMENT_SOFTWARE\"\n,\naligner\n)\ndataSet\n.\nsetPropertyValue\n(\n## \"ISSUED_COMMAND\"\n,\ncommand\n)\ndataSet\n.\nsetPropertyValue\n(\n## \"SAMTOOLS_FLAGSTAT\"\n,\ncmdResult\n)\ndataSet\n.\nsetPropertyValue\n(\n## \"TOTAL_READS\"\n,\ntotalReads\n)\ndataSet\n.\nsetPropertyValue\n(\n## \"MAPPED_READS\"\n,\nmappedReads\n)\n# Add the incoming file into the data set\ntransaction\n.\nmoveFile\n(\nincomingPath\n,\ndataSet\n)\n# Get the search service\nsearch_service\n=\ntransaction\n.\ngetSearchService\n()\n# Search for the sample\nsc\n=\nSearchCriteria\n()\nsc\n.\naddMatchClause\n(\nSearchCriteria\n.\nMatchClause\n.\ncreateAttributeMatch\n(\nSearchCriteria\n.\nMatchClauseAttribute\n.\n## CODE\n,\nsample\n));\nfoundSamples\n=\nsearch_service\n.\nsearchForSamples\n(\nsc\n)\nif\nfoundSamples\n.\nsize\n()\n>\n0\n## :\ndataSet\n.\nsetSample\n(\nfoundSamples\n[\n0\n])\nError Handling\n\nAutomatic Retry (auto recovery)\n\nOpenBIS has a complex mechanism to ensure that the data registration via\ndropboxes is atomic. When error occurs during data registration, the\ndropbox will try several times before it gives up on the process. The\nretries can happen to the initial processing of the data, as well as to\nthe registration in application server. Even if these fail there is\nstill a chance to finish the registration. If the registration reaches\nthe certain level it stores the checkpoint on the disk. If at any point\nthe process fails, or the dss goes down it tries to recover from the\ncheckpoint.\nThere are two types of checkpoint files: State files and marker files.\nThere are stored in two different directories. The default location for\nthe state files is\ndatastore_sever/recovery-state\n. This can be changed\nby the property\ndss-recovery-state-dir\nin DSS\nservice.properties\n.\nThe default location for the marker files was\n<store\nlocation>/<share\nid>/recovery-marker\n. This may lead to problems\nif this local is remote. Since version 20.10.6 the default location is\ndatastore_sever/recovery-marker-dir\n. This can be changed by the\nproperty\ndss-recovery-marker-dir\nin DSS\nservice.properties\n.\n## The\nprocess\nfunction will be retried if a\nshould_retry_processing\nfunction is defined in the dropbox script and\nit returns true. There are two configuration settings that affect this\nbehavior. The setting\nprocess-max-retry-count\nlimits the number of\ntimes the process function can be retried. The number of times to retry\nbefore giving up and the waiting periods are defined using properties\nshown in the table below.\nIMPORTANT NOTE: Please note, that the registration is considered as\nfailed only after, the whole retrying / recovery process will fail. It\nmeans that it can take a long time before the .faulty_paths file is\ncreated, even when there is a simple dropbox error.\nTherefor during development of a dropbox we recommend using\ndevelopment mode\n, wich basically sets all retry values to 0, thus disabling the auto-recovery feature.\n## Key\n## Default Value\n## Meaning\nprocess-max-retry-count\n6\nThe maximum number of times the process function can be retried.\nprocess-retry-pause-in-sec\n300\nThe amount of time to wait between retries of the process function.\nmetadata-registration-max-retry-count\n6\nThe number of times registering metadata with the server can be retried.\nmetadata-registration-retry-pause-in-sec\n300\nThe number of times registering metadata with the server can be retried.\nrecovery-max-retry-count\n50\nThe number of times the recovery from checkpoint can be retries.\nrecovery-min-retry-period\n60\nThe amount of time to wait between recovery from checkpoint retries.\n## Manual Recovery\n\nThe registration of data sets with Jython dropboxes has been designed to\nbe quite robust. Nonetheless, there are situations in which problems may\narise. This can especially be a problem during the development of\ndropboxes. Here are the locations and semantics of several important\nfiles and folders that can be useful for debugging a dropbox.\nFile or Folder\n## Meaning\ndatastore_server/log-registrations\nKeeps logs of registrations. See the registration log documentation for more information.\n[store]/[share]/pre-staging\nContains hard-link copies of the original data. Dropbox process operate on these hardlink copies.\n[store]/[share]/staging\nThe location used to prepare data sets for registration.\n[store]/[share]/pre-commit\nWhere data from data sets are kept while register the metadata with the AS. Once metadata registration succeeds, files are moved from this folder into the final store directory.\n[store]/[share]/recovery-marker (before version 20.10.6)\ndatastore_sever/recovery-marker-dir (since version 20.10.6)\nDirectories, one per dropbox, where marker files are kept that indicate that a recovery should happen on an incoming file if it is reprocessed. Deleting a marker file will force the incoming file to be processed as a new file, not a recovery.\n### Classpath / Configuration\n\nIf you want other jython modules to be available to the code that\nimplements the drop box, you will need to modify the\ndatastore_server.conf file and add something like\n-Dpython.path=data/dropboxes/scripts:lib/jython-lib\nTo the JAVA_OPTS environment variable. The line should now look\n## something like this:\nJAVA_OPTS=${JAVA_OPTS:=-server\n-d64\n-Dpython.path=data/dropboxes/scripts:lib/jython-lib}\nIf the Jython dropbox need third-party JAR files they have to be added\nto the core plugin in a sub-folder\nlib/\n.\nValidation scripts\n\n## See\nJython DataSetValidator\n.\n## Global Thread Parameters\n\nIf you want to write a drop box which uses some parameters defined in\nthe service.properties you can access those properties via\nthe\ngetGlobalState\n. Here we show an example how to use:\nGlobal tread properties\ndef\ngetThreadProperties\n(\ntransaction\n## ):\nthreadPropertyDict\n=\n{}\nthreadProperties\n=\ntransaction\n.\ngetGlobalState\n()\n.\ngetThreadParameters\n()\n.\ngetThreadProperties\n()\nfor\nkey\nin\nthreadProperties\n## :\ntry\n## :\nthreadPropertyDict\n[\nkey\n]\n=\nthreadProperties\n.\ngetProperty\n(\nkey\n)\nexcept\n## :\npass\nreturn\nthreadPropertyDict\n# You can later access the thread properties like this:\nthreadPropertyDict\n=\ngetThreadProperties\n(\ntransaction\n)\nincomingRootDir\n=\nthreadPropertyDict\n[\nu\n'incoming-root-dir'\n]\nSending Emails from a Drop box\n\ndef\npost_storage\n(\ncontext\n## ):\nmailClient\n=\ncontext\n.\ngetGlobalState\n()\n.\ngetMailClient\n()\nresults\n=\ncontext\n.\ngetPersistentMap\n()\n.\nget\n(\n## PERSISTANT_KEY_MAP\n)\nsendEmail\n(\nmailClient\n,\nresults\n[\n0\n])\ndef\nprocess\n(\ntransaction\n## ):\ntransaction\n.\ngetRegistrationContext\n()\n.\ngetPersistentMap\n()\n.\nput\n(\n## PERSISTANT_KEY_MAP\n,\n[\nfcId\n])\nJava Dropboxes\n\nThe above examples show how to implement dropboxes in Python. Python,\nhowever, is not the only language option: it is also possible to write\ndropboxes in Java. Whereas Python has the advantage of short turnaround\nand less verbose syntax, Java is a good choice in the dropbox employs\ncomplex logic and/or does not need to be modified frequently. A natural\nprogression is to use Python at the beginning, when creating a new\ndropbox, to take advantage of the short turnaround cycle and then move\nto Java once the dropbox implementation becomes more stable. Since the\nAPI is the same, this language transition process is quite painless.\n### Configuration\n\nAs with other dropboxes, a Java dropbox should be deployed as a core-plugin.\nplugin.properties\n#\n# REQUIRED PARAMETERS\n#\n# The directory to watch for new data sets\nincoming-dir = ${root-dir}/incoming-java-dropbox", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:14", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JavaTopLevelDataSetHandlerV2 or a subclass thereof", "text": "# The handler class. Must be either ch.systemsx.cisd.etlserver.registrator.api.v2.JavaTopLevelDataSetHandlerV2 or a subclass thereof\ntop-level-data-set-handler = ch.systemsx.cisd.etlserver.registrator.api.v2.JavaTopLevelDataSetHandlerV2", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:15", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "The class that implements the dropbox (must implement ch.systemsx.cisd.etlserver.registrator.api.v2.IJavaDataSetRegistrationDropboxV2)", "text": "# The class that implements the dropbox (must implement ch.systemsx.cisd.etlserver.registrator.api.v2.IJavaDataSetRegistrationDropboxV2)\nprogram-class = ch.systemsx.cisd.etlserver.registrator.api.v2.ExampleJavaDataSetRegistrationDropboxV2", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:16", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "The appropriate storage processor", "text": "# The appropriate storage processor\nstorage-processor = ch.systemsx.cisd.etlserver.DefaultStorageProcessor\n\n#\n# OPTIONAL PARAMETERS\n#", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "reference"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_dss-dropboxes:17", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/dss-dropboxes.html", "repo": "openbis", "title": "Dropboxes", "section": "False if incoming directory is assumed to exist.", "text": "# False if incoming directory is assumed to exist.\n# Default - true: Incoming directory will be created on start up if it doesn't exist.\nincoming-dir-create = true\nThe program-class parameter specifies the class that implements the\nlogic of the dropbox. This class must implement the\nIJavaDataSetRegistrationDropboxV2 interface. This class, and any other\ncode it uses, should be packaged in a jar file that is provided with the\ncore-plugin. The name of the jar file can be freely chosen.\n## Implementation\n\nTo implement a dropbox in Java, implement\nthe IJavaDataSetRegistrationDropboxV2 interface, which codifies the\ninteraction between the datastore server and the dropbox. We recommend\nsubclassing AbstractJavaDataSetRegistrationDropboxV2 to bootstrap the\nimplementation of this interface.\nIJavaDataSetRegistrationDropboxV2\n/**\n* The interface that V2 dropboxes must implement. Defines the process method, which is called to\n* handle new data in the dropbox's incoming folder, and various event methods called as the\n* registration process progresses.\n*\n* @author Pawel Glyzewski\n*/\npublic\ninterface\nIJavaDataSetRegistrationDropboxV2\n{\n/**\n* Invoked when new data is found in the incoming folder. Implements the logic of registering\n* and modifying entities.\n*\n* @param transaction The transaction that offers methods for registering and modifying entities\n*            and performing operations on the file system.\n*/\npublic\nvoid\nprocess\n(\nIDataSetRegistrationTransactionV2\ntransaction\n);\n/**\n* Invoked just before the metadata is registered with the openBIS AS. Gives dropbox\n* implementations an opportunity to perform additional operations. If an exception is thrown in\n* this method, the transaction is rolledback.\n*\n* @param context Context of the registration. Offers access to the global state and persistent\n*            map.\n*/\npublic\nvoid\npreMetadataRegistration\n(\nDataSetRegistrationContext\ncontext\n);\n/**\n* Invoked if the transaction is rolledback before the metadata is registered with the openBIS\n## * AS.\n*\n* @param context Context of the registration. Offers access to the global state and persistent\n*            map.\n* @param throwable The throwable that triggered rollback.\n*/\npublic\nvoid\nrollbackPreRegistration\n(\nDataSetRegistrationContext\ncontext\n,\n## Throwable\nthrowable\n);\n/**\n* Invoked just after the metadata is registered with the openBIS AS. Gives dropbox\n* implementations an opportunity to perform additional operations. If an exception is thrown in\n* this method, it is logged but otherwise ignored.\n*\n* @param context Context of the registration. Offers access to the global state and persistent\n*            map.\n*/\npublic\nvoid\npostMetadataRegistration\n(\nDataSetRegistrationContext\ncontext\n);\n/**\n* Invoked after the data has been stored in its final location on the file system and the\n* storage has been confirmed with the AS.\n*\n* @param context Context of the registration. Offers access to the global state and persistent\n*            map.\n*/\npublic\nvoid\npostStorage\n(\nDataSetRegistrationContext\ncontext\n);\n/**\n* Is a function defined that can be used to check if a failed registration should be retried?\n* Primarily for use implementations of this interface that dispatch to dynamic languages.\n*\n* @return true shouldRetryProcessing is defined, false otherwise.\n*/\npublic\nboolean\nisRetryFunctionDefined\n();\n/**\n* Given the problem with registration, should it be retried?\n*\n* @param context Context of the registration. Offers access to the global state and persistent\n*            map.\n* @param problem The exception that caused the registration to fail.\n* @return true if the registration should be retried.\n*/\npublic\nboolean\nshouldRetryProcessing\n(\nDataSetRegistrationContext\ncontext\n,\n## Exception\nproblem\n)\nthrows\nNotImplementedException\n;\n}\nSending Emails in a drop box (simple)\n\nfrom\nch.systemsx.cisd.common.mail\nimport\nEMailAddress\ndef\nprocess\n(\ntransaction\n## ):\nreplyTo\n=\nEMailAddress\n(\n\"manuel.kohler@id.ethz.ch\"\n)\nfromAddress\n=\nreplyTo\nrecipient1\n=\nEMailAddress\n(\n\"recipient1@ethz.ch\"\n)\nrecipient2\n=\nEMailAddress\n(\n\"recipient2@ethz.ch\"\n)\ntransaction\n.\ngetGlobalState\n()\n.\ngetMailClient\n()\n.\nsendEmailMessage\n(\n\"This is the subject\"\n,\n\\\n\"This is the body\"\n,\nreplyTo\n,\nfromAddress\n,\nrecipient1\n,\nrecipient2\n);\nJava Dropbox Example\n\nThis is a simple example of a pure-java dropbox that creates a sample\nand registers the incoming file as a data set of this sample.\nExampleJavaDataSetRegistrationDropboxV2.java\npackage\nch.systemsx.cisd.etlserver.registrator.api.v2\n;\nimport\nch.systemsx.cisd.etlserver.registrator.api.v1.IDataSet\n;\nimport\nch.systemsx.cisd.etlserver.registrator.api.v1.ISample\n;\nimport\nch.systemsx.cisd.openbis.dss.generic.shared.api.internal.v1.IExperimentImmutable\n;\n/**\n* An example dropbox implemented in Java.\n*\n* @author Chandrasekhar Ramakrishnan\n*/\npublic\nclass\nExampleJavaDataSetRegistrationDropboxV2\nextends\nAbstractJavaDataSetRegistrationDropboxV2\n{\n## @Override\npublic\nvoid\nprocess\n(\nIDataSetRegistrationTransactionV2\ntransaction\n)\n{\n## String\nsampleId\n=\n## \"/CISD/JAVA-TEST\"\n;\nISample\nsample\n=\ntransaction\n.\ncreateNewSample\n(\nsampleId\n,\n## \"DYNAMIC_PLATE\"\n);\nIExperimentImmutable\nexp\n=\ntransaction\n.\ngetExperiment\n(\n## \"/CISD/NEMO/EXP-TEST-1\"\n);\nsample\n.\nsetExperiment\n(\nexp\n);\nIDataSet\ndataSet\n=\ntransaction\n.\ncreateNewDataSet\n();\ndataSet\n.\nsetSample\n(\nsample\n);\ntransaction\n.\nmoveFile\n(\ntransaction\n.\ngetIncoming\n().\ngetAbsolutePath\n(),\ndataSet\n);\n}\n}\nJava Code location\nThe Java file should go into a\nlib\nfolder and should be wrapped as a\njar\n. The name does not matter.\nWhile building a jar, the project should have the following\n## dependencies:\nopenBIS-API-dropbox-<version>.jar\n,\nlib-commonbase-<version>.jar\nand\ncisd-hotdeploy-13.01.0.jar\n## . The\nfirst two are available in the distribution in the archives\nopenBIS-API-commonbase-<version>.zip\nand\nopenBIS-API-dropbox-<version>.zip\n, the third one is available in\nthe Ivy repo\n.\nExample path where the created\njar\n## should reside:\nservers/core-plugins/illumina-ngs/2/dss/drop-boxes/register-cluster-alignment-java/lib\nCreate a\njar\nfrom your java dropbox file:\njar\ncvf\nfoo.jar\nfoo.java\nRestart the DSS\nCalling an Aggregation Service from a drop box\n\ndrop box code\n'''\n## @author:\n## Manuel Kohler\n'''\nfrom\nch.systemsx.cisd.openbis.dss.generic.server.EncapsulatedOpenBISService\nimport\ncreateQueryApiServer\ndef\nprocess\n(\ntransaction\n## ):\n# use the etl server session token\nsession_token\n=\ntransaction\n.\ngetOpenBisServiceSessionToken\n()\n# To find out do SQL on the openBIS DB: select code from data_stores;\ndss\n=\n## \"STANDARD\"\n# folder name under the reporting_plugins\nservice_key\n=\n\"reporting_experimental\"\n# some parameters which are handed over\nd\n=\n{\n\"param1\"\n## :\n\"hello\"\n,\n\"param2\"\n## :\n\"from a drop box\"\n}\n# connection to the openbis server returns IQueryApiServer\ns\n=\ncreateQueryApiServer\n(\n\"http://127.0.0.1:8888/openbis/openbis/\"\n,\n\"600\"\n)\n# Actual call\n# Parameters: String sessionToken, String dataStoreCode,String serviceKey, Map<String, Object> parameters)\ns\n.\ncreateReportFromAggregationService\n(\nsession_token\n,\ndss\n,\nservice_key\n,\nd\n)\nKnown limitations\n\n## Blocking\n\nRegistering/updating a large number of entities can cause other\nconcurrent operations that try to modify the same or related entities to\nbe blocked. This limitation applies to both dropboxes and batch\noperations triggered from the web UI. Lists of operations that are\nblocked are presented below. Each list contains operations that cannot\nbe performed when a specific kind of entity is being registered/updated.\n## Experiment:\ncreating/updating an experiment in the same project\nupdating the same space\nupdating the same project\nupdating the same experiment\n## Sample:\ncreating/updating an experiment in the same project\ncreating/updating a sample in the same experiment\nupdating the same space\nupdating the same project\nupdating the same experiment\nupdating the same sample\nData set:\ncreating/updating an experiment in the same project\ncreating/updating a sample in the same experiment\ncreating a dataset in the same experiment\nupdating the same space\nupdating the same project\nupdating the same experiment\nupdating the same sample\n## Material:\nupdating the same material", "timestamp": "2025-09-18T09:38:29.827659Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_software-developer-documentation_server-side-extensions_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/software-developer-documentation/server-side-extensions/index.html", "repo": "openbis", "title": "Server-Side Extensions", "section": "Server-Side Extensions", "text": "## Server-Side Extensions\n\n## Core Plugins\n## Motivation\n## Core Plugins Folder Structure\n### Merging Configuration Data\nEnabling Modules and Disabling Plugins\n## Enabling Modules\nDisabling Core Plugins by Property\nDisabling Core Plugins by Marker File\n## Core Plugin Dependency\nRules for Plugin Writers\nUsing Java libraries in Core Plugins\n## Custom Application Server Services\n## Introduction\nHow to write a custom AS service core plugin\nHow to use a custom AS service\nAPI Listener Core Plugin (V3 API)\n## Introduction\n## Core Plugin\nPlugin.properties\nlib\n## Example - Logging\n## Example - Loggin Sources\n## Dropboxes\n## Jython Dropboxes\n## Introduction\n## Simple Example\n## More Realistic Example\n## Model\n## Details\n### Dropbox Configuration\nDevelopment mode\nJython version\nJython API\nIDataSetRegistrationTransaction\nTransDatabase queries\n## Events / Registration Process Hooks\n## Events Table\n### Typical Usage Table\n## Example Scripts\nDelete, Move, or Leave Alone on Error\n## Summary\n## Example\n## Search\n## API\n## Experiment\nSample and Data Set\n## Authorization Service\n## API\n## Example\n## Combined Example\nError Handling\nAutomatic Retry (auto recovery)\n## Manual Recovery\n### Classpath / Configuration\nValidation scripts\n## Global Thread Parameters\nSending Emails from a Drop box\nJava Dropboxes\n### Configuration\n## Implementation\nSending Emails in a drop box (simple)\nJava Dropbox Example\nCalling an Aggregation Service from a drop box\nKnown limitations\n## Blocking", "timestamp": "2025-09-18T09:38:29.833655Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_changelog_CHANGELOG:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/changelog/CHANGELOG.html", "repo": "openbis", "title": "OpenBIS Change Log", "section": "Version 20.10.11 (02 Dec 2024)", "text": "OpenBIS Change Log\n\n## Version 20.10.11 (02 Dec 2024)\n\n## Core\n\nBugfix: Fix Property Types DB, remove vocabulary type from converted Varchar types (BIS-1069)\nBugfix: SFTP now reports the right timestamps (BIS-1432)\nBugfix: Name collision of testing PNG with the production one, image export does not work for uncompressed data (BIS-1439)\nBugfix: Search fails if integer dynamic property is evaluated and used for sorting (BIS-1448)\nBugfix: SearchFiles ignores fetch options (BIS-1487)\nBugfix: Number of DSS Java threads gradually increasing to critically high values (BIS-1640)\n## ELN\n\nImprovement: Move addition of new widget for properties in ELN Settings to top of list (BIS-918)\nImprovement: Login with SSO/SwitchAAI was not showing the loading spinning clock (BIS-1488)\nImprovement: “+New” dropdown list in forms replaced, this improves usability in small screens/tables (BIS-1512)\nBugfix: Now experiments from Experiments/Collections table in Project page can be deleted (BIS-983)\nBugfix: Uniqueness of QRcodes was only checked against objects a user has access to, now to all (BIS-1038)\nBugfix: URL display in text fields changed in 20.10.9 (BIS-1461)\nBugfix: Error after login in 20.10.9 reported by external users (BIS-1485)\nBugfix: Failing to download files with comma in name (BIS-1515)\nBugfix: XLSX Import failure when using “Ignore if Exists” on certain scenarios (BIS-1528)\nBugfix: XLSX Import failure if images are embedded in text fields (BIS-1534)\nBugfix: XLSX Import failure if code fields are not uppercase, now they uppercase automatically (BIS-1545)\nBugfix: XLSX Import failure of ELN Standard Technologies master data (BIS-1582)\nBugfix: Barcode/QR scanner view broken in 20.10.9.1 (BIS-1533)\nBugfix: Samples without experiments get blank edit form (BIS-1537)\nBugfix: ELN lims dropbox ignores plugin.properties config (BIS-1593)\nBugfix: Advanced search date widget not configured with openBIS date and timestamp format (BIS-1616)\nBugfix: Search for products does not work when creating a Template for a Request (BIS-1617)\n## Version 20.10.10 (10 Oct 2024)\n\n## Core\n\nNew Feature: Forward all properties from plugins we ship to the as and dss service.properties (BIS-1399)\nNew Feature: Forward all properties from plugins we ship to OS environment variables (BIS-1587)- New Feature: Make SFTP Certificate configurable on the dss service.properties (BIS-1401)\nNew Feature: Make SFTP Certificate configurable on the dss service.properties (BIS-1401)\nImprovement: Remove rotateLogFiles from datastore_server.sh and datamover.sh (BIS-1396)\n## ELN\n\nNew Feature: Move InstanceProfile.js parameters to the ELN Settings UI (BIS-1400)\n## Version 20.10.9.1 (16 Aug 2024)\n\n## ELN\n\nBugfix: ELN UI - Blanc Sample fields (BIS-1460)\n## Version 20.10.9 (31 Jul 2024)\n\n## Core\n\nImprovement: V3 API - Allow to use existing sessionToken with Java and JS facade (BIS-1336)\nImprovement: PDF Export - Formulas within spreadsheets are not converted to calculated values (BIS-1076)\nImprovement: PDF Export - Added section names in exported pdfs (BIS-1335)\nImprovement: Excel Import - Import the exported image data (BIS-1050)\nImprovement: Excel Import - Masterdata import, existing plugins support (BIS-1096)\nImprovement: Excel Import - Large imports support (BIS-1059)\nImprovement: Excel Export - “pdf” folder from exports renamed to “hierarchy” (BIS-1338)\nImprovement: Excel Export - Empty Experiment folders from export removed (BIS-1337)\nImprovement: Excel Export - Use identifiers instead of PermID in object properties for exported pdf (BIS-1055)\nBugfix: PDF Export - Failed PDF export when there are several colored spans (BIS-1371)\nBugfix: Excel Import - Exception during variable substitution (BIS-1058, BIS-1373)\nBugfix: Excel Import - Issue with import of exported object with data in spreadsheet (BIS-1049)\nBugfix: Excel Export - Object Properties References missing on xlsx (BIS-1051)\n## ELN\n\nImprovement: ELN UI - Spreadsheet Improvements (BIS-1025)\nImprovement: ELN UI - Object/Experiment Links in word processor (BIS-1029)\nImprovement: ELN UI - ELN UI: GFB - Notes taking Widget for Experiments (BIS-1003)\nImprovement: ELN UI - GFB - Object/Experiment Links in spreadsheet (BIS-1004)\nImprovement: ELN UI - Customise ELN Welcome Page (BIS-1087)\nImprovement: ELN UI - Change behavior of eln-lims-dropbox script with hidden files (BIS-1093)\nBugfix: ELN UI - Blanc Sample Form (BIS-1369)\nBugfix: ELN UI - ELN-LIMS Dropbox dataset with metadata.json containing UTF-8 Characters fails (BIS-1052)\n### Bugfix: ELN UI - Large Configurations Support ~ large vocabularies fail (BIS-1389)\nBugfix: ELN UI - ELN-LIMS URL Encoding Issues (BIS-1057)\nBugfix: ELN UI - ELN-LIMS loading without DSS (BIS-1028)\n## Admin\n\nImprovement: Admin UI - Show identifier (name) for object property in Tables (BIS-1261)\nImprovement: Admin UI - Show user account in admin UI (BIS-776)\n## Version 20.10.8 (29 May 2024)\n\n## Core\n\nNew Feature: Additional configuration parameters for MultiDataSetArchiveSanityCheckMaintenanceTask (SSDM-14098)\nNew Feature: Registration of shared/space objects can be prevented at the system level (SSDM-14067)\nNew Feature: Extend User and Email field length in PersonPE and Database (BIS-799)\nNew Feature: Timezone of timestamp changed to database timezone (SSDM-14228)\nNew Feature: EXCEL Master Data Importer API (preview, non final) (BIS-772, BIS-773, BIS-994, BIS-999, BIS-1010, BIS-1011, BIS-1025, BIS-1040)\nNew Feature: EXCEL Master Data Exporter API (preview, to be changed) (BIS-772, BIS-773, BIS-994, BIS-999, BIS-1010, BIS-1011, BIS-1025, BIS-1040)\nImprovement: Excel Export - Add support for cells bigger than 32k (BIS-789, BIS-790)\nImprovement: Excel Export - internal name space types are skipped from updates done by non system users (BIS-793)\nBugfix: Excel Import - Keep Dynamic Properties Dynamic (SSDM-14224)\nBugfix: UserManagementMaintenanceTask: role assignment error (SSDM-14261)\nBugfix: Deadlock on display settings (SSDM-14263)\nBugfix: DataSetAndPathInfoDBConsistencyCheckTask causing endless repetitions of tryGetDataSetLocation, causing AS logs to dramatically blow up (SSDM-14237)\nBugfix: DSS Becomes Zombie when Dropbox folder is unreachable (SSDM-14074)\nBugfix: MultiDataSetArchiveSanityCheckMaintenanceTask fails for h5ar files (SSDM-14124)\nBugfix: MultiDataSetArchiver sanity check skips checksum verification if pathinfo db entries are missing (SSDM-14125)\nBugfix: Enabling extra logging switches causes problems on AS start up (SSDM-14207)\nBugfix: FastDownloadServlet and V3.searchFiles do not release data set locks (SSDM-14203)\nBugfix: DSS threads do not release data set locks before dying (SSDM-14204)\n## ELN\n\nBugfix: Problem with selecting checkboxes at Object Browser > Register Objects popup in Firefox (SSDM-14195)\nBugfix: ELN: not clickable checkboxes in multi-select dropdowns in popups (SSDM-14226)\nBugfix: Typo in create inventory space form (SSDM-14133)\nBugfix: Fix Toolbar Plugin Not Loading (BIS-991)\nBugfix: Login screen disabled on small width screens / Android (BIS-984)\nBugfix: Remove Life Sciences and Basic ELN Types from Installer, Link on Community Github (BIS-800)\n## Admin\n\nImprovement: Improve the error message shown when a user with an already existing userid is attempted to be created (SSDM-14194)\n## Version 20.10.7.3 (23 November 2023)\n\n## ELN\n\nBugfix: Circular Annotations deletion (SSDM-14135)\n## Admin\n\nImprovement: XLS Master Data Importer: Make Version Optional, vocabularies bug fix (SSDM-14129)\n## Version 20.10.7.2 (13 October 2023)\n\n## ELN\n\nImprovement: Make object property a link (SSDM-13901)\nImprovement: Support spaces for identifiers separation in the PASTE ANY search (SSDM-13829)\nImprovement: Support PermIDs in the PASTE ANY Parent/Children Search (SSDM-13830)\nImprovement: Rename “Scan barcode” to “Scan QR codes/ barcodes” everywhere in ELN UI (SSDM-13842)\nBugfix: Collection /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION not created in new multi-group instances (SSDM-14068)\nBugfix: XLS Imports and ELN UI don’t take URLs with certain special characters (SSDM-13905)\nBugfix: Issues with calculations in ELN spreadsheets (SSDM-13736)\nBugfix: Multi-position box deletion bug (SSDM-13834)\nBugfix: General Settings of the ELN overwrite the general settings of the Group in the multiple group instance (SSDM-13934)\nBugfix: Remove system pop up from the back button (support issues on mobile/tablets) (SSDM-13894)\nBugfix: Sample form can’t update Objects when project-samples is disabled (SSDM-13962)\nBugfix: Explicit “false” is not saved for “boolean” properties (SSDM-14093)\n## Admin\n\nImprovement: XLS Master Data Importer: Make Version Optional (SSDM-13961)\n## Core\n\nNew Feature: V3 API : Import (BIS-771)\nNew Feature: V3 API : Provide bundles with all V3 API JS files (BIS-761)\nBugfix: Usage Reporting Task : Task fails with NPE if samples without space were created (SSDM-14065)\nBugfix: User Management Task : Task removes roles that could have been configured on the json config (SSDM-13940)\nBugfix: User Management Task : Task should not create exiting user space if a user is de-activated and re-activated (SSDM-13421)\nBugfix: Properties : Fix properties that contain both a value and a link to a controlled vocabulary term (SSDM-13843)\nBugfix: PAT : Remove hash from validity PAT warning emails (SSDM-14099)\n## Version 20.10.7.1 (25 July 2023)\n\n## ELN\n\nImprovement: ELN Dropbox provides now a report with all error messages when registration fails. (SSDM-13794)\nImprovement: Navigation further avoids to show empty folders in some situations. (SSDM-13827)\nBugfix: Back button behaviour was in some situations incorrect. Example: pressing back after navigating to an experiment from an identifier in a table. (SSDM-13811)\nBugfix: Zenodo Export form did break in two situations, now fixed. (SSDM-13813, SSDM-13824)\nBugfix: Dataset Viewer was not showing the list of files on the DataSet form, only on the Collection/Sample forms, is now fixed. (SSDM-13827)\n## Admin\n\nImprovement: Vocabulary term template now provides an explanation on how to use it. (SSDM-13817)\n## Version 20.10.7 (5 July 2023)\n\n## Core\n\nImprovement: Improved SFTP Folder listing performance (SSDM-13489)\nImprovement: Improved SFTP download performance (SSDM-13490)\nImprovement: V3 autogenerated code behaviour can be overridden by providing a code as it works on V1 (SSDM-12646)\nBugfix: V3 rights for creation/update are now consistent with what is intended. Before in some cases POWER USER was necessary for things USER should be enough (SSDM-13718)\nBugfix: V3 Removal of property type that already has some values now works correctly (SSDM-13784)\nBugfix: User Management maintenance task now reuses the same space for a user if it gets deactivated/activated again (SSDM-13421)\nBugfix: User Management maintenance task now assign rights correctly when moving a user from one group to another instead of rights getting lost (SSDM-13716)\nBugfix: DataSetArchiverOrphanFinderTask fix erroneous reporting of missing tar files when using archivers on multi-group instances with sub-folders (SSDM-13725)\n## ELN\n\nRemoval: Plasmapper 2.0 integration since the external service was been decommissioned (SSDM-13664)\nNew Feature: New Barcode / QR Code widget supporting scanner and camera in all places where barcodes could be used on the UI (SSDM-12100)\nNew Feature: Mobile Support, navigation component can be collapsed . (SSDM-12100)\nNew Feature: Dataset table in Object and Collection Forms (SSDM-13683)\nNew Feature: NOT operator on Advance Search (SSDM-13427)\nImprovement: XLS Templates no longer contain names of types, to avoid long verbose names (SSDM-12531)\nImprovement: Number is now formatted with separators following the US locale (SSDM-13640)\nImprovement: Project View now separates the overview  from the list of collections/experiments (SSDM-13643)\nImprovement: Navigation menu refreshes when moving objects (SSDM-13720)\nImprovement: Navigation menu refreshes when copying objects (SSDM-13786)\nImprovement: Navigation menu nodes are not cached, this helps use cases when nodes are updated out of user control (SSDM-13785)\nBugfix: Problem deleting spaces(SSDM-13676)\nBugfix: Some label rendering glitches fixed (SSDM-13692)\nBugfix: Label incorrectly named renamed from Name to Code on Objected move menu (SSDM-13728)\nBugfix: Some glitches with repeated columns tables fixed (SSDM-13712)\nBugfix: XLS Dataset exports (SSDM-13693)\nBugfix: Windows Postgres version detection (SSDM-13709)\nAdmin UI\n\nImprovement: XLS Imports now skips DB sanity check used by large migration greatly lowering import times (SSDM-13788)\nBugfix: Now unofficial terms stay unofficial if the official checkbox is not checked. (SSDM-13730)\nELN/Admin UI\n\nImprovement: High resolution logos and icons (SSDM-13504)\nImprovement: Navigation, removal of empty nodes (SSDM-13671)\n## Version 20.10.6 (26 April 2023)\n\n## Core\n\nNew Feature: OpenBIS class to interact with AS and DSS with methods to handle uploads and semantic annotations. (SSDM-13017)\nNew Feature: Dataset Creation from V3 API: Java, javascript and python3 facades support. (SSDM-13253) right arrow openBISV3API-RegisterDataSets\nNew Feature: V3 API search criteria methods withSampleProperty, withVocabularyProperty added (SSDM-12986)\nNew Feature: Maintenance Task that un-archives all data from a certain point in time and verifies checksums (SSDM-12971) right arrow Maintenance Tasks - MultiDataSetArchiveSanityCheckMaintenanceTask\nNew Feature: Maintenance Task : Delete temporary stage directories from failed registrations from Dropboxes and sharding directories (SSDM-11605) right arrow Enabled by default, no action required\nImprovement: Performance improvements when adding children and creating new entities on the persistence layer. As used by XLS Imports. (SSDM-13033, SSDM-13195, SSDM-13207)\nImprovement: Performance improvements when using general search from the V1 API as used by the Core UI (SSDM-13449)\nImprovement: Make dropbox recovery marker directory configurable (SSDM-12056)\nImprovement: Make dropbox .faulty-paths file creation more consistent (SSDM-13501)\nImprovement: User management task make instance admins configurable (SSDM-13080)\nBugfix: ArchivingByRequestTask : NPE when a data set does not have an experiment (SSDM-13172)\nBugfix: MultiDataSetArchiver : make unarchiving more robust with StrongLink (SSDM-13084)\nBugfix: DSS registration on AS -  Wait for safe registration (SSDM-13327)\nBugfix: PAT support for single sign on setups (SSDM-13362)\nAdmin UI / ELN\n\nNew Feature: New navigation component. (SSDM-12451, SSDM-11608, SSDM-12480, SSDM-13274, SSDM-13118, SSDM-12479, SSDM-12098)right arrow User Documentation - Navigation Menu\nNew Feature: Table XLS Exports of metadata and master data. (SSDM-13206, SSDM-13163, SSDM-13256, SSDM-13450, SSDM-13463, SSDM-13414, SSDM-13420) right arrow User Documentation - Tables\nNew Feature: XLS Imports, zip support (SSDM-13422)\nBugfix: XLS Imports, Allow “”-”” in codes.  Allow updating properties if the file contains a $property. All deletions to require delete tag and other bug fixes  (SSDM-13320, SSDM-13343, SSDM-13447, SSDM-13446)\nAdmin UI\n\nNew Feature: Info view. (SSDM-13133)\n## ELN\n\nNew Feature: Dropbox monitor (SSDM-13135)right arrow User Documentation - Dropbox Monitor\nNew Feature: Barcode Scanner using camera from the main menu right arrow User Documentation - Barcodes\nNew Feature: New Plugin Interface methods and improve the interface documentation (SSDM-13203, SSDM-13265) right arrow ELN-LIMS WEB UI Extensions\nImprovement: Removal of REQUEST.ORDER_NUMBER (SSDM-13199)\nImprovement: XLS Menu items renames (SSDM-13202)\nImprovement: Better error messages and email notifications on the ELN-LIMS dropbox. (SSDM-11306)\nImprovement: Publication API Support for Multi Group setups (Zenodo and research collection exports) (SSDM-11744)\nImprovement: Performance improvement when building a dropdown with thousands of items on Chrome browsers (SSDM-13549)\nImprovement: ELN Dropbox helper tool updated (SSDM-13360)\nImprovement: Use the Vocabulary Term URL on the VIEW mode on forms (SSDM-13436)\nImprovement: +new button in ELN is only be added if object type is specified (SSDM-13268)\nImprovement: Show permId and Identifiers more prominently (SSDM-13197)\nImprovement: remove +Add button when adding parents in ELN (SSDM-13169)\nImprovement: Inconsistency in Collection forms between Inventory and Lab notebook (SSDM-12991)\nImprovement: Add column with default barcode in Collection tables (SSDM-12889)\nImprovement: Search by size for datasets (SSDM-12472)\nImprovement: Changes on General Settings in multi-group instances (SSDM-13262)\nImprovement: Match view of folders on the SFTP to the ELN Navigation menu (SSDM-13261, SSDM-12929)\nImprovement: Use default ELN-LIMS settings on new instances instead of forcing manual configuration (SSDM-12892)\nImprovement: Preserve Plugin configurations (SSDM-13413)\nBugfix: Date filter on tables (SSDM-13315, SSDM-13156)\nBugfix: Boolean value to be now treated as Try-Valued, True, False or Null (SSDM-13481, SSDM-13085)\nBugfix: after saving the ELN Settings, the page stays in edit mode, not in view mode (SSDM-13196)\nBugfix: UnicodeEncodeError in OPERATION.generalExports.py (SSDM-13367)\nBugfix: file-authentication: having special characters in name prevents editing in User profile (SSDM-13171)\n## Version 20.10.5 (29 November 2022)\n\n## Core\n\n(SSDM-11550) New Feature : Personal Access Tokens (PAT)\n(SSDM-12514) New Feature : API Event listener for integrations\n(SSDM-12625) New Feature : Active Users Email triggered from Admin UI\n(SSDM-13173) New Feature: Workspace API Extension\n(SSDM-12900) Bugfix : Sessions not properly closed\n(SSDM-12496) Bugfix  : XLS Import Issue with Project property field in XLS import conflict with openBIS Project\n(SSDM-12933) Bugfix  : XLS Import problem with upgrades on some instances - error about object types that already exist\n(SSDM-12988) Improvement : XLS Import Improved Error Message - for object references\n(SSDM-13031) Improvement  : XLS Import Improved Error Message - Missing header\n(SSDM-12996) Improvement : XLS Import Admin UI Integration\n(SSDM-13015) New Feature  : XLS Import Semantic Annotations\n(SSDM-11744) Improvement : Publications API to support multi-group instance (RC and Zenodo)\n(SSDM-12286, SSDM-13163) New Feature : Exports for Master data and Metadata - Core\n(SSDM-12905) Bugfix : MultiDataSetArchiver problems with LTS\n(SSDM-13117) Bugfix :  MultiDataSetArchiver wrongly detected inconsistency between pathinfo DB and filesystem for H5 files\n(SSDM-13084) MultiDataSetArchiver : retry mechanism for unarchiving\n(SSDM-13052) MultiDataSetArchiving : resource does not exist error\n(SSDM-13018) MultiDataSetArchiver : retry sanity check on any exception\n(SSDM-12980) Bugfix: UserManagementTask - Role Assignments - Corner Case Null Pointer\n(SSDM-12976) Improvements: Additional archiver options for unreliable File API backends\n(SSDM-12855) Improvement: Avoid TS Vector too big when migrating to 20.10 + Format improvement\n(SSDM-12936)  Improvement: Disable converting types on XLS Master Data\n## Jupyter Integration:\n\n(SSDM-12909) Bugfix : Session token not automatically saved in Jupyter notebook\nAdmin UI\n\n(SSDM-11675) Improvement : Get rid of Redux and Redux-Saga\n(SSDM-12451) New Feature : New database navigation component in Admin UI (tech preview)\n## ELN-LIMS:\n\n(SSDM-11539) New Feature: Space management in ELN or new admin UI\n(SSDM-11968) Bugfix : Update CKEditor to fix underscore problem\n(SSDM-12220) Bugfix : Show description of boolean fields\n(SSDM-12327) Bugfix : keep parents and children in object templates\n(SSDM-12740) Bugfix : Cannot unselect an object type in Default Object type field in Collection\n(SSDM-12938) Bugfix : Search in property of type “Object” is not restricted to the specified object type\n(SSDM-12946) Bugfix : Dataset metadata not cleared on type change\n(SSDM-12981) Bugfix : Storage Widget Multi position delete corner case\n(SSDM-12997) Bugfix : DataSet container widget is not cleared when changing the sample\n(SSDM-12987) Bugfix : When I flag “Delete also all descendant objects” openbis goes in loop\n(SSDM-12582) Improvement : ELN Inconsistencies\n(SSDM-12584) Improvement : Add missing attributes to dataset search, archiving, unarchiving\n(SSDM-12634) Improvement : Support more Barcodes scanners\n(SSDM-12654) Improvement : Change dropdown for deletion of dependencies in trashcan\n(SSDM-12729) Improvement : Make property “order status” mandatory in requests and orders\n(SSDM-12730) Improvement : Show Description in Project form by default\n(SSDM-12766) Improvement : Export should have names instead of codes\n(SSDM-12809) Improvement : Show warning when leaving an object without saving after adding parents/children\n(SSDM-12930) Improvement : Update Settings texts\n(SSDM-12937) Improvement : Disallow config changes to system spaces,Resolved\n(SSDM-12990) Improvement : ELN Storage : if a storage is deleted, the positions assigned to it are not and it is not possible to delete them afterwards\n(SSDM-13105) Improvement : Superscript and subscript text in CKE editor\n(SSDM-13159) Improvement : Load improvements for Parents Table and Storage Views\n(SSDM-13152) New Feature : Exports for Master data and Metadata - UI\n## Version 20.10.4 (3 August 2022)\n\n## Core\n\n(SSDM-9831)  New Feature: Sample FK Properties : Materials Migration\n(SSDM-10984) New Feature: Excel masterdata spreadsheet rewrite in Java with parser giving error messages by line\n(SSDM-12561) Improvement: Search Engine: Improve search using existing joins\n(SSDM-12527) Improvement: Search Engine: Remove joins and enforce early filtering with subqueries for PropertySearchCriteria\n(SSDM-12661) Improvement: Refactor AbstractMaintenanceTask to AbstractGroupMaintenanceTask\n(SSDM-12554) Improvement: users removed from user management config file are not always disabled\n(SSDM-12574) Improvement: Source of root certificates and checks for certificate chains used in openBIS AS+DSS cert stores\n(SSDM-12656) Improvement: Archiver: Test consistency between data store and pathinfo database BEFORE writing tarball\n(SSDM-12500) Improvement: Archiving: Calculate data set size if not found in database\n(SSDM-12464) Improvement: Multi Data Set Archiving: Check after successful finalization multi_dataset_archive database\n(SSDM-12565) Improvement: Maintenance task to delete unused datasets on scratch share.\n(SSDM-12707) Bugfix: NPE in DataSetArchiverOrphanFinderTask\n(SSDM-12393) Bugfix: DSS startup check for AS MaintenanceTasks\n(SSDM-12703) Bugfix: SFTP shows non-existing files as empty files/folders\n(SSDM-12655) Bugfix: Search complete openBIS repo for places where we open a v3 api session internally and close them\n(SSDM-12782) Bugfix: Fix Vocabulary from Property Type Conversion\n## ELN\n\n(SSDM-12621) New Feature : Creation of spaces\n(SSDM-12622) New Feature : New Processing Plugin Tool View\n(SSDM-12623) New Feature : Custom Imports -> New Upload to Dropbox Tool View\n(SSDM-12551) New Feature : Add filter for parents and children in ELN tables\n(SSDM-12815) New Feature : Barcode Characters Configurable\n(SSDM-12650) New Feature : Virtual FTP to follow current ELN-LIMS settings to categorise spaces and types visibility\n(SSDM-12520) Improvement : Experiment type to Experiment table in project overview and remove selection of Experiment type in project overview\n(SSDM-12765) Bugfix: Freezing failing with SwitchAAI IDs\n(SSDM-12518) Bugfix: Issue with boolean values in tables\n(SSDM-12694) Bugfix: Storage deletion in ELN\n(SSDM-12735) Bugfix: Navigation Tree doesn’t show data sets for the children of a sample\n(SSDM-12771) Bugfix: cannot create request with new product added to request on multi group instances\n(SSDM-12767) Bugfix: On first creation the JS settings from plugins where not being respected\n(SSDM-12651) Bugfix: Batch upload of storage positions avoids repeating box names\n(SSDM-12732) Bugfix: SFTP shows non-existing files as empty files/folders\n## Version 20.10.3.1 (13 June 2022)\n\n## Core\n\n(SSDM-12045) Improvement : UserManagementMaintenanceTask - Improved template\n(SSDM-12485) Improvement : UserManagementMaintenanceTask - Create empty mapping file\n(SSDM-12081) Bugfix : freezing affects trashcan\n(SSDM-12530) Bugfix : poor performance of events search maintenance task - memory leak\n(SSDM-12556) Bugfix : poor performance of events search maintenance task - fetching too many events\n## ELN\n\n(SSDM-11623) Improvement : Multi Group Support - Group configuration is only applied to its spaces\n(SSDM-12370) Improvement : Truncate long lists of parents/children displayed in tables\n(SSDM-12468) Improvement : Improved performance for tables\n(SSDM-12519) Improvement : add property “$show_in_project_overview” to ENTRY object type\n(SSDM-12309) Bugfix : object type Chemical shows storage widget even if this is disabled in the Settings\n(SSDM-12400) Bugfix : missing scroll down in new XLS batch upload template\n(SSDM-12412) Bugfix : when a new type is created in a multi-group instance, show in main menu is automatically enabled in all settings\n(SSDM-12417) Bugfix : error without text given if I do not add a Label for parents in ELN Settings\n(SSDM-12459) Bugfix : CKEditor missing some features and displaying wrong layout\n(SSDM-12477) Bugfix : description is not shown in MULTILINEVARCHAR fields\n(SSDM-12522) Bugfix : selection of experiments to show in project overview does not work properly\n(SSDM-12526) Bugfix : delete message for object type ENTRY shows html tags\n## Version 20.10.3 (7 March 2022)\n\n## Core\n\n(SSDM-11728) Bugfix : Dynamic Properties evaluation fails if sample components are accessed\n(SSDM-12059) Bugfix : SFTP : datasets connected only to samples are not shown\n(SSDM-12033) Bugfix : SFTP: delay for a user to access the recently added group\n(SSDM-11556) Improvement : export-master-data.py should export fields descriptions\n(SSDM-12293) Improvement : UserManagementMaintenanceTask: Allow to assign roles to all user spaces\n(SSDM-12051) Improvement : Extend DeletedObject by identifier and entity type\n(SSDM-11784) Improvement : Upgrade apache sshd library (for our sftp service needed)\n(SSDM-11953) Improvement : Update R version on JupyterHub image to 4.1x\n(SSDM-11978) Improvement : Upgrade Jackson to 2.9.10.8\n(SSDM-12031) Improvement : Upgrade to latest jetty 9.4 version\n(SSDM-11579) New Feature : AS Maintenance Task - Lib folder not being loaded\n(SSDM-11354) New Feature : Query Engine : Caching Implementation\n(SSDM-11954) New Feature : Maintenance task which removed deleted data sets from the archive\n(SSDM-12110) Remove CIFEX from openBIS\n## ELN\n\n(SSDM-9305) Bugfix : Hints for children set in Settings are not shown when editing objects\n(SSDM-12184) Bugfix : Plain Text Widget and Monospace font saves HTML tags\n(SSDM-10064) Bugfix : Typo correction in Request form\n(SSDM-11256) Bugfix : Error when two storages with same code are created in 2 different ELN_SETTINGS\n(SSDM-11339) Bugfix : Move folders of disabled users to “Others disabled” in multi-group instances\n(SSDM-11819) Bugfix : REAL data type : Missing comparator for advanced search and wrong sorting for properties\n(SSDM-11977) Bugfix : Using bold text in ENTRY title breaks alphabetical sorting in main menu\n(SSDM-11980) Bugfix : Data Set Upload form does not work after a failed login attempt (ID#18459457)\n(SSDM-12055) Bugfix : Parents and children sections are not always displayed in the same place in object forms\n(SSDM-12072) Bugfix : Rescaling images embedded in text fields in Collection table overview\n(SSDM-12227) Bugfix : Move objects does not always work correctly\n(SSDM-12229) Bugfix : ELN Filters all text received from server instead only needed causing Glitch with plain txt\n(SSDM-12310) Bugfix : Rich Text: Images with non-ASCII characters in the file name are not shown\n(SSDM-12323) Bugfix : Sample deletion confirmation popup unexpected HTML tags - (SSDM-10066) Improvement : Delete Experiment/Project should delete Object/Datasets/Experiment on the Experiment/Project\n(SSDM-12049) Bugfix : Export metadata and data is not working\n(SSDM-12073) Bugfix : Could not add new user\n(SSDM-12361) Bugfix : Object templates cannot be created in group_settings of multi-group instance\n(SSDM-12057) Bugfix : User name stored in a session token has different letter case than the actual user name (ID#18459293)\n(SSDM-10474) Improvement : Collection Forms Views Configured by Individual Collection\n(SSDM-12103) Improvement : Change “deleted” message to “moved to trashcan” for entries that are moved to the trashcan\n(SSDM-11098) Improvement : Keep order of properties for types as specified in admin UI\n(SSDM-11533) Improvement : Show parents/children name in addition to identifiers in ELN tables\n(SSDM-12194) Improvement : Barcodes: Making check on minimum length more robust and validate custom barcodes\n(SSDM-12146) Improvement : Modify ELN masterdata plugin to avoid creation of default spaces in multigroup instances\n(SSDM-12218) Improvement : Tables Improvements for Rich Text Content\n(SSDM-8701) New Feature : XLS Import\n(SSDM-12312) New Feature : Allow extra tool actions\n(SSDM-10071) New Feature : Entity history\n(SSDM-9646) New Feature : Add select all/deselect all to all tables in ELN\n(SSDM-10541) New Feature : Add move all to ELN tables\n(SSDM-11664) Table Widget : Common table widget for ELN and NG UI\n(SSDM-11951) Table Widget : Sorting by multiple columns\n(SSDM-12023) Table Widget : Filtering by date ranges\n(SSDM-12025) Table Widget : Dropdown filter for Boolean properties\n(SSDM-12149) Table Widget : Show by default more columns in ELN tables\n(SSDM-12250) Table Widget : Sticky first column\npyBIS\n\n(SSDM-11738) : get_samples() with children\nNew Admin UI\n\n(SSDM-12150) : New Feature : XLS Import\n(SSDM-11169) : New Feature : Property types overview\n(SSDM-11727) : Remove the concept of local property types\n## Version 20.10.2.3 (15 November 2021)\n\n## ELN\n\nFix security vulnerability.\n## Version 20.10.2.2 (30 November 2021)\n\n## Core\n\n(SSDM-11586) Bugfix: Pybis - uses session from last login when used in JupyterHub\n(SSDM-11792) Bugfix: Pybis - remove the usage of environment variables in Jupyter Authenticator and Pybis\n(SSDM-11879) Bugfix: Can’t edit samples in Core UI when project samples disabled\n(SSDM-11462) Bugfix: V3 API - Nested fetched sort options don’t work as expected\n(SSDM-11917) Bugfix: V3 API - Don’t break when assigning project to sample on project samples disabled.\n(SSDM-11859) Bugfix: V1 API - reverting a deletion via coreUI or ELN is very inefficient\n(SSDM-11863) Bugfix: Python Master data export doesn’t escape special characters on description\n(SSDM-11948) Bugfix: Multi Data Set Unarchiving making more robust in case of deleted data sets\n(SSDM-11964) Bugfix: user management task constantly recreates user space\n(SSDM-11602) V3 API - getRights: Adding DELETE and updating EDIT\n(SSDM-11884) Permanent deletion should show dependent deletion sets\n(SSDM-11885) Improve postregistration in case of error\n## ELN\n\n(SSDM-10078) Bugfix: Non deletable datasets can’t be moved to trashcan\n(SSDM-10301) Bugfix: 2nd level of Parents/Children now shows on Parents/Children table\n(SSDM-11425) Bugfix: Enter dates manually not always work\n(SSDM-11622) Bugfix: Multi Group Storage Support: Storage and Templates are now created for the selected group settings instead randomly\n(SSDM-11876) Bugfix: Multi Group Ordering Support: create request in a multigroup instance fails because the wrong space is used\n(SSDM-11734) Bugfix: Advanced Search - Selecting Type OR BUG\n(SSDM-11786) Bugfix: installer fail when folder exists but is empty\n(SSDM-11949) Bugfix: Archiving helper\n(SSDM-11972) Bugfix: ELN Data Set View doesn’t show metadata and files of Data Sets of type UNKNOWN\n(SSDM-11844) Multi Group Storage Support: Editing Users/Groups from the ELN/LIMS\n(SSDM-11670) Side Menu Links Plugin Template\n(SSDM-9867) Advance search shows dropdown for well known values.\n(SSDM-10681) Rename “use as template” to “copy to Experiment” for protocols addition\n(SSDM-11455) Storage Tool that shows storage left for all users\n(SSDM-11557) Remove UNKOWN type from More dropdown in ELN Project\n(SSDM-11958) Show UNKNOWN dataset type on navigation menus by default\n(SSDM-11733) eln-lims dropbox metadata registration support\n(SSDM-11732) eln-lims dropbox metadata.json template export in ELN UI\n(SSDM-11855) Multi Group Storage Support: When creating storage positions only available groups show, when updating only the ones belonging to the same group as position\n## Version 20.10.2.1 (6 October 2021)\n\n## Core\n\n(SSDM-11740) Fix SFTP to use session token\n## ELN\n\n(SSDM-11799) Can not create copy of an object with children\nVersion 20.10.2 GA (General Availability) (22 September 2021)\n\n## Core\n\n(SSDM-10942) V3 API search : Improve partial match search\n(SSDM-10941) V3 API search : Searching for several words does not scale efficiently\n(SSDM-10971) MaintenanceTaskPlugin allows to run MaintenanceTask at specified days/times\n(SSDM-10831) Entity Deletion History - maintenance task\n(SSDM-11000) V3 API search : Implement prefix matching efficient search\n(SSDM-11080) Bugfix : Relationships history : Sample-project relations are not stored/returned\n(SSDM-11124) Bugfix : Issue with deletion of MICROSCOPY_EXPERIMENT and objects\n(SSDM-11227) Bugfix : openbis_statistics_server build not working out of the box\n(SSDM-11165) Cloning a dropbox\n(SSDM-10832) V3 API : Entity Deletion History\n(SSDM-11166) Bad performance of MicroscopyThumbnailCreationTask\n(SSDM-11252) openBIS capabilities config for adding a parent for which the user has only observer rights\n(SSDM-10988) Bugfix : V3 API search : Add missing fields in partial match search\n(SSDM-11323) Bugfix : Null pointer is thrown in 20.10 using getSpace() from SampleImmutable in V2\n(SSDM-11158) Bugfix : V3 API search : Number of results after translation has changed error\n(SSDM-10497) V3 API search : NOT implementation\n(SSDM-10231) V3 API search : Full Text Search on Standard Engine\n(SSDM-11271) Plugin that generates thumbnails should allow to set the number of concurrent thumbs to do\n(SSDM-11268) SFTP Hierarchy resolver that shows the tree the same way as on the ELN-LIMS with Microscopy/Flow enabled\n(SSDM-11267) SFTP Hierarchy resolver that shows the tree the same way as on the ELN-LIMS UI\n(SSDM-11237) Make locale settings in all databases consistent\n(SSDM-11388) Bugfix : V3 API search : Search not working properly for some empty search criteria\n(SSDM-11223) API : Enable Compression in Jetty for the API\n(SSDM-10994) V3 API search: Sorting by multiple properties\n(SSDM-10962) Disable unnecessary plugings in the default Docker installation\n(SSDM-11420) Bugfix : Widget for addition of datasets when creating new jupyter notebook does not have scrollbar\n(SSDM-11577) Bugfix : Moving objects to space called SHARED_MATERIALS does not work\n(SSDM-11559) Bugfix : Cannot remove widget assignment in ELN settings\n## ELN\n\n(SSDM-10940) Warning when searching for more than 3 words in global search\n(SSDM-10986) Anonymous login in ELN\n(SSDM-11086) Matching mode : Remember the last selected option in user settings\n(SSDM-11091) Bugfix : Editing a storage position does not work\n(SSDM-11049) Bugfix : Data set registration with file with leading space\n(SSDM-11118) Bugfix : Error Messages and wrong order in ELN\n(SSDM-11133) Bugfix : Incorrect automatic experiment code generation\n(SSDM-11174) Bugfix : Opening microscopy image viewer breaks “New…” and “More…” dropdowns\n(SSDM-11151) Bugfix : ELN hangs when copying a sample with STORAGE_POSITION child\n(SSDM-11213) Bugfix : ‘visible’ flag at the property level is not respected\n(SSDM-11132) Bugfix : Deletion of rows in spreadsheet component does not work\n(SSDM-11238) Bugfix : Error asigning storage\n(SSDM-11013) The limit of 50 samples on ELN Tree counts the ones you want to hide for the total\n(SSDM-11269) The limit of 50 samples on ELN Tree doesn’t allow to expand after certain level\n(SSDM-11305) Move Object should allow to choose all child objects and datasets on the same space and project to be included\n(SSDM-11270) Extend plugin system to allow hiding datasets by type\n(SSDM-10944) Prototype NG_UI table integration in ELN\n(SSDM-11215) Bugfix : Flag ‘create-continuous-sample-codes’ is not respected in some places\n(SSDM-10968) Add space management to ELN\n(SSDM-11347) Bugfix : Overlapping messages during user registration in ELN\n(SSDM-11005) Bugfix : Vulneraibility check of Util.showError()\n(SSDM-11341) Bugfix: Hide Nagios dataset type from ELN UI\n(SSDM-11555) Bugfix : Export ignores first 3 digits of MULTILINE_VARCHAR fields\n(SSDM-11540) Bugfix : Export does not always work if objects contain a spreadheet\nNew Admin UI\n\n(SSDM-10936) Bugfix : Bug with Order of Requests with missing quantity\n(SSDM-11346) Bugfix : Remove user in new admin UI does not work\n(SSDM-10833) Entity deletion history\n(SSDM-10939) Add SWITCH aai login to NG UI\n(SSDM-11178) Make new admin UI and ELN consistent\nVersion 20.10.1 EA (Early Access) (12 March 2021)\n\n## Core\n\n(SSDM-10320) Bugfix : Installer fails to Upgrade\n(SSDM-10316) Bugfix : SWITCH AAI user management tasks adds user folders each time it runs\n(SSDM-10385) Bugfix : SSLError when trying to connect to OpenBIS from JupyterHub\n(SSDM-10317) Bugfix : Missing material properties in full text search\n(SSDM-10306) Dataset Uploader : Accepting modern certificate authorities\n(SSDM-10366) Unique property values support (database changes only)\n(SSDM-10382) Bugfix : sample_identifier column doesn’t update on some row updates on the database\n(SSDM-10332) New Search Engine : Full text search aggregation running on the database\n(SSDM-10304) openBIS sync : do not synchronize internally managed master data\n(SSDM-10140) Don’t start if incorrect Postgres version is found\n(SSDM-10416) Disable Lucene character escape function in openBIS 20.10\n(SSDM-10473) Bugfix : tsvector_document of experiments_all not updated when project is moved\n(SSDM-10493) Bugfix : tsvector_document, sample_identifier and space_id not corretly updated when project moved to another space\n(SSDM-10469) V3 API - add “openbis-version” on the getServerInformation\n(SSDM-10413) V3 API - a method that would return available query databases is missing\n(SSDM-10395) V3 API - add “deletePersons” method\n(SSDM-10429) Bugfix : fix login issue in JupyterHub\n(SSDM-10405) New Search Engine : Nested AND/OR (Implementation)\n(SSDM-10304) openBIS sync : do not synchronize internally managed master data\n(SSDM-10574) Bugfix : Sorting by a non existing property on sample/experiment/dataset search lead to elements not containing it to not to appear on the results\n(SSDM-10566) Bugfix : V3 API search : Search by code issues\n(SSDM-10538) Bugfix : Project samples - inconsistent sample space and project after sample space change\n(SSDM-10471) Bugfix : Fix issue with DSS check script\n(SSDM-9413) Statistics collection for openBIS\n(SSDM-10702) Bugfix : V3 API search : Search by code issues\n(SSDM-10797) Bugfix : Search Engine Bug : String Equals ending in space not matching\n(SSDM-10894) Bugfix : UserManagementMaintenanceTask fails with stacked file authentication services\n(SSDM-10679) Bugfix : Possible Authorization Bug on DSS Search\n(SSDM-10611) Bugfix : obis doesn’t work with git-annex version 8\n(SSDM-10707) Bugfix : Wildcard behavior in coreUI advance search\n(SSDM-10696) Change entity-history.enabled to true by default\n(SSDM-10539) Work on integration between LDAP and SWITCH AAI\n(SSDM-10782) Modify user management task for multi-group to support shared inventory spaces\n(SSDM-10677) V3 API : Make wildcard search configurable\n(SSDM-10911) Bugfix : V3 API: Global search compatibility fixes\n(SSDM-10830) Entity Deletion History - new database table\n(SSDM-10411) V3 API - a method for plugins evaluation is missing\n(SSDM-10390) Search Engine : minor performance issues found during the new UI performance testing\n(SSDM-10196) New Search engine : Missing criteria methods\n## ELN\n\n(SSDM-10000) ELN - Barcodes Follow Up\n(SSDM-10149) Plugin Toolbar Extension\n(SSDM-10309) User Manager Improvements\n(SSDM-10387) Bugfix : ELN success message of batch uploads says ‘samples’ instead of ‘objects’\n(SSDM-10931) Bugfix : ELN Navigation tree doesn’t show data sets\n(SSDM-10913) Bugfix : ### ELN Global search compatibility fixes (UI)\n(SSDM-10904) Bugfix : Fix bug in DSS eln-lims-api reporting-plugin: openBIS java.lang.IllegalStateException: zip file closed\n(SSDM-10940) Warning when searching for more than 3 words in global search\n(SSDM-10936) Bugfix : Bug with Order of Requests with missing quantity\n(SSDM-10519) ELN UI : Fix Full Text Search sorting to score and show rank\nNew Admin UI\n\n(SSDM-10186) NEW openBIS UI - Group Management page\n(SSDM-10401) NEW openBIS UI - Plugins management page\n(SSDM-10432) NEW openBIS UI - queries execution\n(SSDM-10402) NEW openBIS UI - queries management\n(SSDM-10431) NEW openBIS UI - plugins evaluation\n(SSDM-10663) NEW openBIS UI - “Initial value” field is not shown when needed\n(SSDM-10646) NEW openBIS UI - revise internal property type check\n(SSDM-10433) NEW openBIS UI - table overviews\n(SSDM-10420) NEW openBIS UI - use the new naming (e.g ‘Object’ instead of ‘Sample’)\n(SSDM-10912) NEW openBIS UI : enable the UI core plugin in the installer by default\n(SSDM-10428) Bugfix : NEW openBIS UI - maintain the application path in URL\nVersion 20.10.0 RC (Release Candidate) (27 October 2020)\n\nNew features and Major Changes compared to release 19.06\nExtensions to the data model:\nDate Data Type: Intended to be use when timestamps are not needed.\nSample Property Type: Allows to link Samples without using the Parent/Children relationships.\nSample Relationship Properties: Allows to add information to relationship connections.\nChanges to the data model:\nThe Internally Managed and Internal Namespace concepts for properties have been merged. Now there is only Internally managed. Only the SYSTEM user can modify these.\nSearch engine: Completely rewritten to be faster, scale better and lower memory consumption. Queries will now behave like classic database queries instead of fuzzy full text search queries.\nAdmin UI Currently a preview, will replace the Core UI on the future.\n\n## ELN-LIMS UI\n\nImproved plugin system for the UI.\nMicroscopy and Flow Cytometry UI are now ELN plugins.\nMayor Technology Upgrades, now using:\nJava 11\n## Postgres 11\n## Bioformats 6.5.1\n## Jetty 9.4.30\nand other many upgrades.\nand lots, lots, lots of bug fixes.\nFor more details see sprint change log between S301 and S334.\n## Deprecated\n\nAs a rule of a thumb, deprecated features should stop being used since they can be removed in future releases.\nMaterial entity type is deprecated: Sample Property Types can be used instead for the same use cases.\nFile Type is deprecated.\nManaged Properties are deprecated.\nCore UI is deprecated. For admin tasks use the new Admin UI.\nV1 API: A reminder that all new developments should be done using the V3 API, V1 even if still kept for backwards compatibility with old plugins is not developed anymore.\nGeneralInformationService\nGeneralInformationChangingService\n## V3 API\n\nisInternalNamesSpace & setInternalNameSpace : Now manage the same flag “Internally Managed”\nFetchOptions.cacheMode : The new search engine ignores this, always getting the results from the database.\nEntityWithPropertiesSortOptions.fetchedFieldsScore : The new search engine ignores this, only full text search has a weights system to sort results, non usable on standard queries.\nAbstractEntitySearchCriteria.withProperty : deprecated in favour of using withXXXProperty, XXX being String, Date, Boolean or Number, check Javadoc for more details.\nAbstractEntitySearchCriteria.withAnyProperty : deprecated in favour of using withAnyXXXProperty, XXX being String, Date, Boolean or Number, check Javadoc for more details.\nHot-deployed (aka “predeployed”) Java plugins (i.e. entity validation plugin, dynamic property plugin, managed property plugin) are deprecated. WARNING: using them may lead to a deadlock during the Application Server startup.\n## Removed\n\nThe technologies for proteomics and screening where deprecated on 19.06 not been provided by the installer. Now they are finally removed and openBIS instances with these technologies CAN’T be upgraded. The upgrade procedure will detect their presence and prevent the upgrade.", "timestamp": "2025-09-18T09:38:29.839964Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_changelog_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/changelog/index.html", "repo": "openbis", "title": "Changelog", "section": "Changelog", "text": "## Changelog\n\nOpenBIS Change Log\n## Version 20.10.11 (02 Dec 2024)\n## Core\n## ELN\n## Version 20.10.10 (10 Oct 2024)\n## Core\n## ELN\n## Version 20.10.9.1 (16 Aug 2024)\n## ELN\n## Version 20.10.9 (31 Jul 2024)\n## Core\n## ELN\n## Admin\n## Version 20.10.8 (29 May 2024)\n## Core\n## ELN\n## Admin\n## Version 20.10.7.3 (23 November 2023)\n## ELN\n## Admin\n## Version 20.10.7.2 (13 October 2023)\n## ELN\n## Admin\n## Core\n## Version 20.10.7.1 (25 July 2023)\n## ELN\n## Admin\n## Version 20.10.7 (5 July 2023)\n## Core\n## ELN\nAdmin UI\nELN/Admin UI\n## Version 20.10.6 (26 April 2023)\n## Core\nAdmin UI / ELN\nAdmin UI\n## ELN\n## Version 20.10.5 (29 November 2022)\n## Core\n## Jupyter Integration:\nAdmin UI\n## ELN-LIMS:\n## Version 20.10.4 (3 August 2022)\n## Core\n## ELN\n## Version 20.10.3.1 (13 June 2022)\n## Core\n## ELN\n## Version 20.10.3 (7 March 2022)\n## Core\n## ELN\npyBIS\nNew Admin UI\n## Version 20.10.2.3 (15 November 2021)\n## ELN\n## Version 20.10.2.2 (30 November 2021)\n## Core\n## ELN\n## Version 20.10.2.1 (6 October 2021)\n## Core\n## ELN\nVersion 20.10.2 GA (General Availability) (22 September 2021)\n## Core\n## ELN\nNew Admin UI\nVersion 20.10.1 EA (Early Access) (12 March 2021)\n## Core\n## ELN\nNew Admin UI\nVersion 20.10.0 RC (Release Candidate) (27 October 2020)\nAdmin UI Currently a preview, will replace the Core UI on the future.\n## ELN-LIMS UI\n## Deprecated\n## V3 API\n## Removed\n### Pending 20.10 Configuration Changes\n## Version 20.10.10\n1. Changes to Datastore logs configuration\n## Version 20.10.9\n1. Changes to ELN LIMS Dropbox, new configuration keys for DSS service.properties.\n### Configuration:\n2. Configuration of download-url for Application Server service.properties\n## Version 20.10.6\n1. Changes on ELN LIMS Dropbox, new configuration key for DSS service.properties. This change is OPTIONAL.\n2. Changes to User Management Task, new configuration key for the configuration file. This change is OPTIONAL.\n3. Technology Upgrade: Postgres 15. This change is OPTIONAL.\n## Version 20.10.3\nVersion 20.10.2 GA (General Availability)\nVersion 20.10.1 EA (Early Access)\nRelease 20.10.0 RC\n## Technology Upgrade: Postgres 11\nTechnology Upgrade: Java 11\n## Technology Upgrade: Search Engine", "timestamp": "2025-09-18T09:38:29.843370Z", "source_priority": 2, "content_type": "reference"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_changelog_pending-configuration-changes:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/changelog/pending-configuration-changes.html", "repo": "openbis", "title": "Pending 20.10 Configuration Changes", "section": "Pending 20.10 Configuration Changes", "text": "### Pending 20.10 Configuration Changes\n\n## Version 20.10.10\n\n1. Changes to Datastore logs configuration\n\nDatastore server will no longer create a separate log file everytime it starts up.\nIn order to revert this change, uncomment\nrotateLogFiles\ncall from\n<INSTALLATION_DIR>/servers/datastore_server/datastore_server.sh\n(line 184)\necho\n-n\n\"Starting Data Store Server \"\n# rotateLogFiles $LOGFILE $MAXLOGS  # <- uncomment this line to bring back log rotation\n## Version 20.10.9\n\n1. Changes to ELN LIMS Dropbox, new configuration keys for DSS service.properties.\n\nThe ELN-LIMS dropbox validation has been overhauled so that it can happen in two widely configurable steps.\nFirst, files are allowed to be removed; this is aimed at removing files that are knowingly considered unnecessary.\nSecondly, the remains were validated for acceptability. This should allow administrators to choose the level of strictness in every particular environment.\nThe current default configuration matches previous behavior.\n### Configuration:\n\neln-lims-dropbox-discard-files-patterns\nAllows to specify comma-separated regular expressions of filenames that will be discarded by dropbox script during dataset creation.\n## Default setting:\neln-lims-dropbox-discard-files-patterns=\n(no files are discarded)\neln-lims-dropbox-illegal-files-patterns\nAllows to specify comma-separated regular expressions of filenames that will be considered as illegal by the dropbox script. Their presence in the dropbox directory will automatically abort the dataset creation.\n## Default setting:\neln-lims-dropbox-illegal-files-patterns=desktop\\\\.ini,\nIconCache\\\\.db,\nthumbs\\\\.db,\n\\\\..*,\n.*'.*,\n.*~.*,\n.*\\\\$.*,\n.*%.*\nNote: regular expressions in this configuration are\nJava patterns\nand special characters need to be escaped\n2. Configuration of download-url for Application Server service.properties\n\nThe ELN-LIMS “Print PDF” functionality has been reworked and to produce proper PDFs a\ndownload-url\nparameter (which is a base URL for Web client access to the application server) in AS service.properties needs to be set.\nWhen the machine is behind a reverse proxy, it needs to match the actual domain of the server\ndownload-url\n- Base URL. Contains protocol, domain, and port. (e.g https://localhost:8443)\n## Version 20.10.6\n\n1. Changes on ELN LIMS Dropbox, new configuration key for DSS service.properties. This change is OPTIONAL.\n\nmail.addresses.dropbox-errors\nAllows you to set a list of mails to get notified of registration errors.\n2. Changes to User Management Task, new configuration key for the configuration file. This change is OPTIONAL.\n\n## List<String>\ninstanceAdmins\nAllows you to set instance admins on the global configuration and they will be created.\n3. Technology Upgrade: Postgres 15. This change is OPTIONAL.\n\nopenBIS can now run using Postgres 15. Upgrades of Postgres should be taken carefully and with proper backup procedures in place.\nIt is required to upgrade the database directory format manually. Please check the\nofficial documentation\n## Version 20.10.3\n\nAfter migrating to 20.10.3 please make sure you don’t have in ELN settings properties that have BOTH custom widget set to “Word Processor” and at the same time they are listed in “Forced Disable RTF” section. If your ELN instance happens to have such properties please remove them from both “Custom Widgets” as well as “Forced Disable RTF” sections. This way they will behave as normal text fields. In case you would like to allow styling/formatting of your property values (rich text), please make sure the custom widget is set to “Word Processor” and the property is not listed in “Force Disable RTF” section.\nIn openBIS version 20.10.3 a mechanism for displaying property values in ELN has been changed. Starting from that version any HTML tags contained in the property values won’t be interpreted by a web browser anymore. Instead, such HTML tags will be displayed as normal text. The only exception to that rule are properties that have custom widget set to “Word Processor” in the ELN settings. HTML tags in such properties will be still recognized, interpreted and rendered by a web browser. For instance,\nvalue\n=\n\"<b>text</b>\"\nwill be rendered as “text” in bold font.\nIn some cases, due to this change in the ELN property rendering mechanism, after the upgrade to 20.10.3 properties may contain unwanted HTML tags when displayed in ELN. What was rendered as “value” in older versions of ELN, may after the upgrade to 20.10.3 become “\nvalue\n”. If that is the case with your installation, then you can clean such problematic property values using a Java tool created for that purpose.\nThe tool can be downloaded here. The tools requires Java 11 or newer. It connects with a chosen openBIS server and fetches/updates the data using openBIS V3 API.\n## IMPORTANT:\nPlease note that it is normal for rich text property values (i.e. properties with custom widget set to “Word Processor”) to contain HTML tags. HTML is used for styling/formatting of such property values. The tool should not normally be used for such properties.\n## Tool usage:\nShow statistics about property values that contain HTML tags (by default it searches for property values that start with: “\n, contain: “\n” and end with: “\n”)\njava\n-jar\nopenbis-html-properties-cleaner.jar\nstats\n--openbis-url=https://openbis-xxx.com\nList property values that contain HTML tags for a given entity kind and property type\njava\n-jar\nopenbis-html-properties-cleaner.jar\nlist\n--openbis-url=https://openbis-xxx.com\n--entity-kind=SAMPLE\n--property-type=AUTHORS\nFix property values that contain HTML tags for a given entity kind and property type\njava\n-jar\nopenbis-html-properties-cleaner.jar\nfix\n--openbis-url=https://openbis-xxx.com--entity-kind=SAMPLE\n--property-type=AUTHORS\nVersion 20.10.2 GA (General Availability)\n\nThe default value of “project-samples-enabled” paramenter in service.properties of the openbis project has been changed to “true”. This property controls whether it is possible to have samples related to a project directly.\nVersion 20.10.1 EA (Early Access)\n\n## WARNING:\nDowngrade from 20.10.1 EA to 20.10.0 RC is NOT possible (20.10.1 EA version includes database schema changes)\nThe default value of\nentity-history.enabled\nproperty has been changed to ‘true’. The property controls gathering information about deleted entities in ‘events’ table (for more details please check: Installation and Administrator Guide of the openBIS Server\nhere\n).\nProperty collect-statistics is added with “true” as the default value. When the value is “true” (and not overridden by the environment variable) openBIS sends usage statistics to a dedicated server on startup as well as on the 1st day of the month.\nIn the\nservice.properties\nfile the property collect-statistics is added with “true” as the default value. When the value is “true” (and not overridden by the environment variable) openBIS sends usage statistics to a dedicated server on startup as well as on the 1st day of the month. Usage statistics include only openBIS version, country and number of users.\n## DISABLE_OPENBIS_STATISTICS\nenvironment variable, when set to “true” the statistics is not collected regardless of the value of the property collect-statistics.\nRelease 20.10.0 RC\n\nThis release includes migration to a new version of Postgres, as well as other technology upgrades that reduce the amount of flags and configuration keys needed (configuration files included on new installations don’t have them).\nBefore you upgrade, you need to take care of:\n## Technology Upgrade: Postgres 11\n\nIf the installer was used: It is required to upgrade the database directory format manually. Please check the\nofficial documentation\nIf the docker image was used:\nLog in interactively into your 19.06.5 Docker container and make binary database dumps with the same name of the databases openbis_prod and pathinfo_prod into the ROOT of your mounted state directory.\nStop your 19.06.5 Docker container.\nRename postgresql_data into the state directory to postgresql_data_19_6 since it is not compatible with Postgres 11. It is not recommended to delete it until you are sure you are not going to need it.\nStart your 20.10.0 Docker container, it should detect the presence of openbis_prod and  pathinfo_prod, automatically populating the database from them.\nWhen the process is finish test that the system looks like you expect.\nFeel free to Delete or move somewhere else the openbis_prod and  pathinfo_prod backups so the upgrade procedure is not repeated.\nTechnology Upgrade: Java 11\n\nUpgrading to Java 11 forces to remove some old flags on the next files, Java 11 will fail to start with them:\nopenbis.conf : remove -d64\ndatastore_server.conf : remove -d64\n## Technology Upgrade: Search Engine\n\nNow configuration keys related with the old one are ignored, the next configuration keys will be ignored and can be removed from AS service.properties:\nhibernate\n.\nsearch\n.\nindex\n-\nbase\nhibernate\n.\nsearch\n.\nindex\n-\nmode\nhibernate\n.\nsearch\n.\nbatch\n-\nsize\nhibernate\n.\nsearch\n.\nmaxResults\nhibernate\n.\nsearch\n.\nworker\n.\nexecution\nhibernate\n.\nbatch\n.\nsessionCache\n.\nmaxEntities", "timestamp": "2025-09-18T09:38:29.846539Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_archiving-datasets:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/archiving-datasets.html", "repo": "openbis", "title": "Archiving Datasets", "section": "Archiving Datasets", "text": "## Archiving Datasets\n\nManual archiving\n\nopenBIS core UI\n\nArchiving can be triggered by doing the following steps:\ngo to an experiment/collection or an object.\nswitch to the tab “Data Sets”. There will be in the lower right corner the button ‘Archiving’.\nclick on the button and choose either ‘Copy to Archive’ or ‘Move to Archive’.\nif you did not select any data set, all data sets will be archived. If you have selected some data sets you can choose if you want to archive only them or all the data sets accessible in the table.\nBecause archiving does not happens immediately the status (called ‘Archiving Status’ in data set tables) of the data sets will be changed to BACKUP_PENDING or ARCHIVE_PENDING.\nTo make archived data sets available again repeat the steps, but choose ‘Unarchive’.\nIf you want to disallow archiving, choose ‘Lock’. Remember that you can do this only for available data sets. The ‘Archiving Status’ will change to ‘AVAILABLE (LOCKED)’. To make archiving possible again, choose ‘Unlock’.\nNote that the recommended way is to\nnot use\nthe core-UI for archiving, but to use the ELN-LIMS for this, as detailed below.\n## ELN-LIMS\n\nInstead of triggering archiving direclty, via the ELN archiving can only be\nrequested\n. The maintenance task\nArchivingByRequestTask\nis required. It triggers the actual archiving. For details on archiving and unarchiving via ELN UI see\narchive\nAutomatic archiving\n\nArchiving can be automated by the Auto Archiver. This is a\nmaintenance task\nwhich triggers archiving of data sets fullfulling some conditions (e.g. not accessed since a while). Note that the auto archiver does not itself perform archiving. It only automates the selection of data sets to be archived. For all configuration parameters see\nAutoArchiverTask\n.\n## Archiving Policies\n\nAn archiving policy can be set up to select from all non-archived data sets candidates data sets to be archived. These are either data sets not accessed since some days or data sets marked by a tag. If nothing is specified, all candidates will be archived.\nThe policy can be specified by\npolicy.class\nproperty. It has to be the fully-qualified name of a Java class implementing\nch.systemsx.cisd.etlserver.IAutoArchiverPolicy\n. All properties starting with\npolicy.\nspecify the policy further.\nch.systemsx.cisd.etlserver.plugins.GroupingPolicy\n\n## Description\n: Policy which tries to find a group of data sets with a total size from a specified interval. This is important in case of\nMulti Data Set Archiving\n. Grouping can be defined by space, project, collection, object, data set type or a combination of those. Groups can be merged if they are too small. Several grouping keys can be specified.\nSearching for an appropriate group of data sets for auto archiving is logged. If no group could be found an admin is notified via email (email address specified in\nlog.xml\n). The email contains the search log.\n### Configuration\n## :\n## Property Key\n## Description\nminimal-archive-size\nThe total size (in bytes) of the selected data sets has to be equal or the larger than this value. Default: 0\nmaximal-archive-size\nThe total size (in bytes) of the selected data sets has to be equal or the less than this value. Default: Unlimited\ngrouping-keys\nComma separated list of grouping keys. A grouping key has the following form: <basic key 1>#<basic key 2>#…#\n[:merge] A basic key is from the following vocabulary: All, Space, Project, Experiment, Sample, DataSetType, DataSet. All basic keys of a group key define a grouping of all data set candidates. In each group all data sets have the all attributes defined by the basic keys in common. Note, that basic key All means no grouping. For example: Experiment#DataSetType means that the candidates are grouped according to experiment and data set type. The optional :merge is used when no group fulfills the total size condition and there are at least two groups with total size below minimal-archive-size. In this case groups which are too small will be merged until the total size condition is fulfilled. If a grouping key doesn’t lead to a group of data set fulfilling the total size condition the next grouping key is used until a matching group is found. If for a grouping key more than one matching group is found the oldest one will be chosen. If merging applies for more than two groups the oldest groups will be merged first. The age of a group is defined by the most recent access time stamp. Examples: Grouping policy by experiment: DataSetType#Experiment, DataSetType#Project, DataSetType#Experiment#Sample Grouping policy by space: DataSetType#Space, DataSetType#Project:merge, DataSetType#Experiment:merge, DataSetType#Experiment#Sample:merge, DataSet:merge\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask\ninterval = 10 days\narchive-candidate-discoverer.class = ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer\narchive-candidate-discoverer.tags = /admin-user/archive\npolicy.class = ch.systemsx.cisd.etlserver.plugins.GroupingPolicy\npolicy.minimal-archive-size =  30000000000\npolicy.maximal-archive-size = 150000000000\npolicy.grouping-keys = Space#DataSetType, Experiment#Sample:merge\nIn this example the candidates are unarchived data sets which have been\ntagged by the user\nadmin-user\nwith the tag\narchive\n. The policy tries to\nfind a group of data sets with total size between 30 Gb and 150 Gb. It\nfirst looks for groups where all data sets are of the same type and from\nthe same space. If no group is found it tries to find groups where all\ndata sets are from the same experiment and sample (data set with no\nsamples are assigned to\nno_sample\n). If no matching groups are found\nand at least two groups are below the minimum the policy tries to merge\ngroups to a bigger group until the bigger group match the size\ncondition. If no group can be found an email will be sent describing in\ndetail the several steps of finding a matching group.", "timestamp": "2025-09-18T09:38:29.849542Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_authentication-systems:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/authentication-systems.html", "repo": "openbis", "title": "Authentication Systems", "section": "Authentication Systems", "text": "## Authentication Systems\n\nopenBIS currently supports 3 authentication systems: a\nself-contained system based on a UNIX-like passwd file, LDAP, and Single Sign\nOn (e.g., through SWITCHaai). Beside these, there are also so called\nstacked\nauthentication methods\navailable. Stacked authentication methods use\nmultiple authentication systems in the order indicated by the name. The\nfirst authentication system being able to provide an entry for a\nparticular user id will be used. If you need full control over what\nauthentication systems are used in what order, you can define your own\nstacked authentication service in the Spring application context file:\n$INSTALL_PATH/servers/openBIS-server/jetty/webapps/openbis/WEB-INF/classes/genericCommonContext.xml.\nThe default authentication configuration\n\nIn the template service properties, we set\nauthentication-service\n= file-ldap-caching-authentication-service\n,\nwhich means that file-based authentication, and LDAP are used for\nauthentication, in this order. As LDAP are not configured in\nthe template service properties, this effectively corresponds\nto\nfile-authentication-service\n, however when LDAP is\nconfigured, they are picked up on server start and are used to\nauthenticate users when they are not found in the local\npasswd\nfile.\nFurthermore, as it is a caching authentication service, it will cache\nauthentication results from LDAP in file\n$INSTALL_PATH/servers/openBIS-server/jetty/etc/passwd_cache\n. See section\n## Authentication Cache\nbelow for details on this caching.\nThe file based authentication system\n\nThis authentication schema uses the file\n$INSTALL_PATH/servers/openBIS-server/jetty/etc/passwd\nto determine whether a login to the\nsystem is successful or not.\nThe script\n$INSTALL_PATH/servers/openBIS-server/jetty/bin/passwd.sh\ncan be used to maintain\nthis file. This script supports the options:\npasswd\nlist\n|\n[\nremove\n|\nshow\n|\ntest\n]\n<userid>\n|\n[\nadd\n|\nchange\n]\n<userid>\n[\noption\n[\n...\n]]\n--help\n## :\n## Prints\nout\na\ndescription\nof\nthe\noptions.\n[\n-P,--change-password\n]\n## :\n## Read\nthe\nnew\npassword\nfrom\nthe\nconsole,\n[\n-e,--email\n]\n## VAL\n## :\n## Email\naddress\nof\nthe\nuser.\n[\n-f,--first-name\n]\n## VAL\n## :\n## First\nname\nof\nthe\nuser.\n[\n-l,--last-name\n]\n## VAL\n## :\n## Last\nname\nof\nthe\nuser.\n[\n-p,--password\n]\n## VAL\n## :\n## The\npassword.\nA new user can be added with\nprompt>\npasswd.sh\nadd\n[\n-f\n<first\nname>\n]\n[\n-l\n<last\nname>\n]\n[\n-e\n<email>\n]\n[\n-p\n<password>\n]\n<username>\nIf no password is provided with the\n-p\noption, the system will ask for\na password of the new user on the console. Please note that providing a\npassword on the command line can be a security risk, because the\npassword can be found in the shell history, and, for a short time, in\nthe\nps\ntable. Thus\n-p\nis not recommended in normal operation.\nThe password of a user can be tested with\nprompt>\npasswd.sh\ntest\n<username>\nThe system will ask for the current password on the console and then\nprint whether the user was authenticated successfully or not.\nAn account can be changed with\nprompt>\npasswd.sh\nchange\n[\n-f\n<first\nname>\n]\n[\n-l\n<last\nname>\n]\n[\n-e\n<email>\n]\n[\n## -P\n]\n<username>\nAn account can be removed with\nprompt>\npasswd.sh\nremove\n<username>\nThe details of an account can be queried with\nprompt>\npasswd.sh\nshow\n<username>\nAll accounts can be listed with\nprompt>\npasswd.sh\nlist\nThe password file contains each user in a separate line. The fields of\neach line are separated by colon and contain (in this order):\n## User Id\n,\n## Email Address\n,\n## First Name\n,\n## Last Name\nand\n## Password Hash\n.\n## The\n## Password Hash\nfield represents the\nsalted\n## SHA1\nhash of the user’s password in\nBASE64 encoding\n.\nThe interface to LDAP\n\nTo work with an LDAP server, you need to provide the server URL with\n(example) and set the\nauthentication-service\n=\nldap-authentication-service\nldap.server.url\n=\nldap://d.ethz.ch/DC\n=\nd,DC\n=\nethz,DC\n=\nch\nand the details of an LDAP account who is allowed to make queries on the\nLDAP server with (example)\nldap.security.principal.distinguished.name\n=\n## CN\n=\ncarl,OU\n=\nEthUsers,DC\n=\nd,DC\n=\nethz,DC\n=\nch\nldap.security.principal.password\n=\nCarls_LDAP_Password\nNote: A space-separated list of URLs can be provided if distinguished\nname and password  are valid for all specified LDAP servers.\n## Authentication Cache\n\nIf configuring a caching authentication service like\nfile-ldap-caching-authentication-service\n, authentication results\nfrom remote authentication services like LDAP are cached\nlocally in the openBIS Application Server. The advantage is a faster\nlogin time on repeated logins when one or more remote authentication\nservices are slow. The disadvantage is that changes to data in the\nremote authentication system (like a changed password or email address)\nare becoming known to openBIS only with a delay. In order to minimize\nthis effect, the authentication caching performs “re-validation” of\nauthentication requests asynchronously. That means it doesn’t block the\nuser from logging in because it is performed in different thread than\nthe login.\nThere are two service properties which give you control over the working\nof the authentication cache:\nauthentication.cache.time\nlets you set for how long (after putting\nit into the cache) a cache entry (read: “user name and password”)\nwill be kept if the user does not have a successful login to openBIS\nin this period of time (as successful logins will trigger\nre-validation and thus renewal of the cache entry). The default is\n28h, which means that users logging into the system every day will\nnever experience a delay from slow remote authentication systems. A\nnon-positive value will disable caching.\nauthentication.cache.time-no-revalidation\nlets you set for how\nlong (after putting it into the cache) a cache entry will\nnot\nbe\nre-validated if the login was successful. This allows you to reduce\nthe load that openBIS creates on the remote authentication servers\nfor successful logins of the same user. The default is 1h. Setting\nit to 0 will always trigger re-validation, setting it to\nauthentication.cache.time\nwill not perform re-validation at all\nand thus expire every cache entry after that time.\nAn administrator with shell access to the openBIS Application Server can\nsee and change the current cache entries in the\nfile\n$INSTALL_PATH/servers/openBIS-server/jetty/etc/passwd_cache\n. The format is the same\nas for the file-based authentication system (see section\nThe file based\nauthentication system\nabove), but has an additional field\n## Cached At\nadded to the end of each line.\n## Cached At\nis the time (in milli-seconds\nsince start of the Unix epoch, which is midnight\n## Universal Time\n## Coordinated\n, 1 January 1970) when the entry was cached. Removing a line\nfrom this file will remove the corresponding cache entry. The\nauthentication cash survives openBIS Application Server restarts because\nof this persisted file. If you need to clear the cache altogether, it is\nsufficient to remove the\npasswd_cache\nfile at any time. No server\nrestart is needed to make changes to this file take effect.\nYou can switch off authentication caching by either\nsetting\nauthentication.cache.time\n=\n-1\n, or by choosing an\nauthentication service which does not have\ncaching\nin its name.\n## Anonymous Login\n\nIn order to allow anonymous login a certain user known by openBIS has to be specified. This is\ndone by the property\nuser-for-anonymous-login\n. The value is the user\nID. The display settings and the authorization settings of this user are\nused for the anonymous login.\nAnonymous login is possible with URL parameter\nanonymous\nset to\ntrue\nor by property\ndefault-anonymous-login\nin web configuration properties\n(see\n## Web Client Customization\n). Note, that for the ELN client the property\ndefault-anonymous-login\nisn’t used. Anonymous login needs only the property\nuser-for-anonymous-login\nfor an existing user with some rights.\n## Single Sign On Authentication\n\nCurrently only Shibboleth SSO is supported. For more details see\n## Single Sign On Authentication\n.", "timestamp": "2025-09-18T09:38:29.852682Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_authorization:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/authorization.html", "repo": "openbis", "title": "Authorization", "section": "Authorization", "text": "## Authorization\n\nAuthorization is a logic that decides whether a given user is allowed to perform a given operation on a given resource. In openBIS authorization decides if entities like spaces, projects, experiments, objects, datasets, can be created, read, updated or deleted by a given user.\nSimilar to other IT systems, openBIS access rights can be defined only for groups of resources rather than for each individual resource separately. In openBIS the purpose of such groups is served by spaces and projects. It means that an openBIS user can be given access to a specific space or a specific project (and all the entities that belong to that space or to that project) but cannot be given access just to a single experiment, object or dataset from that space or that project.\nApart from access to a space or a project, a user can be given openBIS instance access rights. With such rights a user can access any space and any project within that openBIS installation.\nHaving defined the 3 scopes (i.e. instance, space and project), we need to learn how to control what operations a user can perform on entities that belong to these scopes. This aspect in openBIS is controlled with “roles”. There are 4 roles available:\nOBSERVER - can see objects in a given scope\nUSER - as OBSERVER + can create/update objects in a given scope\nPOWER_USER -  as USER + can delete objects in a given scope\nADMIN - as POWER_USER + update/delete the scope itself + assign rights within the scope\nThe above roles together with instance, space and project scopes that we have defined earlier give us the following combinations:\nPROJECT_OBSERVER - can see the project and all the entities that belong to the project\nPROJECT_USER - as PROJECT_OBSERVER + can create/update entities in the project\nPROJECT_POWER_USER - as PROJECT_USER + can delete entities in the project\nPROJECT_ADMIN - as PROJECT_POWER_USER + can update/delete the project + assign rights to the project\nSPACE_OBSERVER - can see the space and all the entities that belong to the space\nSPACE_USER - as SPACE_OBSERVER + can create/update entities in the space\nSPACE_POWER_USER - as SPACE_USER + can delete entities in the space\nSPACE_ADMIN - as SPACE_POWER_USER + can update/delete the space + can assign rights to the space\nINSTANCE_OBSERVER - can see everything\nINSTANCE_ADMIN - space admin rights to all spaces + can create openBIS types (masterdata)\nPlease note that instance scope can be combined only with OBSERVER and ADMIN roles.\nWARNING: The project scope is disabled by default. To enable it for all users you have to change openBIS service.properties as follows:\nauthorization.project-level.enabled = true\nauthorization.project-level.users = .*\nThe “enabled” property controls whether the project scope can be used in general, while the “users” property defines exactly which users can use it. Setting “enabled” property to “true” will only make the project roles appear in “Roles” configuration tool in openBIS generic UI. These roles can be then assigned to users and saved. Still these roles won’t be used until a name of the user they are defined for matches the “users” regexp.\nLast part of the openBIS authorization puzzle are users and user groups. So far we always assumed that a scope and a role will be directly assigned to a user, e.g. “John Doe” is an ADMIN of space “TEST”. Such an approach is absolutely fine and works great until the number of users we have to manage is relatively small. As the user base grows and so the maintenance overhead, it becomes handy to find users with the same access rights, put them into a user group and assign the rights to the user group rather than to each individual user. This way by simply assigning a user to a group we give him/her all the rights that are defined for that group. It leads to a simpler, more consistent and easier to maintain configuration.", "timestamp": "2025-09-18T09:38:29.857252Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/index.html", "repo": "openbis", "title": "Advanced Configuration", "section": "Advanced Configuration", "text": "### Advanced Configuration\n\n### openBIS Server Configuration\n### Application Server Configuration\n## Database Settings\nData Store Server Configuration\n### Optional Application Server Configuration\nThe base URL for Web client access to the data store server.\nExport data limit in bytes, default to 10Gib\n## Deleted Entity History\n## Login Page - Banners\n## Client Customization\n### Configuration\nWeb client customizations\nData Set Upload Client Customizations\n## Examples\nFull web-client.properties Example\n## Configuring File Servlet\nChanging the Capability-Role map\nCapability Role Map for V3 API\n### Optional Datastore Server Configuration\nConfiguring DSS Data Sources\n## Authentication Systems\nThe default authentication configuration\nThe file based authentication system\nThe interface to LDAP\n## Authentication Cache\n## Anonymous Login\n## Single Sign On Authentication\n## Authorization\n## Maintenance Tasks\n## Maintenance Task Classification\n## Introduction\n## Feature\nArchivingByRequestTask\nAutoArchiverTask\nBlastDatabaseCreationMaintenanceTask\nDeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask\nReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask\nDeleteFromArchiveMaintenanceTask\nDeleteFromExternalDBMaintenanceTask\nEventsSearchMaintenanceTask\nExperimentBasedArchivingTask\nHierarchicalStorageUpdater\nMultiDataSetDeletionMaintenanceTask\nMultiDataSetUnarchivingMaintenanceTask\nMultiDataSetArchiveSanityCheckMaintenanceTask\nPathInfoDatabaseFeedingTask\nPostRegistrationMaintenanceTask\nRevokeUserAccessMaintenanceTask\nUserManagementMaintenanceTask\nConsistency and other Reports\nDataSetArchiverOrphanFinderTask\nDataSetAndPathInfoDBConsistencyCheckTask\nMaterialExternalDBSyncTask\n## Mapping File\n### UsageReportingTask\nPersonalAccessTokenValidityWarningTask\nConsistency Repair and Manual Migrations\nBatchSampleRegistrationTempCodeUpdaterTask\nCleanUpUnarchivingScratchShareTask\nDataSetRegistrationSummaryTask\nDynamicPropertyEvaluationMaintenanceTask\nDynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask\nFillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask\nPathInfoDatabaseChecksumCalculationTask\nPathInfoDatabaseRefreshingTask\nRemoveUnusedUnofficialTermsMaintenanceTask\nResetArchivePendingTask\nSessionWorkspaceCleanUpMaintenanceTask\nMaterialsMigration\n## Microscopy Maintenance Tasks\nMicroscopyThumbnailsCreationTask\nDeleteFromImagingDBMaintenanceTask\n## Proteomics Maintenance Tasks\nUser Group Management for Multi-groups openBIS Instances\n## Introduction\n### Configuration\n### Static Configurations\nAS service.properties\nDSS service.properties\n### Dynamic Configurations\n## Section\nglobalSpaces\n## Section\ncommonSpaces\n## Section\ncommonSamples\n## Section\ncommonExperiments\n## Section\ninstanceAdmins\n(since version 20.10.6)\n## Section\ngroups\nWhat UserManagementMaintenanceTask does\nContent of the Report File sent by UsageReportingTask\nCommon use cases\nAdding a new group\nMaking a user an group admin\nRemove a user from a group\nAdding more disk space\nManual configuration of Multi-groups openBIS instances\nMasterdata and entities definition\n## Spaces\n## Projects\n## Collections\n## Objects\nRights management\n## Archiving Datasets\nManual archiving\nopenBIS core UI\n## ELN-LIMS\nAutomatic archiving\n## Archiving Policies\nch.systemsx.cisd.etlserver.plugins.GroupingPolicy\nMulti data set archiving\n## Introduction\nImportant technical details\n## Workflows\nSimple workflow\nStaging workflow\nReplication workflow\nStaging and replication workflow\nClean up\n### Configuration steps\nClean up Unarchiving Scratch Share\nDeletion of archived Data Sets\nRecovery from corrupted archiving queues\nMaster data import/export\n## Querying Project Database\nCreate Read-Only User in PostgreSQL\n## Enable Querying\nConfigure Authorization for Querying\nShare IDs\n## Motivation\n## Syntax\n## Resolving Rules\n## Example\n## Sharing Databases\n## Introduction\nShare Databases without Mapping File\nShare Databases with Mapping File\nMapping all DSSs on one\nMapping all DSSs on one per module\n## Overwriting Parameters\n## Overwriting Generic Settings\nopenBIS Sync\n## Introduction\nData Source Service Configuration\nUse case: One Datasource - One or more Harvester\nData Source Service Document\n### Harvester Service Configuration\nWhat HarvesterMaintenanceTask does\nMaster Data Synchronization Rules\nopenBIS Logging\nRuntime changes to logging", "timestamp": "2025-09-18T09:38:29.859242Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_logging:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/logging.html", "repo": "openbis", "title": "openBIS Logging", "section": "Command", "text": "openBIS Logging\n\nRuntime changes to logging\n\nThe script\n$INSTALL_PATH/servers/openBIS-server/jetty/bin/configure.sh\ncan be used to change the logging behavior of openBIS application server while the server is running.\nThe script is used like this: configure.sh [command] [argument]\nThe table below describes the possible commands and their arguments.\n## Command\nArgument(s)\n## Default Value\n## Description\nlog-service-calls\n‘on’, ‘off’\n‘off’\nTurns on / off detailed service call logging.\nWhen this feature is enabled, openBIS will log about start and end of every service call it executes to file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_service_calls.txt\nlog-long-running-invocations\n‘on’, ‘off’\n‘on’\nTurns on / off logging of long running invocations.\nWhen this feature is enabled, openBIS will periodically create a report of all service calls that have been in execution more than 15 seconds to file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_long_running_threads.txt.\ndebug-db-connections\n‘on’, ‘off’\n‘off’\nTurns on / off logging about database connection pool activity.\nWhen this feature is enabled, information about every borrow and return to database connection pool is logged to openBIS main log in file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_log.txt\nlog-db-connections\nno argument / minimum connection age (in milliseconds)\n5000\nWhen this command is executed without an argument, information about every database connection that has been borrowed from the connection pool is written into openBIS main log in file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_log.txt\nIf the “minimum connection age” argument is specified, only connections that have been out of the pool longer than the specified time are logged. The minimum connection age value is given in milliseconds.\nrecord-stacktrace-db-connections\n‘on’, ‘off’\n‘off’\nTurns on / off logging of stacktraces.\nWhen this feature is enabled AND debug-db-connections is enabled, the full stack trace of the borrowing thread will be recorded with the connection pool activity logs.\nlog-db-connections-separate-log-file\n‘on’, ‘off’\n‘off’\nTurns on / off database connection pool logging to separate file.\nWhen this feature is disabled, the database connection pool activity logging is done only to openBIS main log. When this feature is enabled, the activity logging is done ALSO to file $INSTALL_PATH/servers/openBIS-server/jetty/log/openbis_db_connections.txt.", "timestamp": "2025-09-18T09:38:29.862885Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_maintenance-tasks:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/maintenance-tasks.html", "repo": "openbis", "title": "Maintenance Tasks", "section": "Maintenance Tasks", "text": "## Maintenance Tasks\n\n## Maintenance Task Classification\n\n## Category\n## Feature\nConsistency and other Reports\nConsistency Repair and Manual Migrations\n## Relevancy\n## Default\n## Relevant\n## Rare\n## Deprecated\n## Introduction\n\nA maintenance task is a process which runs once or in regular time intervals. It is defined by a\ncore plugin\nof type\nmaintenance-tasks\n. Usually a maintenance task can only run on AS or DSS but not in both environments.\nThe following properties are common for all maintenance tasks:\n## Property Key\n## Description\nclass\nThe fully-qualified Java class name of the maintenance task. The class has to implement IMaintenanceTask.\nexecute-only-once\nA flag which has to be set to true if the task should be executed only once. Default value: false\ninterval\nA time interval (in seconds) which defines the pace of execution of the maintenance task. Can be specified with one of the following time units: ms, msec, s, sec, m, min, h, hours, d, days. Default time unit is sec. Default value: one day.\nstart\nA time at which the task should be executed the first time. Format: HH:mm. where HH is a two-digit hour (in 24h notation) and mm is a two-digit minute. By default the task is execute at server startup.\nrun-schedule\nScheduling plan for task execution. Properties execute-only-once, interval, and start will be ignored if specified.\n## Crontab syntax:\n## cron:\n<second>\n<minute>\n<hour>\n<day>\n<month>\n<weekday>\n## Examples:\n## cron:\n0\n0\n*\n*\n*\n*\n: the top of every hour of every day.\n## cron:\n*/10\n*\n*\n*\n*\n*\n: every ten seconds.\n## cron:\n0\n0\n8-10\n*\n*\n*\n: 8, 9 and 10 o’clock of every day.\n## cron:\n0\n0\n6,19\n*\n*\n*\n: 6:00 AM and 7:00 PM every day.\n## cron:\n0\n0/30\n8-10\n*\n*\n*\n: 8:00, 8:30, 9:00, 9:30, 10:00 and 10:30 every day.\n## cron:\n0\n0\n9-17\n*\n*\n## MON-FRI\n: on the hour nine-to-five weekdays.\n## cron:\n0\n0\n0\n25\n12\n?\n: every Christmas Day at midnight.\n## Non-crontab syntax:\nComma-separated list of definitions with following syntax:\n[[<counter>.]<week\nday>]\n[<month\nday>[.<month>]]\n<hour>[:<minute>]\nwhere\n<counter>\ncounts the specified week day of the month.\n<week\nday>\nis\n## MO\n,\n## MON\n,\n## TU\n,\n## TUE\n,\n## WE\n,\n## WED\n,\n## TH\n,\n## THU\n,\n## FR\n,\n## FRI\n,\n## SA\n,\n## SAT\n,\n## SU\n, or\n## SUN\n(ignoring case).\n<month>\nis either the month number (followed by an optionl ‘.’) or\n## JAN\n,\n## FEB\n,\n## MAR\n,\n## APR\n,\n## MAY\n,\n## JUN\n,\n## JUL\n,\n## AUG\n,\n## SEP\n,\n## OCT\n,\n## NOV\n, or\n## DEC\n(ignoring case).\n## Examples:\n6,\n18\n: every day at 6 AM and 6 PM.\n## 3.FR\n22:15\n: every third friday of a month at 22:15.\n1.\n15:50\n: every first day of a month at 3:50 PM.\n## SAT\n1:30\n: every saturday at 1:30 AM.\n## 1.Jan\n5:15,\n1.4.\n5:15,\n1.7\n5:15,\n1.\n## OCT\n5:15\n: every first day of a quarter at 5:15 AM.\nrun-schedule-file\nFile where the timestamp for next execution is stored. It is used if run-schedule is specified. Default:\n<installation\nfolder>/<plugin\nname>_<class\nname>\nretry-intervals-after-failure\nOptional comma-separated list of time intervals (format as for interval) after which a failed execution will be retried. Note, that a maintenance task will be execute always when the next scheduled timepoint occurs. This feature allows to execute a task much earlier in case of temporary errors (e.g. temporary unavailibity of another server).\n## Feature\n\nArchivingByRequestTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Triggers archiving for data sets where the ‘requested\narchiving’ flag is set. Waits with archiving until enough data sets for\na group come together. This is necessary for taped-base archiving where\nthe files to be stored have to be larger than a minimum size.\n### Configuration\n## :\n## Property Key\n## Description\nkeep-in-store\nIf true the archived data set will not be removed from the store. That is, only a backup will be created. Default: false\nminimum-container-size-in-bytes\nMinimum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 10 GB\nmaximum-container-size-in-bytes\nMaximum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 80 GB\nconfiguration-file-path\nPath to the configuration file as used by User Group Management. Here only the group keys are needed. They define a set of groups. If there is no configuration file at the specified path this set is empty.A data set requested for archiving belongs the a specified group if its space starts with the group key followed by an underscore character ‘_’. Otherwise it belongs to no group. This maintenance task triggers archiving an archive container with one or more data set from the same group if the container fits the specified minimum and maximum size. Note, that data sets which do not belong to a group are handled as a group too. If a data set is larger than the maximum container size it will be archived even though the container is to large. The group key (in lower case) is provided to the archiver. The Multi Data Set Archiver will use this for storing the archive container in a sub folder of the same name.\n## Default:\netc/user-management-maintenance-config.json\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.ArchivingByRequestTask\ninterval = 1 d\nminimum-container-size-in-bytes =  20000000000\nmaximum-container-size-in-bytes = 200000000000\nconfiguration-file-path = ../../../data/groups.json\n## Notes:\nIn practice every instance using multi dataset archiving\nfeature and also the ELN-LIMS should have this enabled.\nAutoArchiverTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Triggers archiving of data sets that have not been\narchived yet.\n### Configuration\n## :\n## Property Key\n## Description\nremove-datasets-from-store\nIf true the archived data set will be removed from the store. Default: false\ndata-set-type\nData set type of the data sets to be archived. If undefined all data set of all types might be archived.\nolder-than\nMinimum number of days a data set to be archived hasn’t been accessed. Default: 30\narchive-candidate-discoverer.class\nDiscoverer of candidates to be archived:\nch.systemsx.cisd.etlserver.plugins.AgeArchiveCandidateDiscoverer\n: All data sets with an access time stamp older than specified by property older-than are candidates. This is the default discoverer.\nch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer\n: All data sets which are marked by one of the tags specified by the property\narchive-candidate-discoverer.tags\nare candidates.\npolicy.class\nA policy specifies which data set candidates should be archived. If undefined all candidates will be archived. Has to be a fully-qualified name of a Java class implementing ch.systemsx.cisd.etlserver.IAutoArchiverPolicy.\npolicy.*\nProperties specific for the policy specified by\npolicy.class\n. More about policies can be found here.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask\ninterval = 10 days\narchive-candidate-discoverer.class = ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer\narchive-candidate-discoverer.tags = /admin-user/archive\npolicy.class = ch.systemsx.cisd.etlserver.plugins.GroupingPolicy\npolicy.minimal-archive-size = 1500000\npolicy.maximal-archive-size = 3000000\npolicy.grouping-keys = Space#DataSetType, Space#Experiment:merge\nBlastDatabaseCreationMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\nDefault (ELN-LIMS)\n## Description\n: Creates BLAST databases from FASTA and FASTQ files of\ndata sets and/or properties of experiments, samples, and data sets.\nThe title of all entries of the FASTA and FASTQ files will be extended\nby the string\n## [Data\n## set:\n<data\nset\ncode>,\n## File:\n<path>]\n## . Sequences\nprovide by an entity property will have identifiers of the form\n<entity\nkind>+<perm\nid>+<property\ntype>+<time\nstamp>\n. This allows to\ndetermine where the matching sequences are stored in openBIS. A sequence\ncan be a nucleic acid sequence or an amino acid sequence.\nFor each data set a BLAST nucl and prot databases will be created (if\nnot empty) by the tool\nmakeblastdb\n. For all entities of a specified\nkind and type one BLAST database (one for nucleic sequences and one\nfor amino acid sequences) will be created from the plain sequences\nstored in the specified property (white spaces will be removed). In\naddition an index is created by the tool\nmakembindex\nif the sequence\nfile of the database (file type\n.nsq\n) is larger than 1MB. The name of\nthe databases are\n<data\nset\ncode>-nucl/prot\nand\n<entity\nkind>+<entity\ntype\ncode>+<property\ntype\ncode>+<time\nstamp>-nucl/prot\n.\nThese databases are referred in the virtual database\nall-nucl\n## (file:\nall-nucl.nal\n) and\nall-prot\n## (file:\nall-prot.pal\n).\nIf a data set is deleted the corresponding BLAST nucl and prot databases\nwill be automatically removed the next time this maintenance task runs.\nIf an entity of specified type has been modified the BLAST databases\nwill be recalculated the next time this maintenance task runs.\nWorks only if BLAST+ tool suite has been installed. BLAST+ can be\ndownloaded from\nftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/\n## Notes:\nIt comes pre-configured with the ELN-LIMS but if additional\nproperties need to scanned they should be added to the plugin.properties\n### Configuration\n## :\n## Property Key\n## Description\ndataset-types\nComma-separated list of regular expressions of data set types. All FASTA and FASTQ files from those data sets are handled. All data sets of types not matching at least one of the regular expression are not handled.\nentity-sequence-properties\nComma-separated list of descriptions of entity properties with sequences. A description is of the form\n<entity\nkind>+<entity\ntype\ncode>+<property\ntype\ncode>\nwhere\n<entity\nkind>\nis either\n## EXPERIMENT\n,\n## SAMPLE\nor\n## DATA_SET\n(Materials are not supported).\nfile-types\nSpace separated list of file types. Data set files of those file types have to be FASTA or FASTQ files. Default:\n.fasta\n.fa\n.fsa\n.fastq\nblast-tools-directory\nPath in the file system where all BLAST tools are located. If it is not specified or empty the tools directory has to be in the PATH environment variable.\nblast-databases-folder\nPath to the folder where all BLAST databases are stored. Default:\n<data\nstore\nroot>/blast-databases\nblast-temp-folder\nPath to the folder where temporary FASTA files are stored.  Default:\n<blast-databases-folder>/tmp\nlast-seen-data-set-file\nPath to the file which stores the id of the last seen data set. Default:\n<data\nstore\nroot>/last-seen-data-set-for-BLAST-database-creation\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.BlastDatabaseCreationMaintenanceTask\ninterval = 1 h\ndataset-types = BLAST-.+\nentity-sequence-properties = SAMPLE+OLIGO+SEQUENCE, EXPERIMENT+YEAST+PLASMID_SEQUENCE\nblast-tools-directory = /usr/local/ncbi/blast/bin\nDeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Deletes data sets which have been deleted on AS.\n## Note\nIf this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.\n### Configuration\n## :\n## Property Key\n## Description\nlast-seen-data-set-file\nPath to a file which will store the code of the last data set handled. Default:\ndeleteDatasetsAlreadyDeletedFromApplicationServerTaskLastSeen\ntiming-parameters.max-retries\nMaximum number of retries in case of currently not available filesystem of the share containing the data set. Default:11\ntiming-parameters.failure-interval\nWaiting time (in seconds) between retries. Default: 10\nchunk-size\nNumber of data sets deleted together. The task is split into deletion tasks with maximum number of data sets. Default: No chunk size. That is, all data sets to be deleted are deleted in one go.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask\ninterval = 60\nlast-seen-data-set-file = lastSeenDataSetForDeletion.txt\nReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Releases data set locks held by dead threads (e.g. threads that have been finished after a thread pool had been shrunk).\n## Note\nIf this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.ReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask\ninterval = 60\nDeleteFromArchiveMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Deletes archived data sets which have been deleted on\nAS. This tasks needs the archive plugin to be configured in\nservice.properties.\n## This\ntask\nonly\nworks\nwith\nnon\nmulti\ndata\nset\narchivers.\n### Configuration\n## :\n## Property Key\n## Description\nstatus-filename\nPath to a file which will store the technical ID of the last data set deletion event on AS.\nchunk-size\nMaximum number of entries deleted in one maintenance task run. Default: Unlimited\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DeleteFromArchiveMaintenanceTask\ninterval = 3600\nstatus-filename = ../archive-cleanup-status.txt\nDeleteFromExternalDBMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Deletes database entries which are related to data sets\ndeleted in AS. The database is can be any relational database accessible\nby DSS.\n### Configuration\n## :\n## Property Key\n## Description\ndata-source\nKey of a data source configured in\nservice.properties\nor in a core plugin of type ‘data-sources’. A data source defines the credentials to access the database.\nsynchronization-table\nName of the table which stores the technical ID of the last data set deletion event on AS. This is ID is used to ask AS for all new data set deletion events. Default value:\n## EVENTS\nlast-seen-event-id-column\nName of the column in the database table defined by property\nsynchronization-table\nwhich stores the ID of the last data set deletion event. Default value:\n## LAST_SEEN_DELETION_EVENT_ID\ndata-set-table-name\nComma-separated list of table names which contain stuff related to data sets to be deleted. In case of cascading deletion only the tables at the beginning of the cascade should be mentioned. Default value:\nimage_data_sets\n,\nanalysis_data_sets\n.\ndata-set-perm-id\nName of the column in all tables defined by\ndata-set-table-name\nwhich stores the data set code. Default value:\n## PERM_ID\nchunk-size\nMaximum number of entries deleted in one maintenance task run. Default: Unlimited\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DeleteFromExternalDBMaintenanceTask\ninterval = 300\ndata-source = proteomics-db\ndata-set-table-name = data_sets\nEventsSearchMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Default\n## Description\n: Populates EVENTS_SEARCH database table basing on\nentries from EVENTS database table. EVENTS_SEARCH table contains the\nsame information as EVENTS table but in a more search friendly format\n(e.g. a single entry in EVENTS table may represent a deletion of\nmultiple objects deleted at the same time, in EVENT_SEARCH table such\nentry is split into separate entries - one for each deleted object.).\nThis is set up automatically.\n### Configuration:\nThere are no specific configuration parameters for this task.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.events_search.EventsSearchMaintenanceTask\ninterval = 1 day\nExperimentBasedArchivingTask\n\n## Environment\n## : DSS\n## Relevancy:\nrare, used when no MultiDataSetArchiver is used and\nAutoArchiverTask is too complex.\n## Description\n: Archives all data sets of experiments which fulfill\nsome criteria. This tasks needs the archive plugin to be configured in\nservice.properties\n.\n### Configuration\n## :\n## Property Key\n## Description\nexcluded-data-set-types\nComma-separated list of data set types. Data sets of such types are not archived. Default: No data set type is excluded.\nestimated-data-set-size-in-KB.\nSpecifies for the data set type\nthe average size in KB. If\nis DEFAULT it will be used for all data set types with unspecified estimated size.\nfree-space-provider.class\nFully qualified class name of the free space provider (implementing\nch.systemsx.cisd.common.filesystem.IFreeSpaceProvider\n). Depending on the free space provider additional properties, all starting with prefix\nfree-space-provider\n## .,  might be needed. Default:\nch.systemsx.cisd.common.filesystem.SimpleFreeSpaceProvider\nmonitored-dir\nPath to the directory to be monitored by the free space provider.\nminimum-free-space-in-MB\nMinimum free space in MB. If the free space is below this limit the task archives data sets. Default: 1 GB\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.ExperimentBasedArchivingTask\ninterval = 86400\nminimum-free-space-in-MB = 2048\nmonitored-dir = /my-data/\nestimated-data-set-size-in-KB.RAW_DATA = 12000\nestimated-data-set-size-in-KB.DEFAULT = 35000\nIf there is not enough free space the task archives all data sets\nexperiment by experiment until free space is above the specified limit.\nThe oldest experiments are archived first. The age of an experiment is\ndetermined by the youngest modification/registration time stamp of all\nits data sets which are not excluded by data set type or archiving\nstatus.\nThe free space is only calculated once when the task starts to figure\nout whether archiving is necessary or not. This value is than used\ntogether with estimated data set sizes to get an estimated free space\nwhich is used for the stopping criteria. Why not calculating the free\nspace again with the free space provider after the data sets of an\nexperiment have been archived? The reason is that providing the free\nspace might be an expensive operation. This is the case when archiving\nmeans removing data from a database which have been fed by data from\ndata sets of certain type. In this case archiving (i.e. deleting) those\ndata in the database do not automatically frees disk space because\nfreeing disk space is for databases often an expensive operation.\nThe DSS admin will be informed by an e-mail about which experiments have\nbeen archived.\nHierarchicalStorageUpdater\n\n## Environment\n## : DSS\n## Description\n: Creates/updates a mirrot of the data store. Data set\nare organized hierachical in accordance to their experiment and samples\n## Relevancy:\n## Deprecated\n### Configuration\n## :\n## Property Key\n## Description\nstoreroot-dir-link-path\nPath to the root directory of the store as to be used for creating symbolic links. This should be used if the path to the store as seen by clients is different than seen by DSS.\nstoreroot-dir\nPath to the root directory of the store. Used if storeroot-dir-link-path is not specified.\nhierarchy-root-dir\nPath to the root directory of mirrored store.\nlink-naming-strategy.class\nFully qualified class name of the strategy to generate the hierarchy (implementing\nch.systemsx.cisd.etlserver.plugins.IHierarchicalStorageLinkNamingStrategy\n). Depending on the actual strategy additional properties, all starting with prefix\nlink-naming-strategy\n## .,  mighty be needed. Default:\nch.systemsx.cisd.etlserver.plugins.TemplateBasedLinkNamingStrategy\nlink-source-subpath.\nLink source subpath for the specified data set type. Only files and folder in this relative path inside a data set will be mirrored. Default: The complete data set folder will be mirroed.\nlink-from-first-child.\nFlag which specifies whether only the first child of or the complete folder (either the data set or the one specified by link-source-subpath.\n## ). Default: False\nwith-meta-data\nFlag, which specifies whether directories with meta-data.tsv and a link should be created or only links. The default behavior is to create links-only. Default: false\nlink-naming-strategy.template\nThe exact form of link paths produced by TemplateBasedLinkNamingStrategy is defined by this template.\nThe variables\ndataSet\n,\ndataSetType\n,\nsample\n,\nexperiment\n, project and space will be recognized and replaced in the actual link path.\n## Default:\n${space}\n/\n${project}\n/\n${experiment}\n/\n${dataSetType}+${sample}+${dataSet}\nlink-naming-strategy.component-template\nIf defined, specifies the form of link paths for component datasets. If undefined, component datasets links are formatted with\nlink-naming-strategy.template\n.\nWorks as\nlink-naming-strategy.template\n, but has these additional variables:\ncontainerDataSetType\n,\ncontainerDataSet\n, `containerSample.\n## Default: Undefined.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.HierarchicalStorageUpdater\nstoreroot-dir = ${root-dir}\nhierarchy-root-dir = ../../mirror\nlink-naming-strategy.template = ${space}/${project}/${experiment}/${sample}/${dataSetType}-${dataSet}\nlink-naming-strategy.component-template = ${space}/${project}/${experiment}/${containerSample}/${containerDataSetType}-${containerDataSet}/${dataSetType}-${dataSet}\nMultiDataSetDeletionMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Relevant\n## Description\n: Deletes data sets which are already deleted on AS also\nfrom multi-data-set archives. This maintenance task works only if the\nMulti Data Set Archiver\nis\nconfigured. It does the following:\nExtracts the not-deleted data sets of a TAR container with deleted\ndata sets into the store.\nMarks them as\nnot present in archive\n.\nDeletes the TAR containers with deleted data sets.\nRequests archiving of the non-deleted data sets.\nThe last step requires that the maintenance task\nArchivingByRequestTask\nis configured.\n### Configuration\n## :\n## Property Key\n## Description\nlast-seen-event-id-file\nFile which contains the last seen event id.\nmapping-file\nOptional file which maps data sets to share ids and archiving folders (for details see Mapping File for Share Ids and Archiving Folders). If not specified the first share which has enough free space and which isn’t a unarchiving scratch share will be used for extracting the not-deleted data sets.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetDeletionMaintenanceTask\ninterval = 1 d\nlast-seen-event-id-file = ${storeroot-dir}/MultiDataSetDeletionMaintenanceTask-last-seen-event-id.txt\nmapping-file = etc/mapping.tsv\n## NOTE\n: Should be configured on any instance using the multi dataset\narchiver when the archive data should be deletable.\nMultiDataSetUnarchivingMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Relevant\n## Description\n: Triggers unarchiving of multi data set archives. Is\nonly needed if the configuration property\ndelay-unarchiving\nof the\nMulti Data Set Archiver\nis\nset\ntrue\n.\nThis maintenance task allows to reduce the stress of the tape system by\notherwise random unarchiving events triggered by the users.\n### Configuration\n: No specific properties.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetUnarchivingMaintenanceTask\ninterval = 1 d\nstart = 01:00\nMultiDataSetArchiveSanityCheckMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Performs sanity check of multi data set archives. The check compares archives stored in final and replica destinations with the\ninformation kept in the path info database. The check verifies: folder structure, file sizes and checksums. In case of found inconsistencies a\nnotification email is sent to chosen email addresses.\nThe task loads a list of archives to be checked from the multi data set archiver database. The archives from the list can be checked either\nchronologically (the oldest archive is checked first) or can be processed in a random order (see\ncheck-in-random-order\nproperty). The list of\narchives can be narrowed down to a specific time window (see\ncheck-from-date\nand\ncheck-to-date\nproperties). During a single run the task can check\nall the archives from the list or just a chosen number of archives (see\nrun-size\nproperty).\nThe task provides additional properties (see\nrun-probability\nand\nrun-max-random-delay\n) that can be used to introduce some randomness to the time\nthe task executes. This can be especially handy in cases when we want to have multiple instances of openBIS that share the same configuration for the\ntask but at the same time we want to desynchronize the task execution among these instances not to cause a peak load on the common archive storage.\n## Warning\nThe task assumes MultiDataSetArchiver task is configured (the\ntask uses some of the multi data set archiver configuration properties\ne.g. final destination location).\n### Configuration\n## :\n## Property Key\n## Mandatory\n## Default Value\n## Description\nstatus-file\ntrue\nPath to a JSON file that keeps a list of already checked archive containers\nnotify-emails\ntrue\nList of emails to notify about problematic archive containers\ncheck-from-date\nfalse\nnull\n“From date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm\ncheck-to-date\nfalse\nnull\n“To date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm\ncheck-in-random-order\nfalse\nfalse\nIf set to “true” then archive containers are checked in a random order. If set to “false” then the containers are checked in chronological order (from the oldest to the newest). Allowed values: true / false.\nrun-probability\nfalse\n1.0\nControls the probability of a task run (0.0 value means the task run will be always skipped, 1.0 value means the task run will always be executed normally). Float values between (0,1.0] are allowed.\nrun-max-random-delay\nfalse\n0\nMaximum delay before the run. The actual delay before each run is randomly chosen from range [0, run-max-random-delay]. Different time units are allowed: sec/s, min/m, hours/h, days/d.\nrun-size\nfalse\n-1\nMaximum number of archives to be checked in a single run, where -1 means all found archives will be checked. Integer values > 0 or equal to -1 are allowed.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetArchiveSanityCheckMaintenanceTask\ninterval = 1d\ncheck-from-date = 2023-01-01 00:00:00\ncheck-to-date = 2023-12-31 23:59:59\ncheck-in-random-order = true\nrun-probability = 0.25\nrun-max-random-delay = 2h\nrun-size = 1\nnotify-emails = test1@email.com, test2@email.com\nstatus-file = ../../multi-dataset-sanity-check-statuses.json\nPathInfoDatabaseFeedingTask\n\n## Environment\n## : DSS\n## Relevancy:\nDefault, is part of the post registration task\n## Description\n: Feeds the pathinfo database with file paths of all data\nsets in the store. It can be used as a maintenance task as well as a\npost registration task. As a maintenance task it is needed to run only\nonce if a\nPostRegistrationMaintenanceTask\nis configured. This task\nassumes a data source with for ‘path-info-db’.\nIf used as a maintenance task the data sets are processed in the order\nthey are registered. The registration time stamp of the last processed\ndata set is the starting point when the task is executed next time.\n### Configuration\n## :\n## Property Key\n## Description\ncompute-checksum\n## If\ntrue\nthe CRC32 checksum (and optionally a checksum of the type specified by\nchecksum-type\n) of all files will be calculated and stored in pathinfo database. Default value:\nfalse\nchecksum-type\nOptional checksum type. If specified and\ncompute-checksum\n=\ntrue\ntwo checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by\nMessageDigest.getInstance(<checksum\ntype>)\n. For more details see\nOracle docs\n.\ndata-set-chunk-size\nNumber of data sets requested from AS in one chunk if it is used as a maintenance task. Default: 1000\nmax-number-of-chunks\nMaximum number of chunks of size data-set-chunk-size are processed if it is used as a maintenance task. If it is <= 0 and\ntime-limit\nisn’t defined all data sets are processed. Default: 0\ntime-limit\nLimit of execution time of this task if it is used as a maintenance task. The task is stopped before reading next chunk if the time has been used up. If it is specified it is an alternative way to limit the number of data sets to be processed instead of specifying\nmax-number-of-chunks\n. This parameter can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask\nexecute-only-once = true\ncompute-checksum = true\nPostRegistrationMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: A tasks which runs a sequence of so-called\npost-registration tasks for each freshly registered data set.\n### Configuration\n## :\n## Property Key\n## Description\nignore-data-sets-before-date\nDefines a registration date. All data sets registered before this date are ignored. Format:\nyyyy-MM-dd\n, where\nyyyy\nis a four-digit year,\n## MM\nis a two-digit month, and\ndd\nis a two-digit day. Default value: no restriction.\nlast-seen-data-set-file\nPath to a file which stores the code of the last data set successfully post-registered. Default value:\nlast-seen-data-set.txt\ncleanup-tasks-folder\nPath to a folder which stores serialized clean-up tasks always created before a post-registration task is executed. These clean-up tasks are executed on start up of DSS after a server crash. Default value:\nclean-up-tasks\npost-registration-tasks\nComma-separated list of keys of post-registration task configuration. Each key defines (together with a ‘.’) the prefix of all property keys defining the post-registration task. They are executed in the order their key appear in the list.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.postregistration.PostRegistrationMaintenanceTask\ninterval = 60\ncleanup-tasks-folder = ../cleanup-tasks\nignore-data-sets-before-date = 2011-01-27\nlast-seen-data-set-file = ../last-seen-data-set\npost-registration-tasks = eager-shuffling, eager-archiving\neager-shuffling.class = ch.systemsx.cisd.etlserver.postregistration.EagerShufflingTask\neager-shuffling.share-finder.class = ch.systemsx.cisd.openbis.dss.generic.shared.ExperimentBasedShareFinder\neager-archiving.class = ch.systemsx.cisd.etlserver.postregistration.ArchivingPostRegistrationTask\nRevokeUserAccessMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Check if the users are available on the configured\nauthentication services, if they are not available, are automatically\ndisabled and their id renamed with the disable date.\nFor this to work the services should be able to list the available\nusers. If you use any service that doesn’t allow it, the task\nautomatically disables itself because is impossible to know if the users\nare active or not.\n## Service\n## Compatible\nCrowdAuthenticationService\n## NO\nDummyAuthenticationService\n## NO\nNullAuthenticationService\n## NO\nFileAuthenticationService\n## YES\nLDAPAuthenticationService\n## YES\n### Configuration\n## :\nThis maintenance task automatically uses the services already configured\non the server.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.RevokeUserAccessMaintenanceTask\ninterval = 60 s\nUserManagementMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Creates users, spaces, samples, projects and\nexperiments for all members of an LDAP authorization group or an\nexplicit list of user ids. A configuration file (in JSON format) will be\nread each time this task is executed. All actions are logged in an audit\nlog file. For more details see\nUser Group Management for Multi-groups openBIS Instances\n### Configuration:\n## Property Key\n## Description\nconfiguration-file-path\nRelative or absolute path to the configuration file. Default:\netc/user-management-maintenance-config.json\naudit-log-file-path\nRelative or absolute path to the audit log file. Default:\nlogs/user-management-audit_log.txt\nshares-mapping-file-path\nRelative or absolute path to the mapping file for data store shares. This is optional. If not specified the mapping file will not be managed by this maintenance task.\nfilter-key\nKey which is used to filter LDAP results. Will be ignored if\nldap-group-query-template\n## is specified. Default value:\nou\nldap-group-query-template\nDirect LDAP query template. It should have ‘%’ character which will be replaced by an LDAP key as specified in the configuration file.\ndeactivate-unknown-users\n## If\ntrue\na user unknown by the authentication service will be deactivated. It should be set to\nfalse\nif no authenication service can be asked (like in Single-Sign-On). Default:\ntrue\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.UserManagementMaintenanceTask\nstart = 02:42\ninterval = 1 day\nConsistency and other Reports\n\nDataSetArchiverOrphanFinderTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Finds archived data sets which are no longer in openBIS\n(at least not marked as present-in-archive). A report will be created\nand sent to the specified list of e-mail addresses (mandatory\nproperty\nemail-addresses\n). The task also looks for data sets which are\npresent-in-archive but actually not found in the archive.\nThis orphan finder task only works for Multi Data Set Archiver. It\ndoesn’t work for RsyncArchiver, TarArchiver or ZipArchiver.\n### Configuration\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DataSetArchiverOrphanFinderTask\ninterval = 60 s\nemail-addresses = email1@bsse.ethz.ch, email2@bsse.ethz.ch\n## Notes:\nThis is a consistency check task. It checks consistency for\ndatasets with the flag present-in-archive.\nDataSetAndPathInfoDBConsistencyCheckTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Checks that the file information in pathinfo database\nis consistent with the information the file system provides. This is\ndone for all recently registered data sets. Note, archived data sets are\nskipped. After all data sets (in the specified checking time interval)\nhave been checked the task checks them again.\n### Configuration\n## :\n## Property Key\n## Description\nchecking-time-interval\nTime interval in the past which defines the range of data sets to be checked. That is, all data sets with registration date between now minus checking-time-interval and now will be checked. Can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n. Default value: one day.\npausing-time-point\n## Optional time point. Format:\nHH:mm\n. where\n## HH\nis a two-digit hour (in 24h notation) and\nmm\nis a two-digit minute.\nWhen specified this task stops checking after the specified pausing time point and continues when executed the next time or the next day if start or\ncontinuing-time-point\nis specified.\nAfter all data sets have been checked the task checks again all data sets started by the oldest one specified by\nchecking-time-interval\n.\ncontinuing-time-point\n## Time point where checking continous. Format:\nHH:mm\n. where\n## HH\nis a two-digit hour (in 24h notation) and\nmm\nis a two-digit minute. Ignored when\npausing-time-point\nisn’t specified. Default value: Time when the task is executed.\nchunk-size\nMaximum number of data sets retrieved from AS. Ignored when\npausing-time-point\nisn’t specified. Default value: 1000\nstate-file\nFile to store registration time stamp and code of last considered data set. This is only used when pausing-time-point has been specified. Default:\n<store\nroot>/DataSetAndPathInfoDBConsistencyCheckTask-state.txt\n## Example\n: The following example checks all data sets of the last ten\nyears. It does the check only during the night and continues next night.\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.DataSetAndPathInfoDBConsistencyCheckTask\ninterval = 1 days\nstart = 23:15\npausing-time-point = 5:00\nchecking-time-interval = 3653 days\nMaterialExternalDBSyncTask\n\n## Environment\n## : AS\n## Relevancy:\n## Deprecated\n## Description\n: Feeds a report database with recently added or modified\nmaterials.\n### Configuration\n## :\n## Property Key\n## Description\ndatabase-driver\nFully qualified name of the JDBC driver class.\ndatabase-url\nURL to access the database server.\ndatabase-username\nUser name of the database. Default: User who started openBIS AS.\ndatabase-password\nOptional password of the database user.\nmapping-file\nPath to the file containing configuration information of mapping material types and material properties to tables and columns in the report database.\nread-timestamp-sql\nThe SQL select statement which returns one column of type time stamp for the time stamp of the last report. If the result set is empty the time stamp is assumed to be 1970-01-01. If the result set has more than one row the first row is used.\nupdate-timestamp-sql\nThe SQL statement which updates or adds a time stamp. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp.\ninsert-timestamp-sql\nThe SQL statement to add a time stamp the first time. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp. Default: same as\nupdate-timestamp-sql\n.\n## Example\n## :\nservice.properties of AS\n<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.MaterialExternalDBSyncTask\n<task id>.interval = 120\n<task id>.read-timestamp-sql = select timestamp from timestamp\n<task id>.update-timestamp-sql = update timestamp set timestamp = ?\n<task id>.insert-timestamp-sql = insert into timestamp values(?)\n<task id>.mapping-file = ../report-mapping.txt\n<task id>.database-driver = org.postgresql.Driver\n<task id>.database-url = jdbc:postgresql://localhost/material_reporting\n## Mapping File\n\nThe mapping file is a text file describing the mapping of the data (i.e.\nmaterial codes and material properties) onto the report database. It\nmakes several assumptions on the database schema:\nOne table per material type. There are only table of materials to be\nreported.\nEach table has a column which contains the material code.\nThe entries are unique.\nThe material code is a string not longer than 60 characters.\nEach table has one column for each property type. Again, there are\nonly column for properties to be reported.\nThe data type of the column should match the data type of the\n## properties:\nMATERIAL:  only the material code (string) will be reported.\nMaximum length: 60\nCONTROLLEDVOCABULARY: the label (if defined) or the code will be\nreported. Maximum length: 128\nTIMESTAMP: timestamp\nINTEGER: integer of any number of bits (maximum 64).\nREAL: fixed or floating point number\nall other data types are mapped to text.\nThe general format of the mapping file is as follows:\n[\n<\n## Material\n## Type\n## Code\n>\n## :\n<\ntable\n## Name\n>\n,\n<\ncode\ncolumn\nname\n>\n]\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n...\n[\n<\n## Material\n## Type\n## Code\n>\n## :\n<\ntable\n## Name\n>\n,\n<\ncode\ncolumn\nname\n>\n]\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n...\n## Example:\nmapping.txt\n# Some comments\n[\n## GENE\n## :\n## GENE\n,\n## GENE_ID\n]\n## GENE_SYMBOLS\n## :\nsymbol\n[\n## SIRNA\n## :\nsi_rna\n,\ncode\n]\n## INHIBITOR_OF\n## :\nsuppressed_gene\n## SEQUENCE\n## :\nNucleotide_sequence\n## Some rules:\nEmpty lines and lines starting with ‘#’ will be ignored.\nTable and column names can be upper or lower case or mixed.\nMaterial type codes and property type codes have to be in upper\ncase.\n## Warning\nIf you put a foreign key constraint on the material code of one of the material properties, you need to define the constraint checking as DEFERRED in order to not get a constraint violation. The reason is that this task will\nnot\norder the\n## INSERT\nstatements by its dependencies, but in alphabetical order.\n### UsageReportingTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Creates a daily/weekly/monthly report to a list of\ne-mail recipients about the usage (i.e. creation of experiments, samples\nand data sets) by users or groups. For more details see\n## User Group\nManagement for Multi-groups openBIS\n## Instances\n.\nIn order to be able to send an e-mail the following properties in\nservice.properties\nhave to be defined:\nmail\n.\nfrom\n=\nopenbis\n@<\nhost\n>\nmail\n.\nsmtp\n.\nhost\n=\n<\n## SMTP\nhost\n>\nmail\n.\nsmtp\n.\nuser\n=\n<\ncan\nbe\nempty\n>\nmail\n.\nsmtp\n.\npassword\n=\n<\ncan\nbe\nempty\n>\n### Configuration\n## :\n## Property Key\n## Description\ninterval\nDetermines the length of period: daily if less than or equal one day, weekly if less than or equal seven days, monthly if above seven days. The actual period is always the day/week/month before the execution day\nemail-addresses\nComma-separated e-mail addresses which will receive the report as an attached text file (format: TSV).\nuser-reporting-type\nType of reporting individual user activities. Possible values are\nNONE: No reporting\nALL: Activities inside and outside groups and for all users\nOUTSIDE_GROUP_ONLY: Activities outside groups and users of no groups\nDefault: ALL\nspaces-to-be-ignored\nOptional list of comma-separated space codes of all the spaces which should be ignored for the report.\nconfiguration-file-path\nOptional configuration file defining groups.\ncount-all-entities\n## If\ntrue\nshows the number of all entities (collections, objects, data sets) in an additional column. Default:\nfalse\n## Example\n## :\n### class = ch.systemsx.cisd.openbis.generic.server.task.UsageReportingTask\ninterval = 7 days\nemail-addresses = ab@c.de, a@bc.de\nPersonalAccessTokenValidityWarningTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n### Automatic Configuration\n## :\nThis task is automatically configured, added and run with a default interval of 1 day. If needed, the default interval can be changed. In order to do\nthat please configure the task just like any other maintenance task in\ncore-plugins\nfolder.\n## Description\n: Sends out warning emails about soon to be expired PATs (Personal Access Tokens). Emails are sent to PATs owners. Each email contains\na list of PATs that have the remaining validity period shorter than the\npersonal-access-tokens-validity-warning-period\ndefined in\n## AS\nservice.properties\n. The task does not send any information about the already expired PATs. It removes them.\nFor more details on Personal Access Tokens please see\n## Personal Access Tokens\n.\nIn order to be able to send an e-mail the following properties in\nservice.properties\nhave to be defined:\nmail\n.\nfrom\n=\nopenbis\n@<\nhost\n>\nmail\n.\nsmtp\n.\nhost\n=\n<\n## SMTP\nhost\n>\nmail\n.\nsmtp\n.\nuser\n=\n<\ncan\nbe\nempty\n>\nmail\n.\nsmtp\n.\npassword\n=\n<\ncan\nbe\nempty\n>\n## Example\n## :\nclass = ch.systemsx.cisd.openbis.generic.server.pat.PersonalAccessTokenValidityWarningTask\ninterval = 7 d\nConsistency Repair and Manual Migrations\n\nBatchSampleRegistrationTempCodeUpdaterTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Replaces temporary sample codes (i.e. codes matching\nthe regular expression\nTEMP\\\\.[a-zA-Z0-9\\\\-]+\\\\.[0-9]+\n) by normal\ncodes (prefix specified by sample type plus number). This maintenance\ntask is only needed when\ncreate-continuous-sample-codes\nis set\ntrue\nin\nservice.properties\nof AS.\n## Example\n## :\nplugin.properties\nclass\n=\nch.systemsx.cisd.openbis.generic.server.task.BatchSampleRegistrationTempCodeUpdaterTask\nCleanUpUnarchivingScratchShareTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Removes data sets from the unarchiving scratch share\nwhich have status ARCHIVED and which are present in archive. For more\ndetails see\nMulti data set\narchiving\n.\n### Configuration\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.CleanUpUnarchivingScratchShareTask\ninterval = 60 s\n## Notes:\nRecommended cleanup task to run on every instance.\nDataSetRegistrationSummaryTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Sends a data set summary report to a list of e-mail\nrecipients in regular time intervals. The report contains all new data\nsets registered since the last report. Selected properties can be\nincluded into the report. The data sets are grouped by the data set\ntype.\nIn order to be able to send an e-mail the following properties in\nservice.properties\nhave to be defined:\nmail\n.\nfrom\n=\nopenbis\n@<\nhost\n>\nmail\n.\nsmtp\n.\nhost\n=\n<\n## SMTP\nhost\n>\nmail\n.\nsmtp\n.\nuser\n=\n<\ncan\nbe\nempty\n>\nmail\n.\nsmtp\n.\npassword\n=\n<\ncan\nbe\nempty\n>\n### Configuration:\n## Property Key\n## Description\ninterval\nInterval (in seconds) between regular checks whether to create a report or not. This value should be set to 86400 (1 day). Otherwise the same report might be sent twice or no report will be sent.\nstart\nTime the report will be created. A good values for this parameter is some early time in the morning like in the example below.\ndays-of-week\nComma-separated list of numbers denoting days of week (Sunday=1, Monday=2, etc.). This parameter should be used if reports should be sent weekly or more often.\ndays-of-month\nComma-separated list of numbers denoting days of month. Default value of this parameter is 1.\nemail-addresses\nComma-separated list of e-mail addresses.\nshown-data-set-properties\nOptional comma-separated list of data set properties to be included into the report.\ndata-set-types\nRestrict the report to the specified comma-separated data set types.\nconfigured-content\nUse the specified content as the body of the email.\nA report is sent at each day which is either a specified day of week or\nday of month. If only weekly reports are needed the parameter\ndays-of-month\nshould be set to an empty string.\n## Example\n## :\nservice.properties of AS\n<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.DataSetRegistrationSummaryTask\n<task id>.interval = 86400\n<task id>.start = 1:00\n<task id>.data-set-types = RAW_DATA, MZXML_DATA\n<task id>.email-addresses = albert.einstein@princeton.edu, charles.darwin@evolution.org\nThis means that on the 1st day of every month at 1:00 AM openBIS sends\nto the specified e-mail recipients a report about the data sets of types\nRAW_DATA and MZXML_DATA that have been uploaded in the previous month.\nDynamicPropertyEvaluationMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Re-evaluates dynamic properties of all entities\n### Configuration\n## :\n## Property Key\n## Description\nclass\nch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask\ninterval = 3600\nDynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Deprecated\n## Description\n: Re-evaluates dynamic properties of all samples which\nrefer via properties of type MATERIAL directly or indirectly to\nmaterials changed since the last re-evaluation.\n### Configuration\n## :\n## Property Key\n## Description\nclass\nch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask\ntimestamp-file\nPath to a file which will store the timestamp of the last evaluation. Default value:\n../../../data/DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask-timestamp.txt\n.\ninitial-timestamp\nInitial timestamp of the form\n## YYYY-MM-DD\n(e.g. 2013-09-15) which will be used the first time when the timestamp file doesn’t exist or has an invalid value. This is a mandatory property.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask\ninterval = 7 days\ninitial-timestamp = 2012-12-31\nFillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Queries openBIS database to find data sets without a\nsize filled in, then queries the pathinfo DB to see if the size info is\navailable there; if it is available, it fills in the size from the\npathinfo information. If it is not available, it does nothing. Data sets\nfrom openBIS database are fetched in chunks (see data-set-chunk-size\nproperty). After each chunk the maintenance tasks checks whether a time\nlimit has been reached (see time-limit property). If so, it stops\nprocessing. A code of the last processed data set is stored in a file\n(see last-seen-data-set-file property). The next run of the maintenance\ntask will process data sets with a code greater than the one saved in\nthe “last-seen-data-set-file”. This file is deleted periodically (see\ndelete-last-seen-data-set-file-interval) to handle a situation where\ncodes of new data sets are lexicographically smaller than the codes of\nthe old datasets. Deleting the file is also needed when pathinfo\ndatabase entries are added after a data set has been already processed\nby the maintenance task.\n### Configuration\n## :\n## Property Key\n## Description\nlast-seen-data-set-file\nPath to a file that will store a code of the last handled data set. Default value: “fillUnknownDataSetSizeTaskLastSeen”\ndelete-last-seen-data-set-file-interval\nA time interval (in seconds) which defines how often the “last-seen-data-set-file” file should be deleted. The parameter can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n. Default value: 7 days.\ndata-set-chunk-size\nNumber of data sets requested from AS in one chunk. Default: 100\ntime-limit\nLimit of execution time of this task. The task is stopped before reading next chunk if the time has been used up. This parameter can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n.\n## Example:\nplugin.properties\n<task id>.class = ch.systemsx.cisd.etlserver.plugins.FillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask\n<task id>.interval = 86400\n<task id>.data-set-chunk-size = 1000\n<task id>.time-limit = 1h\n## NOTE\n: Useful in scenarios where the path info feeding sub task of\npost registration task fails.\nPathInfoDatabaseChecksumCalculationTask\n\n## Environment\n## : DSS\n## Relevancy:\nRare, often the CRC32 is calculated during the post\nregistration.\n## Description\n: Calculates the CRC32 checksum (and optionally a\nchecksum of specified type) of all files in the pathinfo database with\nunknown checksum. This task is needed to run only once. It assumes a\ndata source for key ‘path-info-db’.\n### Configuration\n## :\n## Property Key\n## Description\nchecksum-type\nOptional checksum type. If specified two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by\nMessageDigest.getInstance(<checksum\ntype>)\n. For more details see http://docs.oracle.com/javase/8/docs/api/java/security/MessageDigest.html#getInstance-java.lang.String-.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseChecksumCalculationTask\nexecute-only-once = true\nchecksum-type = SHA-256\nPathInfoDatabaseRefreshingTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Refreshes the pathinfo database with file metadata of\nphysical and available data sets in the store. This task assumes a data\nsource with for ‘path-info-db’.\nThe data sets are processed in the inverse order they are registered.\nOnly a maximum number of data sets are processed in one run. This is\nspecified by\nchunk-size\n.\n## Warning\nUnder normal circumstances this maintenance task is never needed, because the content of a physical data set is\nnever\nchanged by openBIS itself.\nOnly in the rare cases that the content of physical data sets have to be changed this maintenance task allows to refresh the file meta data in the pathinfo database.\n### Configuration\n## :\n## Property Key\n## Description\ntime-stamp-of-youngest-data-set\nTime stamp of the youngest data set to be considered. The format has to be\n<4\ndigit\nyear>-<month>-<day>\n<hour>:<minute>:<second>\n.\ncompute-checksum\n## If\ntrue\nthe CRC32 checksum (and optionally a checksum of the type specified by\nchecksum-type\n) of all files will be calculated and stored in pathinfo database. Default value: true\nchecksum-type\nOptional checksum type. If specified and\ncompute-checksum\n=\ntrue\ntwo checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by\nMessageDigest.getInstance(<checksum\ntype>)\n. For more details see\nOracle doc\n.\nchunk-size\nNumber of data sets requested from AS in one chunk. Default: 1000\ndata-set-type\nOptional data set type. If specified, only data sets of the specified type are considered. Default: All data set types.\nstate-file\nFile to store registration time stamp and code of last considered data set. Default:\n<store\nroot>/PathInfoDatabaseRefreshingTask-state.txt\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseRefreshingTask\ninterval = 30 min\ntime-stamp-of-youngest-data-set = 2014-01-01 00:00:00\ndata-set-type = HCS_IMAGE\nRemoveUnusedUnofficialTermsMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Removes unofficial unused vocabulary terms. For more details about unofficial vocabulary terms see\n## Ad Hoc Vocabulary Terms\n.\n### Configuration:\n## Property Key\n## Description\nolder-than-days\nUnofficial terms are only deleted if they have been registered more than the specified number of days ago. Default: 7 days.\n## Example\n## :\nservice.properties of AS\n<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.RemoveUnusedUnofficialTermsMaintenanceTask\n<task id>.interval = 86400\n<task id>.older-than-days = 30\nResetArchivePendingTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: For each data set not present in archive and status\nARCHIVE_PENDING the status will be set to AVAILABLE if there is no\ncommand in the DSS data set command queues referring to it.\n### Configuration\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.ResetArchivePendingTask\ninterval = 60 s\nSessionWorkspaceCleanUpMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Default\n## Description\n: Cleans up session workspace folders of no longer active\nsessions. This maintenance plugin is automatically added by default with\na default interval of 1 hour. If a manually configured version of the\nplugin is detected then the automatic configuration is skipped.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.SessionWorkspaceCleanUpMaintenanceTask\ninterval = 1 day\nMaterialsMigration\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Migrates the Materials entities and types to use a\nSample based model using Sample Properties. It automatically creates and\nassigns sample types, properties and entities.\nIt allows to execute the migration and to delete of the old Materials\nmodel in separate steps.\nDeleting Materials and material types requires the migration to have\nbeen a success,  before the deletion a validation check is run.\n## Example\n## :\nThis maintenance task can be directly configured on the AS\nservice.properties\nservice.properties\nmaintenance-plugins = materials-migration", "timestamp": "2025-09-18T09:38:29.873212Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_maintenance-tasks:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/maintenance-tasks.html", "repo": "openbis", "title": "Maintenance Tasks", "section": "Maintenance Tasks", "text": "## Maintenance Tasks\n\n## Maintenance Task Classification\n\n## Category\n## Feature\nConsistency and other Reports\nConsistency Repair and Manual Migrations\n## Relevancy\n## Default\n## Relevant\n## Rare\n## Deprecated\n## Introduction\n\nA maintenance task is a process which runs once or in regular time intervals. It is defined by a\ncore plugin\nof type\nmaintenance-tasks\n. Usually a maintenance task can only run on AS or DSS but not in both environments.\nThe following properties are common for all maintenance tasks:\n## Property Key\n## Description\nclass\nThe fully-qualified Java class name of the maintenance task. The class has to implement IMaintenanceTask.\nexecute-only-once\nA flag which has to be set to true if the task should be executed only once. Default value: false\ninterval\nA time interval (in seconds) which defines the pace of execution of the maintenance task. Can be specified with one of the following time units: ms, msec, s, sec, m, min, h, hours, d, days. Default time unit is sec. Default value: one day.\nstart\nA time at which the task should be executed the first time. Format: HH:mm. where HH is a two-digit hour (in 24h notation) and mm is a two-digit minute. By default the task is execute at server startup.\nrun-schedule\nScheduling plan for task execution. Properties execute-only-once, interval, and start will be ignored if specified.\n## Crontab syntax:\n## cron:\n<second>\n<minute>\n<hour>\n<day>\n<month>\n<weekday>\n## Examples:\n## cron:\n0\n0\n*\n*\n*\n*\n: the top of every hour of every day.\n## cron:\n*/10\n*\n*\n*\n*\n*\n: every ten seconds.\n## cron:\n0\n0\n8-10\n*\n*\n*\n: 8, 9 and 10 o’clock of every day.\n## cron:\n0\n0\n6,19\n*\n*\n*\n: 6:00 AM and 7:00 PM every day.\n## cron:\n0\n0/30\n8-10\n*\n*\n*\n: 8:00, 8:30, 9:00, 9:30, 10:00 and 10:30 every day.\n## cron:\n0\n0\n9-17\n*\n*\n## MON-FRI\n: on the hour nine-to-five weekdays.\n## cron:\n0\n0\n0\n25\n12\n?\n: every Christmas Day at midnight.\n## Non-crontab syntax:\nComma-separated list of definitions with following syntax:\n[[<counter>.]<week\nday>]\n[<month\nday>[.<month>]]\n<hour>[:<minute>]\nwhere\n<counter>\ncounts the specified week day of the month.\n<week\nday>\nis\n## MO\n,\n## MON\n,\n## TU\n,\n## TUE\n,\n## WE\n,\n## WED\n,\n## TH\n,\n## THU\n,\n## FR\n,\n## FRI\n,\n## SA\n,\n## SAT\n,\n## SU\n, or\n## SUN\n(ignoring case).\n<month>\nis either the month number (followed by an optionl ‘.’) or\n## JAN\n,\n## FEB\n,\n## MAR\n,\n## APR\n,\n## MAY\n,\n## JUN\n,\n## JUL\n,\n## AUG\n,\n## SEP\n,\n## OCT\n,\n## NOV\n, or\n## DEC\n(ignoring case).\n## Examples:\n6,\n18\n: every day at 6 AM and 6 PM.\n## 3.FR\n22:15\n: every third friday of a month at 22:15.\n1.\n15:50\n: every first day of a month at 3:50 PM.\n## SAT\n1:30\n: every saturday at 1:30 AM.\n## 1.Jan\n5:15,\n1.4.\n5:15,\n1.7\n5:15,\n1.\n## OCT\n5:15\n: every first day of a quarter at 5:15 AM.\nrun-schedule-file\nFile where the timestamp for next execution is stored. It is used if run-schedule is specified. Default:\n<installation\nfolder>/<plugin\nname>_<class\nname>\nretry-intervals-after-failure\nOptional comma-separated list of time intervals (format as for interval) after which a failed execution will be retried. Note, that a maintenance task will be execute always when the next scheduled timepoint occurs. This feature allows to execute a task much earlier in case of temporary errors (e.g. temporary unavailibity of another server).\n## Feature\n\nArchivingByRequestTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Triggers archiving for data sets where the ‘requested\narchiving’ flag is set. Waits with archiving until enough data sets for\na group come together. This is necessary for taped-base archiving where\nthe files to be stored have to be larger than a minimum size.\n### Configuration\n## :\n## Property Key\n## Description\nkeep-in-store\nIf true the archived data set will not be removed from the store. That is, only a backup will be created. Default: false\nminimum-container-size-in-bytes\nMinimum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 10 GB\nmaximum-container-size-in-bytes\nMaximum size of an archive container which has one or more data set. This is important for Multi Data Set Archiving. Default: 80 GB\nconfiguration-file-path\nPath to the configuration file as used by User Group Management. Here only the group keys are needed. They define a set of groups. If there is no configuration file at the specified path this set is empty.A data set requested for archiving belongs the a specified group if its space starts with the group key followed by an underscore character ‘_’. Otherwise it belongs to no group. This maintenance task triggers archiving an archive container with one or more data set from the same group if the container fits the specified minimum and maximum size. Note, that data sets which do not belong to a group are handled as a group too. If a data set is larger than the maximum container size it will be archived even though the container is to large. The group key (in lower case) is provided to the archiver. The Multi Data Set Archiver will use this for storing the archive container in a sub folder of the same name.\n## Default:\netc/user-management-maintenance-config.json\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.ArchivingByRequestTask\ninterval = 1 d\nminimum-container-size-in-bytes =  20000000000\nmaximum-container-size-in-bytes = 200000000000\nconfiguration-file-path = ../../../data/groups.json\n## Notes:\nIn practice every instance using multi dataset archiving\nfeature and also the ELN-LIMS should have this enabled.\nAutoArchiverTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Triggers archiving of data sets that have not been\narchived yet.\n### Configuration\n## :\n## Property Key\n## Description\nremove-datasets-from-store\nIf true the archived data set will be removed from the store. Default: false\ndata-set-type\nData set type of the data sets to be archived. If undefined all data set of all types might be archived.\nolder-than\nMinimum number of days a data set to be archived hasn’t been accessed. Default: 30\narchive-candidate-discoverer.class\nDiscoverer of candidates to be archived:\nch.systemsx.cisd.etlserver.plugins.AgeArchiveCandidateDiscoverer\n: All data sets with an access time stamp older than specified by property older-than are candidates. This is the default discoverer.\nch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer\n: All data sets which are marked by one of the tags specified by the property\narchive-candidate-discoverer.tags\nare candidates.\npolicy.class\nA policy specifies which data set candidates should be archived. If undefined all candidates will be archived. Has to be a fully-qualified name of a Java class implementing ch.systemsx.cisd.etlserver.IAutoArchiverPolicy.\npolicy.*\nProperties specific for the policy specified by\npolicy.class\n. More about policies can be found here.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.AutoArchiverTask\ninterval = 10 days\narchive-candidate-discoverer.class = ch.systemsx.cisd.etlserver.plugins.TagArchiveCandidateDiscoverer\narchive-candidate-discoverer.tags = /admin-user/archive\npolicy.class = ch.systemsx.cisd.etlserver.plugins.GroupingPolicy\npolicy.minimal-archive-size = 1500000\npolicy.maximal-archive-size = 3000000\npolicy.grouping-keys = Space#DataSetType, Space#Experiment:merge\nBlastDatabaseCreationMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\nDefault (ELN-LIMS)\n## Description\n: Creates BLAST databases from FASTA and FASTQ files of\ndata sets and/or properties of experiments, samples, and data sets.\nThe title of all entries of the FASTA and FASTQ files will be extended\nby the string\n## [Data\n## set:\n<data\nset\ncode>,\n## File:\n<path>]\n## . Sequences\nprovide by an entity property will have identifiers of the form\n<entity\nkind>+<perm\nid>+<property\ntype>+<time\nstamp>\n. This allows to\ndetermine where the matching sequences are stored in openBIS. A sequence\ncan be a nucleic acid sequence or an amino acid sequence.\nFor each data set a BLAST nucl and prot databases will be created (if\nnot empty) by the tool\nmakeblastdb\n. For all entities of a specified\nkind and type one BLAST database (one for nucleic sequences and one\nfor amino acid sequences) will be created from the plain sequences\nstored in the specified property (white spaces will be removed). In\naddition an index is created by the tool\nmakembindex\nif the sequence\nfile of the database (file type\n.nsq\n) is larger than 1MB. The name of\nthe databases are\n<data\nset\ncode>-nucl/prot\nand\n<entity\nkind>+<entity\ntype\ncode>+<property\ntype\ncode>+<time\nstamp>-nucl/prot\n.\nThese databases are referred in the virtual database\nall-nucl\n## (file:\nall-nucl.nal\n) and\nall-prot\n## (file:\nall-prot.pal\n).\nIf a data set is deleted the corresponding BLAST nucl and prot databases\nwill be automatically removed the next time this maintenance task runs.\nIf an entity of specified type has been modified the BLAST databases\nwill be recalculated the next time this maintenance task runs.\nWorks only if BLAST+ tool suite has been installed. BLAST+ can be\ndownloaded from\nftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/\n## Notes:\nIt comes pre-configured with the ELN-LIMS but if additional\nproperties need to scanned they should be added to the plugin.properties\n### Configuration\n## :\n## Property Key\n## Description\ndataset-types\nComma-separated list of regular expressions of data set types. All FASTA and FASTQ files from those data sets are handled. All data sets of types not matching at least one of the regular expression are not handled.\nentity-sequence-properties\nComma-separated list of descriptions of entity properties with sequences. A description is of the form\n<entity\nkind>+<entity\ntype\ncode>+<property\ntype\ncode>\nwhere\n<entity\nkind>\nis either\n## EXPERIMENT\n,\n## SAMPLE\nor\n## DATA_SET\n(Materials are not supported).\nfile-types\nSpace separated list of file types. Data set files of those file types have to be FASTA or FASTQ files. Default:\n.fasta\n.fa\n.fsa\n.fastq\nblast-tools-directory\nPath in the file system where all BLAST tools are located. If it is not specified or empty the tools directory has to be in the PATH environment variable.\nblast-databases-folder\nPath to the folder where all BLAST databases are stored. Default:\n<data\nstore\nroot>/blast-databases\nblast-temp-folder\nPath to the folder where temporary FASTA files are stored.  Default:\n<blast-databases-folder>/tmp\nlast-seen-data-set-file\nPath to the file which stores the id of the last seen data set. Default:\n<data\nstore\nroot>/last-seen-data-set-for-BLAST-database-creation\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.BlastDatabaseCreationMaintenanceTask\ninterval = 1 h\ndataset-types = BLAST-.+\nentity-sequence-properties = SAMPLE+OLIGO+SEQUENCE, EXPERIMENT+YEAST+PLASMID_SEQUENCE\nblast-tools-directory = /usr/local/ncbi/blast/bin\nDeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Deletes data sets which have been deleted on AS.\n## Note\nIf this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.\n### Configuration\n## :\n## Property Key\n## Description\nlast-seen-data-set-file\nPath to a file which will store the code of the last data set handled. Default:\ndeleteDatasetsAlreadyDeletedFromApplicationServerTaskLastSeen\ntiming-parameters.max-retries\nMaximum number of retries in case of currently not available filesystem of the share containing the data set. Default:11\ntiming-parameters.failure-interval\nWaiting time (in seconds) between retries. Default: 10\nchunk-size\nNumber of data sets deleted together. The task is split into deletion tasks with maximum number of data sets. Default: No chunk size. That is, all data sets to be deleted are deleted in one go.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DeleteDataSetsAlreadyDeletedInApplicationServerMaintenanceTask\ninterval = 60\nlast-seen-data-set-file = lastSeenDataSetForDeletion.txt\nReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Releases data set locks held by dead threads (e.g. threads that have been finished after a thread pool had been shrunk).\n## Note\nIf this task isn’t configured neither in service.properties nor as a core plugin it will be established automatically by using default configuration and running every 5 minutes.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.ReleaseDataSetLocksHeldByDeadThreadsMaintenanceTask\ninterval = 60\nDeleteFromArchiveMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Deletes archived data sets which have been deleted on\nAS. This tasks needs the archive plugin to be configured in\nservice.properties.\n## This\ntask\nonly\nworks\nwith\nnon\nmulti\ndata\nset\narchivers.\n### Configuration\n## :\n## Property Key\n## Description\nstatus-filename\nPath to a file which will store the technical ID of the last data set deletion event on AS.\nchunk-size\nMaximum number of entries deleted in one maintenance task run. Default: Unlimited\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DeleteFromArchiveMaintenanceTask\ninterval = 3600\nstatus-filename = ../archive-cleanup-status.txt\nDeleteFromExternalDBMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Deletes database entries which are related to data sets\ndeleted in AS. The database is can be any relational database accessible\nby DSS.\n### Configuration\n## :\n## Property Key\n## Description\ndata-source\nKey of a data source configured in\nservice.properties\nor in a core plugin of type ‘data-sources’. A data source defines the credentials to access the database.\nsynchronization-table\nName of the table which stores the technical ID of the last data set deletion event on AS. This is ID is used to ask AS for all new data set deletion events. Default value:\n## EVENTS\nlast-seen-event-id-column\nName of the column in the database table defined by property\nsynchronization-table\nwhich stores the ID of the last data set deletion event. Default value:\n## LAST_SEEN_DELETION_EVENT_ID\ndata-set-table-name\nComma-separated list of table names which contain stuff related to data sets to be deleted. In case of cascading deletion only the tables at the beginning of the cascade should be mentioned. Default value:\nimage_data_sets\n,\nanalysis_data_sets\n.\ndata-set-perm-id\nName of the column in all tables defined by\ndata-set-table-name\nwhich stores the data set code. Default value:\n## PERM_ID\nchunk-size\nMaximum number of entries deleted in one maintenance task run. Default: Unlimited\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DeleteFromExternalDBMaintenanceTask\ninterval = 300\ndata-source = proteomics-db\ndata-set-table-name = data_sets\nEventsSearchMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Default\n## Description\n: Populates EVENTS_SEARCH database table basing on\nentries from EVENTS database table. EVENTS_SEARCH table contains the\nsame information as EVENTS table but in a more search friendly format\n(e.g. a single entry in EVENTS table may represent a deletion of\nmultiple objects deleted at the same time, in EVENT_SEARCH table such\nentry is split into separate entries - one for each deleted object.).\nThis is set up automatically.\n### Configuration:\nThere are no specific configuration parameters for this task.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.events_search.EventsSearchMaintenanceTask\ninterval = 1 day\nExperimentBasedArchivingTask\n\n## Environment\n## : DSS\n## Relevancy:\nrare, used when no MultiDataSetArchiver is used and\nAutoArchiverTask is too complex.\n## Description\n: Archives all data sets of experiments which fulfill\nsome criteria. This tasks needs the archive plugin to be configured in\nservice.properties\n.\n### Configuration\n## :\n## Property Key\n## Description\nexcluded-data-set-types\nComma-separated list of data set types. Data sets of such types are not archived. Default: No data set type is excluded.\nestimated-data-set-size-in-KB.\nSpecifies for the data set type\nthe average size in KB. If\nis DEFAULT it will be used for all data set types with unspecified estimated size.\nfree-space-provider.class\nFully qualified class name of the free space provider (implementing\nch.systemsx.cisd.common.filesystem.IFreeSpaceProvider\n). Depending on the free space provider additional properties, all starting with prefix\nfree-space-provider\n## .,  might be needed. Default:\nch.systemsx.cisd.common.filesystem.SimpleFreeSpaceProvider\nmonitored-dir\nPath to the directory to be monitored by the free space provider.\nminimum-free-space-in-MB\nMinimum free space in MB. If the free space is below this limit the task archives data sets. Default: 1 GB\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.ExperimentBasedArchivingTask\ninterval = 86400\nminimum-free-space-in-MB = 2048\nmonitored-dir = /my-data/\nestimated-data-set-size-in-KB.RAW_DATA = 12000\nestimated-data-set-size-in-KB.DEFAULT = 35000\nIf there is not enough free space the task archives all data sets\nexperiment by experiment until free space is above the specified limit.\nThe oldest experiments are archived first. The age of an experiment is\ndetermined by the youngest modification/registration time stamp of all\nits data sets which are not excluded by data set type or archiving\nstatus.\nThe free space is only calculated once when the task starts to figure\nout whether archiving is necessary or not. This value is than used\ntogether with estimated data set sizes to get an estimated free space\nwhich is used for the stopping criteria. Why not calculating the free\nspace again with the free space provider after the data sets of an\nexperiment have been archived? The reason is that providing the free\nspace might be an expensive operation. This is the case when archiving\nmeans removing data from a database which have been fed by data from\ndata sets of certain type. In this case archiving (i.e. deleting) those\ndata in the database do not automatically frees disk space because\nfreeing disk space is for databases often an expensive operation.\nThe DSS admin will be informed by an e-mail about which experiments have\nbeen archived.\nHierarchicalStorageUpdater\n\n## Environment\n## : DSS\n## Description\n: Creates/updates a mirrot of the data store. Data set\nare organized hierachical in accordance to their experiment and samples\n## Relevancy:\n## Deprecated\n### Configuration\n## :\n## Property Key\n## Description\nstoreroot-dir-link-path\nPath to the root directory of the store as to be used for creating symbolic links. This should be used if the path to the store as seen by clients is different than seen by DSS.\nstoreroot-dir\nPath to the root directory of the store. Used if storeroot-dir-link-path is not specified.\nhierarchy-root-dir\nPath to the root directory of mirrored store.\nlink-naming-strategy.class\nFully qualified class name of the strategy to generate the hierarchy (implementing\nch.systemsx.cisd.etlserver.plugins.IHierarchicalStorageLinkNamingStrategy\n). Depending on the actual strategy additional properties, all starting with prefix\nlink-naming-strategy\n## .,  mighty be needed. Default:\nch.systemsx.cisd.etlserver.plugins.TemplateBasedLinkNamingStrategy\nlink-source-subpath.\nLink source subpath for the specified data set type. Only files and folder in this relative path inside a data set will be mirrored. Default: The complete data set folder will be mirroed.\nlink-from-first-child.\nFlag which specifies whether only the first child of or the complete folder (either the data set or the one specified by link-source-subpath.\n## ). Default: False\nwith-meta-data\nFlag, which specifies whether directories with meta-data.tsv and a link should be created or only links. The default behavior is to create links-only. Default: false\nlink-naming-strategy.template\nThe exact form of link paths produced by TemplateBasedLinkNamingStrategy is defined by this template.\nThe variables\ndataSet\n,\ndataSetType\n,\nsample\n,\nexperiment\n, project and space will be recognized and replaced in the actual link path.\n## Default:\n${space}\n/\n${project}\n/\n${experiment}\n/\n${dataSetType}+${sample}+${dataSet}\nlink-naming-strategy.component-template\nIf defined, specifies the form of link paths for component datasets. If undefined, component datasets links are formatted with\nlink-naming-strategy.template\n.\nWorks as\nlink-naming-strategy.template\n, but has these additional variables:\ncontainerDataSetType\n,\ncontainerDataSet\n, `containerSample.\n## Default: Undefined.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.HierarchicalStorageUpdater\nstoreroot-dir = ${root-dir}\nhierarchy-root-dir = ../../mirror\nlink-naming-strategy.template = ${space}/${project}/${experiment}/${sample}/${dataSetType}-${dataSet}\nlink-naming-strategy.component-template = ${space}/${project}/${experiment}/${containerSample}/${containerDataSetType}-${containerDataSet}/${dataSetType}-${dataSet}\nMultiDataSetDeletionMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Relevant\n## Description\n: Deletes data sets which are already deleted on AS also\nfrom multi-data-set archives. This maintenance task works only if the\nMulti Data Set Archiver\nis\nconfigured. It does the following:\nExtracts the not-deleted data sets of a TAR container with deleted\ndata sets into the store.\nMarks them as\nnot present in archive\n.\nDeletes the TAR containers with deleted data sets.\nRequests archiving of the non-deleted data sets.\nThe last step requires that the maintenance task\nArchivingByRequestTask\nis configured.\n### Configuration\n## :\n## Property Key\n## Description\nlast-seen-event-id-file\nFile which contains the last seen event id.\nmapping-file\nOptional file which maps data sets to share ids and archiving folders (for details see Mapping File for Share Ids and Archiving Folders). If not specified the first share which has enough free space and which isn’t a unarchiving scratch share will be used for extracting the not-deleted data sets.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetDeletionMaintenanceTask\ninterval = 1 d\nlast-seen-event-id-file = ${storeroot-dir}/MultiDataSetDeletionMaintenanceTask-last-seen-event-id.txt\nmapping-file = etc/mapping.tsv\n## NOTE\n: Should be configured on any instance using the multi dataset\narchiver when the archive data should be deletable.\nMultiDataSetUnarchivingMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Relevant\n## Description\n: Triggers unarchiving of multi data set archives. Is\nonly needed if the configuration property\ndelay-unarchiving\nof the\nMulti Data Set Archiver\nis\nset\ntrue\n.\nThis maintenance task allows to reduce the stress of the tape system by\notherwise random unarchiving events triggered by the users.\n### Configuration\n: No specific properties.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetUnarchivingMaintenanceTask\ninterval = 1 d\nstart = 01:00\nMultiDataSetArchiveSanityCheckMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Performs sanity check of multi data set archives. The check compares archives stored in final and replica destinations with the\ninformation kept in the path info database. The check verifies: folder structure, file sizes and checksums. In case of found inconsistencies a\nnotification email is sent to chosen email addresses.\nThe task loads a list of archives to be checked from the multi data set archiver database. The archives from the list can be checked either\nchronologically (the oldest archive is checked first) or can be processed in a random order (see\ncheck-in-random-order\nproperty). The list of\narchives can be narrowed down to a specific time window (see\ncheck-from-date\nand\ncheck-to-date\nproperties). During a single run the task can check\nall the archives from the list or just a chosen number of archives (see\nrun-size\nproperty).\nThe task provides additional properties (see\nrun-probability\nand\nrun-max-random-delay\n) that can be used to introduce some randomness to the time\nthe task executes. This can be especially handy in cases when we want to have multiple instances of openBIS that share the same configuration for the\ntask but at the same time we want to desynchronize the task execution among these instances not to cause a peak load on the common archive storage.\n## Warning\nThe task assumes MultiDataSetArchiver task is configured (the\ntask uses some of the multi data set archiver configuration properties\ne.g. final destination location).\n### Configuration\n## :\n## Property Key\n## Mandatory\n## Default Value\n## Description\nstatus-file\ntrue\nPath to a JSON file that keeps a list of already checked archive containers\nnotify-emails\ntrue\nList of emails to notify about problematic archive containers\ncheck-from-date\nfalse\nnull\n“From date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm\ncheck-to-date\nfalse\nnull\n“To date” of the time window to be checked. Date in format yyyy-MM-dd HH:mm\ncheck-in-random-order\nfalse\nfalse\nIf set to “true” then archive containers are checked in a random order. If set to “false” then the containers are checked in chronological order (from the oldest to the newest). Allowed values: true / false.\nrun-probability\nfalse\n1.0\nControls the probability of a task run (0.0 value means the task run will be always skipped, 1.0 value means the task run will always be executed normally). Float values between (0,1.0] are allowed.\nrun-max-random-delay\nfalse\n0\nMaximum delay before the run. The actual delay before each run is randomly chosen from range [0, run-max-random-delay]. Different time units are allowed: sec/s, min/m, hours/h, days/d.\nrun-size\nfalse\n-1\nMaximum number of archives to be checked in a single run, where -1 means all found archives will be checked. Integer values > 0 or equal to -1 are allowed.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetArchiveSanityCheckMaintenanceTask\ninterval = 1d\ncheck-from-date = 2023-01-01 00:00:00\ncheck-to-date = 2023-12-31 23:59:59\ncheck-in-random-order = true\nrun-probability = 0.25\nrun-max-random-delay = 2h\nrun-size = 1\nnotify-emails = test1@email.com, test2@email.com\nstatus-file = ../../multi-dataset-sanity-check-statuses.json\nPathInfoDatabaseFeedingTask\n\n## Environment\n## : DSS\n## Relevancy:\nDefault, is part of the post registration task\n## Description\n: Feeds the pathinfo database with file paths of all data\nsets in the store. It can be used as a maintenance task as well as a\npost registration task. As a maintenance task it is needed to run only\nonce if a\nPostRegistrationMaintenanceTask\nis configured. This task\nassumes a data source with for ‘path-info-db’.\nIf used as a maintenance task the data sets are processed in the order\nthey are registered. The registration time stamp of the last processed\ndata set is the starting point when the task is executed next time.\n### Configuration\n## :\n## Property Key\n## Description\ncompute-checksum\n## If\ntrue\nthe CRC32 checksum (and optionally a checksum of the type specified by\nchecksum-type\n) of all files will be calculated and stored in pathinfo database. Default value:\nfalse\nchecksum-type\nOptional checksum type. If specified and\ncompute-checksum\n=\ntrue\ntwo checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by\nMessageDigest.getInstance(<checksum\ntype>)\n. For more details see\nOracle docs\n.\ndata-set-chunk-size\nNumber of data sets requested from AS in one chunk if it is used as a maintenance task. Default: 1000\nmax-number-of-chunks\nMaximum number of chunks of size data-set-chunk-size are processed if it is used as a maintenance task. If it is <= 0 and\ntime-limit\nisn’t defined all data sets are processed. Default: 0\ntime-limit\nLimit of execution time of this task if it is used as a maintenance task. The task is stopped before reading next chunk if the time has been used up. If it is specified it is an alternative way to limit the number of data sets to be processed instead of specifying\nmax-number-of-chunks\n. This parameter can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseFeedingTask\nexecute-only-once = true\ncompute-checksum = true\nPostRegistrationMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: A tasks which runs a sequence of so-called\npost-registration tasks for each freshly registered data set.\n### Configuration\n## :\n## Property Key\n## Description\nignore-data-sets-before-date\nDefines a registration date. All data sets registered before this date are ignored. Format:\nyyyy-MM-dd\n, where\nyyyy\nis a four-digit year,\n## MM\nis a two-digit month, and\ndd\nis a two-digit day. Default value: no restriction.\nlast-seen-data-set-file\nPath to a file which stores the code of the last data set successfully post-registered. Default value:\nlast-seen-data-set.txt\ncleanup-tasks-folder\nPath to a folder which stores serialized clean-up tasks always created before a post-registration task is executed. These clean-up tasks are executed on start up of DSS after a server crash. Default value:\nclean-up-tasks\npost-registration-tasks\nComma-separated list of keys of post-registration task configuration. Each key defines (together with a ‘.’) the prefix of all property keys defining the post-registration task. They are executed in the order their key appear in the list.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.postregistration.PostRegistrationMaintenanceTask\ninterval = 60\ncleanup-tasks-folder = ../cleanup-tasks\nignore-data-sets-before-date = 2011-01-27\nlast-seen-data-set-file = ../last-seen-data-set\npost-registration-tasks = eager-shuffling, eager-archiving\neager-shuffling.class = ch.systemsx.cisd.etlserver.postregistration.EagerShufflingTask\neager-shuffling.share-finder.class = ch.systemsx.cisd.openbis.dss.generic.shared.ExperimentBasedShareFinder\neager-archiving.class = ch.systemsx.cisd.etlserver.postregistration.ArchivingPostRegistrationTask\nRevokeUserAccessMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Check if the users are available on the configured\nauthentication services, if they are not available, are automatically\ndisabled and their id renamed with the disable date.\nFor this to work the services should be able to list the available\nusers. If you use any service that doesn’t allow it, the task\nautomatically disables itself because is impossible to know if the users\nare active or not.\n## Service\n## Compatible\nCrowdAuthenticationService\n## NO\nDummyAuthenticationService\n## NO\nNullAuthenticationService\n## NO\nFileAuthenticationService\n## YES\nLDAPAuthenticationService\n## YES\n### Configuration\n## :\nThis maintenance task automatically uses the services already configured\non the server.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.RevokeUserAccessMaintenanceTask\ninterval = 60 s\nUserManagementMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Creates users, spaces, samples, projects and\nexperiments for all members of an LDAP authorization group or an\nexplicit list of user ids. A configuration file (in JSON format) will be\nread each time this task is executed. All actions are logged in an audit\nlog file. For more details see\nUser Group Management for Multi-groups openBIS Instances\n### Configuration:\n## Property Key\n## Description\nconfiguration-file-path\nRelative or absolute path to the configuration file. Default:\netc/user-management-maintenance-config.json\naudit-log-file-path\nRelative or absolute path to the audit log file. Default:\nlogs/user-management-audit_log.txt\nshares-mapping-file-path\nRelative or absolute path to the mapping file for data store shares. This is optional. If not specified the mapping file will not be managed by this maintenance task.\nfilter-key\nKey which is used to filter LDAP results. Will be ignored if\nldap-group-query-template\n## is specified. Default value:\nou\nldap-group-query-template\nDirect LDAP query template. It should have ‘%’ character which will be replaced by an LDAP key as specified in the configuration file.\ndeactivate-unknown-users\n## If\ntrue\na user unknown by the authentication service will be deactivated. It should be set to\nfalse\nif no authenication service can be asked (like in Single-Sign-On). Default:\ntrue\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.UserManagementMaintenanceTask\nstart = 02:42\ninterval = 1 day\nConsistency and other Reports\n\nDataSetArchiverOrphanFinderTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Finds archived data sets which are no longer in openBIS\n(at least not marked as present-in-archive). A report will be created\nand sent to the specified list of e-mail addresses (mandatory\nproperty\nemail-addresses\n). The task also looks for data sets which are\npresent-in-archive but actually not found in the archive.\nThis orphan finder task only works for Multi Data Set Archiver. It\ndoesn’t work for RsyncArchiver, TarArchiver or ZipArchiver.\n### Configuration\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.DataSetArchiverOrphanFinderTask\ninterval = 60 s\nemail-addresses = email1@bsse.ethz.ch, email2@bsse.ethz.ch\n## Notes:\nThis is a consistency check task. It checks consistency for\ndatasets with the flag present-in-archive.\nDataSetAndPathInfoDBConsistencyCheckTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Checks that the file information in pathinfo database\nis consistent with the information the file system provides. This is\ndone for all recently registered data sets. Note, archived data sets are\nskipped. After all data sets (in the specified checking time interval)\nhave been checked the task checks them again.\n### Configuration\n## :\n## Property Key\n## Description\nchecking-time-interval\nTime interval in the past which defines the range of data sets to be checked. That is, all data sets with registration date between now minus checking-time-interval and now will be checked. Can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n. Default value: one day.\npausing-time-point\n## Optional time point. Format:\nHH:mm\n. where\n## HH\nis a two-digit hour (in 24h notation) and\nmm\nis a two-digit minute.\nWhen specified this task stops checking after the specified pausing time point and continues when executed the next time or the next day if start or\ncontinuing-time-point\nis specified.\nAfter all data sets have been checked the task checks again all data sets started by the oldest one specified by\nchecking-time-interval\n.\ncontinuing-time-point\n## Time point where checking continous. Format:\nHH:mm\n. where\n## HH\nis a two-digit hour (in 24h notation) and\nmm\nis a two-digit minute. Ignored when\npausing-time-point\nisn’t specified. Default value: Time when the task is executed.\nchunk-size\nMaximum number of data sets retrieved from AS. Ignored when\npausing-time-point\nisn’t specified. Default value: 1000\nstate-file\nFile to store registration time stamp and code of last considered data set. This is only used when pausing-time-point has been specified. Default:\n<store\nroot>/DataSetAndPathInfoDBConsistencyCheckTask-state.txt\n## Example\n: The following example checks all data sets of the last ten\nyears. It does the check only during the night and continues next night.\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.DataSetAndPathInfoDBConsistencyCheckTask\ninterval = 1 days\nstart = 23:15\npausing-time-point = 5:00\nchecking-time-interval = 3653 days\nMaterialExternalDBSyncTask\n\n## Environment\n## : AS\n## Relevancy:\n## Deprecated\n## Description\n: Feeds a report database with recently added or modified\nmaterials.\n### Configuration\n## :\n## Property Key\n## Description\ndatabase-driver\nFully qualified name of the JDBC driver class.\ndatabase-url\nURL to access the database server.\ndatabase-username\nUser name of the database. Default: User who started openBIS AS.\ndatabase-password\nOptional password of the database user.\nmapping-file\nPath to the file containing configuration information of mapping material types and material properties to tables and columns in the report database.\nread-timestamp-sql\nThe SQL select statement which returns one column of type time stamp for the time stamp of the last report. If the result set is empty the time stamp is assumed to be 1970-01-01. If the result set has more than one row the first row is used.\nupdate-timestamp-sql\nThe SQL statement which updates or adds a time stamp. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp.\ninsert-timestamp-sql\nThe SQL statement to add a time stamp the first time. The statement has to contain a ‘?’ symbol as the placeholder of the actual time stamp. Default: same as\nupdate-timestamp-sql\n.\n## Example\n## :\nservice.properties of AS\n<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.MaterialExternalDBSyncTask\n<task id>.interval = 120\n<task id>.read-timestamp-sql = select timestamp from timestamp\n<task id>.update-timestamp-sql = update timestamp set timestamp = ?\n<task id>.insert-timestamp-sql = insert into timestamp values(?)\n<task id>.mapping-file = ../report-mapping.txt\n<task id>.database-driver = org.postgresql.Driver\n<task id>.database-url = jdbc:postgresql://localhost/material_reporting\n## Mapping File\n\nThe mapping file is a text file describing the mapping of the data (i.e.\nmaterial codes and material properties) onto the report database. It\nmakes several assumptions on the database schema:\nOne table per material type. There are only table of materials to be\nreported.\nEach table has a column which contains the material code.\nThe entries are unique.\nThe material code is a string not longer than 60 characters.\nEach table has one column for each property type. Again, there are\nonly column for properties to be reported.\nThe data type of the column should match the data type of the\n## properties:\nMATERIAL:  only the material code (string) will be reported.\nMaximum length: 60\nCONTROLLEDVOCABULARY: the label (if defined) or the code will be\nreported. Maximum length: 128\nTIMESTAMP: timestamp\nINTEGER: integer of any number of bits (maximum 64).\nREAL: fixed or floating point number\nall other data types are mapped to text.\nThe general format of the mapping file is as follows:\n[\n<\n## Material\n## Type\n## Code\n>\n## :\n<\ntable\n## Name\n>\n,\n<\ncode\ncolumn\nname\n>\n]\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n...\n[\n<\n## Material\n## Type\n## Code\n>\n## :\n<\ntable\n## Name\n>\n,\n<\ncode\ncolumn\nname\n>\n]\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n<\n## Property\n## Type\n## Code\n>\n## :\n<\ncolumn\nname\n>\n...\n## Example:\nmapping.txt\n# Some comments\n[\n## GENE\n## :\n## GENE\n,\n## GENE_ID\n]\n## GENE_SYMBOLS\n## :\nsymbol\n[\n## SIRNA\n## :\nsi_rna\n,\ncode\n]\n## INHIBITOR_OF\n## :\nsuppressed_gene\n## SEQUENCE\n## :\nNucleotide_sequence\n## Some rules:\nEmpty lines and lines starting with ‘#’ will be ignored.\nTable and column names can be upper or lower case or mixed.\nMaterial type codes and property type codes have to be in upper\ncase.\n## Warning\nIf you put a foreign key constraint on the material code of one of the material properties, you need to define the constraint checking as DEFERRED in order to not get a constraint violation. The reason is that this task will\nnot\norder the\n## INSERT\nstatements by its dependencies, but in alphabetical order.\n### UsageReportingTask\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Creates a daily/weekly/monthly report to a list of\ne-mail recipients about the usage (i.e. creation of experiments, samples\nand data sets) by users or groups. For more details see\n## User Group\nManagement for Multi-groups openBIS\n## Instances\n.\nIn order to be able to send an e-mail the following properties in\nservice.properties\nhave to be defined:\nmail\n.\nfrom\n=\nopenbis\n@<\nhost\n>\nmail\n.\nsmtp\n.\nhost\n=\n<\n## SMTP\nhost\n>\nmail\n.\nsmtp\n.\nuser\n=\n<\ncan\nbe\nempty\n>\nmail\n.\nsmtp\n.\npassword\n=\n<\ncan\nbe\nempty\n>\n### Configuration\n## :\n## Property Key\n## Description\ninterval\nDetermines the length of period: daily if less than or equal one day, weekly if less than or equal seven days, monthly if above seven days. The actual period is always the day/week/month before the execution day\nemail-addresses\nComma-separated e-mail addresses which will receive the report as an attached text file (format: TSV).\nuser-reporting-type\nType of reporting individual user activities. Possible values are\nNONE: No reporting\nALL: Activities inside and outside groups and for all users\nOUTSIDE_GROUP_ONLY: Activities outside groups and users of no groups\nDefault: ALL\nspaces-to-be-ignored\nOptional list of comma-separated space codes of all the spaces which should be ignored for the report.\nconfiguration-file-path\nOptional configuration file defining groups.\ncount-all-entities\n## If\ntrue\nshows the number of all entities (collections, objects, data sets) in an additional column. Default:\nfalse\n## Example\n## :\n### class = ch.systemsx.cisd.openbis.generic.server.task.UsageReportingTask\ninterval = 7 days\nemail-addresses = ab@c.de, a@bc.de\nPersonalAccessTokenValidityWarningTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n### Automatic Configuration\n## :\nThis task is automatically configured, added and run with a default interval of 1 day. If needed, the default interval can be changed. In order to do\nthat please configure the task just like any other maintenance task in\ncore-plugins\nfolder.\n## Description\n: Sends out warning emails about soon to be expired PATs (Personal Access Tokens). Emails are sent to PATs owners. Each email contains\na list of PATs that have the remaining validity period shorter than the\npersonal-access-tokens-validity-warning-period\ndefined in\n## AS\nservice.properties\n. The task does not send any information about the already expired PATs. It removes them.\nFor more details on Personal Access Tokens please see\n## Personal Access Tokens\n.\nIn order to be able to send an e-mail the following properties in\nservice.properties\nhave to be defined:\nmail\n.\nfrom\n=\nopenbis\n@<\nhost\n>\nmail\n.\nsmtp\n.\nhost\n=\n<\n## SMTP\nhost\n>\nmail\n.\nsmtp\n.\nuser\n=\n<\ncan\nbe\nempty\n>\nmail\n.\nsmtp\n.\npassword\n=\n<\ncan\nbe\nempty\n>\n## Example\n## :\nclass = ch.systemsx.cisd.openbis.generic.server.pat.PersonalAccessTokenValidityWarningTask\ninterval = 7 d\nConsistency Repair and Manual Migrations\n\nBatchSampleRegistrationTempCodeUpdaterTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Replaces temporary sample codes (i.e. codes matching\nthe regular expression\nTEMP\\\\.[a-zA-Z0-9\\\\-]+\\\\.[0-9]+\n) by normal\ncodes (prefix specified by sample type plus number). This maintenance\ntask is only needed when\ncreate-continuous-sample-codes\nis set\ntrue\nin\nservice.properties\nof AS.\n## Example\n## :\nplugin.properties\nclass\n=\nch.systemsx.cisd.openbis.generic.server.task.BatchSampleRegistrationTempCodeUpdaterTask\nCleanUpUnarchivingScratchShareTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Default\n## Description\n: Removes data sets from the unarchiving scratch share\nwhich have status ARCHIVED and which are present in archive. For more\ndetails see\nMulti data set\narchiving\n.\n### Configuration\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.CleanUpUnarchivingScratchShareTask\ninterval = 60 s\n## Notes:\nRecommended cleanup task to run on every instance.\nDataSetRegistrationSummaryTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Sends a data set summary report to a list of e-mail\nrecipients in regular time intervals. The report contains all new data\nsets registered since the last report. Selected properties can be\nincluded into the report. The data sets are grouped by the data set\ntype.\nIn order to be able to send an e-mail the following properties in\nservice.properties\nhave to be defined:\nmail\n.\nfrom\n=\nopenbis\n@<\nhost\n>\nmail\n.\nsmtp\n.\nhost\n=\n<\n## SMTP\nhost\n>\nmail\n.\nsmtp\n.\nuser\n=\n<\ncan\nbe\nempty\n>\nmail\n.\nsmtp\n.\npassword\n=\n<\ncan\nbe\nempty\n>\n### Configuration:\n## Property Key\n## Description\ninterval\nInterval (in seconds) between regular checks whether to create a report or not. This value should be set to 86400 (1 day). Otherwise the same report might be sent twice or no report will be sent.\nstart\nTime the report will be created. A good values for this parameter is some early time in the morning like in the example below.\ndays-of-week\nComma-separated list of numbers denoting days of week (Sunday=1, Monday=2, etc.). This parameter should be used if reports should be sent weekly or more often.\ndays-of-month\nComma-separated list of numbers denoting days of month. Default value of this parameter is 1.\nemail-addresses\nComma-separated list of e-mail addresses.\nshown-data-set-properties\nOptional comma-separated list of data set properties to be included into the report.\ndata-set-types\nRestrict the report to the specified comma-separated data set types.\nconfigured-content\nUse the specified content as the body of the email.\nA report is sent at each day which is either a specified day of week or\nday of month. If only weekly reports are needed the parameter\ndays-of-month\nshould be set to an empty string.\n## Example\n## :\nservice.properties of AS\n<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.DataSetRegistrationSummaryTask\n<task id>.interval = 86400\n<task id>.start = 1:00\n<task id>.data-set-types = RAW_DATA, MZXML_DATA\n<task id>.email-addresses = albert.einstein@princeton.edu, charles.darwin@evolution.org\nThis means that on the 1st day of every month at 1:00 AM openBIS sends\nto the specified e-mail recipients a report about the data sets of types\nRAW_DATA and MZXML_DATA that have been uploaded in the previous month.\nDynamicPropertyEvaluationMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Re-evaluates dynamic properties of all entities\n### Configuration\n## :\n## Property Key\n## Description\nclass\nch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask\ninterval = 3600\nDynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Deprecated\n## Description\n: Re-evaluates dynamic properties of all samples which\nrefer via properties of type MATERIAL directly or indirectly to\nmaterials changed since the last re-evaluation.\n### Configuration\n## :\n## Property Key\n## Description\nclass\nch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask\ntimestamp-file\nPath to a file which will store the timestamp of the last evaluation. Default value:\n../../../data/DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask-timestamp.txt\n.\ninitial-timestamp\nInitial timestamp of the form\n## YYYY-MM-DD\n(e.g. 2013-09-15) which will be used the first time when the timestamp file doesn’t exist or has an invalid value. This is a mandatory property.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationTriggeredByMaterialChangeMaintenanceTask\ninterval = 7 days\ninitial-timestamp = 2012-12-31\nFillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Queries openBIS database to find data sets without a\nsize filled in, then queries the pathinfo DB to see if the size info is\navailable there; if it is available, it fills in the size from the\npathinfo information. If it is not available, it does nothing. Data sets\nfrom openBIS database are fetched in chunks (see data-set-chunk-size\nproperty). After each chunk the maintenance tasks checks whether a time\nlimit has been reached (see time-limit property). If so, it stops\nprocessing. A code of the last processed data set is stored in a file\n(see last-seen-data-set-file property). The next run of the maintenance\ntask will process data sets with a code greater than the one saved in\nthe “last-seen-data-set-file”. This file is deleted periodically (see\ndelete-last-seen-data-set-file-interval) to handle a situation where\ncodes of new data sets are lexicographically smaller than the codes of\nthe old datasets. Deleting the file is also needed when pathinfo\ndatabase entries are added after a data set has been already processed\nby the maintenance task.\n### Configuration\n## :\n## Property Key\n## Description\nlast-seen-data-set-file\nPath to a file that will store a code of the last handled data set. Default value: “fillUnknownDataSetSizeTaskLastSeen”\ndelete-last-seen-data-set-file-interval\nA time interval (in seconds) which defines how often the “last-seen-data-set-file” file should be deleted. The parameter can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n. Default value: 7 days.\ndata-set-chunk-size\nNumber of data sets requested from AS in one chunk. Default: 100\ntime-limit\nLimit of execution time of this task. The task is stopped before reading next chunk if the time has been used up. This parameter can be specified with one of the following time units:\nms\n,\nmsec\n,\ns\n,\nsec\n,\nm\n,\nmin\n,\nh\n,\nhours\n,\nd\n,\ndays\n. Default time unit is\nsec\n.\n## Example:\nplugin.properties\n<task id>.class = ch.systemsx.cisd.etlserver.plugins.FillUnknownDataSetSizeInOpenbisDBFromPathInfoDBMaintenanceTask\n<task id>.interval = 86400\n<task id>.data-set-chunk-size = 1000\n<task id>.time-limit = 1h\n## NOTE\n: Useful in scenarios where the path info feeding sub task of\npost registration task fails.\nPathInfoDatabaseChecksumCalculationTask\n\n## Environment\n## : DSS\n## Relevancy:\nRare, often the CRC32 is calculated during the post\nregistration.\n## Description\n: Calculates the CRC32 checksum (and optionally a\nchecksum of specified type) of all files in the pathinfo database with\nunknown checksum. This task is needed to run only once. It assumes a\ndata source for key ‘path-info-db’.\n### Configuration\n## :\n## Property Key\n## Description\nchecksum-type\nOptional checksum type. If specified two checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by\nMessageDigest.getInstance(<checksum\ntype>)\n. For more details see http://docs.oracle.com/javase/8/docs/api/java/security/MessageDigest.html#getInstance-java.lang.String-.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseChecksumCalculationTask\nexecute-only-once = true\nchecksum-type = SHA-256\nPathInfoDatabaseRefreshingTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: Refreshes the pathinfo database with file metadata of\nphysical and available data sets in the store. This task assumes a data\nsource with for ‘path-info-db’.\nThe data sets are processed in the inverse order they are registered.\nOnly a maximum number of data sets are processed in one run. This is\nspecified by\nchunk-size\n.\n## Warning\nUnder normal circumstances this maintenance task is never needed, because the content of a physical data set is\nnever\nchanged by openBIS itself.\nOnly in the rare cases that the content of physical data sets have to be changed this maintenance task allows to refresh the file meta data in the pathinfo database.\n### Configuration\n## :\n## Property Key\n## Description\ntime-stamp-of-youngest-data-set\nTime stamp of the youngest data set to be considered. The format has to be\n<4\ndigit\nyear>-<month>-<day>\n<hour>:<minute>:<second>\n.\ncompute-checksum\n## If\ntrue\nthe CRC32 checksum (and optionally a checksum of the type specified by\nchecksum-type\n) of all files will be calculated and stored in pathinfo database. Default value: true\nchecksum-type\nOptional checksum type. If specified and\ncompute-checksum\n=\ntrue\ntwo checksums are calculated: CRC32 checksum and the checksum of specified type. The type and the checksum are stored in the pathinfo database. An allowed type has to be supported by\nMessageDigest.getInstance(<checksum\ntype>)\n. For more details see\nOracle doc\n.\nchunk-size\nNumber of data sets requested from AS in one chunk. Default: 1000\ndata-set-type\nOptional data set type. If specified, only data sets of the specified type are considered. Default: All data set types.\nstate-file\nFile to store registration time stamp and code of last considered data set. Default:\n<store\nroot>/PathInfoDatabaseRefreshingTask-state.txt\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.path.PathInfoDatabaseRefreshingTask\ninterval = 30 min\ntime-stamp-of-youngest-data-set = 2014-01-01 00:00:00\ndata-set-type = HCS_IMAGE\nRemoveUnusedUnofficialTermsMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Rare\n## Description\n: Removes unofficial unused vocabulary terms. For more details about unofficial vocabulary terms see\n## Ad Hoc Vocabulary Terms\n.\n### Configuration:\n## Property Key\n## Description\nolder-than-days\nUnofficial terms are only deleted if they have been registered more than the specified number of days ago. Default: 7 days.\n## Example\n## :\nservice.properties of AS\n<task id>.class = ch.systemsx.cisd.openbis.generic.server.task.RemoveUnusedUnofficialTermsMaintenanceTask\n<task id>.interval = 86400\n<task id>.older-than-days = 30\nResetArchivePendingTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Rare\n## Description\n: For each data set not present in archive and status\nARCHIVE_PENDING the status will be set to AVAILABLE if there is no\ncommand in the DSS data set command queues referring to it.\n### Configuration\n## :\nplugin.properties\nclass = ch.systemsx.cisd.etlserver.plugins.ResetArchivePendingTask\ninterval = 60 s\nSessionWorkspaceCleanUpMaintenanceTask\n\n## Environment\n## : AS\n## Relevancy:\n## Default\n## Description\n: Cleans up session workspace folders of no longer active\nsessions. This maintenance plugin is automatically added by default with\na default interval of 1 hour. If a manually configured version of the\nplugin is detected then the automatic configuration is skipped.\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.generic.server.task.SessionWorkspaceCleanUpMaintenanceTask\ninterval = 1 day\nMaterialsMigration\n\n## Environment\n## : AS\n## Relevancy:\n## Relevant\n## Description\n: Migrates the Materials entities and types to use a\nSample based model using Sample Properties. It automatically creates and\nassigns sample types, properties and entities.\nIt allows to execute the migration and to delete of the old Materials\nmodel in separate steps.\nDeleting Materials and material types requires the migration to have\nbeen a success,  before the deletion a validation check is run.\n## Example\n## :\nThis maintenance task can be directly configured on the AS\nservice.properties\nservice.properties\nmaintenance-plugins = materials-migration\n\n\n\nmaterials-migration.class = ch.systemsx.cisd.openbis.generic.server.task.MaterialsMigration\nmaterials-migration.execute-only-once = true\nmaterials-migration.doMaterialsMigrationInsertNew = true\nmaterials-migration.doMaterialsMigrationDeleteOld = true\n## Microscopy Maintenance Tasks\n\nMicroscopyThumbnailsCreationTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Relevant\n## Description\n: Creates thumbnails for already registered microscopy\ndata sets.\n### Configuration:\n## Property Key\n## Description\nmaximum-number-of-workers\nIf specified the creation will be parallelized among several workers. The actual number of workers depends on the number CPUs. There will be not more than 50% of CPUs used.\nstate-file\nName of the file which stores the registration time stamp of the last successfully handled data set. Default:\nMicroscopyThumbnailsCreationTask-state.txt\nscript-path\nPath to the jython script which specifies the thumbnails to be generated. The script should have defined the method\nprocess(transaction,\nparameters,\ntablebuilder)\nas for\nJythonIngestionService\n(see Jython-based Reporting and Processing Plugins). Note, that tablebuilder will be ignored. In addition the global variables\nimage_config\nand\nimage_data_set_structure\n## are defined:\nimage_data_set_structure: It is an object of the class\nImageDataSetStructure\n. Information about channels, series numbers etc. can be requested.\nimage_config: It is an object of the class\nSimpleImageContainerDataConfig\n. It should be used to specify the thumbnails to be created. Currently only\nsetImageGenerationAlgorithm()\nis supported.\nmain-data-set-type-regex\nRegular expression for the type of data sets which have actual images. Default:\n## MICROSCOPY_IMG\ndata-set-thumbnail-type-regex\nRegular expression for the type of data sets which have thumbnails. This is used to test whether there are already thumbnails or not. Default:\n## MICROSCOPY_IMG_THUMBNAIL\nmax-number-of-data-sets\nThe maximum number of data sets to be handle in a run of this task. If zero or less than zero all data sets will be handled. Default: 1000\ndata-set-container-type\nType of the data set container. Default:\n## MICROSCOPY_IMG_CONTAINER\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.etl.MicroscopyThumbnailsCreationTask\ninterval = 1 h\nscript-path = specify_thumbnail_generation.py\nwith\nspecify_thumbnail_generation.py\nfrom\nch.systemsx.cisd.openbis.dss.etl.dto.api.impl\nimport\nMaximumIntensityProjectionGenerationAlgorithm\nfrom\nsets\nimport\n## Set\ndef\n_get_series_num\n## ():\nseries_numbers\n=\n## Set\n()\nfor\nimage_info\nin\nimage_data_set_structure\n.\ngetImages\n## ():\nseries_numbers\n.\nadd\n(\nimage_info\n.\ntryGetSeriesNumber\n())\nreturn\nseries_numbers\n.\npop\n()\ndef\nprocess\n(\ntransaction\n,\nparameters\n,\ntableBuilder\n## ):\nseriesNum\n=\n_get_series_num\n()\nif\nint\n(\nseriesNum\n)\n%\n2\n==\n0\n## :\nimage_config\n.\nsetImageGenerationAlgorithm\n(\nMaximumIntensityProjectionGenerationAlgorithm\n(\n## \"MICROSCOPY_IMG_THUMBNAIL\"\n,\n256\n,\n128\n,\n\"thumbnail.png\"\n))\nDeleteFromImagingDBMaintenanceTask\n\n## Environment\n## : DSS\n## Relevancy:\n## Relevant\n## Description\n: Deletes database entries from the imaging database.\nThis is special variant of\nDeleteFromExternalDBMaintenanceTask\nwith the same configuration parameters.\n### Configuration\n## : See\nDeleteFromExternalDBMaintenanceTask\n## Example\n## :\nplugin.properties\nclass = ch.systemsx.cisd.openbis.dss.etl.DeleteFromImagingDBMaintenanceTask\ndata-source = imaging-db\n## Proteomics Maintenance Tasks\n", "timestamp": "2025-09-18T09:38:29.873212Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_master-data:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/master-data.html", "repo": "openbis", "title": "Master data import/export", "section": "Core", "text": "Master data import/export\n\nThe master data of openBIS comprises all entity/property types, property\nassignments, vocabularies etc. needed for your customized installation\nto work. The system offers a way to export/import master data via Jython\nscripts. More information on how to do create such scripts and run them\nmanually see the advanced guide\nJython Master Data Scripts\n.\nA master data script can be run automatically by start up of the AS if\nit is defined in an AS core plugin. The script path should be\n$INSTALL_PATH/servers/core-plugins/<module\nname>/<version\nnumber>/as/initialize-master-data.py\n.\nFor more details about the folder structure of core plugins see\n## Core\n## Plugins\n. If there are several\ncore plugins with master data scripts the scripts will be executed in\nalphabetical order of the module names. For example, the master data\nscript of module\nscreening-optional\nwill be executed after the master\ndata script of module\nscreening\nhas been executed.\nExecution of master data script can be suppressed by\ndisabling\ninitialize-master-data\ncore plugin. For more details see\n## Core Plugins\n.", "timestamp": "2025-09-18T09:38:29.878571Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Introduction", "text": "Multi data set archiving\n\n## Introduction\n\nMulti data set archiver is a tool to archive several datasets together\nin chunks of relatively large size. When a group of datasets is selected\nfor archive it is verified if they are all together of proper size and\nthen they are being stored as one big container file (tar) on the\ndestination storage.\nWhen unarchiving data sets from a multi data set archive the following\n## rules are obeyed:\nUnarchiving of data sets from different containers is possible as\nlong as the maximum unarchiving cap specified in the\nplugin.properties file is not exceeded.\nAll data sets from a container are unarchived even though\nunarchiving has been requested only for a sub set.\nThe data sets are unarchived into a share which is marked as an\nunarchiving scratch share.\nIn case of not enough free space in the scratch share the oldest\n(defined by modification time stamp) data sets are removed from the\nscratch share to free space. For those data sets the archiving\nstatus is set back to ARCHIVED.\nTo test the archiver find the datasets you want to archive in openbis\nGUI and “add to archive”.\nImportant technical details\n\nThe archiver requires configuration of three important entities.\nAn archive destination (e.g. on Strongbox).\nA PostgreSQL database for mapping information (i.e. which data set\nis in which container file).\nAn unarchiving scratch share.\nMulti dataset archiver is not compatible with other archivers. You\nshould have all data available before configuring this archiver.\n## Workflows\n\nThe multi data set archiver can be configured for four different\nworkflows. The workflow is selected by the presence/absence of the\nproperties\nstaging-destination\nand\nreplicated-destination\n.\nSimple workflow\n\nNone of the properties\nstaging-destination\nand\nreplicated-destination\nare present.\nWait for enough free space on the archive destination.\nStore the data set in a container file directly on the archive\ndestination.\nPerform sanity check. That is, getting the container file to the\nlocal disk and compare the content with the content of all data sets\nin the store.\nAdd mapping data to the PostgreSQL database.\nRemove data sets from the store if requested.\nUpdate archiving status for all data sets.\nStaging workflow\n\n## Property\nstaging-destination\nis specified but\nreplicated-destination\nis not.\nStore the data sets in a container file in the staging folder.\nWait for enough free space on the archive destination.\nCopy the container file from the staging folder to the archive\ndestination.\nPerform sanity check.\nRemove container file from the staging folder.\nAdd mapping data to the PostgreSQL database.\nRemove data sets from the store if requested.\nUpdate archiving status for all data sets.\nReplication workflow\n\n## Property\nreplicated\n-destination is specified but\nstaging\n-destination is not.\nWait for enough free space on the archive destination.\nStore the data set in a container file directly on the archive\ndestination.\nPerform sanity check.\nAdd mapping data to the PostgreSQL database.\nWait until the container file has also been copied (by some external\nprocess) to a replication folder.\nRemove data sets from the store if requested.\nUpdate archiving status for all data sets.\n## Some remarks:\nSteps 5 to 7 will be performed asynchronously from the first four\nsteps because step 5 can take quite long. In the meantime the next\narchiving task can already be performed.\nIf the container file isn’t replicated after some time archiving\nwill be rolled back and scheduled again.\nStaging and replication workflow\n\nWhen both properties\nstaging-destination\nand\nreplicated-destination\nare present staging and replication workflow will be combined.\nClean up\n\nIn case archiving fails all half-baked container files have to be\nremoved. By default this is done immediately.\nBut in context of tape archiving systems (e.g. Strongbox) immediate\ndeletion might not always be possible all the time. In this case a\ndeletion request is schedule. The request will be stored in file. It\nwill be handled in a separate thread in regular time intervals (polling\ntime). If deletion isn’t possible after some timeout an e-mail will be\nsent. Such deletion request will still be handled but the e-mail allows\nmanual intervention/deletion. Note, that deletion requests for\nnon-existing files will always be handled successfully.\n### Configuration steps\n\nDisable existing archivers\nFind all properties of a form\narchiver.*\nin\nservers/datastore_server/etc/service.properties\nand remove\nthem.\nFind all DSS core plugins of type\nmiscellaneous\nwhich define\nan archiver. Disable them by adding an empty marker file\nnamed\ndisabled\n.\nEnable archiver\nConfigure a new DSS core plugin of type\nmiscellaneous\n## :\nmulti-dataset-archiver/1/dss/miscellaneous/archiver/plugin.properties\narchiver.class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.MultiDataSetArchiver", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Temporary folder (needed for sanity check). Default: Value provided by Java system property java.io.tmpdir. Usually /tmp", "text": "# Temporary folder (needed for sanity check). Default: Value provided by Java system property java.io.tmpdir. Usually /tmp\n# archiver.temp-folder = <java temp folder>", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:2", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Staging folder (needed for 'staging workflow' and 'staging and replication workflow')", "text": "# Staging folder (needed for 'staging workflow' and 'staging and replication workflow')\narchiver.staging-destination = path/to/local/stage/area", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:3", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Replication folder (needed for 'replication workflow' and 'staging and replication workflow')", "text": "# Replication folder (needed for 'replication workflow' and 'staging and replication workflow')\narchiver.replicated-destination = path/to/mounted/replication/folder", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:4", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "The archiver will refuse to archive group of data sets, which together are smaller than this value", "text": "# The archiver will refuse to archive group of data sets, which together are smaller than this value\narchiver.minimum-container-size-in-bytes = 15000000", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:5", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "The archiver will refuse to archive group of data sets, which together are bigger than this value.", "text": "# The archiver will refuse to archive group of data sets, which together are bigger than this value.\n# The archiver will ignore this value, when archiving single data set.\narchiver.maximum-container-size-in-bytes = 35000000", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:6", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "This variable is meant for another use case, than this archiver, but is shared among all archivers.", "text": "# This variable is meant for another use case, than this archiver, but is shared among all archivers.\n# For this archiver it should be specified for something safely larger than maximum-container-size-in-bytes\narchiver.batch-size-in-bytes = 80000000", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:7", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "(since version 20.10.4) Check consistency between file meta data of the files in the store and from the pathinfo database.", "text": "# (since version 20.10.4) Check consistency between file meta data of the files in the store and from the pathinfo database.\n# If path info db entries are missing or are different than the original files kept in the store, then the archiving is aborted.\n# If \"sanity-check-verify-checksums\" is also set to \"true\", then this consistency check will also verify that the entries stored in the\n# path info db do have the checksums filled in.\n# Default value: true\n# check-consistency-between-store-and-pathinfo-db = true", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:8", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Archiving can be speed up if setting this flag to false (default value: true). But this works only if the data sets", "text": "# Archiving can be speed up if setting this flag to false (default value: true). But this works only if the data sets\n# to be archived do not contain hdf5 files which are handled as folders (like the thumbnail h5ar files in screening/microscopy).\n# archiver.hdf5-files-in-data-set = true", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:9", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Whether all data sets should be archived in a top level directory of archive or with sharding (the way data sets are stored in openbis internal store)", "text": "# Whether all data sets should be archived in a top level directory of archive or with sharding (the way data sets are stored in openbis internal store)\n# archiver.with-sharding = false", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:10", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Polling time for evaluating free space on archive destination", "text": "# Polling time for evaluating free space on archive destination\n# archiver.waiting-for-free-space-polling-time = 1 min", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:11", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Maximum waiting time for free space on archive destination", "text": "# Maximum waiting time for free space on archive destination\n# archiver.waiting-for-free-space-time-out = 4 h", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:12", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "If set to true, then an initial waiting time will be added before starting a sanity check.", "text": "# If set to true, then an initial waiting time will be added before starting a sanity check.\n# If the sanity check fails, it will be retried. The time between each sanity check attempt is doubled,\n# starting from the initial waiting time up to the maximum waiting time (see properties below).\n# Default: false\narchiver.wait-for-sanity-check = true", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:13", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Initial waiting time before starting a sanity check. Works only if 'wait-for-sanity-check = true'", "text": "# Initial waiting time before starting a sanity check. Works only if 'wait-for-sanity-check = true'\n# Default: 10sec\narchiver.wait-for-sanity-check-initial-waiting-time = 120 sec", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:14", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Maximum total waiting time for failed sanity check attempts. Works only if 'wait-for-sanity-check = true'", "text": "# Maximum total waiting time for failed sanity check attempts. Works only if 'wait-for-sanity-check = true'\n# Default: 30min\narchiver.wait-for-sanity-check-max-waiting-time = 5 min", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:15", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "If set to \"true\", then the sanity check will verify that checksums of the original files and checksums of the archived files are the same.", "text": "# If set to \"true\", then the sanity check will verify that checksums of the original files and checksums of the archived files are the same.\n# If \"check-consistency-between-store-and-pathinfo-db\" is also set to \"true\", then the checksums stored in the path info db will be used\n# for the verification of all files. Otherwise, the checksums of the original files may be either taken from the path info db or be calculated on the fly (depending on what is available).\n# Default: true\narchiver.sanity-check-verify-checksums = true", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:16", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "A template of a shell command to be executed before unarchiving. The template may use ${container-path} and ${container-id} variables which will be replaced with an absolute container path (full path of the tar file to be unarchived)", "text": "# A template of a shell command to be executed before unarchiving. The template may use ${container-path} and ${container-id} variables which will be replaced with an absolute container path (full path of the tar file to be unarchived)\n# and a container id (id of the container to be unarchived used in the archiving database). The command created from the template is executed only once for a given container (just before the first unarchiving attempt) and is not retried.\n# The unarchiver waits for the command to finish before proceeding. If the command exits with status zero, then the unarchiving is started. If the command exits with a non-zero value, then the archiving is marked as failed.\n#\n# Example: tar -tf ${container-path}\n# Default: null\narchiver.unarchiving-prepare-command-template", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:17", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "If set to true, then the unarchiver waits for T flag to be removed from the file in the final destination before it tries to read the file.", "text": "# If set to true, then the unarchiver waits for T flag to be removed from the file in the final destination before it tries to read the file.\n# Default: false\narchiver.unarchiving-wait-for-t-flag = true", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:18", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Maximum total waiting time for failed unarchiving attempts.", "text": "# Maximum total waiting time for failed unarchiving attempts.\n# Default: null\narchiver.unarchiving-max-waiting-time = 1d", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:19", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Polling time for waiting on unarchiving.", "text": "# Polling time for waiting on unarchiving.\n# Default: null\narchiver.unarchiving-polling-time = 5 min", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:20", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "If set to true, then the archiver waits for T flag to be set on the file in the replicated destination. The check is done before a potential sanity check of the replicated file (see 'finalizer-sanity-check').", "text": "# If set to true, then the archiver waits for T flag to be set on the file in the replicated destination. The check is done before a potential sanity check of the replicated file (see 'finalizer-sanity-check').\n# Default: false\narchiver.finalizer-wait-for-t-flag = true", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:21", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "If set to true, then a sanity check for the replicated destination is also performed (in addition to a sanity check for the final destination which is always executed).", "text": "# If set to true, then a sanity check for the replicated destination is also performed (in addition to a sanity check for the final destination which is always executed).\n# Default: false\narchiver.finalizer-sanity-check = true", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:22", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Minimum required free space at final destination before triggering archiving if > 0. This threshold can be", "text": "# Minimum required free space at final destination before triggering archiving if > 0. This threshold can be\n# specified as a percentage of total space or number of bytes. If both are specified the threshold is given by\n# the maximum of both values.\n# archiver.minimum-free-space-at-final-destination-in-percentage\n# archiver.minimum-free-space-at-final-destination-in-bytes", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:23", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Minimum free space on archive destination after container file has been added.", "text": "# Minimum free space on archive destination after container file has been added.\n# archiver.minimum-free-space-in-MB = 1024", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:24", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Polling time for waiting on replication. Only needed if archiver.replicated-destination is specified.", "text": "# Polling time for waiting on replication. Only needed if archiver.replicated-destination is specified.\n# archiver.finalizer-polling-time = 1 min", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:25", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Maximum waiting time for replication finished.  Only needed if archiver.replicated-destination is specified.", "text": "# Maximum waiting time for replication finished.  Only needed if archiver.replicated-destination is specified.\n# archiver.finalizer-max-waiting-time = 1 d", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:26", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Maximum total size (in MB) of data sets that can be scheduled for unarchiving at any given time. When not specified, defaults to 1 TB.", "text": "# Maximum total size (in MB) of data sets that can be scheduled for unarchiving at any given time. When not specified, defaults to 1 TB.\n# Note also that the value specified must be consistent with the scratch share size.\n# maximum-unarchiving-capacity-in-megabytes = 200000", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:27", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Delay unarchiving. Needs MultiDataSetUnarchivingMaintenanceTask.", "text": "# Delay unarchiving. Needs MultiDataSetUnarchivingMaintenanceTask.\n# archiver.delay-unarchiving = false", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:28", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Size of the buffer used for copying data. Default value: 1048576 (i.e. 1 MB). This value is only important in case of accurate", "text": "# Size of the buffer used for copying data. Default value: 1048576 (i.e. 1 MB). This value is only important in case of accurate\n# measurements of data transfer rates. In case of expected fast transfer rates a larger value (e.g. 10 MB) should be used.\n# archiver.buffer-size = 1048576", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:29", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Maximum size of the writing queue for copying data. Reading from the data store and writing to the TAR file is", "text": "# Maximum size of the writing queue for copying data. Reading from the data store and writing to the TAR file is\n# done in parallel. The default value 5 * archiver.buffer-size.\n# archiver.maximum-queue-size-in-bytes = 5242880", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:30", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Path (absolute or relative to store root) of an empty file. If this file is present starting", "text": "# Path (absolute or relative to store root) of an empty file. If this file is present starting\n# archiving will be paused until this file has been removed.\n# This property is useful for archiving media/facilities with maintenance downtimes.\n# archiver.pause-file = pause-archiving", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:31", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Time interval between two checks whether pause file still exists or not.", "text": "# Time interval between two checks whether pause file still exists or not.\n# archiver.pause-file-polling-time = 10 min\n\n#-------------------------------------------------------------------------------------------------------\n# Clean up properties\n#\n# A comma-separated list of path to folders which should be cleaned in a separate thread\n#archiver.cleaner.file-path-prefixes-for-async-deletion = <absolute path 1>, <absolute path 2>, ...", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:32", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "A folder which will contain deletion request files. This is a mandatory property if", "text": "# A folder which will contain deletion request files. This is a mandatory property if\n# archiver.cleaner.file-path-prefixes-for-async-deletion is specified.\n#archiver.cleaner.deletion-requests-dir = <some local folder>", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:33", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Polling time interval for looking and performing deletion requests. Default value is 10 minutes.", "text": "# Polling time interval for looking and performing deletion requests. Default value is 10 minutes.\n#archiver.cleaner.deletion-polling-time = 10 min", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:34", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Time out of deletion requests. Default value is one day.", "text": "# Time out of deletion requests. Default value is one day.\n#archiver.cleaner.deletion-time-out = 24 h", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:35", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Optional e-mail address. If specified every integer multiple of the timeout period an e-mail is send to", "text": "# Optional e-mail address. If specified every integer multiple of the timeout period an e-mail is send to\n# this address listing all deletion requests older than specified timeout.\n#archiver.cleaner.email-address = <some valid e-mail address>", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:36", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Optional e-mail address for the 'from' field.", "text": "# Optional e-mail address for the 'from' field.\n#archiver.cleaner.email-from-address = <some well-formed e-mail address>", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:37", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Subject for the 'subject' field. Mandatory if an e-mail address is specified.", "text": "# Subject for the 'subject' field. Mandatory if an e-mail address is specified.\n#archiver.cleaner.email-subject = Deletion failure", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:38", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of", "text": "# Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of\n# lines. Two lines for each deletion request. One for the absolute file path and one of the request time stamp.\n# Mandatory if an e-mail address is specified.\n#archiver.cleaner.email-template = The following files couldn't be deleted:\\n${file-list}", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_multi-data-set-archiving:39", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/multi-data-set-archiving.html", "repo": "openbis", "title": "Multi data set archiving", "section": "Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of", "text": "# Template with variable ${file-list} for the body text of the e-mail. The variable will be replaced by a list of\n# lines. Two lines for each deletion request. One for the absolute file path and one of the request time stamp.\n# Mandatory if an e-mail address is specified.\n#archiver.cleaner.email-template = The following files couldn't be deleted:\\n${file-list}\n\n\n\n#-------------------------------------------------------------------------------------------------------\n# The following properties are necessary in combination with data source configuration\nmulti-dataset-archive-database.kind = prod\nmulti-dataset-archive-sql-root-folder = datastore_server/sql/multi-dataset-archive\nYou should make sure that all destination directories exist and\nDSS has read/write privileges before attempting archiving\n(otherwise the operation will fail).\nAdd the core plugin module name (here\nmulti-dataset-archiver\n)\nto the property\nenabled-modules\nof\ncore-plugin.properties\n.\nEnable PostgreSQL datasource\nConfigure a new DSS core plugin of type\ndata-sources\n## :\nmulti-dataset-archiver/1/dss/data-sources/multi-dataset-archiver-db/plugin.properties\nversion-holder-class = ch.systemsx.cisd.openbis.dss.generic.server.plugins.standard.archiver.dataaccess.MultiDataSetArchiverDBVersionHolder\ndatabaseEngineCode = postgresql\nbasicDatabaseName = multi_dataset_archive\nurlHostPart = ${multi-dataset-archive-database.url-host-part:localhost}\ndatabaseKind = ${multi-dataset-archive-database.kind:prod}\nscriptFolder = ${multi-dataset-archive-sql-root-folder:}\nowner = ${multi-dataset-archive-database.owner:}\npassword = ${multi-dataset-archive-database.password:}\nCreate a share which will be used exclusively as a scratch share for\nunarchiving. To mark it for this purpose add a\nshare.properties\nfile to the share (e.g.\n<mounted\nshare>/store/1/share.properties\n)\nwith property\nunarchiving-scratch-share\n=\ntrue\n.\nIn addition the maximum size of the share can be specified. Example:\nshare.properties\nunarchiving-scratch-share = true\nunarchiving-scratch-share-maximum-size-in-GB = 100\nIt is recommended to do archiving in a separate queue in order to\navoid situation when fast processing plugin tasks are not processes\nbecause multi data set archiving tasks can take quite long. If one\nof the two workflows with replication is selected\n(i.e.\narchiver.replicated-destination\n) a second processing plugin\n## (ID\n## Archiving\n## Finalizer\n) is used. It should run in a queue\ndifferent from the queue used for archiving. The following setting\nin DSS\nservice.properties\n## covers all workflows:\nservice.properties\ndata-set-command-queue-mapping = archiving:Archiving|Copying data sets to archive, unarchiving:Unarchiving, archiving-finalizer:Archiving Finalizer\nClean up Unarchiving Scratch Share\n\n(Since version 20.10.4) Data sets in the unarchiving scratch share can\nbe removed any times because they are already present in archive.\nNormally this happens during unarchving if there is not enough free\nspace available in the scratch share. But this may fail for some reason.\nThis can lead to the effect that unarchiving doesn’t work because they\nare data sets in the scratch share which can be removed because they are\narchived.\nTherefore, it is recommended to setup a\nCleanUpUnarchivingScratchShareTask\nwhich removes data sets from the scratch share which fulfill the\n## following conditions:\nThe data set is in state ARCHIVED and the flag\npresentInArchive\nis set.\nThe data set is found in the Multi Data Set Archive database and the\ncorresponding TAR archive file exists.\nDeletion of archived Data Sets\n\n(Since version 20.10.3) Archived data sets can be deleted permanently.\nBut they are still in the container files. In order to remove them also\nfrom the container files a\nMultiDataSetDeletionMaintenanceTask\nhas to be configured.\nRecovery from corrupted archiving queues\n\nIn case the queues with the archiving commands get corrupted, they\ncannot be used any more, they need to be deleted before the DSS starts\nand a new one will be created. The typical scenario where this happens\nis when you get out of space on the disk where the queues are stored.\nThe following steps describe how to recover from such a situation.\nFinding out the data sets that are in ‘ARCHIVE_PENDING’ status.\n## SELECT\nid\n,\nsize\n,\npresent_in_archive\n,\nshare_id\n,\nlocation\n## FROM\nexternal_data\n## WHERE\nstatus\n=\n## 'ARCHIVE_PENDING'\n;\nopenbis_prod\n=>\n## SELECT\nid\n,\nsize\n,\npresent_in_archive\n,\nshare_id\n,\nlocation\n## FROM\nexternal_data\n## WHERE\nstatus\n=\n## 'ARCHIVE_PENDING'\n;\ndata_id\n|\nsize\n|\npresent_in_archive\n|\nshare_id\n|\nlocation\n---------+-------------+--------------------+----------+-----------------------------------------------------------------------\n3001\n|\n34712671864\n|\nf\n|\n1\n|\n585\n## D8354\n-\n92\n## A3\n-\n4\n## C24\n-\n9621\n-\n## F6B7063A94AC\n/\n17\n/\n65\n/\na4\n/\n20170712111421297\n-\n37998\n3683\n|\n29574172672\n|\nf\n|\n1\n|\n585\n## D8354\n-\n92\n## A3\n-\n4\n## C24\n-\n9621\n-\n## F6B7063A94AC\n/\n39\n/\n6\nc\n/\nb0\n/\n20171106181516927\n-\n39987\n3688\n|\n53416316928\n|\nf\n|\n1\n|\n585\n## D8354\n-\n92\n## A3\n-\n4\n## C24\n-\n9621\n-\n## F6B7063A94AC\n/\nca\n/\n3\nb\n/\n93\n/\n20171106183212074\n-\n39995\n3692\n|\n47547908096\n|\nf\n|\n1\n|\n585\n## D8354\n-\n92\n## A3\n-\n4\n## C24\n-\n9621\n-\n## F6B7063A94AC\n/\nb7\n/\n26\n/\n85\n/\n20171106185354378\n-\n40002\nThe data sets found, can be or not in the archiving process. This is\nnot easy to find out instantly. It’s easier just to execute the\nabove statement in subsequent days.\nIf the data sets are still in ‘ARCHIVE_PENDING’ after a sensible\namount of time (1 week for example) and there is no other issues,\nlike the archiving destination is not available there is a good\nchange, they are really stuck on the process.\nReaching this point, the data sets are most likely still on the data\nstore as indicated by the combination of share ID and location\nindicated. Verify this! If they are not there hope they are archived\nor you are on trouble.\nIf they are on the store, you need to set the status to available\nagain using a SQL statement.\nopenbis_prod\n=>\n## UPDATE\nexternal_data\n## SET\nstatus\n=\n## 'AVAILABLE'\n,\npresent_in_archive\n=\n'f'\n## WHERE\nid\n## IN\n(\n## SELECT\nid\n## FROM\ndata\nwhere\ncode\nin\n(\n'20170712111421297-37998'\n,\n'20171106181516927-39987'\n));\nIf there is half copied files on the archive destination, these need\nto be delete too, to find them run the next queries.\nTo find out the containers:\n## SELECT\n*\n## FROM\ndata_sets\n## WHERE\n## CODE\n## IN\n(\n'20170712111421297-37998'\n,\n'20171106181516927-39987'\n,\n'20171106183212074-39995'\n,\n'20171106185354378-40002'\n);\nmulti_dataset_archive_prod\n=>\n## SELECT\n*\n## FROM\ndata_sets\n## WHERE\n## CODE\n## IN\n(\n'20170712111421297-37998'\n,\n'20171106181516927-39987'\n,\n'20171106183212074-39995'\n,\n'20171106185354378-40002'\n);\nid\n|\ncode\n|\nctnr_id\n|\nsize_in_bytes\n-----+-------------------------+---------+---------------\n294\n|\n20170712111421297\n-\n37998\n|\n60\n|\n34712671864\n295\n|\n20171106185354378\n-\n40002\n|\n61\n|\n47547908096\n296\n|\n20171106183212074\n-\n39995\n|\n61\n|\n53416316928\n297\n|\n20171106181516927\n-\n39987\n|\n61\n|\n29574172672\n(\n4\nrows\n)\nmulti_dataset_archive_prod\n=>\n## SELECT\n*\n## FROM\ncontainers\n## WHERE\nid\n## IN\n(\n60\n,\n61\n);\nid\n|\npath\n|\nunarchiving_requested\n----+---------------------------------------------+-----------------------\n60\n|\n20170712111421297\n-\n37998\n-\n20171108\n-\n105339\n.\ntar\n|\nf\n61\n|\n20171106185354378\n-\n40002\n-\n20171108\n-\n130342\n.\ntar\n|\nf\n## Note\nWe have never seen it but if there is a container with data\nsets in different archiving status then, you need to recover the\nARCHIVED data sets from the container and copy them manually to the\ndata store before being able to continue.\nmulti_dataset_archive_prod\n=>\n## SELECT\n*\n## FROM\ndata_sets\n## WHERE\nctnr_id\n## IN\n(\n## SELECT\nctnr_id\n## FROM\ndata_sets\n## WHERE\n## CODE\n## IN\n(\n'20170712111421297-37998'\n,\n'20171106181516927-39987'\n,\n'20171106183212074-39995'\n,\n'20171106185354378-40002'\n));\nAfter deleting the files clean up the multi dataset archiver\ndatabase.\nopenbis_prod\n=>\n## DELETE\n## FROM\ncontainers\n## WHERE\nid\n## IN\n(\n## SELECT\nctnr_id\n## FROM\ndata_sets\n## WHERE\n## CODE\n## IN\n(\n'20170712111421297-37998'\n,\n'20171106181516927-39987'\n,\n'20171106183212074-39995'\n,\n'20171106185354378-40002'\n));", "timestamp": "2025-09-18T09:38:29.884580Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_openbis-sync:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/openbis-sync.html", "repo": "openbis", "title": "openBIS Sync", "section": "Introduction", "text": "openBIS Sync\n\n## Introduction\n\nSync is a service of openBIS and comes with every instance.\nSync allows to synchronize two openBIS instances using the OAI-PMH protocol.\nThis protocol has two participants:\nOne instance (called\n## Data\n## Source\n) provides the data (types, meta-data and data sets).\nAnother instance (called\n## Harvester\n) grabs these data and makes them available.\nIn regular time intervals, the\n## Harvester\ninstance will synchronize its data with the data on the\n## Data\n## Source\ninstance.\nSynchronization will add and/or delete data to the\n## Harvester\ninstance.\nAn openBIS instance can be one or both\n## Data\n## Source\nand\n## Harvester\nsince these are separate services.\nAn openBIS instance only needs one\n## Data\n## Source\nservice. Even with many participants since it decides what to share depending on the user requesting the information.\nAn openBIS instance only needs one\n## Harvester\nservice. Even with many participants since it goes through a list of\n## Data\n## Source\n.\nData Source Service Configuration\n\n## The\n## Data\n## Source\ninstance provides a service based on the ResourceSync Framework Specification (see\nhttp://www.openarchives.org/rs/1.1/resourcesync\n).\nThis service is configured by default in all new installations as a core plugin as\ncore plugin\nmodule\nopenbis-sync\n.\nSo in theory all openBIS instances are by default a\n## Data\n## Source\n. This should not worry admins, since users cannot access any data though the\n## Data\n## Source\nendpoint that they could not already with the UI or standard API.\nAs a\n## Data\n## Source\nis key to learn to configure the module\nopenbis-sync\n, this module has two config files:\n# Main Configuration File\n./core-plugins/openbis-sync/2/dss/servlet-services/resource-sync/plugin.properties\n# Configuration file providing an AS datasource to the DSS\n./core-plugins/openbis-sync/2/dss/data-sources/openbis-db/plugin.properties\nIs strongly encouraged that any overrides to the values of any configuration file should be done forwarding\nsuch properties to AS service.properties keys.\nThe main configuration file defines the URL where the\n## Data\n## Source\nwill be available.\nWhen using the default settings for the openBIS installation is not necessary to modify it.\nSpecial attention to\nrequest-handler.file-service-repository-path\nthat should point to the\nfile-server\n.\nplugin.properties\n# ./core-plugins/openbis-sync/2/dss/servlet-services/resource-sync/plugin.properties\nclass\n=\nch.systemsx.cisd.openbis.dss.generic.server.oaipmh.OaipmhServlet\npath\n=\n${openbis-sync.servlet-services.resource-sync.path:/datastore_server/re-sync/*}\nrequest-handler\n=\n${openbis-sync.servlet-services.resource-sync.request-handler:ch.ethz.sis.openbis.generic.server.dss.plugins.sync.datasource.DataSourceRequestHandler}\nrequest-handler.server-url\n=\n${server-url}/openbis\nrequest-handler.download-url\n=\n${download-url}\nrequest-handler.file-service-repository-path\n=\n${openbis-sync.servlet-services.resource-sync.request-handler.file-service-repository-path:../../data/file-server}\nauthentication-handler\n=\n${openbis-sync.servlet-services.resource-sync.authentication-handler:ch.systemsx.cisd.openbis.dss.generic.server.oaipmh.BasicHttpAuthenticationHandler}\nThe second configuration file is just there to create an AS database data source.\nSpecial attention to\ndatabaseKind\n, if is not the default\nprod\n.\nplugin.properties\n# ./core-plugins/openbis-sync/2/dss/data-sources/openbis-db/plugin.properties\n#\n# Data source used to determine which entities have been deleted\n#\ndatabaseEngineCode\n=\n${openbis-sync.data-sources.openbis-db.databaseEngineCode:postgresql}\nbasicDatabaseName\n=\n${openbis-sync.data-sources.openbis-db.basicDatabaseName:openbis}\n# This needs to match the databaseKind in the openBIS service.properties\ndatabaseKind\n=\n${database.kind:prod}\nIn practice by default the service should be available at\n## <DSS\nbase\nURL>/datastore_server/re-sync\n.\nPlease test this is available in your instance by opening such URL in your browser, using one of the 3 verbs available:\nhttps://openbis-sis-ci-sprint.ethz.ch/datastore_server/re-sync/?verb=about.xml\nhttps://openbis-sis-ci-sprint.ethz.ch/datastore_server/re-sync/?verb=capabilitylist.xml\nhttps://openbis-sis-ci-sprint.ethz.ch:443/datastore_server/re-sync/?verb=resourcelist.xml\nYou will be asked with the openBIS user and password you want to access the service and the service will return an XML file.\nUse case: One Datasource - One or more Harvester\n\nThe key is the fact that the XML file contains only the information visible for the selected user.\nIt is recommended to create a\nuser\non the\n## Data\n## Source\nthat can only see the set of data needed for a\n## Harvester\n.\nThis way is possible to shared different sets of data, using different\nusers\n, for example:\nsync-datasource-user-inventory: This user is OBSERVER of all inventory SPACE.\nsync-datasource-user-project-a: This user is OBSERVER of project A.\nIs recommended that technical users are created on the file authentication system of the instance and their\nrights configured on  the admin UI.\nData Source Service Document\n\nBy default, the URL of the service is\n## <DSS\nbase\nURL>/datastore_server/re-sync\nand supports the verbs mentioned previously.\nFor example for\nhttps://openbis-sis-ci-sprint.ethz.ch/datastore_server/re-sync/?verb=about.xml\n## we get:\n<urlset\nxsi:schemaLocation=\n\"https://sis.id.ethz.ch/software/#openbis/xdterms/ ./xml/xdterms.xsd https://sis.id.ethz.ch/software/#openbis/xmdterms/\"\n>\n<rs:ln\nhref=\n\"https://openbis-sis-ci-sprint.ethz.ch:443/datastore_server/re-sync/?verb=about.xml\"\nrel=\n\"describedby\"\n/>\n<rs:md\ncapability=\n\"description\"\n/>\n<url>\n<loc>\nhttps://openbis-sis-ci-sprint.ethz.ch:443/datastore_server/re-sync/?verb=capabilitylist.xml\n</loc>\n<rs:md\ncapability=\n\"capabilitylist\"\n/>\n</url>\n</urlset>\nFrom capabilities described in the ResourceSync Framework Specification only\nresourcelist\nis supported.\nThe resourcelist returns an XML with all metadata of the data source openBIS instance.\nThis includes master data, meta data including file meta data.\nTwo optional URL parameters filter the data by spaces:\nblack_list\n: comma-separated list of regular expressions. All\nentities which belong to a space which matches one of the regular\nexpressions of this list will be suppressed.\nwhite_list\n: comma-separated list of regular expressions. If\ndefined only entities which belong to a space which matches one of\nthe regular expressions of this list will be delivered (if not\nsuppressed by the black list).\n## Remarks:\nBasic HTTP authentication is used for authentication.\nThe resourcelist capability returns only data visible for the user\nwhich did the authentication.\n### Harvester Service Configuration\n\nThis service needs to be configured in a case by case basis.\n## A\n## Harvester\ncan sync with one or more\n## Data\n## Source\nopenBIS instances.\nFor that a\n## Harvester\nmaintenance task\nhas to be configured\non the\n## Harvester\nopenBIS instance.\n## The\n## Harvester\nis a DSS maintenance task. An example config file follows:\nplugin.properties\nclass\n=\nch.ethz.sis.openbis.generic.server.dss.plugins.sync.harvester.HarvesterMaintenanceTask\ninterval\n=\n1 d\nharvester-config-file\n=\n../../data/harvester-config.txt\nThe only specific property of\nHarvesterMaintenanceTask\nis\nharvester-config-file\nwhich is absolute or relative path to the actual\nconfiguration file. This separation in two configuration files has been\ndone because\nplugin.properties\nis only read once (at start up of DSS).\nThus changes in Harvester configuration would be possible without\nrestarting DSS.\nHere is an example of a typical configuration:\nharvester-config.txt\n## [LAB1]\nresource-list-url\n=\nhttps://<data source host>:<DSS port>/datastore_server/re-sync\ndata-source-openbis-url\n=\nhttps://<data source host>:<AS port>/openbis/openbis\ndata-source-dss-url\n=\nhttps://<data source host>:<DSS port>/datastore_server\ndata-source-auth-realm\n=\n## OAI-PMH\ndata-source-auth-user\n=\n<data source user id>\ndata-source-auth-pass\n=\n<data source password>\nspace-black-list\n=\n## SYSTEM\nspace-white-list\n=\n## LAB1.*\nharvester-user\n=\n<harvester user id>\nharvester-pass\n=\n<harvester user password>\nkeep-original-timestamps-and-users\n=\nfalse\nharvester-tmp-dir\n=\ntemp\nlast-sync-timestamp-file\n=\n../../data/last-sync-timestamp-file_HRVSTR.txt\nlog-file\n=\nlog/synchronization.log\nemail-addresses\n=\n<e-mail 1>, <e-mail 2>, ...\ntranslate-using-data-source-alias\n=\ntrue\nverbose\n=\ntrue\n#dry-run = true\nThe configuration file can have one or many section for each openBIS\ninstance. Each section start with an arbitrary name in square\nbrackets. This name becomes the alias of the instance.\nWarning: This alias SHOULD only contain letters and numbers.\nOther characters MIGHT look like the work but can lead to errors when parsing the prefix.\n<data\nsource\nhost>\n,\n## <DSS\nport>\nand\n## <AS\nport>\nhave to be host\nname and ports of the Data Source openBIS instance as seen by the\nHarvester instance.\n<data\nsource\nuser\nid>\nand\n<data\nsource\npassword>\nare the\ncredential to access the Data Source openBIS instance. Only data\nseen by this user is harvested.\nspace-black-list\nand\nspace-white-list\nhave the same meaning\nas\nblack_list\nand\nwhite_list\nas specified above in the Data\nSource section.\n<harvester\nuser\nid>\nand\n<harvester\nuser\npassword>\nare the\ncredential to access the Harvester openBIS instance. It has to be a\nuser with instance admin rights.\n## Temporary\nfiles created during harvesting are stored\nin\nharvester-tmp-dir\nwhich is a path relative to the root of the\ndata store. The root store is specified by\nstoreroot-dir\nin\n## DSS\nservice.properties\n. The default value is\ntemp\n.\nBy default the original timestamps (registration timestamps and\nmodification timestamps) and users (registrator and modifier) are\nsynchronized. If necessary users will be created. With the\nconfiguration property\nkeep-original-timestamps-and-users\n=\nfalse\nno timestamps and users will be synchronized.\n## The\nlast-sync-timestamp-file\nis a relative or absolute path to the\nfile which store the last timestamp of synchronization.\n## The\nlog-file\nis a relative or absolute path to the file where\nsynchronization information is logged. This information does not\nappear in the standard DSS log file.\nIn case of an error an e-mail is sent to the specified e-mail\naddresses.\ntranslate-using-data-source-alias\nis a flag which controls whether\nthe code of spaces, types and materials should have a prefix or not.\nIf true the prefix will be the name in the square bracket followed\nby an underscore. The default value of this flag is false.\nverbose\nflag adds to the synchronization log added, updated and\n## deleted items. Default:\nfalse\nor\ntrue\nif\ndry-run\nflag is set.\ndry-run\nflag allows to run without changing Harvester openBIS\ninstance. This allows to check config errors or errors with the Data\nSource openBIS instance. A dry run will be performed even if this\n## flag is set. Default:\nfalse\nmaster-data-update-allowed\nflag allows to update master data as\nplugins, property types, entity types and entity assignments. Note,\nthat master data can still be added if this flag is\nfalse\n.\n## Default:\nfalse\nproperty-unassignment-allowed\nflag allows to unassign property\nassignments, that is, removing property types from entity types.\n## Default:\nfalse\ndeletion-allowed\nflag allows deletion of entities on the Harvester\n## openBIS instance. Default:\nfalse\nkeep-original-timestamps-and-users\nflag yields that time stamps\nand users are copied from the Data Source to the Harvester.\nOtherwise the entities will have harvester user and the actual\n## registration time stamp. Default:\ntrue\nkeep-original-frozen-flags\nflag yields that the frozen flags are\ncopied from the Data Source to the Harvester. Otherwise entities\nwhich are frozen on the Data Source are not frozen on the Harvester.\n## Default:\ntrue\nThis DSS service access the main openBIS database directly in order to\nsynchronize timestamps and users. If the name of this database isn’t\n{{openbis_prod}} the property\ndatabase.kind\nin DSS service.properties\nshould be defined with the same value as the same property in AS service.properties.\nWhat HarvesterMaintenanceTask does\n\nIn the first step it reads the configuration file from the file path\nspecified by\nharvester-config-file\nin\nplugins.properties\n. Next, the\nfollowing steps will be performed in DRY RUN mode. That is, all data are\nread, parsed and checked but nothing is changed on the Harvester. If no\nerror occured and the\ndry-run\nflag isn’t set the same steps are\nperformed but this time the data is changed (i.e. synced) on the\n## Harvester.\nRead meta data from the Data Source.\nDelete entities from the Harvester which are no longer on the Data\nSource (if\ndeletion-allowed\nflag is set).\nRegister/update master data.\nRegister/update spaces, projects, experiments, samples and\nmaterials.\nRegister/update attachments.\nSynchronize files from the file service.\nRegister/update data sets.\nUpdate timestamps and users (if\nkeep-original-timestamps-and-users\nflag is set).\nUpdate frozen flags (if\nkeep-original-frozen-flags\nflag is set).\nData are registered if they do not exists on the Harvester.\nOtherwise they are updated if the Data Source version has a\nmodification timestamp which is after the last time the\nHarvesterMaintenanceTask has been performed\n## If\ntranslate-using-data-source-alias\nflag is set a prefix is added\nto spaces, types and materials when created.\nTo find out if an entity already exist on the Harvester the perm ID\nis used.\nMaster Data Synchronization Rules\n\nNormally all master data are registered/updated if they do not exists or\nthey are older. But for internal vocabularies and property types\ndifferent rules apply. Internal means that the entity (i.e. a vocabulary\nor a property type) is managed internally (visible by the prefix ‘$’ in\nits code) and has been registered by the system user.\nInternal vocabularies and property types will not be created or\nupdated on the Harvester.\nAn internal vocabulary or property type of the Data Source which\ndoesn’t exist on the Harvester leads to an error.\nAn internal property type which exists on the Data Source and the\nHarvester but have different data type leads to an error.\nTerms of an internal vocabulary are added if they do not exists on\nthe Harvester.", "timestamp": "2025-09-18T09:38:29.890884Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_optional-application-server-configuration:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/optional-application-server-configuration.html", "repo": "openbis", "title": "Optional Application Server Configuration", "section": "Optional Application Server Configuration", "text": "### Optional Application Server Configuration\n\nThe base URL for Web client access to the data store server.\n\ndownload-url = ${host-address}:${port}\nExport data limit in bytes, default to 10Gib\n\nexport.data-limit=10737418240\n## Deleted Entity History\n\nLogging the history of deleted entities can be enabled / disabled in the AS service.properties using setting\nentity-history.enabled = \\[true | false\\]\nSince 20.10.1 the default value is true (meaning, entity history is enabled). Before 20.10.1 the default value was false.\nDeleted entity history can be queried with script\n$INSTALL_PATH/bin/show-history.sh\n.\n## Login Page - Banners\n\nTo add banners to the main openBIS change\nloginHeader.html\npage. It is\nstored in the same directory as\nindex.html\n. Note that if the height of\nloginHeader.html\nis too big, the content may overlap with the rest of\nthe openBIS login page.\nExample of the\nloginHeader.html\n## :\n<center><img src=\"images/banner.gif\"></center>\nFor announcements you have to edit the\nindex.html\nfile. Here is an example showing the tail:\n<\ninput\nstyle\n=\n\"margin-left: 200px\"\ntype\n=\n\"submit\"\nid\n=\n\"openbis_login_submit\"\nvalue\n=\n## \"Login\"\n>\n<\nbr\n>\n<\nbr\n>\n<\nbr\n>\n<\nbr\n>\n<\nspan\nstyle\n=\n\"color:red\"\n>\nDue the server maintenance openBIS\n<\nbr\n>\nwill not be available on 24th of\n<\nbr\n>\nDecember 2023 from 10 am to 3 pm!\n<\nbr\n>\n</\nspan\n>\n</\nform\n>\n</\ndiv\n>\n</\nbody\n>\n</\nhtml\n>\nNote: the current work-around with\nbr\ntags between the lines ensures that the login box is still centered.\n## Client Customization\n\n### Configuration\n\nTo reconfigure some parts of the openBIS Web Client and Data Set Upload\nClient, prepare the configuration file and add the path to the value of\nweb-client-configuration-file\nproperty in openBIS\nservice.properties\n.\nweb-client-configuration-file = etc/web-client.properties\nWeb client customizations\n\nEnable the trashcan. When the trashcan is enabled, deleting entities\nonly marks them as deleted but not deletes them physically (it is\nalso called “logical deletion”). When clicking on the trashcan icon\nin the Web GUI, the user can see all of his deletion operations and\ncan revert them individually. Only an admin can empty the trashcan\nand thus delete the entities physically. Only with enabled trashcan\nis it possible to delete complete hierarchies (e.g. an experiment\nwith samples and datasets attached).\nDefault view mode (\n## SIMPLE/NORMAL\n) that should be used if user\ndoesn’t have it specified in a URL.\nReplacement texts for ‘Experiment’ and ‘Sample’ by\nexperiment-text\nand\nsample-text\n, respectively.\nAnonymous login by default.\nSample, material, experiment and data set\ndetail\nviews\ncan be\n## customized by:\nhiding the sections (e.g. attachments)\n## Additionally\ndata\nset\ndetail\nview\ncan be customized by:\nremoving\n## Smart\n## View\nand\n## File\n## View\nfrom the list of available\nreports in\n## Data\n## View\nsection\nTechnology specific properties with property\ntechnologies\nwhich is\na comma-separated list of technologies. For each technology\nproperties are defined where the property names start with\ntechnology name followed by a dot character.\nData Set Upload Client Customizations\n\nIt is possible to restrict the set of data set types available to the\nuser in the data set uploader. This is useful when there are some data\nset types that a user would never upload; for example, if there are data\nset types that are used only internally exist only to support\nthird-party software.\nThe restriction is specified in the web-client.properties file using\neither a whitelist or a blacklist. If both are specified, the whitelist\nis used. To specify a whitelist, use the key\ncreatable-data-set-types-whitelist\n; for a blacklist, use the key\ncreatable-data-set-types-blacklist\n. The value for the property should\nbe a comma-separated list of regular-expression patterns to match. In\nthe case of the whitelist, data set types that match the specified\npatterns are shown to the user, whereas for the blacklist, the data set\ntypes that match the specified patterns are those that are not shown to\nthe user.\n## Examples\n\nSpecifying a whitelist\nweb-client.properties.\ncreatable-data-set-types-whitelist = .*IMAGE.*, ANALYSIS, THUMBNAIL[0-9]?\nAssume we have the following data set types in our system:\n## PROCESSED-DATA\n,\n## MICROSCOPE-IMAGE\n,\n## IMAGE-SCREENING\n,\n## ANALYSIS\n,\n## ANALYSIS-FEATURES\n,\n## THUMBNAIL1\n,\n## THUMBNAIL-BIG\nIn this case, the follwing data set types will be available to the user:\n## MICROSCOPE-IMAGE\n,\n## IMAGE-SCREENING\n,\n## ANALYSIS\n,\n## THUMBNAIL1\nSpecifying a blacklist\nweb-client.properties.\ncreatable-data-set-types-blacklist = .*IMAGE.*, ANALYSIS, THUMBNAIL[0-9]?\nAssume we have the following data set types in our system:\n## PROCESSED-DATA\n,\n## MICROSCOPE-IMAGE\n,\n## IMAGE-SCREENING\n,\n## ANALYSIS\n,\n## ANALYSIS-FEATURES\n,\n## THUMBNAIL1\n,\n## THUMBNAIL-BIG\nIn this case, the follwing data set types will be available to the user:\n## PROCESSED-DATA\n,\n## ANALYSIS-FEATURES\n,\n## THUMBNAIL-BIG\nFull web-client.properties Example\n\nweb-client.properties\n# Enable the trash can and logical deletion.\n# Default value: false\nenable\n-\ntrash\n=\ntrue\n# Replacement texts for 'Experiment' and 'Sample' in the UI\n# sample-text = Object\n# experiment-text = Collection\n# Default view mode that should be used if user doesn't have it specified in URL.\n# Options: 'NORMAL' (standard or application mode - default), 'SIMPLE' (read-only mode with simplified GUI)\n#\ndefault\n-\nview\n-\nmode\n=\n## SIMPLE\n# Flag specifying whether default login mode is anonymous or not.\n# If true a user-for-anonymous-login has to be defined in service.properties\n# Default value: false\ndefault\n-\nanonymous\n-\nlogin\n=\ntrue\n# Configuration of entity (experiment, sample, data set, material) detail views.\n#\n# Mandatory properties:\n#   - view (entity detail view id)\n#   - types (list of entity type codes)\n# Optional properties:\n#   - hide-sections (list of section ids)\n#   - hide-smart-view (removes \"Smart View\" from Data Set Detail View -> Data View) (generic_dataset_viewer)\n#   - hide-file-view (removes \"File View\" from Data Set Detail View -> Data View) (generic_dataset_viewer)\n# Available sections in entity-detail-views:\n#   generic_dataset_viewer\n#       data-set-data-section\n#       data-set-parents-section\n#       data-set-children-section\n#       query-section\n#   generic_experiment_viewer\n#       data-set-section\n#       attachment-section\n#       query-section\n#       container-sample-section\n#   generic_sample_viewer\n#       container-sample-section\n#       derived-samples-section\n#       parent-samples-section\n#       data-set-section\n#       attachment-section\n#       query-section\n#   generic_material_viewer\n#       query-section\n#\n# Example:\n#\n#detail-views = sample-view, experiment-view, data-view\n#\n#sample-view.view = generic_sample_viewer\n#sample-view.types = STYPE1, STYPE2\n#sample-view.hide-sections = derived-samples-section, container-sample-section\n#\n#experiment-view.view = generic_sample_viewer\n#experiment-view.types = ETYPE1, ETYPE2\n#experiment-view.hide-sections = data-set-section\n#\n#data-view.view = generic_dataset_viewer\n#data-view.types = DSTYPE\n#data-view.hide-smart-view = false\n#data-view.hide-file-view = true\n#technologies = screening\n#screening.image-viewer-enabled = true\n#\n# Only render these types when creating new data sets via the\n# Data Set Upload Client\n#\n#creatable-data-set-types-whitelist=WHITELISTED_TYPE1, WHITELISTED_TYPE2\n#\n# Do not render these types in the Data Set Upload Client.\n# The value of the property is only taken into account if\n# creatable-data-set-types-whitelist is not configured\n#\n#creatable-data-set-types-blacklist=BLACKLISTED_TYPE1, BLACKLISTED_TYPE2\n## Configuring File Servlet\n\nThis service is specially tailored for web applications requiring to\nupload files to the system without using the DataSet concept, it was\nmeant to be used for small images and rich text editors like CKEditor.\n## Property Key\n## Default Value\n## Description\nfile-server.maximum-file-size-in-MB\n10\nMax size of files.\nfile-server.repository-path\n../../../data/file-server\nPath where files will be stored, ideally should be a folder on the same NAS you are storing the DataSets.\nfile-server.download-check\ntrue\nChecks that the user is log in into the system to be able to download files.\nChanging the Capability-Role map\n\nopenBIS uses a map of capabilities to roles to decide what role is\nneeded to perform a given action. The defaults can be overridden by\ncreating a file\netc/capabilities\n. One line in this file has one of the\n## following formats:\n<\n## Capability\n>\n## :\n<\n## Role\n>\n[,\n<\n## ROLE\n>...\n]\n<\n## Capability\n>\n## :\n<\n## Role\n>\n[,\n<\n## ROLE\n>...\n][;\n<\n## Parameter\n>\n=\n<\n## Role\n>\n[,\n<\n## Role\n>...\n]][;\n<\n## Parameter\n>\n=\n<\n## Role\n>\n[,\n<\n## Role\n>\n]]\n...\n<\n## Capability\n>\n## :\n<\n## Parameter\n>\n=\n<\n## Role\n>\n[,\n<\n## Role\n>...\n][;\n<\n## Parameter\n>\n=\n<\n## Role\n>\n[,\n<\n## Role\n>\n]]\n...\nwhich sets a new (minimal) role for the given capability. There is a\nspecial role\n## INSTANCE_DISABLED\nwhich allows to completely disable a\ncapability for an instance. Note: to set multiple roles for single\ncapability use multiple lines in the file.\nThis is the default map:\n## Capability\n## Parameter\n## Default Role\n## Comment\n## WRITE_CUSTOM_COLUMN\n## PROJECT_POWER_USER\n## DELETE_CUSTOM_COLUMN\n## PROJECT_POWER_USER\n## WRITE_FILTER\n## PROJECT_POWER_USER\n## DELETE_FILTER\n## PROJECT_POWER_USER\n## WRITE_EXPERIMENT_SAMPLE\n## PROJECT_USER\n## WRITE_EXPERIMENT_ATTACHMENT\n## PROJECT_USER\n## WRITE_EXPERIMENT_PROPERTIES\n## PROJECT_USER\n## DELETE_EXPERIMENT\n## PROJECT_POWER_USER\n## DELETE_EXPERIMENT_ATTACHMENT\n## PROJECT_POWER_USER\n## WRITE_SAMPLE\n## PROJECT_USER\n## WRITE_SAMPLE_ATTACHMENT\n## PROJECT_USER\n## WRITE_SAMPLE_PROPERTIES\n## PROJECT_USER\n## DELETE_SAMPLE\n## PROJECT_POWER_USER\n## DELETE_SAMPLE_ATTACHMENT\n## PROJECT_POWER_USER\n## WRITE_DATASET\n## PROJECT_POWER_USER\n## WRITE_DATASET_PROPERTIES\n## PROJECT_USER\n## DELETE_DATASET\n## PROJECT_POWER_USER\nDelete datasets (this capability IS NOT enough to delete datasets with deletion_disallow flag set to true in their type - see\n## FORCE_DELETE_DATASET\n)\n## FORCE_DELETE_DATASET\n## INSTANCE_DISABLED\nDelete datasets (this capability IS enough to delete datasets with deletion_disallow flag set to true in their type - see\n## DELETE_DATASET\n)\n## ARCHIVE_DATASET\n## PROJECT_POWER_USER\nMove dataset from data store into archive\n## UNARCHIVE_DATASET\n## PROJECT_USER\nCopy back dataset from archive to data store\n## LOCK_DATA_SETS\n## PROJECT_ADMIN\nPrevent data sets from being archived\n## UNLOCK_DATA_SETS\n## PROJECT_ADMIN\nRelease locked data sets\n## WRITE_EXPERIMENT_SAMPLE_MATERIAL\n## INSTANCE_ADMIN\nRegistration / update of experiments, samples and materials in one go\n## REGISTER_SPACE\n## SPACE_ADMIN\nThe user will become space admin of the freshly created space\n## DELETE_SPACE\n## SPACE_ADMIN\n## UPDATE_SPACE\n## SPACE_ADMIN\n## REGISTER_PROJECT\n## SPACE_POWER_USER\n## WRITE_PROJECT\n## SPACE_POWER_USER\n,\n## PROJECT_ADMIN\n## WRITE_PROJECT_ATTACHMENT\n## SPACE_POWER_USER\n,\n## PROJECT_ADMIN\n## DELETE_PROJECT\n## SPACE_POWER_USER\n,\n## PROJECT_ADMIN\n## DELETE_PROJECT_ATTACHMENT\n## SPACE_POWER_USER\n,\n## PROJECT_ADMIN\n## REGISTER_VOCABULARY\n## INSTANCE_ADMIN\n## WRITE_VOCABULARY\n## INSTANCE_ADMIN\n## DELETE_VOCABULARY\n## INSTANCE_ADMIN\n## WRITE_VOCABULARY_TERM\n## INSTANCE_ADMIN\n## WRITE_UNOFFICIAL_VOCABULARY_TERM\n## PROJECT_USER\n## PURGE\n## PROJECT_ADMIN\nPermanently delete experiments, samples and datasets in the trashcan (this capability IS NOT enough to delete datasets with deletion_disallow flag set to true in their type - see\n## FORCE_PURGE\n)\n## FORCE_PURGE\n## INSTANCE_DISABLED\nPermanently delete experiments, samples and datasets in the trashcan (this capability IS enough to delete datasets with deletion_disallow flag set to true in their type - see\n## PURGE\n)\n## REVERT_DELETION\n## PROJECT_USER\nGet back experiments, samples and datasets from the trashcan\n## ASSIGN_EXPERIMENT_TO_PROJECT\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## ASSIGN_PROJECT_TO_SPACE\n## SPACE_POWER_USER\n,\n## SPACE_ETL_SERVER\n## ASSIGN_SAMPLE_TO_EXPERIMENT\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\nRe-assign a sample to a new experiment (called in ‘register experiment’, ‘update experiment’, ‘update sample’’)\n## UNASSIGN_SAMPLE_FROM_EXPERIMENT\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## ASSIGN_SAMPLE_TO_SPACE\n## SPACE_POWER_USER\n,\n## SPACE_ETL_SERVER\nRe-assign a sample to a new space (called in ‘update sample’)\n## ASSIGN_DATASET_TO_EXPERIMENT\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## ASSIGN_DATASET_TO_SAMPLE\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## SHARE_SAMPLE\n## INSTANCE_ADMIN\n,\n## INSTANCE_ETL_SERVER\n## UNSHARE_SAMPLE\n## INSTANCE_ADMIN\n,\n## INSTANCE_ETL_SERVER\n## ADD_PARENT_TO_SAMPLE\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## ADD_PARENT_TO_SAMPLE\n## SAMPLE\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## ADD_PARENT_TO_SAMPLE\n## PARENT\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## REMOVE_PARENT_FROM_SAMPLE\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## REMOVE_PARENT_FROM_SAMPLE\n## SAMPLE\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## REMOVE_PARENT_FROM_SAMPLE\n## PARENT\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## ADD_CONTAINER_TO_SAMPLE\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## REMOVE_CONTAINER_FROM_SAMPLE\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## ADD_PARENT_TO_DATASET\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## REMOVE_PARENT_FROM_DATASET\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## ADD_CONTAINER_TO_DATASET\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## REMOVE_CONTAINER_FROM_DATASET\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## ASSIGN_ROLE_TO_SPACE_VIA_DSS\n## SPACE_ADMIN\n,\n## INSTANCE_ETL_SERVER\n## CREATE_SPACES_VIA_DSS\n## SPACE_ADMIN\n,\n## INSTANCE_ETL_SERVER\n## CREATE_PROJECTS_VIA_DSS\n## SPACE_POWER_USER\n,\n## SPACE_ETL_SERVER\n## UPDATE_PROJECTS_VIA_DSS\n## SPACE_POWER_USER\n,\n## PROJECT_ADMIN\n,\n## SPACE_ETL_SERVER\n## CREATE_EXPERIMENTS_VIA_DSS\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## UPDATE_EXPERIMENTS_VIA_DSS\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## CREATE_SPACE_SAMPLES_VIA_DSS\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## UPDATE_SPACE_SAMPLES_VIA_DSS\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## CREATE_INSTANCE_SAMPLES_VIA_DSS\n## INSTANCE_ETL_SERVER\n## UPDATE_INSTANCE_SAMPLES_VIA_DSS\n## INSTANCE_ETL_SERVER\n## CREATE_MATERIALS_VIA_DSS\n## INSTANCE_ETL_SERVER\n## UPDATE_MATERIALS_VIA_DSS\n## INSTANCE_ETL_SERVER\n## CREATE_DATA_SETS_VIA_DSS\n## PROJECT_USER\n,\n## SPACE_ETL_SERVER\n## UPDATE_DATA_SETS_VIA_DSS\n## PROJECT_POWER_USER\n,\n## SPACE_ETL_SERVER\n## SEARCH_ON_BEHALF_OF_USER\n## INSTANCE_OBSERVER\nAll search or list operations being performed on behalf of another user. Supposed to be used by a service user for server-to-server communication tasks.\nOlder versions of openBIS used to allow changing entity relationships to\nregular\n## SPACE_USER\n. If you want to get this behavior back, put these\nlines into\netc/capabilities\n## :\n## ASSIGN_EXPERIMENT_TO_PROJECT\n## :\n## SPACE_USER\n## ASSIGN_EXPERIMENT_TO_PROJECT\n## :\n## SPACE_ETL_SERVER\n## ASSIGN_SAMPLE_TO_EXPERIMENT\n## :\n## SPACE_USER\n## ASSIGN_SAMPLE_TO_EXPERIMENT\n## :\n## SPACE_ETL_SERVER\n## UNASSIGN_SAMPLE_FROM_EXPERIMENT\n## :\n## SPACE_USER\n## UNASSIGN_SAMPLE_FROM_EXPERIMENT\n## :\n## SPACE_ETL_SERVER\n## ASSIGN_SAMPLE_TO_SPACE\n## :\n## SPACE_USER\n## ASSIGN_SAMPLE_TO_SPACE\n## :\n## SPACE_ETL_SERVER\n## ASSIGN_DATASET_TO_EXPERIMENT\n## :\n## SPACE_USER\n## ASSIGN_DATASET_TO_EXPERIMENT\n## :\n## SPACE_ETL_SERVER\n## ASSIGN_DATASET_TO_SAMPLE\n## :\n## SPACE_USER\n## ASSIGN_DATASET_TO_SAMPLE\n## :\n## SPACE_ETL_SERVER\n## ADD_PARENT_TO_SAMPLE\n## :\n## SPACE_USER\n## ADD_PARENT_TO_SAMPLE\n## :\n## SPACE_ETL_SERVER\n## REMOVE_PARENT_FROM_SAMPLE\n## :\n## SPACE_USER\n## REMOVE_PARENT_FROM_SAMPLE\n## :\n## SPACE_ETL_SERVER\n## ADD_CONTAINER_TO_SAMPLE\n## :\n## SPACE_USER\n## ADD_CONTAINER_TO_SAMPLE\n## :\n## SPACE_ETL_SERVER\n## REMOVE_CONTAINER_FROM_SAMPLE\n## :\n## SPACE_USER\n## REMOVE_CONTAINER_FROM_SAMPLE\n## :\n## SPACE_ETL_SERVER\n## ADD_PARENT_TO_DATASET\n## :\n## SPACE_USER\n## ADD_PARENT_TO_DATASET\n## :\n## SPACE_ETL_SERVER\n## REMOVE_PARENT_FROM_DATASET\n## :\n## SPACE_USER\n## REMOVE_PARENT_FROM_DATASET\n## :\n## SPACE_ETL_SERVER\n## ADD_CONTAINER_TO_DATASET\n## :\n## SPACE_USER\n## ADD_CONTAINER_TO_DATASET\n## :\n## SPACE_ETL_SERVER\n## REMOVE_CONTAINER_FROM_DATASET\n## :\n## SPACE_USER\n## REMOVE_CONTAINER_FROM_DATASET\n## :\n## SPACE_ETL_SERVER\nCapability Role Map for V3 API\n\nMethod of IApplicationServerApi\n## Default Roles\n## Capability\narchiveDataSets\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## ARCHIVE_DATASET\nconfirmDeletions, forceDeletion == false\n## PROJECT_ADMIN, SPACE_ETL_SERVER\n## CONFIRM_DELETION\nconfirmDeletions, forceDeletion == true\ndisabled\n## CONFIRM_DELETION_FORCED\ncreateAuthorizationGroups\n## INSTANCE_ADMIN\n## CREATE_AUTHORIZATION_GROUP\ncreateCodes\n## PROJECT_USER, SPACE_ETL_SERVER\n## CREATE_CODES\ncreateDataSetTypes\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## CREATE_DATASET_TYPE\ncreateDataSets\n## PROJECT_USER, SPACE_ETL_SERVER\n## CREATE_DATASET\ncreateExperimentTypes\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## CREATE_EXPERIMENT_TYPE\ncreateExperiments\n## PROJECT_USER, SPACE_ETL_SERVER\n## CREATE_EXPERIMENT\ncreateExternalDataManagementSystems\n## INSTANCE_ADMIN\n## CREATE_EXTERNAL_DMS\ncreateMaterialTypes\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## CREATE_MATERIAL_TYPE\ncreateMaterials\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## CREATE_MATERIAL\ncreatePermIdStrings\n## PROJECT_USER, SPACE_ETL_SERVER\n## CREATE_PERM_IDS\ncreatePersons\n## INSTANCE_ADMIN\n## CREATE_PERSON\ncreatePlugins\n## INSTANCE_ADMIN\n## CREATE_PLUGIN\ncreateProjects\n## SPACE_POWER_USER, SPACE_ETL_SERVER\n## CREATE_PROJECT\ncreatePropertyTypes\n## INSTANCE_ADMIN\n## CREATE_PROPERTY_TYPE\ncreateQueries\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## CREATE_QUERY\ncreateRoleAssignments, instance role\n## INSTANCE_ADMIN\n## CREATE_INSTANCE_ROLE\ncreateRoleAssignments, space role\n## SPACE_ADMIN\n## CREATE_SPACE_ROLE\ncreateRoleAssignments, project role\n## PROJECT_ADMIN\n## CREATE_PROJECT_ROLE\ncreateSampleTypes\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## CREATE_SAMPLE_TYPE\ncreateSamples\n## PROJECT_USER, SPACE_ETL_SERVER\n## CREATE_SAMPLE\ncreateSemanticAnnotations\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## CREATE_SEMANTIC_ANNOTATION\ncreateSpaces\n## SPACE_ADMIN, SPACE_ETL_SERVER\n## CREATE_SPACE\ncreateTags\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## CREATE_TAG\ncreateVocabularies\n## INSTANCE_ADMIN\n## CREATE_VOCABULARY\ncreateVocabularyTerms, official terms\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## CREATE_OFFICIAL_VOCABULARY_TERM\ncreateVocabularyTerms, unofficial terms\n## PROJECT_USER, SPACE_ETL_SERVER\n## CREATE_UNOFFICIAL_VOCABULARY_TERM\ndeleteAuthorizationGroups\n## INSTANCE_ADMIN\n## DELETE_AUTHORIZATION_GROUP\ndeleteDataSetTypes\n## INSTANCE_ADMIN\n## DELETE_DATASET_TYPE\ndeleteDataSets\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## DELETE_DATASET\ndeleteExperimentTypes\n## INSTANCE_ADMIN\n## DELETE_EXPERIMENT_TYPE\ndeleteExperiments\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## DELETE_EXPERIMENT\ndeleteExternalDataManagementSystems\n## INSTANCE_ADMIN\n## DELETE_EXTERNAL_DMS\ndeleteMaterialTypes\n## INSTANCE_ADMIN\n## DELETE_MATERIAL_TYPE\ndeleteMaterials\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## DELETE_MATERIAL\ndeleteOperationExecutions\n## PROJECT_USER, SPACE_ETL_SERVER\n## DELETE_OPERATION_EXECUTION\ndeletePlugins\n## INSTANCE_ADMIN\n## DELETE_PLUGIN\ndeleteProjects\n## SPACE_POWER_USER, PROJECT_ADMIN, SPACE_ETL_SERVER\n## DELETE_PROJECT\ndeletePropertyTypes\n## INSTANCE_ADMIN\n## DELETE_PROPERTY_TYPE\ndeleteQueries\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## DELETE_QUERY\ndeleteRoleAssignments, instance role\n## INSTANCE_ADMIN\n## DELETE_INSTANCE_ROLE\ndeleteRoleAssignments, space role\n## SPACE_ADMIN\n## DELETE_SPACE_ROLE\ndeleteRoleAssignments, project role\n## PROJECT_ADMIN\n## DELETE_PROJECT_ROLE\ndeleteSampleTypes\n## INSTANCE_ADMIN\n## DELETE_SAMPLE_TYPE\ndeleteSamples\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## DELETE_SAMPLE\ndeleteSemanticAnnotations\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## DELETE_SEMANTIC_ANNOTATION\ndeleteSpaces\n## SPACE_ADMIN, SPACE_ETL_SERVER\n## DELETE_SPACE\ndeleteTags\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## DELETE_TAG\ndeleteVocabularies\n## INSTANCE_ADMIN\n## DELETE_VOCABULARY\ndeleteVocabularyTerms\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## DELETE_VOCABULARY_TERM\nexecuteAggregationService\n## PROJECT_OBSERVER\n## EXECUTE_AGGREGATION_SERVICES\nexecuteCustomASService\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## EXECUTE_CUSTOM_AS_SERVICE\nexecuteProcessingService\n## PROJECT_USER\n## EXECUTE_PROCESSING_SERVICES\nexecuteQuery\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## EXECUTE_QUERY\nexecuteReportingService\n## PROJECT_OBSERVER\n## EXECUTE_REPORTING_SERVICES\nexecuteSearchDomainService\n## PROJECT_OBSERVER\n## EXECUTE_SEARCH_DOMAIN_SERVICES\nexecuteSql\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## EXECUTE_QUERY\ngetAuthorizationGroups\n## PROJECT_ADMIN\n## GET_AUTHORIZATION_GROUP\ngetDataSetTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_DATASET_TYPE\ngetDataSets\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_DATASET\ngetExperimentTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_EXPERIMENT_TYPE\ngetExperiments\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_EXPERIMENT\ngetExternalDataManagementSystems\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_EXTERNAL_DMS\ngetMaterialTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_MATERIAL_TYPE\ngetMaterials\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_MATERIAL\ngetOperationExecutions\n## PROJECT_USER, SPACE_ETL_SERVER\n## GET_OPERATION_EXECUTION\ngetPersons\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_PERSON\ngetPlugins\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_PLUGIN\ngetProjects\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_PROJECT\ngetPropertyTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_PROPERTY_TYPE\ngetQueries\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_QUERY\ngetRoleAssignments\n## PROJECT_ADMIN\n## GET_ROLE_ASSIGNMENT\ngetSampleTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_SAMPLE_TYPE\ngetSamples\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_SAMPLE\ngetSemanticAnnotations\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_SEMANTIC_ANNOTATION\ngetSessionInformation\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_SESSION\ngetSpaces\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_SPACE\ngetTags\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_TAG\ngetVocabularies\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_VOCABULARY\ngetVocabularyTerms\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_VOCABULARY_TERM\nlockDataSets\n## PROJECT_ADMIN\n## LOCK_DATASET\nrevertDeletions\n## PROJECT_USER, SPACE_ETL_SERVER\n## REVERT_DELETION\nsearchAggregationServices\n## PROJECT_OBSERVER\n## SEARCH_AGGREGATION_SERVICES\nsearchAuthorizationGroups\n## PROJECT_ADMIN\n## SEARCH_AUTHORIZATION_GROUP\nsearchCustomASServices\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_CUSTOM_AS_SERVICES\nsearchDataSetTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_DATASET_TYPE\nsearchDataSets\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_DATASET\nsearchDataStores\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_DATASTORE\nsearchDeletions\n## PROJECT_USER, SPACE_ETL_SERVER\n## SEARCH_DELETION\nsearchExperimentTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_EXPERIMENT_TYPE\nsearchExperiments\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_EXPERIMENT\nsearchExternalDataManagementSystems\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_EXTERNAL_DMS\nsearchGlobally\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_GLOBALLY\nsearchMaterialTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_MATERIAL_TYPE\nsearchMaterials\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_MATERIAL\nsearchObjectKindModifications\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_OBJECT_KIND_MODIFICATION\nsearchOperationExecutions\n## PROJECT_USER, SPACE_ETL_SERVER\n## GET_OPERATION_EXECUTION\nsearchPersons\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## GET_PERSON\nsearchPlugins\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_PLUGIN\nsearchProcessingServices\n## PROJECT_OBSERVER\n## SEARCH_PROCESSING_SERVICES\nsearchProjects\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_PROJECT\nsearchPropertyTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_PROPERTY_TYPE\nsearchQueries\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_QUERY\nsearchReportingServices\n## PROJECT_OBSERVER\n## SEARCH_REPORTING_SERVICES\nsearchRoleAssignments\n## PROJECT_ADMIN\n## SEARCH_ROLE_ASSIGNMENT\nsearchSampleTypes\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_SAMPLE_TYPE\nsearchSamples\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_SAMPLE\nsearchSearchDomainServices\n## PROJECT_OBSERVER\n## SEARCH_SEARCH_DOMAIN_SERVICES\nsearchSemanticAnnotations\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_SEMANTIC_ANNOTATION\nsearchSpaces\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_SPACE\nsearchTags\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_TAG\nsearchVocabularies\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_VOCABULARY\nsearchVocabularyTerms\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## SEARCH_VOCABULARY_TERM\nunarchiveDataSets\n## PROJECT_USER, SPACE_ETL_SERVER\n## UNARCHIVE_DATASET\nunlockDataSets\n## PROJECT_ADMIN\n## UNLOCK_DATASET\nupdateAuthorizationGroups\n## INSTANCE_ADMIN\n## UPDATE_AUTHORIZATION_GROUP\nupdateDataSetTypes\n## INSTANCE_ADMIN\n## UPDATE_DATASET_TYPE\nupdateDataSets\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## UPDATE_DATASET\nupdateDataSets, properties\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## UPDATE_DATASET_PROPERTY\nupdateExperimentTypes\n## INSTANCE_ADMIN\n## UPDATE_EXPERIMENT_TYPE\nupdateExperiments\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_EXPERIMENT\nupdateExperiments, attachments\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_EXPERIMENT_ATTACHMENT\nupdateExperiments, properties\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_EXPERIMENT_PROPERTY\nupdateExternalDataManagementSystems\n## INSTANCE_ADMIN\n## UPDATE_EXTERNAL_DMS\nupdateMaterialTypes\n## INSTANCE_ADMIN\n## UPDATE_MATERIAL_TYPE\nupdateMaterials\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## UPDATE_MATERIAL\nupdateMaterials, properties\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## UPDATE_MATERIAL_PROPERTY\nupdateOperationExecutions\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_OPERATION_EXECUTION\nupdatePersons, activate\n## INSTANCE_ADMIN\n## ACTIVATE_PERSON\nupdatePersons, deactivate\n## INSTANCE_ADMIN\n## DEACTIVATE_PERSON\nupdatePersons, set home space\n## SPACE_ADMIN\n## UPDATE_HOME_SPACE\nupdatePlugins\n## INSTANCE_ADMIN\n## UPDATE_PLUGIN\nupdateProjects\n## SPACE_POWER_USER, PROJECT_ADMIN, SPACE_ETL_SERVER\n## UPDATE_PROJECT\nupdateProjects, attachments\n## SPACE_POWER_USER, PROJECT_ADMIN, SPACE_ETL_SERVER\n## UPDATE_PROJECT_ATTACHMENT\nupdatePropertyTypes\n## INSTANCE_ADMIN\n## UPDATE_PROPERTY_TYPE\nupdateQueries\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## UPDATE_QUERY\nupdateSampleTypes\n## INSTANCE_ADMIN\n## UPDATE_SAMPLE_TYPE\nupdateSamples\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_SAMPLE\nupdateSamples, attachments\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_SAMPLE_ATTACHMENT\nupdateSamples, properties\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_SAMPLE_PROPERTY\nupdateSemanticAnnotations\n## INSTANCE_ADMIN, INSTANCE_ETL_SERVER\n## UPDATE_SEMANTIC_ANNOTATION\nupdateSpaces\n## SPACE_ADMIN, SPACE_ETL_SERVER\n## UPDATE_SPACE\nupdateTags\n## PROJECT_OBSERVER, SPACE_ETL_SERVER\n## UPDATE_TAG\nupdateVocabularies\n## INSTANCE_ADMIN\n## UPDATE_VOCABULARY\nupdateVocabularyTerms, official terms\n## PROJECT_POWER_USER, SPACE_ETL_SERVER\n## UPDATE_OFFICIAL_VOCABULARY_TERM\nupdateVocabularyTerms, unofficial terms\n## PROJECT_USER, SPACE_ETL_SERVER\n## UPDATE_UNOFFICIAL_VOCABULARY_TERM", "timestamp": "2025-09-18T09:38:29.899861Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_optional-datastore-server-configuration:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/optional-datastore-server-configuration.html", "repo": "openbis", "title": "Optional Datastore Server Configuration", "section": "Optional Datastore Server Configuration", "text": "### Optional Datastore Server Configuration\n\nConfiguring DSS Data Sources\n\nIt is quite common that openBIS AS is using a database filled by DSS.\nDepending on the DSS (specified by the DSS code) and the technology\ndifferent databases have to be used.\n### Configuration is best done by AS\ncore\nplugins\nof type\ndss-data-sources\n. The name of the plugin is just the DSS code. The\nfollowing properties of\nplugin.properties\n## are recognized:\n## Property Key\n## Description\ntechnology\nNormally the technology/module folder of the core plugin specifies the technology/module for which this data source has to be configured. If this is not the case this property allows to specify the technology/module independently.\ndatabase-driver\nFully qualified class name of the data base driver, e.g.\norg.postgresql.Driver\n.\ndatabase-url\nURL of the database, e.g.\njdbc:postgresql://localhost/imaging_dev\nusername\nOptional user name needed to access database.\npassword\nOptional password needed to access database.\nvalidation-query\nOptional SQL script to be executed to validate database connections.\ndatabase-max-idle-connections\nThe maximum number of connections that can remain idle in the pool. A negative value indicates that there is no limit. Default: -1\ndatabase-max-active-connections\nThe maximum number of active connections that can be allocated at the same time. A negative value indicates that there is no limit. Default: -1\ndatabase-max-wait-for-connection\nThe maximum number of seconds that the pool will wait for a connection to be returned before throwing an exception. A value less than or equal to zero means the pool is set to wait indefinitely. Default: -1\ndatabase-active-connections-log-interval\nThe interval (in ms) between two regular log entries of currently active database connections if more than one connection is active. Default: Disabled\ndatabase-active-number-connections-log-threshold\nThe number of active connections that will trigger a NOTIFY log and will switch on detailed connection logging. Default: Disabled\ndatabase-log-stacktrace-on-connection-logging\nIf true and logging enabled also stack traces are logged. Default:\nfalse\n## Properties\ndatabase-driver\nand\ndatabase-url\ncan be omitted if a\netc/dss-datasource-mapping\nis defined. For more see\n## Sharing Databases\n.", "timestamp": "2025-09-18T09:38:29.905198Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_querying-project-db:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/querying-project-db.html", "repo": "openbis", "title": "Querying Project Database", "section": "Querying Project Database", "text": "## Querying Project Database\n\nIn some customized versions of openBIS an additional project-specific\ndatabase is storing data from registered data sets. This database can be\nqueried via SQL Select statements in openBIS Web application. In order\nto protect modification of this database by malicious SQL code openBIS\napplication server should access this database as a user which is member\nof a read-only group. The name of this read-only group is project\nspecific.\n## Note\nIt is possible to configure openBIS to query multiple project-specific databases.\nCreate Read-Only User in PostgreSQL\n\nA new user (aka role) is created by\n## CREATE\n## ROLE\n<\nread\n-\nonly\nuser\n>\n## LOGIN\n## NOSUPERUSER\n## INHERIT\n## NOCREATEDB\n## NOCREATEROLE\n;\nThis new user is added to the read-only group by the following command:\n## GRANT\n<\nread\n-\nonly\ngroup\n>\n## TO\n<\nread\n-\nonly\nuser\n>\n;\nThe name of the read-only group can be obtained by having a look into\nthe list of all groups:\n## SELECT\n*\nfrom\n## PG_GROUP\n;\nNote that by default openBIS creates a user\nopenbis_readonly\nwhich has read-only permissions to all database objects. You can use\nthis user to access the openBIS meta database through the openBIS query\ninterface.\n## Enable Querying\n\nTo enable querying functionality for additional databases in openBIS Web  application a\ncore plugin\nof type query-databases has to be created. The following\nplugin.properties\nhave to be specified:\n## Property\n## Description\nlabel\nLabel of the database. It will be used in the Web application in drop down lists for adding / editing customized queries.\ndatabase-driver\nJDBC Driver of the database, e.g. org.postgresql.Driver for postgresql.\ndatabase-url\nJDBC URL to the database containing full database name, e.g. jdbc:postgresql://localhost/database_name for postgresql\ndatabase-username\nAbove-mentioned defined read-only user.\ndatabase-password\nPassword of the read-only user.\nConfigure Authorization for Querying\n\nIn order to configure authorization, two properties can be configured:\n## Property\n## Description\n.data-space\nTo which data-space this database belongs to (optional, i.e. a query database can be configured not to belong to one data space by leaving this configuration value empty).\n.creator-minimal-role\nWhat role is required to be allowed to create / edit queries on this database (optional, default: INSTANCE_OBSERVER if data-space is not set, POWER_USER otherwise).\nThe given parameters data-space and creator-minimal-role are used by openBIS to enforce proper authorization.\nFor example, if\ndata-space = CISD\ncreator-minimal-role = SPACE_ADMIN\nis configured, then for the query database configured with key\ndb1\n## :\nonly a\n## SPACE_ADMIN\non data space\n## CISD\nand an\n## INSTANCE_ADMIN\nare allowed to create / edit queries,\nonly a user who has the\n## OBSERVER\nrole in data space\n## CISD\nis allowed to execute a query.\nFor query databases that do not belong to a space but that have a column with any of the\nmagic column names\n, the query result is filtered on a per-row basis according to what the user executing the query is allowed to see. In detail this means: if the user executing the query is not an instance admin, filter out all rows which belong to a data space where the user doesn’t have a least the observer role. The relationship between a row and a data space is established by means of the experiment / sample / data set whose\npermId\nis given by one of the magical column names.\nFor sensitive data where authorization needs to be enforced, there are\n## two setups possible:\nConfigure a query database that\ndoes not\nbelong to a data space\nand set the creator-minimal-role to\n## INSTANCE_ADMIN\n. Any instance\nadmin can be trusted to understand authorization issues and ensure\nthat only queries are added for this query database that contain a\nproper reference to an experiment / sample / data set. This way, it\ncan be ensured that only properly filtered query results are\nreturned to the user running the query.\nConfigure a query database that\ndoes\nbelong to a specific data\nspace and set the creator-minimal-role to\n## POWER_USER\n## . The\ndatastore server (or whatever server maintains the query database)\nensures that only information related to the configured data space\nis added to the query database. Thus whatever query the power user\nwrites for this database, it will only reveal information from this\ndata space. As only users with\n## OBSERVER\nrole on this data space\nare allowed to execute the query, authorization is enforced properly\nwithout the need of filtering query results.", "timestamp": "2025-09-18T09:38:29.907241Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_required-configuration:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/required-configuration.html", "repo": "openbis", "title": "openBIS Server Configuration", "section": "openBIS Server Configuration", "text": "### openBIS Server Configuration\n\nAfter successful installation, the openBIS configuration files (which are extended Java property files) of the Application Server (AS) and data store server (DSS) should be checked. All necessary adjustments to those files should be made prior to running the system in production.\n### Application Server Configuration\n\nThe openBIS Application Server is configured using the file\n$INSTALL_PATH/servers/openBIS-server/jetty/etc/service.properties\n.\nEach configuration item of the default service.properties file is self-documented by means of inline comments.\n## Database Settings\n\nAll properties starting with\ndatabase.\nspecify the settings for the openBIS database. They are all mandatory.\n## Property\n## Description\ndatabase.engine\nType of database. Currently only postgresql is supported.\ndatabase.create-from-scratch\nIf true the database will be dropped and an empty database will be created. In productive use always set this value to  false  .\ndatabase.script-single-step-mode\nIf true all SQL scripts are executed in single step mode. Useful for localizing errors in SQL scripts. Should be always false in productive mode.\ndatabase.url-host-part\nPart of JDBC URL denoting the host of the database server. If openBIS Application Server and database server are running on the same machine this property should be an empty string.\ndatabase.kind\nPart of the name of the database. The full name reads openbis_<  kind  >.\ndatabase.admin-user\nID of the user on database server with admin rights, like creation of tables. Should be an empty string if default admin user should be used. In case of PostgreSQL the default admin user is assumed to be postgres.\ndatabase.admin-password\nPassword for admin user. Usual an empty string.\ndatabase.owner\nID of the user owning the data. This should generally be openbis. The correspoding role (and password matching the property\ndatabase.owner-password\n) needs to be created on the PostgreSQL database prior to starting openBIS. In case of an empty string, it is the same user who started up the openBIS Application Server.\ndatabase.owner-password\nPassword of the owner.\nAdditional configuration options are outlined\nhere\n.\nData Store Server Configuration\n\nThe openBIS Data Store Server is configured using the file\n$INSTALL_PATH/servers/datastore_server/etc/service.properties\n.\nEach configuration item of the default service.properties file is self-documented by means of inline comments.\nAdditional configuration options are outlined\nhere\n.", "timestamp": "2025-09-18T09:38:29.909353Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_share-ids:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/share-ids.html", "repo": "openbis", "title": "Share IDs", "section": "Motivation", "text": "Share IDs\n\n## Motivation\n\nAn openBIS instance for a facility often needs the possibility that each customer can have its one disk space in the data store. This means a mapping is needed to decided in eager shuffling (by using MappingBasedShareFinder) and archiving (see Archiver for Facilities) to which share and which archive the data set should go.\n## Syntax\n\nFor this purpose a single mapping file is used. It is a tab-separated value file with three columns: Identifier, Share IDs, Archive Folder. The file contains a row with the headers (which can be arbitrary because they are not checked). Each following row are of the form\n## TAB\n## TAB\n.\n: This is a regular expression for an experiment identifier (/\n/\n/\n), a project identifier (/\n/\n), or a space identifier (/\n).\n: This is a comma-separated list of zero-to-many share IDs.\n: This is a comma-separated list of absolute or relative paths to the archive folders. The list can contain zero, one or two paths. When this column is empty then the row should be ignored for archive folder mapping. When the column contains exactly one path, then it is treated as a common archive folder for all data sets no matter of their size. When the column contains two paths, then the first one is an archive folder for “big” data sets and the other is an archive folder for “small” data sets. Which data sets are considered “big” and which are “small” is controlled by “small-data-sets-size-limit” archiver property (see ZIP and TAR archivers). When this column contains two paths then “small-data-sets-size-limit” property becomes mandatory.\n## Resolving Rules\n\nThe mapping algorithm selects for a specified data set a line from the mapping file in four steps:\nPick the entry whose regular expression matches the identifier of the experiment to which the data set belongs. If such an entry exists and if it has a value (archive folder or share IDs, depending on what is needed) this entry will be selected.\nOtherwise pick the entry whose regular expression matches the project identifier and select it if it exists and has a value.\nOtherwise pick the entry whose regular expression matches the space identifier (i.e. /\n) and select it if it exists and has a value.\nOtherwise no entry is picked which means either no share IDs or default archive folder.\nThe mapping file is reloaded if it has been changed. That is, changes in the mapping do not require a restart of DSS.\n## Example\n\nIdentifier\tShare IDs\tArchive Folder\n/MAIER/DEFAULT/EXP1\t7, 2\t/net/miller/archive\n/SMITH\t6\t/net/smith/openbis/archive-big, /net/smith/openbis/archive-small\n## /MAIER/DEFAULT\t2\n/MAIER\t1\t/net/maier/archive\nThe following table shows the archive folder and the list of share IDs for various experiment identifiers:\n/MAIER/DEFAULT/EXP7\t2\t/net/maier/archive\n/MAIER/DEFAULT/EXP1\t7, 2\t/net/miller/archive\n/MAIER/PROJECT-X/EXP1\t1\t/net/maier/archive\n/SMITH/P786/E775\t6\t/net/smith/openbis/archive-big when a data set is considered \"big\" and /net/smith/openbis/archive-small when a data set is considered \"small\"\n/MILLER/AKZU-3/E83\t \t<default archive folder>", "timestamp": "2025-09-18T09:38:29.911972Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_sharing-databases:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/sharing-databases.html", "repo": "openbis", "title": "Sharing Databases", "section": "Sharing Databases", "text": "## Sharing Databases\n\n## Introduction\n\nApplication server and data store server(s) can share the same database.\nFor example, openBIS screening uses a database for image meta data\n(called imaging-db) which is used by DSS to register and delivering\nimages. It is also used by AS to provide information about available\nimages and transformations.\nFor configuration of the data bases\ncore plugin\non the AS and for each DSS have to be defined. For a DSS it is a core plugin of type\ndata-sources\nand for AS it is a core plugin of type\ndss-data-sources\n. Optionally the AS can get configuration parameters from its registered DSS instances by defining a mapping file\netc/dss-datasource-mapping\nfor the AS.\nWhen a DSS is registering itself at the AS all its data source\ndefinitions are provided and stored on the AS. This allows the AS (if a\nmapping file is defined)\nto reduce configuration of core plugins of type\ndss-data-sources\nto a minimum.\nto have only one core plugin of type\ndss-data-sources\nindependent\nof the number of technologies/modules and DSS instances.\nThe AS can have only one data source per pair defined by data store code\nand module code.\nShare Databases without Mapping File\n\nWithout a mapping file specified data sources are independently defined for DSS and AS. For details see\nDSS Data Sources\nand\nAS Data Sources\n, respectively. Note, that the roperties\ndatabase-driver\nand\ndatabase-url\nare mandatory for AS.\nShare Databases with Mapping File\n\nWhen a mapping file is used the configuration doesn’t change for data\nsources defined for DSS. But the configuration parameters for an\nactually used data source in AS can come from three sources:\nAS core plugins of type\ndss-data-sources\nData source definitions as provided by the data stores\nMapping file\netc/dss-datasource-mapping\nAS core plugins no longer need to define the properties\ndatabase-driver\nand\ndatabase-url\nbecause they are provided by DSS\nor the mapping file. The same is true for properties\nusername\nand\npassword.\nIn fact the\nplugin.properties\ncan be empty. Usually\nonly parameters for logging and connection pooling are used.\nThe mapping file is used to pick the right AS core plugin and the right\ndata source provided by the DSS. In addition database credentials can be\noverwritten by the mapping file.\n## Warning\nOnly those properties in a core plugin of type\ndss-data-source\nare overwritten which are\nundefined\n.\nThe mapping file is a text file with lines of the following syntax:\n<data store code pattern>.<module code pattern>.<type> = <value>\nwhere <data store code pattern> and <module code pattern>\nare wildcard patterns for the data store code and module/technology\ncode, repectively. The <type> can have one of the following\n## values:\n## Type\nMeaning of\nconfig\nMapping from the actual data store and module code to an existing AS core plugin of type dss-data-sources. The value should have one of the two following forms:\n[\n]If a code is a star symbol the corresponding actual value of the data store code or module code is used. Note, that the first form (without module code) is usefully only if the data source is define in the service.properties of AS.\ndata-source-code\nThe data source code as provided by the DSS.\nhost-part\nHost part of the data source URL. It contains the host name (or IP address) and optional the port. Overwrites the value provided by the DSS.\nsid\nUnique identifier of the database. In most cases this is the database name. Overwrites the value provided by the DSS.\nusername\nUser name. Overwrites the value provided by the DSS.\npassword\nUser password. Overwrites the value provided by the DSS.\nEmpty lines and lines starting with ‘#’ will be ignored.\nWhen AS needs a data source for a specific data store and module it will\nconsult the mapping file line by line. For each type it considers only\nthe last line matching the actual data store and module code. From this\ninformation it is able to pick the right AS core plugin of\ntype\ndss-data-sources\n, the data source definitions provided by DSS at\nregistration, and the values for the host part of the URL, database\nname, user and password.\nIf there is no matching line of type\nconfig\nfound the AS core plugin\nwith key <actual data store code>[<actual module code>] is\nused.\nIf there is no matching line of type\ndata-source-code\nfound it is\nassumed that the data store has one and only one data source. Thus data\nstore code has to be defined in the mapping file if the data store has\nmore than one data source. Remember, per data store and module there can\nbe only one data source for AS.\nHere are some examples for various use cases:\nMapping all DSSs on one\n\netc/dss-datasource-mapping\n*.*.config\n=\ndss\nThis means that any request for data source for data store x and\nmodule/technology y will be mapped to the same configuration. If one of\nthe properties driver class, URL, user name, and password is missing it\nwill be replaced by the data source definition provided by data store\nserver x at registration. This works only, if all DSS instances have\nonly\none\ndata source specified.\nThe following mapping file is similar:\netc/dss-datasource-mapping\n*.*.config\n=\ndss[*]\nThis means that any request for data source for data store x and\nmodule/technology y will be mapped to AS core plugin DSS of module y.\nMapping all DSSs on one per module\n\netc/dss-datasource-mapping\n*.\nproteomics\n.\nconfig\n=\ndss1\n[\nproteomics\n]\n*.\nproteomics\n.\ndata\n-\nsource\n-\ncode\n=\nproteomics\n-\ndb\n*.\nscreening\n.\nconfig\n=\ndss1\n[\nscreening\n]\n*.\nscreening\n.\ndata\n-\nsource\n-\ncode\n=\nimaging\n-\ndb\nAll DSS instances for the same module are mapped onto an AS core plugin\nnamed DSS1 for the corresponding module. This time the data source code\nis also specified. This is needed if the corrsponding DSS has more than\none data source defined. For example in screening\npath-info-db\nis\noften used in addition to\nimaging-db\nto speed up file browsing in the\ndata store.\n## Overwriting Parameters\n\nReusing the same AS dss-data-sources core plugin is most flexible with\nthe mapping file if no driver, URL, username and password have been\ndefined in such a core plugin. In this case all these parameters come\nform the data source information provided at DSS registration. If DSS\nand AS are running on the same machine AS can usually use these\nparameters. In this case mapping files like  in the previous examples\nare enough.\nThe situation is different if the DSS instances, AS and the database\nserver running on different machines. The following example assumes that\nthe AS and the database server running on the same machine but at least\none of the DSS instances are running on a different machine. In this\ncase the database URL for the such a DSS instances could be different\nthan the URL for the AS.\netc/dss-datasource-mapping\n*.\nscreening\n.\nconfig\n=\ndss1\n[\nscreening\n]\n*.\nscreening\n.\ndata\n-\nsource\n-\ncode\n=\nimaging\n-\ndb\n*.\nscreening\n.\nhost\n-\npart\n=\nlocalhost\nAlso database name (aka sid), user, and password can be overwritten in\nthe same way.\n## Overwriting Generic Settings\n\netc/dss-datasource-mapping\n*.screening.config = dss1[screening]\n*.screening.data-source-code = imaging-db\n*.screening.host-part = localhost\n*.screening.username = openbis\n*.screening.password = !a7zh93jP.\nDSS3.screening.host-part = my.domain.org:1234\nDSS3.screening.username = ob\nDSS3.screening.password = 8uij.hg6\nThis is an example where all DSS instances except DSS3 are accessing the\nsame database server which is on the same machine as the AS. Username\nand password are also set in order to ignore corresponding data source\ndefinitions of all DSS instances. DSS3 uses a different database server\nwhich could be on the same machine as DSS3. Also username and password\nare different.\nNote, that the generic mapping definitions (i.e. definitions with wild\ncards for data store codes or module codes) should appear before the\nmore specific definitions.", "timestamp": "2025-09-18T09:38:29.914973Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html", "repo": "openbis", "title": "User Group Management for Multi-groups openBIS Instances", "section": "Introduction", "text": "User Group Management for Multi-groups openBIS Instances\n\n## Introduction\n\nRunning openBIS as a facility means that different groups share the same\nopenBIS instance. Therefore the following demands have to be addressed\nby correct configuration of such an instance:\nA user should have only access to data of groups to which he or she belongs.\nEach group should have its own disk space on DSS by assigning each group to a specific\nshare\n.\nMake openBIS available for a new group.\nOptional usage reports should be sent regularly.\nIn order to fulfill these demands\na\nUserManagementMaintenanceTask\nhas to be configured on AS\nan\nEagerShufflingTask\nfor the\nPostRegistrationTask\nhas to be configured on DSS.\noptionally a\n### UsageReportingTask\nhas to be configured on AS.\nIf a new group is added\na new share has to be added to the DSS store folder (a symbolic link to an NFS directory)\na group definition has to be added to a configuration file by added LDAP group keys or an explicit list of user ids.\n### Configuration\n\n## Two types of configurations are needed:\nStatic configurations: Changes in these configuration need a restart of openBIS (AS and/or DSS)\nDynamic configurations: Changes apply without the need of a restart of openBIS\n### Static Configurations\n\nThe necessary static configurations have to be specified in two places:\nAS and DSS service.properties.\nAS service.properties\n\nHere an LDAPAuthenticationService (only if needed) and a\n## UserManagementMaintenanceTask are configured:\nAS service.properties\n# Authentication service.\n# Usually a stacked service were first file-based service is asked (for users like etl-server, i.e. DSS)\n# and second the LDAP service if the file-based service fails.\nauthentication-service = file-ldap-authentication-service", "timestamp": "2025-09-18T09:38:29.920974Z", "source_priority": 2, "content_type": "concept"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html", "repo": "openbis", "title": "User Group Management for Multi-groups openBIS Instances", "section": "When a new person is created in the database the authentication service is asked by default whether this", "text": "# When a new person is created in the database the authentication service is asked by default whether this\n# person is known by the authentication service.\n# In the case of single-sign-on this doesn't work. In this case the authentication service shouldn't be asked.\n# and the flag 'allow-missing-user-creation' should be set 'true' (default: 'false')\n#\n# allow-missing-user-creation = false", "timestamp": "2025-09-18T09:38:29.920974Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:2", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html", "repo": "openbis", "title": "User Group Management for Multi-groups openBIS Instances", "section": "The URL of the LDAP server, e.g. \"ldaps://ldaps-hit-1.ethz.ch\"", "text": "# The URL of the LDAP server, e.g. \"ldaps://ldaps-hit-1.ethz.ch\"\nldap.server.url = <LDAP URL>\n# The distinguished name of the security principal, e.g. \"CN=carl,OU=EthUsers,DC=d,DC=ethz,DC=ch\"\nldap.security.principal.distinguished.name = <distinguished name to login to the LDAP server>\n# Password of the LDAP user account that will be used to login to the LDAP server to perform the queries\nldap.security.principal.password = <password of the user to connect to the LDAP server>\n# The search base, e.g. \"ou=users,ou=nethz,ou=id,ou=auth,o=ethz,c=ch\"\nldap.searchBase = <search base>\nldap.queryTemplate = (%s)\nldap.queryEmailForAliases = true", "timestamp": "2025-09-18T09:38:29.920974Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:3", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html", "repo": "openbis", "title": "User Group Management for Multi-groups openBIS Instances", "section": "Maintenance tasks for user management", "text": "# Maintenance tasks for user management\nmaintenance-plugins = user-management, usage-reporting\n\nuser-management.class = ch.systemsx.cisd.openbis.generic.server.task.UserManagementMaintenanceTask\n# Start time in 24h notation\nuser-management.start = 01:15\n# Time interval of execution\nuser-management.interval = 1 days\n# Path to the file with dynamic configuration\nuser-management.configuration-file-path = ../../../data/user-management-maintenance-config.json\n# Path to the file with information which maps groups to data store shares.\n# Will be created by the maintenance task and is needed by DSS (EagerShufflingTask during post registration)\nuser-management.shares-mapping-file-path = ../../../data/shares-mapping.txt\n# Path to the audit log file. Default: logs/user-management-audit_log.txt\n# user-management.audit-log-file-path =", "timestamp": "2025-09-18T09:38:29.920974Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:4", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html", "repo": "openbis", "title": "User Group Management for Multi-groups openBIS Instances", "section": "Maintenance tasks for user management", "text": "# Maintenance tasks for user management\nmaintenance-plugins = user-management, usage-reporting\n\n### usage-reporting.class = ch.systemsx.cisd.openbis.generic.server.task.UsageReportingTask\n# Time interval of execution and also length report period\nusage-reporting.interval = 7 days\n# Path to the file with group definition\nusage-reporting.configuration-file-path = ${user-management.configuration-file-path}\n# User reporting type. Possible values are NONE, ALL, OUTSIDE_GROUP_ONLY. Default: ALL\nusage-reporting.user-reporting-type = OUTSIDE_GROUP_ONLY\n# Comma-separated list of e-mail addresses for report sending\nusage-reporting.email-addresses = <address 1>, <address 2>, ...", "timestamp": "2025-09-18T09:38:29.920974Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_configuration_user-group-management-for-multi-groups-openbis-instances:5", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/configuration/user-group-management-for-multi-groups-openbis-instances.html", "repo": "openbis", "title": "User Group Management for Multi-groups openBIS Instances", "section": "Mail server configuration is needed by UsageReportingTask", "text": "# Mail server configuration is needed by UsageReportingTask\nmail.from = openbis@<host>\nmail.smtp.host = <SMTP host>\nmail.smtp.user = <can be empty>\nmail.smtp.password = <can be empty>\nWith this template configuration the UserManagementMaintenanceTask runs\nevery night at 1:15 am. It reads the configuration\nfile\n<installation\npath>/data/user-management-maintenance-config.json\nand creates\n<installation\npath>/data/shares-mapping.txt\n. Every week a\nusage report file of the previous week is sent to the specified\naddresses.\nFor the LDAP configuration\nldap.server.url\n,\nldap.security.principal.distingished.name\n,\nldap.security.principal.password\nand\nldap.searchBase\nhave to be specified.\nThe LDAP service is not only used for authenticating users but also to\nobtain all users of a group. In the later case an independent query\ntemplate can be specified by the property\nldap-group-query-template\nof\nthe\nplugin.properties\nof\nUserManagementMaintenanceTask\n(since\n20.10.1.1). The % character in this template will be replaced by the\nLDAP group key.\n## Active Directory\n\nIf the LDAP service is actually an Active Directory service the\nconfiguration is a bit different. These are the changes:\n## Remove\nldap.queryTemplate\n. This means that the default\nvalue\n(&(objectClass=organizationalPerson)(objectCategory=person)(objectClass=user)(%s))\nwill be used.\nIt might be necessary to increase the timeout. The default value is\n## 10 second. Example:\nldap.timeout\n=\n1\nmin\nAdd the following line to the AS service.properties:\nAS service.properties\n## user-management.filter-key = memberOf:1.2.840.113556.1.4.1941:\n## Warning\nThe ldap group keys described below in section\n### Dynamic Configurations\nhave to be full distinguished names (DN) like e.g.\nCN=id-sis-source,OU=Custom,OU=EthLists,DC=d,DC=ethz,DC=ch\n. To find the correct DN an LDAP browsing tool (like Apache Directory Studio <https://directory.apache.org/studio/>) might be useful.\nDSS service.properties\n\nHere the PostRegistrationMaintenanceTask has be extended for eager\nshuffling.\nDSS service.properties\n# Lists of post registrations tasks for each data set executed in the specified order.\n# Note, that pathinfo-feeding is already defined.\npost-registration.post-registration-tasks = pathinfo-feeding, eager-shuffling\npost-registration.eager-shuffling.class = ch.systemsx.cisd.etlserver.postregistration.EagerShufflingTask\npost-registration.eager-shuffling.share-finder.class = ch.systemsx.cisd.openbis.dss.generic.shared.MappingBasedShareFinder\n# Path to the file with information which maps groups to data store shares.\npost-registration.eager-shuffling.share-finder.mapping-file = ../../data/shares-mapping.txt\nEager shuffling moves the just registered data set from share 1 to the share of the group as specified in\n<installation\npath>/data/shares-mapping.txt\n. For more details about share mapping see\nMapping File for Share Ids and Archiving Folders\n### Dynamic Configurations\n\nEach time the UserManagementMaintenanceTask is executed it reads the\nconfiguration file specified\nin\nuser-management.configuration-file-path\nof AS\nservice.properties\n.\nIt is a text file in JSON format which has the following structure, that\nneeds to be created manually:\n{\n\"globalSpaces\": [\"<space 1>\", \"<space 2>\", ...],\n## \"commonSpaces\":\n{\n\"<role 1>\": [\"<space post-fix 11>\", \"<space post-fix 12>\", ...],\n\"<role 2>\": [\"<space post-fix 21>\", \"<space post-fix 22>\", ...],\n...\n},\n## \"commonSamples\":\n{\n\"<sample identifier template 1>\": \"<sample type 1>\",\n\"<sample identifier template 2>\": \"<sample type 2>\",\n...\n},\n## \"commonExperiments\":\n[\n{\n\"identifierTemplate\" : \"<experiment identifier template 1>\",\n\"experimentType\"   :  \"<experiment type 1>\",\n\"<property code 1>\"  :  \"<property value 1>\",\n\"<property code 2>\"  :  \"<property value 2>\",\n...\n},\n{\n\"identifierTemplate\" : \"<experiment identifier template 2>\",\n\"experimentType\"   :  \"<experiment type 2>\",\n\"<property code 1>\"  :  \"<property value 1>\",\n\"<property code 2>\"  :  \"<property value 2>\",\n...\n},\n...\n],\n\"instanceAdmins\": [\"<instance admin user id 1>\", \"<instance admin user id 1>\"],\n## \"groups\":\n[\n{\n\"name\": \"<human readable group name 1>\",\n\"key\": \"<unique group key 1>\",\n\"ldapGroupKeys\": [\"<ldap group key 11>\", \"<ldap group key 12>\", ...],\n\"users\": [\"<user id 11>\", \"<user id 12>\", ...],\n\"admins\": [\"<user id 11>\", \"<user id 12>\", ...],\n\"shareIds\": [\"<share id 11>\", \"<share id 12>\", ...],\n\"useEmailAsUserId\": true/false (default: false),\n\"createUserSpace\": true/false (default: true),\n\"userSpaceRole\" : <role> (default: non)\n},\n{\n\"name\": \"<human readable group name 2>\",\n\"key\": \"<unique group key 2>\",\n\"ldapGroupKeys\": [\"<ldap group key 21>\", \"<ldap group key 22>\", ...],\n\"admins\": [\"<user id 21>\", \"<user id 22>\", ...],\n\"shareIds\": [\"<share id 21>\", \"<share id 22>\", ...],\n\"useEmailAsUserId\": true/false (default: false),\n\"createUserSpace\": true/false (default: true),\n\"userSpaceRole\" : <role> (default: non)\n},\n...\n]\n}\n## Example:\n{\n\"globalSpaces\"\n## :\n[\n## \"ELN_SETTINGS\"\n],\n\"commonSpaces\"\n## :\n{\n## \"USER\"\n## :\n[\n## \"INVENTORY\"\n,\n## \"MATERIALS\"\n,\n## \"METHODS\"\n,\n## \"STORAGE\"\n,\n## \"STOCK_CATALOG\"\n],\n## \"OBSERVER\"\n## :\n[\n## \"ELN_SETTINGS\"\n,\n## \"STOCK_ORDERS\"\n]\n},\n\"commonSamples\"\n## :\n{\n## \"ELN_SETTINGS/ELN_SETTINGS\"\n## :\n## \"GENERAL_ELN_SETTINGS\"\n},\n\"commonExperiments\"\n## :\n[\n{\n\"identifierTemplate\"\n## :\n## \"ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION\"\n,\n\"experimentType\"\n## :\n## \"COLLECTION\"\n,\n## \"$NAME\"\n## :\n## \"Templates Collection\"\n,\n## \"$DEFAULT_OBJECT_TYPE\"\n## :\nnull\n,\n## \"$DEFAULT_COLLECTION_VIEW\"\n## :\n## \"LIST_VIEW\"\n},\n{\n\"identifierTemplate\"\n## :\n## \"ELN_SETTINGS/STORAGES/STORAGES_COLLECTION\"\n,\n\"experimentType\"\n## :\n## \"COLLECTION\"\n,\n## \"$NAME\"\n## :\n## \"Storages Collection\"\n,\n## \"$DEFAULT_OBJECT_TYPE\"\n## :\n## \"STORAGE\"\n,\n## \"$DEFAULT_COLLECTION_VIEW\"\n## :\n## \"LIST_VIEW\"\n},\n{\n\"identifierTemplate\"\n## :\n\"PUBLICATIONS/PUBLIC_REPOSITORIES/PUBLICATION_COLLECTION\"\n,\n\"experimentType\"\n## :\n## \"COLLECTION\"\n,\n## \"$NAME\"\n## :\n## \"Publication Collection\"\n,\n## \"$DEFAULT_OBJECT_TYPE\"\n## :\n## \"PUBLICATION\"\n,\n## \"$DEFAULT_COLLECTION_VIEW\"\n## :\n## \"LIST_VIEW\"\n},\n{\n\"identifierTemplate\"\n## :\n## \"STOCK_ORDERS/ORDERS/ORDER_COLLECTION\"\n,\n\"experimentType\"\n## :\n## \"COLLECTION\"\n,\n## \"$NAME\"\n## :\n## \"Order Collection\"\n,\n## \"$DEFAULT_OBJECT_TYPE\"\n## :\n## \"ORDER\"\n,\n## \"$DEFAULT_COLLECTION_VIEW\"\n## :\n## \"LIST_VIEW\"\n},\n{\n\"identifierTemplate\"\n## :\n## \"STOCK_CATALOG/PRODUCTS/PRODUCT_COLLECTION\"\n,\n\"experimentType\"\n## :\n## \"COLLECTION\"\n,\n## \"$NAME\"\n## :\n## \"Product Collection\"\n,\n## \"$DEFAULT_OBJECT_TYPE\"\n## :\n## \"PRODUCT\"\n,\n## \"$DEFAULT_COLLECTION_VIEW\"\n## :\n## \"LIST_VIEW\"\n},\n{\n\"identifierTemplate\"\n## :\n## \"STOCK_CATALOG/REQUESTS/REQUEST_COLLECTION\"\n,\n\"experimentType\"\n## :\n## \"COLLECTION\"\n,\n## \"$NAME\"\n## :\n## \"Request Collection\"\n,\n## \"$DEFAULT_OBJECT_TYPE\"\n## :\n## \"REQUEST\"\n,\n## \"$DEFAULT_COLLECTION_VIEW\"\n## :\n## \"LIST_VIEW\"\n},\n{\n\"identifierTemplate\"\n## :\n## \"STOCK_CATALOG/SUPPLIERS/SUPPLIER_COLLECTION\"\n,\n\"experimentType\"\n## :\n## \"COLLECTION\"\n,\n## \"$NAME\"\n## :\n## \"Supplier Collection\"\n,\n## \"$DEFAULT_OBJECT_TYPE\"\n## :\n## \"SUPPLIER\"\n,\n## \"$DEFAULT_COLLECTION_VIEW\"\n## :\n## \"LIST_VIEW\"\n}\n],\n\"groups\"\n## :\n[\n{\n\"name\"\n## :\n## \"ID SIS\"\n,\n\"key\"\n## :\n## \"SIS\"\n,\n\"ldapGroupKeys\"\n## :\n[\n\"id-sis-source\"\n],\n\"admins\"\n## :\n[\n\"abc\"\n,\n\"def\"\n],\n\"shareIds\"\n## :\n[\n\"2\"\n,\n\"3\"\n],\n\"createUserSpace\"\n## :\nfalse\n}\n]\n}\n## Section\nglobalSpaces\n\nOptional. A list of space codes. If the corresponding spaces do not\nexist they will be created. All users of all groups will have\nSPACE_OBSERVER rights on these spaces. For this reason the\nauthorization group\n## ALL_GROUPS\nwill be created.\n## Section\ncommonSpaces\n\nOptional. The following roles are allowed:\n## ADMIN, USER, POWER_USER, OBSERVER.\nFor each role a list of space post-fix codes are specified. For each\ngroup of the group section a space with code\n<group\nkey>_<space\npost-fix>\nwill be created. Normal users of the\ngroup will have access right SPACE_<ROLE> and admin users will\nhave access right SPACE_ADMIN.\n## Section\ncommonSamples\n\nOptional. A list of key-value pairs where the key is a sample identifier\ntemplate and the value is an existing sample type. The template has the\nform\n<space\npost-fix\ncode>/<sample\npost-fix\ncode>\nThe space post-fix code has to be in one of the lists of common spaces.\nFor each group of the group section a sample with identifier\n<group\nkey>_<space\npost-fix\ncode>/<group\nkey>_<sample\npost-fix\ncode>\nof specified type will be created.\n## Section\ncommonExperiments\n\nOptional. A list of maps where every key represents the different\nexperiment attributes, allowing the not only set the type but also set\nproperty values. The template has the form\n<space\npost-fix\ncode>/<project\npost-fix\ncode>/<experiment\npost-fix\ncode>\nThe space post-fix code has to be in one of the lists of common spaces.\nFor each group of the group section an experiment with identifier\n<group\nkey>_<space\npost-fix\ncode>/<group\nkey>_<project\npost-fix\ncode>/<group\nkey>_<experiment\npost-fix\ncode>\nof specified type will be created.\n## Section\ninstanceAdmins\n(since version 20.10.6)\n\nOptional. A list of users for which INSTANCE_ADMIN rights will be\nestablished. If such users are no longer known by the authetication\nservice they will not be revoked\n.\n## Section\ngroups\n\nA list of group definitions. A group definition has the following\n## sections:\nname\n: The human readable name of the group.\nkey\n: A unique alphanumerical key of the group that follows the same rules as openBIS codes (letters, digits, ‘-’, ‘.’ but no ‘_’), for this particular purpose using only capital letters is recommended. It is used to created the two authorization groups\n<group\nkey>\nand\n<group\nkey>_ADMIN.\nldapGroupKeys\n: A list of group keys known by the LDAP authentication service.\nusers\n: An explicit list of user ids.\nadmins\n: A list of user ids. All admin users have SPACE_ADMIN rights to all spaces (common and user ones) which belong to the group.\nshareIds\n: This is a list of ids of data store shares. This list is only needed if\nshares-mapping-file-path\nhas been specified.\nuseEmailAsUserId\n: (since 20.10.1) If\ntrue\nthe email address will be used instead of the user ID to determine the code of the user’s space. Note, that the ‘@’ symbol in the email address will be replaced by ‘_AT_’. This flag should be used if\n## Single Sign On\nis used for authentication but LDAP for managing the users of a group. Default:\nfalse.\ncreateUserSpace\n: (since 20.10.1) This is a flag that controls a creation of personal user spaces for the users of this group. By default it is set to true, i.e. the personal user spaces will be created. If set to false, then the personal user spaces won’t be created for this group.\nuserSpaceRole\n: Optional access role (either ADMIN, USER, POWER_USER, or OBSERVER) for all users of the group on all personal user spaces. (since version 20.10.3)\nWhat UserManagementMaintenanceTask does\n\nEach time this maintenance task is executed (according to the scheduling\ninterval of\nplugin.properties\n) the JSON configuration file will be\nread first. The task does the following:\nUpdates mapping file of data store shares if\nshares-mapping-file-path\nhas been specified.\nCreates global spaces if they do not exist and allows\nSPACE_OBSERVER access by all users of all groups.\nRevokes all users unknown by the authentication service. These users\nwill not be deleted but deactivated. This includes removing home\nspace and all authorization rights.\nDoes for each specified group the following:\nCreates the following two authorization groups if they do not\n## exist:\n<group\nkey>\n: All users of the group will a member of this\nauthorization group. This group has access rights to common\nspaces as specified.\n<group\nkey>_ADMIN\n: All admin users of the group will be\nmember of this authorization group. This group has\nSPACE_ADMIN rights to all common spaces and all personal\nuser spaces.\nCreates common spaces if they do not exist and assign roles for\nthese space to the authorization groups.\nCreates for each user of the LDAP groups or the explicit list of\nuser ids a personal user space with SPACE_ADMIN access right\n(NOTE: since 20.10.1 creation of personal user spaces can be\ndisabled by setting “createUserSpace” flag in the group\nconfiguration to false). The space code read\n<group\nkey>_<user\nid>[_<sequence\nnumber>]\nA sequence\nnumber will be used if there is already a space with code\n<group\nkey>_<user_id\n>. There are two reason why this can\n## happen:\nA user leaving the group and joining it again later but was\nalways known by the authentication service.\nA user leaving the group and the institution. That it, the\nuser is no longer known by the authentication service. But\nlater another user with the same user id is joining the\ngroup.\nCreates common samples if they do not exist.\nCreates common experiments (and necessary projects) if they do\nnot exist.\nAssigns home spaces in accordance to the following rules:\nIf the user has no home space the personal user space of the\nfirst group of the JSON configuration file will become the home\nspace.\nThe home space will not be changed if its code doesn’t start\nwith\n<group\nkey>_<user\nid>\nfor all groups.\nIf the user leaves a group the home space will be removed.\nNote, if a user is moved from one group to another group the home\nspace of the user will be come the personal user space of the new\ngroup.\nContent of the Report File sent by UsageReportingTask\n\nThe report file is a TSV text file with following columns:\nColumn header\n## Description\nperiod start\nTime stamp of the begin of the reporting period.\nperiod end\nTime stamp of the end of the reporting period.\ngroup name\nIt has one of the three different meanings:An empty string which indicates the summary over all groups and users.The name of the group as specified by key in the dynamic configuration file.The user id for activities outside a group or for users which do not belong to a group.\nnumber of users\nNumber of users of the group.\nidle users\nSpace-separated list of ids of those users which haven’t created a collection (i.e. experiment) or an object (i.e. sample) or registered a data set in the reporting period.\nnumber of new collections\nNumber of collections created.\nnumber of new objects\nNumber of objects created.\nnumber of new data sets\nNumber of data sets registered.\ntotal number of entities\nTotal number of all entities (collections, objects and data sets). Only shown if property count-all-entities = true.\nThe first line in the report (after the column headers) shows always\nthe summary (with unspecified ‘group name’).\n## If\nconfiguration-file-path\nis specified usage for each specified\ngroup (in alphabetic order) is listed.\nFinally usage by individual users follows if\nuser-reporting-type\nisn’t NONE\nCommon use cases\n\nHere are some common uses cases. No openBIS restart is needed for these\nuse cases.\nAdding a new group\n\nIn order to make openBIS available for a new group three things have to\nbe done by an administrator:\nAdd one or more shares to the DSS store. These are symbolic links to\n(remote) disk space which belongs to the new group. Note, that\nsymbolic link has to be a number which is the share ID.\nDefine a new group in the LDAP service and add all persons which\nshould belong to the group. Note, a person can be in more than one\ngroup.\nAdd to the above mentioned JSON configuration file a new section\nunder\ngroups\n.\nMaking a user an group admin\n\nAdd the user ID to the\nadmins\nlist of the group in the JSON\nconfiguration file.\nRemove a user from a group\n\nThe user has to be removed from the LDAP group on the LDAP service.\nAdding more disk space\n\nAdd a new share for the new disk to DSS store.\nAdd the share id to the\nshareIds\nlist.\nManual configuration of Multi-groups openBIS instances\n\nIn order to reproduce the set up of a multi-group openBIS instance\nhandled by the maintenance task, the following steps are necessary.\nNote: We do NOT recommend to use the manual set up of a multi-group\ninstance for productive use.\nMasterdata and entities definition\n\n## Spaces\n\nDefine a prefix for a group\nCreate a\ngroup_prefix_MATERIALS\nspace\nCreate a\ngroup_prefix_METHODS\nspace\nCreate a\ngroup_prefix_ELN_SETTINGS\nspace\nCreate a\ngroup_prefix_STORAGE\nspace\nCreate a\ngroup_prefix_STOCK_CATALOG\nspace\nCreate a\ngroup_prefix_STOCK_ORDERS\nspace\nCreate a\ngroup_prefix_Username\nspace for each user of the group\n## Projects\n\nCreate the /**group_prefix_ELN_SETTINGS/\ngroup_prefix_STORAGES\nproject\n## Collections\n\nCreate the /\ngroup_prefix_ELN_SETTINGS/group_prefix_STORAGES/Group_prefix_STORAGES_COLLECTION\ncollection of type COLLECTION\n## Objects\n\nCreate the  /**group_prefix_ELN_SETTINGS/group_prefix_\n## ELN_SETTINGS\nobject of type\n## GENERAL_ELN_SETTINGS\nRights management\n\nCreate a group_prefix User group in openBIS\nCreate a group_prefix_ADMIN User group in openBIS\nAssign every group member to group_prefix User group\nAssign the admin to group_prefix_ADMIN User group\nAssign group_prefix User group SPACE_USER rights to the following\n## spaces:\ngroup_prefix_MATERIALS\ngroup_prefix_METHODS\ngroup_prefix_STORAGE\ngroup_prefix_STOCK_CATALOG\nAssign group_prefix User group SPACE_OBSERVER rights to the following spaces:\ngroup_prefix_ELN_SETTINGS\ngroup_prefix_STOCK_ORDERS\nAssign group_prefix_ADMIN SPACE_ADMIN rights to the following folders:\ngroup_prefix_MATERIALS\ngroup_prefix_METHODS\ngroup_prefix_STORAGE\ngroup_prefix_STOCK_CATALOG\ngroup_prefix_ELN_SETTINGS\ngroup_prefix_STOCK_ORDERS\nAssign each single user SPACE_ADMIN rights to his/her\ngroup_prefix_Username\nspace", "timestamp": "2025-09-18T09:38:29.920974Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_architecture:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/architecture.html", "repo": "openbis", "title": "Architecture", "section": "Architecture", "text": "## Architecture\n\nWe recommend to run\nopenBIS\nas a lightweight Docker container, fostering portability across environments and platforms.\n### Requirements\n\nRefer to the official documentation pages on Docker Engine (aka Docker CE) to learn more about requirements and\ninstallation instructions\nof the packages needed for running docker containers.\nWe recommend to run the openBIS Docker container on top of an Ubuntu server for running the application in production -\n### System Requirements\n.\nRead more on\n## Docker Architecture\nto familiarize with its core concepts.\n## Application Layout\n\nopenBIS can be split into distinct sub-units, which are virtualized either all-in-one or within multiple Docker containers.\nIndependently of the scenario, we recommend clients to communicate with the application via a reverse proxy.", "timestamp": "2025-09-18T09:38:29.925429Z", "source_priority": 2, "content_type": "concept"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_configuration:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/configuration.html", "repo": "openbis", "title": "Basic configuration", "section": "Environment Variables", "text": "Basic configuration\n\n## Environment Variables\n\n## Variable\nDefault value\n## Description\n## OPENBIS_ADMIN_PASS\n123456789\nAdministrator password to openBIS instance.\n## OPENBIS_DATA\n/data/openbis\nDirectory for openBIS persistent data.\n## OPENBIS_DB_ADMIN_PASS\nmysecretpassword\nPostgreSQL superuser password.\n## OPENBIS_DB_ADMIN_USER\npostgres\nPostgreSQL superuser name.\n## OPENBIS_DB_APP_PASS\nmysecretpassword\nPassword for application user connecting to the database.\n## OPENBIS_DB_APP_USER\nopenbis\nUsername for application user connecting to the database.\n## OPENBIS_DB_HOST\nopenbis-db\nName of container running PostgreSQL database.\n## OPENBIS_ETC\n/etc/openbis\nDirectory for openBIS configuration files.\n## OPENBIS_HOME\n/home/openbis\nDirectory for openBIS installation binaries.\n## OPENBIS_LOG\n/var/log/openbis\nDirectory for openBIS log files.\n## OPENBIS_FQDN\nopenbis.domain\nFull qualified domain name of openBIS service.\n### Configuration Files\n\nopenBIS offers the ability to pass in configuration files like, e.g., capabilities files. Those can be deployed in any directory mounted as a volume in the openBIS docker container. It needs to be ensured that the associated AS and DSS properties are pointing to the correct file paths.\n## Note\nIt is necessary to store any data files that needs to be preserved inside the\nopenbis-app-data\nvolume, or inside some other docker volume. Likewise, any configuration files should be stored within the\nopenbis-app-etc\nvolume. Failure to do so yields undesired behavior in the sense that changes made to the files within the container are being lost when the container goes down.\n## Examples\n\nSuppy a json file for storing personal access tokens\n\nEnable the feature\nAdd the following line to the AS service.properties:\npersonal\n-\naccess\n-\ntokens\n-\nenabled\n=\ntrue\nEnsure the AS property\npersonal-access-tokens-file-path\nis pointing to the correct path to where the json file is located\nAssuming the DSS root-dir points to the default dir,\n/data/openbis\n, then the personal-access-tokens.json could be stored in this directory:\npersonal\n-\naccess\n-\ntokens\n-\nfile\n-\npath\n=\n/\ndata\n/\nopenbis\n/\npersonal\n-\naccess\n-\ntokens\n.\njson\nCreate a PAT and monitor the contents of the json file\npersonal-access-tokens.html#typical-application-workflow\nModify the AS capabilities file\n\nFor this, it is not needed to\n## Core Plugins\n\nIt is possible to make adjustments to core-plugins shipped with the openBIS installer. To do so, just start up openBIS at least once. This will copy the contents of the core-plugins directory to a sub-directory\ncore-plugins\nwhich is stored within the docker volume\nopenbis-app-etc\n. Any customizations made here will persist restarts of the application, as well as upgrades of the openbis-docker image.\n## Warning\nBe careful when making changes to code of core-plugins since they might break when new releases are published. Please consider reading the\nChange Log published with each release\n.\nIf the application fails to start after changes to the core-plugins have been made, you can always revert to the original state of the core-plugins by removing the\ncore-plugins\nfolder within the\nopenbis-app-etc\nvolume.\nBesides adjustments to existing plugins, it is also possible to\ncreate new plugins from scratch\n.\n## Examples\n\nCustomize the InstanceProfile.js\n\nThis file is part of the\neln-lims\n## core-plugin. It is located here:\n<openbis-app-etc>/core-plugins/eln-lims/1/as/webapps/eln-lims/html/etc/InstanceProfile.js\nMake any changes to this file\nE.g., change\nthis.showSemanticAnnotations\n=\nfalse;\nto\nthis.showSemanticAnnotations\n=\ntrue;\n.\nRestart the container\nMost changes made to the configuration of the openBIS application require a restart in order to be applied. Assuming the container running the openBIS application is named\nopenbis-app\n, this is achieved as follows:\ndocker\nrestart\nopenbis\n-\napp", "timestamp": "2025-09-18T09:38:29.928444Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_environments:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/environments.html", "repo": "openbis", "title": "Environments", "section": "Environments", "text": "## Environments\n\nProduction, testing and development\n\nopenbis-app - https://hub.docker.com/r/openbis/openbis-server\n\n## The\nopenbis-app\nimage is designed and supported for deploying openBIS in production, testing and development environments.\nThe openBIS service running in container named\nopenbis-app\ncan be connected to a\ncontainerized PostgreSQL database\nor to any managed or cloud-native PostgreSQL service.\nA reverse HTTP proxy is required in front. It can be\nset up as a container\n, as an independent ingress controller, or as a cloud-based content delivery service.\nWe recommend to orchestrate all containers using Docker Compose, for which we’re providing\nexamples\nto use as a template.", "timestamp": "2025-09-18T09:38:29.931444Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/index.html", "repo": "openbis", "title": "Docker", "section": "Docker", "text": "## Docker\n\n## Quickstart\n## Architecture\n### Requirements\n## Application Layout\n## Environments\nProduction, testing and development\nopenbis-app - https://hub.docker.com/r/openbis/openbis-server\n## Release Cycle\n## Source Repositories\nSource code\nDocker images\n### Usage\n## Docker Containers\n## Docker Compose\n## Docker Network\n## Storage Volumes\n## Database\n## Application\n## Ingress\n## Nginx\nApache httpd\nHAProxy\nBasic configuration\n## Environment Variables\n### Configuration Files\n## Examples\nSuppy a json file for storing personal access tokens\nModify the AS capabilities file\n## Core Plugins\n## Examples\nCustomize the InstanceProfile.js\n## Verification\n## References", "timestamp": "2025-09-18T09:38:29.934162Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_quickstart:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/quickstart.html", "repo": "openbis", "title": "Quickstart", "section": "Quickstart", "text": "## Quickstart\n\nCreate virtual network.\n$ docker network create openbis-network --driver bridge;\nRun database container.\n$ docker run --detach \\\n--name openbis-db \\\n--hostname openbis-db \\\n--network openbis-network \\\n-v openbis-db-data:/var/lib/postgresql/data \\\n-e POSTGRES_PASSWORD=mysecretpassword \\\n-e PGDATA=/var/lib/postgresql/data/pgdata \\\npostgres:15;\nRun application container.\n$ docker run --detach \\\n--name openbis-app \\\n--hostname openbis-app \\\n--network openbis-network \\\n--pid host \\\n-p 8080:8080 \\\n-p 8081:8081 \\\n-v openbis-app-data:/data \\\n-v openbis-app-etc:/etc/openbis \\\n-v openbis-app-logs:/var/log/openbis \\\n-e OPENBIS_ADMIN_PASS=\"123456789\" \\\n-e OPENBIS_DATA=\"/data/openbis\" \\\n-e OPENBIS_DB_ADMIN_PASS=\"mysecretpassword\" \\\n-e OPENBIS_DB_ADMIN_USER=\"postgres\" \\\n-e OPENBIS_DB_APP_PASS=\"mysecretpassword\" \\\n-e OPENBIS_DB_APP_USER=\"openbis\" \\\n-e OPENBIS_DB_HOST=\"openbis-db\" \\\n-e OPENBIS_ETC=\"/etc/openbis\" \\\n-e OPENBIS_HOME=\"/home/openbis\" \\\n-e OPENBIS_LOG=\"/var/log/openbis\" \\\n-e OPENBIS_FQDN=\"local.openbis.ch\" \\\nopenbis/openbis-app:20.10.7;\nRun local ingress container.\n$ docker run --detach \\\n--name openbis-ingress \\\n--hostname openbis-ingress \\\n--network openbis-network \\\n--pid host \\\n-p 443:443 \\\n-e OPENBIS_HOST=\"openbis-app\" \\\nopenbis/openbis-local:latest;\nVerify connectivity.\n$ curl -v https://local.openbis.ch/openbis/webapp/eln-lims/version.txt", "timestamp": "2025-09-18T09:38:29.935994Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_references:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/references.html", "repo": "openbis", "title": "References", "section": "References", "text": "## References\n\nopenBIS: a flexible framework for managing and analyzing complex data in biology research\nopenBIS official documentation\nopenBIS official image\n## Docker Engine\n## Docker Compose\nPostgreSQL official image\n## NGINX Documentation\n## HAProxy Documentation\n## Apache HTTP Server Documentation", "timestamp": "2025-09-18T09:38:29.938180Z", "source_priority": 2, "content_type": "reference"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_release-cycle:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/release-cycle.html", "repo": "openbis", "title": "Release Cycle", "section": "Release Cycle", "text": "## Release Cycle\n\nThe official\nopenbis-app\nimages available on\n## Docker Hub\nare tagged by major release published on the\nopenBIS download page\nwith the latest bugfix patches included. The official openBIS installation package is based on the latest official image of\nUbuntu LTS Linux\n. In addition, all containers are rebuilt and republished at least on a monthly basis to include operating system patches.", "timestamp": "2025-09-18T09:38:29.940389Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_source-repositories:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/source-repositories.html", "repo": "openbis", "title": "Source Repositories", "section": "Source Repositories", "text": "## Source Repositories\n\nSource code\n\nThe source code of all builds and helper scripts is published in the\nopenBIS Continous Integration repository\n. This is the only official location of openBIS source code supported by the openBIS team of ETH Zurich Scientific IT Services.\nDocker images\n\nContainer images are published on\n## Docker Hub\n. This is the only official location of openBIS Docker images supported by the openBIS team of ETH Zurich Scientific IT Services.", "timestamp": "2025-09-18T09:38:29.941762Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_usage:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/usage.html", "repo": "openbis", "title": "Usage", "section": "Usage", "text": "### Usage\n\n## Docker Containers\n\nOur recommendation is to run openBIS within a\nthree-container setup\n, in particular when aiming at\nrunning openBIS in production\n## :\nopenbis-ingress\n: Runs a\nreverse HTTP Proxy\nfor managing and securing HTTP requests in between the client and the application.\nopenbis-app\n: Runs a\nJava Runtime Environment\n, including the openBIS Application Server (AS) and openBIS Data Store Server (DSS).\nopenbis-db\n: Runs a\nPostgreSQL\ndatabase, to handle all data transactions.\n## Container\n## Image\n## Port\n## Description\nopenbis-db\npostgres15\n5432/tcp\nPostgreSQL database listens on port 5432 and accepts connection from openbis-app.\nopenbis-app\nopenbis-app\n8080/tcp\nJava Virtual Machine with openBIS Application Server listens on port 8080.\nopenbis-app\nopenbis-app\n8081/tcp\nJava Virtual Machine with openBIS Data Store Server listens on port 8081.\nopenbis-ingress\napache2\n443/tcp\nApache HTTP server listens on port 443 and is configured as reverse proxy to ports 8080 and 8081.\n## Docker Compose\n\nDocker Compose is a tool for defining and running multi-container applications. It simplifies the control of the entire openBIS service, making it easy to control the application workflow in a single, comprehensible YAML configuration file, allows to create, start and stop all services by issuing a single command.\nWe are providing a basic\ndocker-compose.yml\n, which is ready to use.\nTo run the application navigate to the sub-directory where you’ve downloaded the\ndocker-compose.yml\nto and then execute\ndocker\n-\ncompose\npull\ndocker\n-\ncompose\nup\n-\nd\nFor advanced use, consider to modify the file according to your needs (\nmore details\n). Note that this example does not include an ingress controler. For full examples, proceed to [Ingress].\nThe sections below provides a brief description of the individual components used in the proposed multi-container setup.\n## Docker Network\n\nThe virtual bridge network\nopenbis-network\nallows all containers deployed with openBIS to connect to each other. The following example creates a network using the bridge network driver, which all running containers will be communicating accross.\nTo manually create the network, execute:\ndocker\nnetwork\ncreate\nopenbis\n-\nnetwork\n--\ndriver\nbridge\n## Storage Volumes\n\nThe use of Docker volumes is preferred for\npersisting data\ngenerated and utilized by containers. For proper operation, the data directory of openBIS, main configuration files and logs are to be mounted as  persistent volumes. This ensures that data can be accessed from different containers, and allows data to persist container restarts. By utilizing the option\n-v\nopenbis-data:/data\n, a persistent storage named\nopenbis-data\nis created and mounted as\n/data\nwithin the active container. This applies analogically to all other persistent volumes.\n## Container\nPersistent volume\n## Mountpoint\n## Description\nopenbis-db\nopenbis-db-data\n/var/lib/postgresql/data\nPostgreSQL database configuration and data directory.\nopenbis-app\nopenbis-app-data\n/data\nApplication data directory for data store files.\nopenbis-app\nopenbis-app-etc\n/etc/openbis\nApplication configuration files.\nopenbis-app\nopenbis-app-logs\n/var/log/openbis\nApplication log files.\n## Database\n\n## The\ndatabase container\nopenbis-db\nprovides a relational database through\nPostgreSQL server\nto guarantee persistence for any data created while running openBIS. This includes user and authorization data, openBIS entities and their metadata, as well as index information about all datasets. It is required to have database superuser privileges.\n$ docker run --detach \\\n--name openbis-db \\\n--hostname openbis-db \\\n--network openbis-network \\\n-v openbis-db-data:/var/lib/postgresql/data \\\n-e POSTGRES_PASSWORD=mysecretpassword \\\n-e PGDATA=/var/lib/postgresql/data \\\npostgres:15;\nThe running database container can be inspected to fetch logs. The database has started up successfully when the openbis-db container logs the following message: “database system is ready to accept connections”:\n$ docker logs openbis-db;\n2024-01-19 18:37:50.984 UTC [1] LOG:  database system is ready to accept connections\nThe volume created (\nopenbis-db-data\n) can be inspected to check the mountpoint where the database data is physically stored.\n$ docker volume inspect openbis-db-data;\n[\n{\n\"CreatedAt\": \"2024-01-19T19:37:48+01:00\",\n\"Driver\": \"local\",\n\"Labels\": null,\n\"Mountpoint\": \"/var/lib/docker/volumes/openbis-db-data/_data\",\n\"Name\": \"openbis-db-data\",\n\"Options\": null,\n\"Scope\": \"local\"\n}\n]\n## Application\n\n## The\napplication container\nopenbis-app\nprovides Java runtime and consists of two Java processes - the\nopenBIS Application Server\n(openBIS AS) and the -\nopenBIS Data Store Server\n(openBIS DSS). The\nopenBIS AS\nmanages the metadata and links to the data, while the\nopenBIS DSS\nmanages the data themselves operating on a managed part of the file system.\n$ docker run --detach \\\n--name openbis-app \\\n--hostname openbis-app \\\n--network openbis-network \\\n--pid host \\\n-p 8080:8080 \\\n-p 8081:8081 \\\n-v openbis-app-data:/data \\\n-v openbis-app-etc:/etc/openbis \\\n-v openbis-app-logs:/var/log/openbis \\\n-e OPENBIS_ADMIN_PASS=\"123456789\" \\\n-e OPENBIS_DATA=\"/data/openbis\" \\\n-e OPENBIS_DB_ADMIN_PASS=\"mysecretpassword\" \\\n-e OPENBIS_DB_ADMIN_USER=\"postgres\" \\\n-e OPENBIS_DB_APP_PASS=\"mysecretpassword\" \\\n-e OPENBIS_DB_APP_USER=\"openbis\" \\\n-e OPENBIS_DB_HOST=\"openbis-db\" \\\n-e OPENBIS_ETC=\"/etc/openbis\" \\\n-e OPENBIS_HOME=\"/home/openbis\" \\\n-e OPENBIS_LOG=\"/var/log/openbis\" \\\n-e OPENBIS_FQDN=\"openbis.domain\" \\\nopenbis/openbis-app:20.10.7;\nThe state of the running application container can be inspected by fetching the container logs:\n$ docker logs openbis-app;\n2024-01-23 11:06:19,310 INFO  [main] OPERATION.ETLDaemon - Data Store Server ready and waiting for data.\nDocker volumes mounted by\nopenbis-app\ncan be inspected to check where data files, configuration files and logs are physically stored.\n$ docker volume inspect openbis-app-data openbis-app-etc openbis-app-logs;\n[\n{\n\"CreatedAt\": \"2024-01-20T12:24:49+01:00\",\n\"Driver\": \"local\",\n\"Labels\": null,\n\"Mountpoint\": \"/var/lib/docker/volumes/openbis-app-data/_data\",\n\"Name\": \"openbis-app-data\",\n\"Options\": null,\n\"Scope\": \"local\"\n},\n{\n\"CreatedAt\": \"2024-01-20T12:24:49+01:00\",\n\"Driver\": \"local\",\n\"Labels\": null,\n\"Mountpoint\": \"/var/lib/docker/volumes/openbis-app-etc/_data\",\n\"Name\": \"openbis-app-etc\",\n\"Options\": null,\n\"Scope\": \"local\"\n},\n{\n\"CreatedAt\": \"2024-01-20T12:24:49+01:00\",\n\"Driver\": \"local\",\n\"Labels\": null,\n\"Mountpoint\": \"/var/lib/docker/volumes/openbis-app-logs/_data\",\n\"Name\": \"openbis-app-logs\",\n\"Options\": null,\n\"Scope\": \"local\"\n}\n]\n## Ingress\n\n## An\ningress container\nacts as reverse proxy and performs Transport Layer Security (TLS) termination. The examples provided below only cover the base functionality. They should be extended to handle complex access control scenarios and to comply with firewall rules. In each of the examples below, the ingress controller configures TLS, and it is configured as a reverse proxy to handle requests to the path\n/openbis\n(directed to port 8080) and to\n/datastore_server\n(directed to port 8081).\n## Nginx\n\nIn order to use nginx as an ingress container, it is required to deploy the following files, as provided on our\nsource repository\n## :\ndocker-compose-nginx.yml\nnginx config\n, to be placed in sub-directory\nnginx\nTo run the application, you need to:\nhave docker and docker-compose installed\nensure that valid certificate and key files are deployed in the sub-directory\ncerts\nfrom within the directory where you’ve deployed the\ndocker-compose-nginx.yml\n, run\ndocker-compose\n-f\ndocker-compose-nginx.yml\nup\n-d\nApache httpd\n\nIn order to use apache-httpd as an ingress container, it is required to deploy the following files, as provided on our\nsource repository\n## :\ndocker-compose-httpd.yml\napache-httpd config\n, to be placed in sub-directory\nhttpd\nTo run the application, you need to:\nhave docker and docker-compose installed\nensure that valid certificate and key files are deployed in the sub-directory\ncerts\nfrom within the directory where you’ve deployed the\ndocker-compose-httpd.yml\n, run\ndocker-compose\n-f\ndocker-compose-httpd.yml\nup\n-d\nHAProxy\n\nIn order to use haproxy as an ingress container, it is required to deploy the following files, as provided on our\nsource repository\n## :\ndocker-compose-haproxy.yml\nhaproxy config\n, to be placed in sub-directory\nhaproxy\nTo run the application, you need to:\nhave docker and docker-compose installed\nensure that valid certificate and key files are deployed in the sub-directory\ncerts\nfrom within the directory where you’ve deployed the\ndocker-compose-haproxy.yml\n, run\ndocker-compose\n-f\ndocker-compose-haproxy.yml\nup\n-d", "timestamp": "2025-09-18T09:38:29.945735Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_docker_verification:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/docker/verification.html", "repo": "openbis", "title": "Verification", "section": "Verification", "text": "## Verification\n\nCheck status of running openBIS Application Server.\n$ docker exec -it openbis-app /home/openbis/servers/openBIS-server/jetty/bin/status.sh;\nopenBIS Application Server is running (pid 24538)\nCheck version of running openBIS Application Server.\n$ docker exec -it openbis-app /home/openbis/servers/openBIS-server/jetty/bin/version.sh;\n20.10.7.4 (r1701090021)\nCheck the password file for file based authentication.\n$ docker exec -it openbis-app /home/openbis/servers/openBIS-server/jetty/bin/passwd.sh list;\nUser ID               First Name            Last Name             Email\nadmin\netlserver\nCheck connectivity to port 8080 of openBIS Application Server.\n$ docker exec -it openbis-app wget -q --output-document - http://localhost:8080/openbis/webapp/eln-lims/version.txt;\n20.10.7.4\nExamine a process of  openBIS Data Store Server.\n$ docker exec -it openbis-app pgrep -af DataStoreServer;\n25503 java -server -Djavax.net.ssl.trustStore=etc/openBIS.keystore --add-exports java.xml/jdk.xml.internal=ALL-UNNAMED -Dnative.libpath=lib/native -classpath lib/slf4j-log4j12-1.6.2.jar:lib/datastore_server.jar:lib/common.jar:lib/dbmigration-20.10.7-r1688387419.jar:lib/activation-1.1.1.jar:lib/ascii-table-1.2.0.jar:lib/aspectjweaver-1.8.12.jar:lib/authentication-20.10.7-r1688387419.jar:lib/autolink-dataset-uploader-api-zip4j_1.3.2.jar:lib/autolink-dropboxReporter-jyson-1.0.2.jar:lib/autolink-eln-lims-api-htmlcleaner-2.23.jar:lib/autolink-eln-lims-api-zip4j_1.3.2.jar:lib/autolink-password-reset-api-persistentkeyvaluestore.jar:lib/autolink-zenodo-exports-api-job-scheduler.jar:lib/base64-2.3.9.jar:lib/bcel-6.0-SNAPSHOT.jar:lib/bcpg-1.59.jar:lib/bcprov-1.59.jar:lib/bioformats-6.5.1.jar:lib/builder-commons-1.0.2.jar:lib/cisd-args4j-9.11.2.jar:lib/cisd-cifex-r1550129411.jar:lib/cisd-hotdeploy-13.01.0.jar:lib/cisd-image-readers-bioformats-r1553067167.jar:lib/cisd-image-readers-imagej-r1553067167.jar:lib/cisd-image-readers-jai-r1553067167.jar:lib/cisd-image-readers-r1553067167.jar:lib/cisd-openbis-knime-server-13.6.0.r29301.jar:lib/classmate-1.3.0.jar:lib/common.jar:lib/commonbase.jar:lib/commons-cli-1.2.jar:lib/commons-codec-1.10.jar:lib/commons-collections-4.01.jar:lib/commons-collections4-4.1.jar:lib/commons-compress-1.8.jar:lib/commons-csv-1.2.jar:lib/commons-dbcp-1.3-CISD.jar:lib/commons-fileupload-1.3.3.jar:lib/commons-io-2.6.jar:lib/commons-lang3-3.11.jar:lib/commons-logging-1.1.1.jar:lib/commons-pool-1.5.6.jar:lib/commons-text-1.6.jar:lib/datastore_server-20.10.7-r1688387419.jar:lib/datastore_server_plugin-dsu-20.10.7-r1688387419.jar:lib/datastore_server_plugin-plasmid-20.10.7-r1688387419.jar:lib/datastore_server_plugin-yeastx-20.10.7-r1688387419.jar:lib/dbmigration-20.10.7-r1688387419.jar:lib/docx4j-6.1.2.jar:lib/dom4j-1.6.1.jar:lib/ehcache-2.10.0.jar:lib/eodsql-2.2-CISD.jar:lib/fast-md5-2.6.1.jar:lib/ftpserver-core-1.0.6.jar:lib/guava-25.0-jre.jar:lib/h2-1.1.115.jar:lib/hamcrest-core-1.3.jar:lib/hamcrest-integration-1.3.jar:lib/hamcrest-library-1.3.jar:lib/httpclient-4.3.6.jar:lib/httpcore-4.3.3.jar:lib/ij-1.43u.jar:lib/image-viewer-0.3.6.jar:lib/istack-commons-runtime-3.0.5.jar:lib/jackcess-1.2.2.jar:lib/jackson-annotations-2.9.10.jar:lib/jackson-core-2.9.10.jar:lib/jackson-databind-2.9.10.8.jar:lib/jandex-2.0.3.Final.jar:lib/javacsv-2.0.jar:lib/javassist-3.20.0.GA.jar:lib/javax.annotation-api-1.3.2.jar:lib/javax.jws-3.1.2.2.jar:lib/jaxb-api-2.3.0.jar:lib/jaxb-core-2.3.0.jar:lib/jaxb-runtime-2.3.0.jar:lib/jboss-logging-3.3.0.Final.jar:lib/jboss-transaction-api_1.2_spec-1.0.0.Final.jar:lib/jcommon.jar:lib/jetty-client-9.4.44.v20210927.jar:lib/jetty-deploy-9.4.44.v20210927.jar:lib/jetty-http-9.4.44.v20210927.jar:lib/jetty-io-9.4.44.v20210927.jar:lib/jetty-security-9.4.44.v20210927.jar:lib/jetty-server-9.4.44.v20210927.jar:lib/jetty-servlet-9.4.44.v20210927.jar:lib/jetty-util-9.4.44.v20210927.jar:lib/jetty-webapp-9.4.44.v20210927.jar:lib/jetty-xml-9.4.44.v20210927.jar:lib/jfreechart-1.0.13.jar:lib/jline-0.9.94.jar:lib/jsonrpc4j-1.5.3.jar:lib/jsoup-1.14.2.jar:lib/jython-2.5.2.jar:lib/log4j-1.2.15.jar:lib/mail-1.4.3.jar:lib/marathon-spring-util-1.2.5.jar:lib/mina-core-2.0.7.jar:lib/openbis-20.10.7-r1688387419.jar:lib/openbis-common.jar:lib/openbis-mobile-r29271.jar:lib/openbis_api-20.10.7-r1688387419.jar:lib/pngj-0.62.jar:lib/poi-3.17.jar:lib/poi-ooxml-3.17.jar:lib/poi-ooxml-schemas-3.17.jar:lib/postgresql-42.5.0.jar:lib/reflections-0.9.10.jar:lib/restrictionchecker-1.0.2.jar:lib/screening-20.10.7-r1688387419.jar:lib/serializer-2.7.2.jar:lib/servlet-api-3.1.0.jar:lib/sis-base-18.09.0.jar:lib/sis-file-transfer-19.03.1.jar:lib/sis-jhdf5-19.04.0.jar:lib/slf4j-1.6.2.jar:lib/slf4j-api-1.7.24.jar:lib/slf4j-log4j12-1.6.2.jar:lib/spring-aop-5.0.17.RELEASE.jar:lib/spring-beans-5.0.17.RELEASE.jar:lib/spring-context-5.0.17.RELEASE.jar:lib/spring-context-support-5.0.17.RELEASE.jar:lib/spring-core-5.0.17.RELEASE.jar:lib/spring-expression-5.0.17.RELEASE.jar:lib/spring-jcl-5.0.17.RELEASE.jar:lib/spring-jdbc-5.0.17.RELEASE.jar:lib/spring-orm-5.0.17.RELEASE.jar:lib/spring-tx-5.0.17.RELEASE.jar:lib/spring-web-5.0.17.RELEASE.jar:lib/spring-webmvc-5.0.1.RELEASE.jar:lib/sshd-common.jar:lib/sshd-core-2.7.0.jar:lib/sshd-sftp-2.7.0.jar:lib/stax-api-1.0.1.jar:lib/stax2-api-3.0.4.jar:lib/truezip-6.8.1.jar:lib/txw2-2.3.0.jar:lib/validation-api-1.0.0.GA.jar:lib/wstx-asl-4.0.0.jar:lib/xalan-2.7.2.jar:lib/xml-apis-1.3.03.jar:lib/xml-io-1.0.3.jar:lib/xmlbeans-2.6.0.jar:lib/xoai-common.jar:lib/xoai-data-provider-4.2.0.jar:ext-lib/*.jar ch.systemsx.cisd.openbis.dss.generic.DataStoreServer\nCheck connectivity to the database.\n$ docker exec -it openbis-db psql -U openbis openbis_prod -c \"select id,user_id,email from persons\";\nPassword for user openbis:\nid |  user_id  | email\n----+-----------+-------\n1 | system    |\n2 | etlserver |\n3 | admin     |\n(3 rows)", "timestamp": "2025-09-18T09:38:29.947738Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_standalone_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/index.html", "repo": "openbis", "title": "Standalone", "section": "Standalone", "text": "## Standalone\n\n### System Requirements\n## Architecture\n### Hardware Configuration\nCPU and Memory Configuration\n## Postgres Memory Settings\n## Tuning Of Hardware Settings In Case Of Issues\n## Operating System\n## Third-Party Packages\n### Additional Requirements\n### openBIS Server Installation\nContents of openBIS Installer Tarball\n### Installation Steps\nStarting and Stopping the openBIS Application Server and Data Store Server\n## Start Server\n## Stop Server", "timestamp": "2025-09-18T09:38:29.949730Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_standalone_installation:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/installation.html", "repo": "openbis", "title": "openBIS Server Installation", "section": "openBIS Server Installation", "text": "### openBIS Server Installation\n\nContents of openBIS Installer Tarball\n\nThe server distribution is a\ngzipped\ntar\nfile named\nopenBIS-installation-standard-technologies-<version>.tar.gz\n. It contains the following files:\n## console.properties:\n### Installation configuration file\n## extract.sh:\nhelper script for installation\n## jul.config:\nLog configuration for the openBIS install process\nopenBIS-installer.jar\nJava archive containing openBIS\nrun-console.sh\n### Installation script\n### Installation Steps\n\nCreate a service user account, i.e. an unprivileged, regular user account.\nDo not run openBIS as root!\nGunzip the distribution on the server machine into some temporary folder:\nmkdir\ntmp\nmv\nxvfz\nopenBIS-installation-standard-technologies-<release-number>.tar.gz\ntmp/\ncd\ntmp/\ntar\nxvfz\nopenBIS-installation-standard-technologies-<release-number>.tar.gz\nCustomize the\nconsole.properties\nfile by specifying values for at least the following parameters:\n## INSTALL_PATH\n,\n## DSS_ROOT_DIR\n,\n## INSTALLATION_TYPE\n,\n## ELN-LIMS\n, and\n## ELN-LIMS-LIFE-SCIENCES\n. Each parameter is documented inline.\n## Run installation script:\n./run-console.sh\nWhen done, openBIS is installed in the directory specified as\n## INSTALL_PATH\nin the\nconsole.properties\n. Within this system admin documentation pages, we’re referring this path as\n## $INSTALL_PATH\n.\n## Note\nPlease be aware that the directory where openBIS is installed should not already exist. Users should specify the directory where they want to install openBIS (in the console.properties) and this directory will be created by the installation procedure. If the directory already exists, the installation will fail, except from when the installer detects that it already contains an existing openBIS installation. In the latter case, the installer will try to upgrade the existing release to the one to be installed by invoking $INSTALL_PATH/bin/upgrade.sh.", "timestamp": "2025-09-18T09:38:29.951732Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_standalone_starting-and-stopping:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/starting-and-stopping.html", "repo": "openbis", "title": "Starting and Stopping the openBIS Application Server and Data Store Server", "section": "Start Server", "text": "Starting and Stopping the openBIS Application Server and Data Store Server\n\n## Start Server\n\nThe openBIS application server is started as follows:\nprompt> <installation folder>/bin/bisup.sh\nOn startup the openBIS server creates the openBIS database (named\nopenbis_prod\nby default) and checks the connection with the remote authentication services (if they are configured). Log files can be found in\n<installation\nfolder>/servers/openBIS-server/jetty/logs\n. Also the following command shows the log:\n<installation\nfolder>/bin/bislog.sh\n## Warning\nUnless otherwise configured through running the installation script or within the database itself, the first user logged in into the system will have full administrator rights (role\n## INSTANCE_ADMIN\n).\nCommonly, the application server is configured to access a local data store via the data store server. This has to be started after the AS:\nprompt> <installation folder>/bin/dssup.sh\nThe application server and the data store server can also be started one after the other using a single command:\nprompt> <installation folder>/bin/allup.sh\n## Stop Server\n\nThe application server is stopped as follows:\nprompt> <installation folder>/bin/bisdown.sh\nTo only stop the data store server:\nprompt> <installation folder>/bin/dssdown.sh\nTo stop both the data store server and then the applicaiton server:\nprompt> <installation folder>/bin/alldown.sh", "timestamp": "2025-09-18T09:38:29.954734Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_system-documentation_standalone_system-requirements:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/system-documentation/standalone/system-requirements.html", "repo": "openbis", "title": "System Requirements", "section": "System Requirements", "text": "### System Requirements\n\n## Architecture\n\nAs of today, openBIS can be deployed on the AMD64 (x86_64) architecture.\nSupport for ARM architecture is currently being developed.\n### Hardware Configuration\n\nStarting from openBIS version 20.10.0, openBIS memory and CPU usage requirements have remarkably dropped. The following guidelines cannot be applied to previous versions.\nBelow we provide recommended (virtual) hardware and database (PostgreSQL) server settings for three common use-cases:\n## Parameter\n## Small\n## Medium\n## Big\nDefault ELN LIMS UI using Generic or Life-Sciences Technologies\nx\nx\nx\nOld core UI still actively used\nx\nx\nup to 5 concurrent users\nx\nx\nx\nup to 20 concurrent users\nx\nx\nmore than 20 concurrent users\nx\nPlease bear in mind that, the more customised an openBIS installation is, the more the recommended settings may vary from the optimal ones.\nCPU and Memory Configuration\n\n## Scenario\nNumber of CPUs\nTotal memory\nMemory allocated to OS\nMemory allocated to PostgreSQL\nMemory allocated to openBIS Application Server\nMemory allocated to openBIS Data Store Server\n## Small\n2 modern x86 CPU cores\n## 4 GB\n## 1.5 GB\n## 1 GB\n## 1 GB\n## 0.5 GB\n## Medium\n2-4 modern x86 CPU cores\n## 8 GB\n## 2 GB\n## 2 GB\n## 3GB\n## 1 GB\n## Big\n4-8 modern x86 CPU cores\n## 16 GB\n## 3 GB\n## 3 GB\n## 8 GB\n## 2 GB\n## Postgres Memory Settings\n\nMemory-related settings of your PostgreSQL server can be obtained from https://pgtune.leopard.in.ua/. For the “small” scenario, use the below template:\n## Parameter\n## Value\nDB Version\n15\nOS Type\n(depends on your infrastructure)\nDB Type\n## Web Applicaiton\nTotal Memory (RAM)\n## 3 GB\nNumber of CPUs\n2\nNumber of Connections\n50\nData Storage\n(depends on your infrastructure)\nAfter clicking on “Generate”, you get the matching settings of the postgresql.conf displayed, jointly with the commands to be used to apply these (ALTER SYSTEM).\n## Tuning Of Hardware Settings In Case Of Issues\n\n## Symptom\n## Recommended Action\nLong query execution times\nIncrease CPU number and/or AS & Postgres memory settings, reconfigure Postgres and openBIS memory settings following the recommended settings provided by https://pgtune.leopard.in.ua/.\nAS log shows out of memory errors\nIncrease AS Memory. This easily happens in old installations using the Legacy Core UI that requires additional memory for cache.\nDSS log shows out of memory errors\nIncrease DSS Memory.\n## Operating System\n\nWe recommend to set up openBIS on a Linux operating system. We provide support for installing and operating openBIS on supported\nUbuntu Server LTS releases\n.\nOperating System: Linux / MacOS X\n## Third-Party Packages\n\nThe following software packages are required:\nThe binaries\nbash\n,\nawk\n,\nsed\nand\nunzip\nneed to be installed and in the\n## PATH\nof the openBIS user.\nJava Runtime Environment: recent versions of Oracle JRE 11 or OpenJDK 11\nPostgreSQL 15\n### Additional Requirements\n\nAn SMTP server needs to be accessible if you want openBIS to send out notifications via mail. We recommend to use the a local mail transfer agent such as Postfix configued for message sending.", "timestamp": "2025-09-18T09:38:29.958111Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_command-line-tool:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/command-line-tool.html", "repo": "openbis", "title": "openBIS Command Line Tool (oBIS)", "section": "1. Prerequisites", "text": "openBIS Command Line Tool (oBIS)\n\noBIS is a command-line tool that makes it possible to handle data sets tracked by OpenBIS,\nwhere users have complete freedom to structure and manipulate the data as they wish, while retaining\nthe benefits of openBIS.\nWith oBIS, it is possible not only to handle datasets stored in OpenBIS but also available to keep\nonly metadata send to openBIS, while the data itself is managed externally, by the user. In this\ncase, OpenBIS is aware of its existence and the data can be used for provenance tracking.\n## 1. Prerequisites\n\npython 3.6 or higher\ngit 2.11 or higher\ngit-annex 6 or higher\n### Installation guide\n### 2. Installation\n\npip3\ninstall\nobis\n## Since\nobis\nis based on\npybis\n, the pip command will also install pybis and all its dependencies.\n3. Quick start guide\n\nConfigure your openBIS Instance\n# global settings to be use for all obis repositories\nobis\nconfig\n-g\nset\nopenbis_url\n=\nhttps://localhost:8888\nobis\nconfig\n-g\nset\nuser\n=\nadmin\n## Download Physical Dataset\n# create a physical (-p) obis repository with a folder name\nobis\ninit\n-p\ndata1\ncd\ndata1\n# check configuration\nobis\nconfig\nget\nis_physical\n# download dataset giving a single permId\nobis\ndownload\n20230228091119011\n-58\n## Upload Physical Dataset\n# create a physical (-p) obis repository with a folder name\nobis\ninit\n-p\ndata1\ncd\ndata1\n# check configuration\nobis\nconfig\nget\nis_physical\n# upload as many files or folder as you want (-f) to an existing object as type RAW_DATA\nobis\nupload\n20230228133001314\n-59\n## RAW_DATA\n-f\nyour_file_a\n-f\nyour_file_b\n### 4. Usage\n\n4.1 Help is your friend!\n\n$\nobis\n--help", "timestamp": "2025-09-18T09:38:29.965121Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_command-line-tool:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/command-line-tool.html", "repo": "openbis", "title": "openBIS Command Line Tool (oBIS)", "section": "Options:", "text": "## Options:\n--version\n## Show\nthe\nversion\nand\nexit.\n-q,\n--quiet\n## Suppress\nstatus\nreporting.\n-s,\n--skip_verification\n## Do\nnot\nverify\ncerficiates\n-d,\n--debug\n## Show\nstack\ntrace\non\nerror.\n--help\n## Show\nthis\nmessage\nand\nexit.", "timestamp": "2025-09-18T09:38:29.965121Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_command-line-tool:2", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/command-line-tool.html", "repo": "openbis", "title": "openBIS Command Line Tool (oBIS)", "section": "Commands:", "text": "## Commands:\naddref\n## Add\nthe\ngiven\nrepository\nas\na\nreference\nto\nopenBIS.\nclone\n## Clone\nthe\nrepository\nfound\nin\nthe\ngiven\ndata\nset\nid.\ncollection\nGet/set\nsettings\nrelated\nto\nthe\ncollection.\ncommit\n## Commit\nthe\nrepository\nto\ngit\nand\ninform\nopenBIS.\nconfig\nGet/set\nconfigurations.\ndata_set\nGet/set\nsettings\nrelated\nto\nthe\ndata\nset.\ndownload\n## Download\nfiles\nof\na\ndata\nset.\ninit\n## Initialize\nthe\nfolder\nas\na\ndata\nrepository.\ninit_analysis\n## Initialize\nthe\nfolder\nas\nan\nanalysis\nfolder.\nmove\n## Move\nthe\nrepository\nfound\nin\nthe\ngiven\ndata\nset\nid.\nobject\nGet/set\nsettings\nrelated\nto\nthe\nobject.\nremoveref\n## Remove\nthe\nreference\nto\nthe\ngiven\nrepository\nfrom\nopenBIS.\nrepository\nGet/set\nsettings\nrelated\nto\nthe\nrepository.\nsettings\n## Get\nall\nsettings.\nstatus\n## Show\nthe\nstate\nof\nthe\nobis\nrepository.\nsync\n## Sync\nthe\nrepository\nwith\nopenBIS.\ntoken\ncreate/show\na\nopenBIS\ntoken\nupload\n## Upload\nfiles\nto\nform\na\ndata\nset.\nTo show detailed help for a specific command, type\nobis\n<command>\n--help\n## :\n$\nobis\ncommit\n--help\n### Usage:\nobis\ncommit\n[\n## OPTIONS\n]\n[\n## REPOSITORY\n]\n## Options:\n-m,\n--msg\n## TEXT\n## A\nmessage\nexplaining\nwhat\nwas\ndone\n.\n-a,\n--auto_add\n## Automatically\nadd\nall\nuntracked\nfiles.\n-i,\n--ignore_missing_parent\n## If\nparent\ndata\nset\nis\nmissing,\nignore\nit.\n--help\n## Show\nthis\nmessage\nand\nexit.\n5. Work modes\n\noBIS command line tool can work in two modes depending on how data is stored:\nStandard Data Store mode\nExternal Data Store mode\n## Warning:\nEach repository can work in a single mode only! Mixing modes is not supported.\nDepending on the mode, some commands may be unavailable or behave differently. Please read details\nin the adequate section.\nHere is a short summary of which commands are available in given modes:\n## Command\nStandard Data Store\nExternal Data Store\naddref\n❌\n✅\nclone\n❌\n✅\ncollection get\n✅\n✅\ncollection set\n✅\n✅\ncollection clear\n❌\n✅\ncommit\n❌\n✅\nconfig get\n✅\n✅\nconfig set\n✅\n✅\nconfig clear\n❌\n✅\ndata_set get\n❌\n✅\ndata_set set\n❌\n✅\ndata_set clear\n❌\n✅\ndata_set search\n✅\n❌\ndownload\n✅\n❌\ninit\n❌\n✅\ninit -p\n✅\n❌\ninit_analysis\n❌\n✅\nmove\n❌\n✅\nobject get\n✅\n✅\nobject set\n✅\n✅\nobject clear\n❌\n✅\nobject search\n✅\n❌\nremoveref\n❌\n✅\nrepository get\n❌\n✅\nrepository set\n❌\n✅\nrepository clear\n❌\n✅\nsettings get\n❌\n✅\nsettings set\n❌\n✅\nsettings clear\n❌\n✅\nstatus\n❌\n✅\nsync\n❌\n✅\ntoken\n✅\n✅\nupload\n✅\n❌\n## Login\nSome commands like\ndownload\nor\nupload\nwill connect to OpenBIS instance. At that time, oBIS will\nuse username configured in\n.obis/config.json\nand will ask for password whenever session expires or\nusername changes.\n5.1 Standard Data Store\n\nStandard Data Store mode depicts a workflow where datasets are stored directly in the OpenBIS\ninstance. In this mode user can download/upload files to OpenBIS, search for objects/datasets\nfulfilling filtering criteria\nand get/set properties of objects/collections represented by datasets in current repository.\n## 5.1.1 Commands\n\ncollection\nobis\ncollection\nget\n[\nkey1\n]\n[\nkey2\n]\n...\nobis\ncollection\nset\n[\nkey1\n]=[\nvalue1\n]\n,\n[\nkey2\n]=[\nvalue2\n]\n...\n## With\ncollection\ncommand, obis crawls through current repository and gathers all data set ids and\nthen - if\ndata set is connected directly to a collection - gets or sets given properties to it in OpenBIS\nNote some property names may require to be encapsulated in ‘’, e.g. ‘$name’\nconfig\nobis\nconfig\nget\n[\nkey\n]\nobis\nconfig\nset\n[\nkey\n]=[\nvalue\n]\n## With\nconfig\ncommand, obis can get/set config of a local repository, e.g. when setting access link\nto OpenBIS instance\nThe settings are saved within the obis repository, in the\n.obis\nfolder, as JSON files, or\nin\n~/.obis\nfor the global settings. They can be added/edited manually, which might be useful when\nit comes to integration with other tools.\n## Example\n.obis/config.json\n{\n\"fileservice_url\"\n## :\nnull,\n\"git_annex_hash_as_checksum\"\n## :\ntrue,\n\"hostname\"\n## :\n\"bsse-bs-dock-5-160.ethz.ch\"\n,\n\"is_physical\"\n## :\ntrue,\n\"openbis_url\"\n## :\n\"http://localhost:8888\"\n}\ndata_set\nobis\ndata_set\nsearch\n[\n## OPTIONS\n]\n## Options:\n-object_type,\n--object_type\n## TEXT\n## Object\ntype\ncode\nto\nfilter\nby\n-space,\n--space\n## TEXT\n## Space\ncode\n-project,\n--project\n## TEXT\n## Full\nproject\nidentification\ncode\n-experiment,\n--experiment\n## TEXT\n## Full\nexperiment\ncode\n-object,\n--object\n## TEXT\n## Object\nidentification\ninformation,\nit\ncan\nbe\npermId\nor\nidentifier\n-type,\n--type\n## TEXT\n## Type\ncode\n-registration-date,\n--registration-date\n## TEXT\n## Registration\ndate,\nit\ncan\nbe\nin\nthe\nformat\n\"oYYYY-MM-DD\"\n(\ne.g.\n\">2023-01-31\"\n,\n\"=2023-01-31\"\n,\n\"<2023-01-31\"\n)\n-modification-date,\n--modification-date\n## TEXT\n## Modification\ndate,\nit\ncan\nbe\nin\nthe\nformat\n\"oYYYY-MM-DD\"\n(\ne.g.\n\">2023-01-31\"\n,\n\"=2023-01-31\"\n,\n\"<2023-01-31\"\n)\n-property\n## TEXT\n## Property\ncode\n-property-value\n## TEXT\n## Property\nvalue\n-save,\n--save\n## TEXT\n## Directory\nname\nto\nsave\nresults\n-r,\n--recursive\n## Search\ndata\nrecursively\n## With\ndata_set\nsearch\ncommand, obis connects to a configured OpenBIS instance and searches for all\ndata sets that fulfill given filtering criteria or by using object identification string.\nAt least one search option must be specified.\nSearch results can be downloaded into a file by using\nsave\noption.\nRecursive option enables searching for datasets of children samples or datasets\nNote: Filtering by\n-project\nmay not work when\n## Project\n## Samples\nare disabled in OpenBIS\nconfiguration.\ndownload\nobis\ndownload\n[\noptions\n]\n[\ndata_set_id\n]\n## Options:\n-from-file,\n--from-file\n## TEXT\n## An\noutput\n## .CSV\nfile\nfrom\n`\nobis\ndata_set\nsearch\n`\ncommand\nwith\nthe\nlist\nof\nobjects\nto\ndownload\ndata\nsets\nfrom\n-f,\n--file\n## TEXT\n## File\nin\nthe\ndata\nset\nto\ndownload\n-\ndownloading\nall\nif\nnot\ngiven.\n-s,\n--skip_integrity_check\n## Flag\nto\nskip\nfile\nintegrity\ncheck\nwith\nchecksums\n## The\ndownload\ncommand downloads, the files of a given data set from the OpenBIS instance specified\nin\nconfig\n. This command requires the DownloadHandler / FileInfoHandler microservices to be running\nand the\nfileservice_url\nneeds to be configured.\ninit\nobis\ninit\n-p\n[\nfolder\n]\nIf a folder is given, obis will initialize that folder as an obis repository that works in the\nStandard Data Store mode.\nIf not, it will use the current folder.\nobject get / set\nobis\ncollection\nget\n[\nkey1\n]\n[\nkey2\n]\n...\nobis\ncollection\nset\n[\nkey1\n]=[\nvalue1\n]\n,\n[\nkey2\n]=[\nvalue2\n]\n...\n## With\nget\nand\nset\ncommands, obis crawls through current repository and gathers all data set ids\nand then - if\ndata set is connected directly to an object - gets or sets given properties to it in OpenBIS\nNote some property names may require to be encapsulated in ‘’, e.g. ‘$name’\nobject search\nobis\nobject\nsearch\n[\n## OPTIONS\n]\n## Options:\n-type,\n--type\n## TEXT\n## Type\ncode\nto\nfilter\nby\n-space,\n--space\n## TEXT\n## Space\ncode\n-project,\n--project\n## TEXT\n## Full\nproject\nidentification\ncode\n-experiment,\n--experiment\n## TEXT\n## Full\nexperiment\n-object,\n--object\n## TEXT\n## Object\nidentification\ninformation,\nit\ncan\nbe\npermId\nor\nidentifier\n-registration-date,\n--registration-date\n## TEXT\n## Registration\ndate,\nit\ncan\nbe\nin\nthe\nformat\n\"oYYYY-MM-DD\"\n(\ne.g.\n\">2023-01-31\"\n,\n\"=2023-01-31\"\n,\n\"<2023-01-31\"\n)\n-modification-date,\n--modification-date\n## TEXT\n## Modification\ndate,\nit\ncan\nbe\nin\nthe\nformat\n\"oYYYY-MM-DD\"\n(\ne.g.\n\">2023-01-31\"\n,\n\"=2023-01-31\"\n,\n\"<2023-01-31\"\n)\n-property\n## TEXT\n## Property\ncode\n-property-value\n## TEXT\n## Property\nvalue\n-save,\n--save\n## TEXT\n## File\nname\nto\nsave\nresults\nin\ncsv\nformat\n-r,\n--recursive\n## Search\ndata\nrecursively\n## With\nobject\nsearch\ncommand, obis connects to a configured OpenBIS instance and searches for all\nobjects/samples that fulfill given filtering criteria or by using object identification string.\nAt least one search option must be specified.\nSearch results can be downloaded into a file by using\nsave\noption.\nRecursive option enables searching for datasets of children samples or datasets\nNote: Filtering by\n-project\nmay not work when\n## Project\n## Samples\nare disabled in OpenBIS\nconfiguration.\nupload\nobis\nupload\n[\nsample_id\n]\n[\ndata_set_type\n]\n[\n## OPTIONS\n]\n## With\nupload\ncommand, a new data set of type\ndata_set_type\nwill be created under\nobject\nsample_id\n. Files and folders specified with\n-f\nflag will be uploaded to a newly created\ndata set.\n## 5.1.2 Examples\n\nCreate an obis repository to work in Standard Data Store mode\n# global settings to be use for all obis repositories\nobis\nconfig\n-g\nset\nopenbis_url\n=\nhttps://localhost:8888\nobis\nconfig\n-g\nset\nuser\n=\nadmin\n# create an obis repository with a folder name\nobis\ninit\n-p\ndata1\ncd\ndata1\n# check configuration\nobis\nconfig\nget\nis_physical\n# search for objects of type BACTERIA in sapce TESTID  in OpenBIS\nobis\nobject\nsearch\n-space\n## TESTID\n-type\n## BACTERIA\n# save search results in a files\nobis\nobject\nsearch\n-space\n## TESTID\n-type\n## BACTERIA\n-save\nresults.csv\nobis\nobject\nsearch\n-space\n## TESTID\n-save\nresults_space.csv\n# upload files to an existing object as type RAW_DATA\nobis\nupload\n20230228133001314\n-59\n## RAW_DATA\n-f\nresults.csv\n-f\nresults_space.csv\ndownload datasets of an object and check properties\n# assuming we are in a configured obis repository\nobis\ndownload\n20230228091119011\n-58\n# set object name to XYZ\nobis\nobject\nset\n'$name'\n=\n## XYZ\n# set children of an object to /TESTID/PROJECT_101/PROJECT_101_EXP_3\nobis\nobject\nset\nchildren\n=\n## /TESTID/PROJECT_101/PROJECT_101_EXP_3\n5.2 External Data Store\n\nExternal Data Store mode allows for orderly management of data in\nconditions that require great flexibility. oBIS makes it possible to track data on a file system,\nwhere users have complete freedom to structure and manipulate the data as they wish, while retaining\nthe benefits of openBIS. With oBIS, only metadata is actually stored and managed by openBIS. The\ndata itself is managed externally, by the user, but openBIS is aware of its existence and the data\ncan be used for provenance tracking.\nUnder the covers, obis takes advantage of publicly available and tested tools to manage data on the\nfile system. In particular, it uses git and git-annex to track the content of a dataset. Using\ngit-annex, even large binary artifacts can be tracked efficiently. For communication with openBIS,\nobis uses the openBIS API, which offers the power to register and track all metadata supported by\nopenBIS.\n## 5.2.1 Settings\n\n## With\nget\nyou retrieve one or more settings. If the\nkey\nis omitted, you retrieve all settings of\nthe\ntype\n## :\nobis\n[\ntype\n]\n[\noptions\n]\nget\n[\nkey\n]\n## With\nset\nyou set one or more settings:\nobis\n[\ntype\n]\n[\noptions\n]\nset\n[\nkey1\n]=[\nvalue1\n]\n,\n[\nkey2\n]=[\nvalue2\n]\n,\n...\n## With\nclear\nyou unset one or more settings:\nobis\n[\ntype\n]\n[\noptions\n]\nclear\n[\nkey1\n]\nWith the type\nsettings\nyou can get all settings at once:\nobis\nsettings\n[\noptions\n]\nget\nThe option\n-g\ncan be used to interact with the global settings. The global settings are stored\nin\n~/.obis\nand are copied to an obis repository when that is created.\n## Following settings exist:\ntype\nsetting\ndescription\ncollection\nid\nIdentifier of the collection the created data set is attached to. Use either this or the object id.\nconfig\nallow_only_https\nDefault is true. If false, http can be used to connect to openBIS.\nconfig\nfileservice_url\nURL for downloading files. See DownloadHandler / FileInfoHandler services.\nconfig\ngit_annex_backend\nGit annex backend to be used to calculate file hashes. Supported backends are SHA256E (default), MD5 and WORM.\nconfig\ngit_annex_hash_as_checksum\nDefault is true. If false, a CRC32 checksum will be calculated for openBIS. Otherwise, the hash calculated by git-annex will be used.\nconfig\nhostname\nHostname to be used when cloning / moving a data set to connect to the machine where the original copy is located.\nconfig\nopenbis_url\nURL for connecting to openBIS (only protocol://host:port, without a path).\nconfig\nopenbis_token\nToken to use when connecting to openBIS. Can be either a session token or a personal access token. Alternatively, it can be a path to a file containing the token.\nconfig\nsession_name\nThe name every personal access token is associated with.\nconfig\nobis_metadata_folder\nAbsolute path to the folder which obis will use to store its metadata. If not set, the metadata will be stored in the same location as the data. This setting can be useful when dealing with read-only access to the data. The clone and move commands will not work when this is set.\nconfig\nuser\nUser for connecting to openBIS.\ndata_set\ntype\nData set type of data sets created by obis.\ndata_set\nproperties\nData set properties of data sets created by obis.\nobject\nid\nIdentifier of the object the created data set is attached to. Use either this or the collection id.\nrepository\ndata_set_id\nThis is set by obis. Is is the id of the most recent data set created by obis and will be used as the parent of the next one.\nrepository\nexternal_dms_id\nThis is set by obis. Id of the external dms in openBIS.\nrepository\nid\nThis is set by obis. Id of the obis repository.\nThe settings are saved within the obis repository, in the\n.obis\nfolder, as JSON files, or\nin\n~/.obis\nfor the global settings. They can be added/edited manually, which might be useful when\nit comes to integration with other tools.\n## Example\n.obis/config.json\n{\n\"fileservice_url\"\n## :\nnull,\n\"git_annex_hash_as_checksum\"\n## :\ntrue,\n\"hostname\"\n## :\n\"bsse-bs-dock-5-160.ethz.ch\"\n,\n\"openbis_url\"\n## :\n\"http://localhost:8888\"\n}\n## Example\n.obis/data_set.json\n{\n\"properties\"\n## :\n{\n## \"K1\"\n## :\n\"v1\"\n,\n## \"K2\"\n## :\n\"v2\"\n}\n,\n\"type\"\n## :\n## \"UNKNOWN\"\n}\n## 5.2.2 Commands\n\ninit\nobis\ninit\n[\nfolder\n]\nIf a folder is given, obis will initialize that folder as an obis in the External Data Store mode.\nIf not, it will use the current folder.\ninit_analysis\nobis\ninit_analysis\n[\noptions\n]\n[\nfolder\n]\nWith init_analysis, a repository can be created which is derived from a parent repository. If it is\ncalled from within a repository, that will be used as a parent. If not, the parent has to be given\nwith the\n-p\noption.\ncommit\nobis\ncommit\n[\noptions\n]\n## The\ncommit\ncommand adds files to a new data set in openBIS. If the\n-m\noption is not used to\ndefine a commit message, the user will be asked to provide one.\nsync\nobis\nsync\nWhen git commits have been done manually, the\nsync\ncommand creates the corresponding data set in\nopenBIS. Note that, when interacting with git directly, use the git annex commands whenever\napplicable, e.g. use “git annex add” instead of “git add”.\nstatus\nobis\nstatus\n[\nfolder\n]\nThis shows the status of the repository folder from which it is invoked, or the one given as a\nparameter. It shows file changes and whether the repository needs to be synchronized with openBIS.\nclone\nobis\nclone\n[\noptions\n]\n[\ndata_set_id\n]\n## The\nclone\ncommand copies a repository associated with a data set and registers the new copy in\nopenBIS. In case there are already multiple copied of the repository, obis will ask from which copy\nto clone.\nTo avoid user interaction, the copy index can be chosen with the option\n-c\nWith the option\n-u\na user can be defined for copying the files from a remote system\nBy default, the file integrity is checked by calculating the checksum. This can be skipped\nwith\n-s\n.\n## Note\n: This command does not work when\nobis_metadata_folder\nis set.\nmove\nobis\nmove\n[\noptions\n]\n[\ndata_set_id\n]\n## The\nmove\ncommand works the same as\nclone\n, except that the old repository will be removed.\nNote: This command does not work when\nobis_metadata_folder\nis set.\naddref / removeref\nobis\naddref\nobis\nremoveref\nObis repository folders can be added or removed from openBIS. This can be useful when a repository\nwas moved or copied without using the\nmove\nor\ncopy\ncommands.\ntoken\nobis\ntoken\nget\n<session_name>\n[\n--validity-days\n]\n[\n--validity-weeks\n]\n[\n--validity-months\n]\nGets or creates a new personal access token (PAT) and stores it in the obis configuration. If\nno\nsession_name\nis provided or is not stored in the configuration, you’ll be asked interactively.\nIf no validity period is provided, the maximum (configured by the server) is used. If a PAT with\nthis\nsession_name\nalready exists and it is going to expire soon (according to server\nsetting\npersonal_access_tokens_validity_warning_period\n), a new PAT will be created, stored in the\nobis configuration and used for every subsequent request.\n## 5.2.3 Examples\n\nCreate an obis repository and commit to openBIS\n# global settings to be use for all obis repositories\nobis\nconfig\n-g\nset\nopenbis_url\n=\nhttps://localhost:8888\nobis\nconfig\n-g\nset\nuser\n=\nadmin\n# create an obis repository with a file\nobis\ninit\ndata1\ncd\ndata1\necho\ncontent\n>>\nexample_file\n# configure the repository\nobis\ndata_set\nset\ntype\n=\n## UNKNOWN\nobis\nobject\nset\nid\n=\n## /DEFAULT/DEFAULT\n# commit to openBIS\nobis\ncommit\n-m\n'message'\nCommit to git and sync manually\n# assuming we are in a configured obis repository\necho\ncontent\n>>\nexample_file\ngit\nannex\nadd\nexample_file\ngit\ncommit\n-m\n'message'\nobis\nsync\nCreate an analysis repository\n# assuming we have a repository 'data1'\nobis\ninit_analysis\n-p\ndata1\nanalysis1\ncd\nanalysis1\nobis\ndata_set\nset\ntype\n=\n## UNKNOWN\nobis\nobject\nset\nid\n=\n## /DEFAULT/DEFAULT\necho\ncontent\n>>\nexample_file\nobis\ncommit\n-m\n'message'\n## 6. Authentication\n\nThere are 2 ways to perform user authentication against OpenBIS.\n## 6.1. Login\n\nObis, internally, stores a session token which is used to connect with OpenBIS. Whenever this token\nis invalidated, obis will ask user to provide credentials to log into OpenBIS again.\n## 6.2. Personal Access Token\n\nSession token is short-lived and its interactive generation makes it unfeasible for usage in automatic\nscripts. An alternative way to authorize is to generate personal access token (PAT), which can be\nconfigured to last for a long periods of time.\nPAT generation is explained in depth in\ntoken\ncommand section.\n7. Big Data Link Services\n\nThe Big Data Link Services can be used to download files which are contained in an obis repository.\nThe services are included in the installation folder of openBIS,\nunder\nservers/big_data_link_services\n. For how to configure and run them, consult\nthe\nREADME.md\nfile.\n8. Rationale for obis\n\nData-provenance tracking tools like openBIS make it possible to understand and follow the research\nprocess. What was studied, what data was acquired and how, how was data analyzed to arrive at final\nresults for publication – this is information that is captured in openBIS. In the standard usage\nscenario, openBIS stores and manages data directly. This has the advantage that openBIS acts as a\ngatekeeper to the data, making it easy to keep backups or enforce access restrictions, etc. However,\nthis way of working is not a good solution for all situations.\nSome research groups work with large amounts of data (e.g., multiple TB), which makes it inefficient\nand impractical to give openBIS control of the data. Other research groups require that data be\nstored on a shared file system under a well-defined directory structure, be it for historical\nreasons or because of the tools they use. In this case as well, it is difficult to give openBIS full\ncontrol of the data.\nFor situations like these, we have developed\nobis\n, a tool for orderly management of data in\nconditions that require great flexibility.\nobis\nmakes it possible to track data on a file system,\nwhere users have complete freedom to structure and manipulate the data as they wish, while retaining\nthe benefits of openBIS. With\nobis\n, only metadata is actually stored and managed by openBIS. The\ndata itself is managed externally, by the user, but openBIS is aware of its existence and the data\ncan be used for provenance tracking.\nobis\nis packaged as a stand-alone utility, which, to be\navailable, only needs to be added to the\n## PATH\nvariable in a UNIX or UNIX-like environment.\nUnder the covers,\nobis\ntakes advantage of publicly available and tested tools to manage data on\nthe file system. In particular, it uses\ngit\nand\ngit-annex\nto track the content of a dataset.\n## Using\ngit-annex\n, even large binary artifacts can be tracked efficiently. For communication with\nopenBIS,\nobis\nuses the openBIS API, which offers the power to register and track all metadata\nsupported by openBIS.\n## 9. Literature\n\nV. Korolev, A. Joshi, V. Korolev, M.A. Grasso, A. Joshi, M.A. Grasso, et al., “PROB: A tool for\ntracking provenance and reproducibility of big data experiments”, Reproduce ‘14. HPCA 2014, vol. 11,\npp. 264-286, 2014.\nhttp://ebiquity.umbc.edu/\nfile_directory\n/papers/693.pdf", "timestamp": "2025-09-18T09:38:29.965121Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_excel-import-service:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/excel-import-service.html", "repo": "openbis", "title": "Excel Import Service", "section": "Excel Import Service", "text": "## Excel Import Service\n\n## Introduction\n\nThe Excel import service reads xls definitions for both types and\nentities and send them to openBIS. It is the replacement of the old\nmaster data scripts adding support for the creation of openBIS entities.\nThe goals are:\nFor common users an import format with the following features to avoid\nthe shortcomings of the old format:\nRecognisable labels as column names.\nMulti-type imports.\nParents/Children creation and linking on a single import.\nFor advanced users like consultants and plugin developers a tool that\nallows to specify on an Excel sheet:\nMetadata model.\nBasic entity structures used for navigation.\n## Modes\n\nTo support different use cases the import service supports the next\nmodes, specifying one of them is mandatory.\nUPDATE IF EXISTS: This one should be the default mode to use to make\nincremental updates.\nIGNORE EXISTING: This mode should be used when the intention is to\nignore updates. Existing entities will be ignored. That way is\npossible to avoid unintentionally updating entities and at the same\ntime adding new ones.\nFAIL IF EXISTS: This mode should be used when the intention is to\nfail if anything is found. That way is possible to avoid making any\nunintentional changes.\n## Organising Definition Files\n\nAll data can be arranged according to the needs of the user, in any\nnumber of files and any number of worksheets. All files have to be in\none directory.\nThe names of the files and worksheets are ignored by the service, the\nuser is advised to use descriptive names that they can quickly\nremember/refer to later.\n## Warning\nIf there are dependencies between files they should be submitted together or an error will be shown.\n## Example:\nWe want to define vocabularies and sample types with properties using\nthese vocabularies. We can arrange our files in several ways:\nput vocabulary and sample types in separate files named i.e\nvocabulary.xls and sample_types.xlsx respectively\nput vocabulary and sample types in different worksheets in the same\nxls file\nput everything in one worksheet in the same file\n## Organising Definitions\n\n## Type definitions:\nThe order of type definitions is not important for the Excel import\nservice, with exception of Vocabularies, those need to be placed before\nthe property types that use them.\n## Entity definitions:\nType definitions for the entities should already exist in the database\nat the time when entities are registered. Generally Entity definitions\nare placed at the end.\nText cell formatting (colours, fonts, font style, text decorations)\n\nAll types of formatting are permitted, and users are encouraged to use\nthem to make their excel files more readable. Adding any non text\nelement (table, clipart) will cause the import to fail.\n(A valid, but not easily readable, example)\nDefinition, rows and sheet formatting\n\nA valid sheet has to start with definition on the first row.\nEach definition has to be separated by one empty row.\nTwo or more consecutive empty rows mark the end of the definitions.\nEmpty spaces at the beginning or end of headers are silently\neliminated.\n## Warning\nIf any content is placed after two consecutive empty rows it will result in an error. This is to alert the user and avoid silently ignoring content.\n## Warning\nHeader rows\nNEED TO BE\na valid attribute of the entity or entity type, property label or property code.\nAny unintended header will result in an error. This is to avoid possible\nmisspellings and avoid silently ignoring content.\n## Entity Types Definitions\n\nAll entity types can be created. There are differences due to the nature of the defined elements themselves.\nVocabulary and Vocabulary Term\n\n## Vocabulary\n## Headers\n## Mandatory\n## Code\n## Yes\n## Description\n## Yes\n## Vocabulary Term\n## Headers\n## Mandatory\n## Code\n## Yes\n## Label\n## Yes\n## Description\n## Yes\n## Example\n## VOCABULARY_TYPE\n## Code\n## Description\n## $STORAGE.STORAGE_VALIDATION_LEVEL\n## Validation Level\n## Code\n## Label\n## Description\n## RACK\n## Rack Validation\n## BOX\n## Box Validation\n## BOX_POSITION\n## Box Position Validation\n## Experiment Type\n\n## Headers\n## Mandatory\n## Code\n## Yes\n## Description\n## Yes\nValidation script\n## Yes\n## Ontology Id\n## No\n## Ontology Version\n## No\n## Ontology Annotation Id\n## No\n## Example\n## EXPERIMENT_TYPE\n## Code\n## Description\nValidation script\n## DEFAULT_EXPERIMENT\ndate_range_validation.py\n## Sample Type\n\n## Headers\n## Mandatory\n## Code\n## Yes\n## Description\n## Yes\nAuto generate codes\n## Yes\nValidation script\n## Yes\nGenerate code prefix\n## Yes\n## Ontology Id\n## No\n## Ontology Version\n## No\n## Ontology Annotation Id\n## No\n## Example\n## SAMPLE_TYPE\n## Code\n## Description\nAuto generate codes\nValidation script\nGenerated code prefix\n## STORAGE_POSITION\n## TRUE\nstorage_position_validation.py\n## STO\n## Dataset Type\n\n## Headers\n## Mandatory\n## Code\n## Yes\n## Description\n## Yes\nValidation script\n## Yes\n## Ontology Id\n## No\n## Ontology Version\n## No\n## Ontology Annotation Id\n## No\n## Example\n## DATASET_TYPE\n## Code\n## Description\nValidation script\n## RAW_DATA\n## Property Type\n\nA property type can exist unassigned to an entity type or assigned to an\nentity type.\n## Headers\n## Mandatory Assigned\n## Mandatory Unassigned\n## Code\n## Yes\n## Yes\n## Mandatory\n## No\n## Yes\nShow in edit views\n## No\n## Yes\n## Section\n## No\n## Yes\nProperty label\n## Yes\n## Yes\nData type\n## Yes\n## Yes\nVocabulary code\n## Yes\n## Yes\n## Description\n## Yes\n## Yes\n## Metadata\n## No\n## No\nDynamic script\n## No\n## No\n## Ontology Id\n## No\n## No\n## Ontology Version\n## No\n## No\n## Ontology Annotation Id\n## No\n## No\nA property type requires a data type to be defined, valid data types\nare.\nData type\n## Description\n## INTEGER\n## REAL\n## VARCHAR\nText of any length but displayed as a single line field.\n## MULTILINE_VARCHAR\nText of any length but displayed as a multi line field.\n## HYPERLINK\n## BOOLEAN\n## CONTROLLEDVOCABULARY\n## XML\n## TIMESTAMP\n## DATE\n## SAMPLE\nSample of any type.\n## SAMPLE:<SAMPLE_TYPE>\nSample of the indicated type.\n## Example Unassigned Property\nIn this case, the property is registered without being assigned to a\ntype, and  the block of property types uses the PROPERTY_TYPE block.\n## PROPERTY_TYPE\n## Code\n## Mandatory\nShow in edit views\n## Section\nProperty label\nData type\nVocabulary code\n## Description\n## $WELL.COLOR_ENCODED_ANNOTATION\n## FALSE\n## TRUE\nColor Annotation\n## CONTROLLEDVOCABULARY\n## $WELL.COLOR_ENCODED_ANNOTATIONS\nColor Annotation for plate wells\n## ANNOTATION.SYSTEM.COMMENTS\n## FALSE\n## TRUE\n## Comments\n## VARCHAR\n## Comments\n## ANNOTATION.REQUEST.QUANTITY_OF_ITEMS\n## FALSE\n## TRUE\nQuantity of Items\n## INTEGER\nQuantity of Items\n## $BARCODE\n## FALSE\n## FALSE\n## Custom Barcode\n## VARCHAR\n## Custom Barcode\n## Example Assigned\nIn this case the property types are assigned to a sample type and the\nblock of property types belong to the entity type block (SAMPLE_TYPE in\nthis case).\n## SAMPLE_TYPE\n## Code\n## Description\nAuto generate codes\nValidation script\nGenerated code prefix\n## ENTRY\n## TRUE\n## ENTRY\n## Code\n## Mandatory\nShow in edit views\n## Section\nProperty label\nData type\nVocabulary code\n## Description\n## Metadata\nDynamic script\n## $NAME\n## FALSE\n## TRUE\nGeneral info\n## Name\n## VARCHAR\n## Name\n## $SHOW_IN_PROJECT_OVERVIEW\n## FALSE\n## TRUE\nGeneral info\nShow in project overview\n## BOOLEAN\nShow in project overview page\n## $DOCUMENT\n## FALSE\n## TRUE\nGeneral info\n## Document\n## MULTILINE_VARCHAR\n## Document\n{ “custom_widget” : “Word Processor” }\n## $ANNOTATIONS_STATE\n## FALSE\n## FALSE\n## Annotations State\n## XML\n## Annotations State\nEntity Type Validation Script and Property Type Dynamic Script\n\nScripts have to reside in\n.py\nfiles in the\nscripts\ndirectory within\nthe folder that contains the Excel files.\n## Within\nscripts,\nfiles can be organised in any suitable setup:\nIn order to refer to a validation or dynamic script\n(e.g.\nstorage_position_validation.py\nbelow), the relative path (from\nthe\nscripts\ndirectory) to the file has to be provided in the relevant\ncolumn. See the example columns below.\n## Example\n## SAMPLE_TYPE\n## Code\n## Description\nAuto generate codes\nValidation scriptƒgre\nGenerated code prefix\n## STORAGE_POSITION\n## TRUE\nstorage_position_validation.py\n## STO\n## Code\n## Mandatory\nShow in edit views\n## Section\nProperty label\nData type\nVocabulary code\n## Description\n## Metadata\nDynamic script\n## $STORAGE_POSITION.STORAGE_CODE\n## FALSE\n## TRUE\n## Physical Storage\n## Storage Code\n## VARCHAR\n## Storage Code\n## $STORAGE_POSITION.STORAGE_RACK_ROW\n## FALSE\n## TRUE\n## Physical Storage\n## Storage Rack Row\n## INTEGER\nNumber of Rows\n## $STORAGE_POSITION.STORAGE_RACK_COLUMN\n## FALSE\n## TRUE\n## Physical Storage\n## Storage Rack Column\n## INTEGER\nNumber of Columns\n## $STORAGE_POSITION.STORAGE_BOX_NAME\n## FALSE\n## TRUE\n## Physical Storage\n## Storage Box Name\n## VARCHAR\n## Box Name\n## $STORAGE_POSITION.STORAGE_BOX_SIZE\n## FALSE\n## TRUE\n## Physical Storage\n## Storage Box Size\n## CONTROLLEDVOCABULARY\n## $STORAGE_POSITION.STORAGE_BOX_SIZE\n## Box Size\n## $STORAGE_POSITION.STORAGE_BOX_POSITION\n## FALSE\n## TRUE\n## Physical Storage\n## Storage Box Position\n## VARCHAR\n## Box Position\n## $STORAGE_POSITION.STORAGE_USER\n## FALSE\n## TRUE\n## Physical Storage\n## Storage User Id\n## VARCHAR\n## Storage User Id\n## $XMLCOMMENTS\n## FALSE\n## FALSE\n## Comments\n## XML\nComments log\n## $ANNOTATIONS_STATE\n## FALSE\n## FALSE\n## Annotations State\n## XML\n## Annotations State\n## Entity Types Update Algorithm\n\n### General Usage\n\nFor every TYPE found in the Excel sheet the next algorithm is performed:\n## IF\n## ITEM\n## NOT\n## EXISTS\nin\nopenBIS\n## :\n## CREATE\n## ITEM\n## ELSE\n## :\n//\n## Doesn\n't exist branch\n## IF\n## FAIL_IF_EXISTS\n## :\n## THROW\n## EXCEPTION\n## IF\n## UPDATE_IF_EXISTS\n## :\n## UPDATE\n## ITEM\n## ELSE\n## IF\n## IGNORE_EXISTING\n## :\n## PASS\n//\n## Ignore\nas\nrequested\n## ELSE\n## :\n## PASS\n//\n## Ignore\nobject\nthat\nhave\nnot\nbeen\nupdated\n## Entity Definitions\n\nMost entities can be created, excluding DataSets. There are differences due to the nature of the defined elements themselves.\n## General Rules:\nHeader order is arbitrary.\nWhen referring to another entity only Identifiers are allowed.\nSample Variables are the only exception.\nVocabulary values in property value rows can be referred to by\neither the vocabulary term code or the vocabulary term label.\n## Warning\nIf a mandatory header is missing it results in an error.\n## Warning\nRepeated headers will result in an error, in case a Property shares Label with an Attribute is encouraged to use the property code instead.\n## Space\n\n## Headers\n## Mandatory\n## Code\n## Yes\n## Description\n## Yes\n## Example\n## SPACE\n## Code\n## Description\n## ELN_SETTINGS\nELN Settings\n## DEFAULT_LAB_NOTEBOOK\n## Default Lab Notebook\n## METHODS\nFolder for methods\n## MATERIALS\nFolder for th materials\n## STOCK_CATALOG\nFolder for the catalog\n## STOCK_ORDERS\nFolder for orders\n## PUBLICATIONS\nFolder for publications\n## Project\n\n## Headers\n## Mandatory\n## Identifier\nYes on UPDATES, ignored on INSERT\n## Code\n## Yes\n## Space\n## Yes\n## Description\n## Yes\n## Example\n## PROJECT\n## Identifier\n## Code\n## Description\n## Space\n## /DEFAULT_LAB_NOTEBOOK/DEFAULT_PROJECT\n## DEFAULT_PROJECT\n## Default Project\n## DEFAULT_LAB_NOTEBOOK\n## /METHODS/PROTOCOLS\n## PROTOCOLS\n## Protocols\n## METHODS\n## /STOCK_CATALOG/PRODUCTS\n## PRODUCTS\n## Products\n## STOCK_CATALOG\n## /STOCK_CATALOG/SUPPLIERS\n## SUPPLIERS\n## Suppliers\n## STOCK_CATALOG\n## /STOCK_CATALOG/REQUESTS\n## REQUESTS\n## Requests\n## STOCK_CATALOG\n## /STOCK_ORDERS/ORDERS\n## ORDERS\n## Orders\n## STOCK_ORDERS\n## /ELN_SETTINGS/TEMPLATES\n## TEMPLATES\n## Templates\n## ELN_SETTINGS\n## /PUBLICATIONS/PUBLIC_REPOSITORIES\n## PUBLIC_REPOSITORIES\n## Public Repositories\n## PUBLICATIONS\n## Experiment\n\n## Headers\n## Mandatory\n## Identifier\nYes on UPDATES, ignored on INSERT\n## Code\n## Yes\n## Project\n## Yes\n## Property Code\n## No\n## Property Label\n## No\n## Example\n## EXPERIMENT\nExperiment type\n## COLLECTION\n## Identifier\n## Code\n## Project\n## Name\nDefault object type\n## /METHODS/PROTOCOLS/GENERAL_PROTOCOLS\n## GENERAL_PROTOCOLS\n## /METHODS/PROTOCOLS\n## General Protocols\n## GENERAL_PROTOCOL\n## /STOCK_CATALOG/PRODUCTS/PRODUCT_COLLECTION\n## PRODUCT_COLLECTION\n## /STOCK_CATALOG/PRODUCTS\n## Product Collection\n## PRODUCT\n## /STOCK_CATALOG/SUPPLIERS/SUPPLIER_COLLECTION\n## SUPPLIER_COLLECTION\n## /STOCK_CATALOG/SUPPLIERS\n## Supplier Collection\n## SUPPLIER\n## /STOCK_CATALOG/REQUESTS/REQUEST_COLLECTION\n## REQUEST_COLLECTION\n## /STOCK_CATALOG/REQUESTS\n## Request Collection\n## REQUEST\n## /STOCK_ORDERS/ORDERS/ORDER_COLLECTION\n## ORDER_COLLECTION\n## /STOCK_ORDERS/ORDERS\n## Order Collection\n## ORDER\n## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION\n## TEMPLATES_COLLECTION\n## /ELN_SETTINGS/TEMPLATES\n## Template Collection\n/PUBLICATIONS/PUBLIC_REPOSITORIES/PUBLICATIONS_COLLECTION\n## PUBLICATIONS_COLLECTION\n## /PUBLICATIONS/PUBLIC_REPOSITORIES\n## Publications Collection\n## PUBLICATION\n## Sample\n\n## Headers\n## Mandatory\n$\n## No\n## Identifier\nYes on UPDATES, ignored on INSERT\n## Code\n## No\n## Project\n## No\n## Experiment\n## No\nAuto generate code\n## No\n## Parents\n## No\n## Children\n## No\n## Property Code\n## No\n## Property Label\n## No\n## Example\n## SAMPLE\nSample type\n## ORDER\n$\n## Identifier\n## Code\n## Space\n## Project\n## Experiment\n## Order Status\n## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE\n## ORDER_TEMPLATE\n## ELN_SETTINGS\n## /ELN_SETTINGS/TEMPLATES\n## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION\nNot yet ordered\nDefining Parent and Children in Samples\n\nParent and child columns can be used to define relations between\nsamples. Samples can be addressed by:\n$ : Variables, only really useful during batch inserts for samples\nwith autogenerated codes since Identifiers can’t be known. Variables\nSHOULD start with $.\n## Identifiers\n## Warning\nParents and children SHOULD be separated by an end of line, each sample should be in its own line.\n## SAMPLE\nSample type\n## ORDER\n$\n## Parents\n## Children\n## Identifier\n## Code\n## Space\n## Project\n## Experiment\n## Order Status\n## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_A\n## ORDER_TEMPLATE\n## ELN_SETTINGS\n## /ELN_SETTINGS/TEMPLATES\n## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION\nNot yet ordered\n## $B\n## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_B\n## ORDER_TEMPLATE\n## ELN_SETTINGS\n## /ELN_SETTINGS/TEMPLATES\n## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION\nNot yet ordered\n## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_A\n## $B\n## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_D\n## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_C\n## ORDER_TEMPLATE\n## ELN_SETTINGS\n## /ELN_SETTINGS/TEMPLATES\n## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION\nNot yet ordered\n## /ELN_SETTINGS/TEMPLATES/ORDER_TEMPLATE_D\n## ORDER_TEMPLATE\n## ELN_SETTINGS\n## /ELN_SETTINGS/TEMPLATES\n## /ELN_SETTINGS/TEMPLATES/TEMPLATES_COLLECTION\nNot yet ordered\nProperties and Sample Variables\n\nAs a general rule, properties would only accept data of the specified\ntype.\nSample properties would typically require an Identifier to be given but\na variable ‘$’ could be used instead for a sample declared at any point\nof the document, including cyclical dependencies. This is useful for\nscenarios where Sample codes are autogenerated and can’t be known in\nadvance.\nMaster Data as a Core Plugin\n\nThe master data plugin is an AS core plugin.\nDirectory structure\n(important)\n## :\nUse standard initialize-master-data.py handle as it is ingested by\nopenbis on startup.\nExcel files\nshould be organised\nin\nmaster-data\ndirectory\nin the same plugin and\nscripts\nshould\nbe contained in\nscripts\ndirectory\nunder master-data.\n## Contents of initialize-master-data.py:\nfrom\nch.ethz.sis.openbis.generic.server.asapi.v3\nimport\nApplicationServerApi\nfrom\nch.systemsx.cisd.openbis.generic.server\nimport\nCommonServiceProvider\nfrom\nch.ethz.sis.openbis.generic.asapi.v3.dto.service.id\nimport\nCustomASServiceCode\nfrom\nch.ethz.sis.openbis.generic.asapi.v3.dto.service\nimport\nCustomASServiceExecutionOptions\nfrom\nch.systemsx.cisd.openbis.generic.server.jython.api.v1.impl\nimport\nMasterDataRegistrationHelper\nimport\nsys\nhelper\n=\nMasterDataRegistrationHelper\n(\nsys\n.\npath\n)\napi\n=\nCommonServiceProvider\n.\ngetApplicationContext\n()\n.\ngetBean\n(\nApplicationServerApi\n.\n## INTERNAL_SERVICE_NAME\n)\nsessionToken\n=\napi\n.\nloginAsSystem\n()\nprops\n=\nCustomASServiceExecutionOptions\n()\n.\nwithParameter\n(\n'xls'\n,\nhelper\n.\nlistXlsByteArrays\n())\n\\\n.\nwithParameter\n(\n'xls_name'\n,\n## 'ELN-LIMS-LIFE-SCIENCES'\n)\n.\nwithParameter\n(\n'update_mode'\n,\n## 'UPDATE_IF_EXISTS'\n)\n\\\n.\nwithParameter\n(\n'scripts'\n,\nhelper\n.\ngetAllScripts\n())\nresult\n=\napi\n.\nexecuteCustomASService\n(\nsessionToken\n,\nCustomASServiceCode\n(\n\"xls-import-api\"\n),\nprops\n)\nThere are following parameters to fill (Easiest is to use\nMasterDataRegistrationHelper to evaluate parameter values):\n‘xls’: Array of excel files. It can be easily acquired by calling\nhelper.listXlsByteArrays or listCsvByteArrays.\n‘xls_name’ - Name for the batch, it is used by versioning system.\n‘update_mode’ - See “Modes” section.\n‘scripts’ - if you have any scripts in your data, provide them here.\nIt is easiest to get it with MasterDataRegistrationHelper\ngetAllScripts function.\n‘results’ object is a summary of what has been created.\n## Example\nFor an complete up to date example, please check the\neln-lims-life-sciences plugin that ships with the installer or on the\n## official Git repository:\nhttps://sissource.ethz.ch/sispub/openbis/-/tree/master/openbis_standard_technologies/dist/core-plugins/eln-lims-life-sciences/1/as\nOr download the complete plugin using the next link:\nhttps://sissource.ethz.ch/sispub/openbis/-/archive/master/openbis-master.zip?path=openbis_standard_technologies/dist/core-plugins/eln-lims-life-sciences\n## Known Limitations\n\nProperty type assignments to entity types cannot be updated since\nthe current V3 API does not support this functionality. This means\nthat a change in the order of assignments or group names during an\nupdate will be ignored.", "timestamp": "2025-09-18T09:38:29.971953Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/index.html", "repo": "openbis", "title": "Advance Features", "section": "Advance Features", "text": "## Advance Features\n\nJupyterHub for openBIS\n## Overview\n## Nomenclature\nPrerequisites for testing in a local environment\nHow to run the official JupyterHub for openBIS image in your local machine\nHow to extend the official JupyterHub for openBIS image\nModify a currently running container - From UI (for users)\n## Check Available Python 2 Libraries\n## Add Python 2 Library\n## Check Available Octave Libraries\n## Add Octave Library\n## Check Available Python 3 Libraries\n## Add Python 3 Library\n## Check Available R Libraries\n## Add R Library\nModify a currently running container - From Console (for admins)\n## Add Python Library\n## Add R Library\nSave the state of a running container as a new image\nExtend a docker image using a docker recipe (for maintenance)\nHow to start a jupyterhub-openbis docker image on a productive JupyterHub server\nOther useful Docker commands\nSave an image as a tar file to share it\nLoad an image from a tar file\nRemove an image\nRemove all stopped containers\n### openBIS ELN Integration Configuration\nTroubleshooting Connectivity to openBIS\nSession is no longer valid. Please log in again error\nSession is no longer valid. The openBIS server has a self-signed certificate\nSession is no longer valid. The session has timeout\nopenBIS Command Line Tool (oBIS)\n## 1. Prerequisites\n### 2. Installation\n3. Quick start guide\n### 4. Usage\n4.1 Help is your friend!\n5. Work modes\n5.1 Standard Data Store\n## 5.1.1 Commands\n## 5.1.2 Examples\n5.2 External Data Store\n## 5.2.1 Settings\n## 5.2.2 Commands\n## 5.2.3 Examples\n## 6. Authentication\n## 6.1. Login\n## 6.2. Personal Access Token\n7. Big Data Link Services\n8. Rationale for obis\n## 9. Literature\nopenBIS Data Modelling\n## Overview\nData model in openBIS ELN-LIMS\n## Inventory\n## Lab Notebook\nopenBIS parents and children\nExamples of parent-child relationships\n## Excel Import Service\n## Introduction\n## Modes\n## Organising Definition Files\n## Organising Definitions\nText cell formatting (colours, fonts, font style, text decorations)\nDefinition, rows and sheet formatting\n## Entity Types Definitions\nVocabulary and Vocabulary Term\n## Experiment Type\n## Sample Type\n## Dataset Type\n## Property Type\nEntity Type Validation Script and Property Type Dynamic Script\n## Entity Types Update Algorithm\n### General Usage\n## Entity Definitions\n## Space\n## Project\n## Experiment\n## Sample\nDefining Parent and Children in Samples\nProperties and Sample Variables\nMaster Data as a Core Plugin\n## Known Limitations", "timestamp": "2025-09-18T09:38:29.975941Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html", "repo": "openbis", "title": "JupyterHub for openBIS", "section": "Warning", "text": "JupyterHub for openBIS\n\n## Warning\nThis guide is not meant to substitute the official Docker documentation. Standard Docker commands are present in sections that are not necessarily related with them.\n## Overview\n\nSIS provides a Docker image for the installation of a JupyterHub server,\navailable at\nhttps://hub.docker.com/r/openbis/\n.\nThis guide explains how to modify JupyterHub docker images and save\nthem. It is aimed at users who are not familiar with Docker, but it\nshould not be considered a substitute of the official Docker\ndocumentation.\n## Warning\nWe advise non expert users, to first test the instructions provided in this guide on their local machine, to familiarise themselves with the process, before making changes on the JupyterHub server.\nDocker images are stateless, which means that after rebooting all\nchanges made will not be saved. This guarantees a stable environment,\nwhich is particularly desirable to run services.\nIf a user wants to introduce changes, the docker image needs to be\nupdated. There are two possibilities for this:\nFor testing\n: Login into the Docker container, modify it and save\nthe modified container as a new image. This method is not\nrecommended for production because it is not compatible with\nofficial JupyterHub Docker image upgrades released by SIS.\nFor correct maintenance\n: Extend the current image using a Docker\nrecipe that includes only your changes. This method is recommended\nfor production, because when a new official JupyterHub Docker image\nis released by SIS, it will be possible to quickly apply the changes\nto this image from the Docker recipe.\n## Nomenclature\n\n## Docker\n: A computer program that performs operating-system-level\nvirtualisation also known as containerisation. The official website can\nbe found here\nhttps://www.docker.com/\n.\nDocker image\n: Docker images describe the environment to virtualise.\nDocker images are stateless.\nDocker container\n: Docker containers provide the environment to\nexecute the images.\nPrerequisites for testing in a local environment\n\nDocker environment\n. All examples shown below require a working\ndocker environment. Please visit\nhttps://www.docker.com\nto\ndownload the Docker Community Edition for your OS.\nJupyterHub Docker image\n. The jupyterhub-openbis images can be\nfound at\nhttps://hub.docker.com/r/openbis/\n. They can be installed\nlocally like any other Docker Hub image.\nopenBIS installation\n(optional).\nHow to run the official JupyterHub for openBIS image in your local machine\n\nAfter downloading the jupyterhub-openbis, find the id of your image.\n$\ndocker\nimages\n## REPOSITORY\n## TAG\n## IMAGE\n## ID\n## CREATED\n## SIZE\nopenbis/jupyterhub-openbis-sis-20180405\nlatest\n585a9adf333b\n23\nhours\nago\n4\n## .75GB\nRun the image with one of the two following commands:\na. if you want to connect to your productive openBIS instance (e.g.\nhttps://openbis-elnlims.ch), use the following command:\ndocker run -e OPENBIS_URL=https://openbis-elnlims.ch -e JUPYTERHUB_INTEGRATION_SERVICE_PORT=8002 -e JUPYTERHUB_PORT=8000 -e CERTIFICATE_KEY=/vagrant/config/certificates/default.key -e CERTIFICATE_CRT=/vagrant/config/certificates/default.crt -p 8000:8000 -p 8081:8081 -p 8001:8001 -p 8002:8002 585a9adf333b ./vagrant/initialize/start_jupyterhub.sh\nb. if you have a local openBIS installation for testing, you can run\nthe following command:\ndocker run -v /Users/juanf/jupyterhub-local/home:/home -v /Users/juanf/jupyterhub-local/config/certificates:/vagrant/config/certificates -e OPENBIS_URL=https://129.132.228.42:8443 -e JUPYTERHUB_INTEGRATION_SERVICE_PORT=8002 -e JUPYTERHUB_PORT=8000 -e CERTIFICATE_KEY=/vagrant/config/certificates/default.key -e CERTIFICATE_CRT=/vagrant/config/certificates/default.crt -p 8000:8000 -p 8081:8081 -p 8001:8001 -p 8002:8002 585a9adf333b ./vagrant/initialize/start_jupyterhub.sh\n## Warning\nPlease note the following configuration options:\n-v /Users/juanf/jupyterhub-local/home:/home\nThis option is only required if you want to store the changes you are making. You need to have a home directory for this. It is not necessary for testing, as the image will provide a default one. This directory should contain a “vagrant” sub directory.\n-v /Users/juanf/jupyterhub-local/config/certificates:/vagrant/config/certificates\nThis option is only required in production environments where you need valid certificates. It is not necssary for testing, as the image will provide a default one.\nOPENBIS_URL= https://129.132.228.42:8443\nBy defaut docker is in bridge mode, which means that your docker container accesses your local machine network directly through it. If you have a local openBIS installation please use your IP address; if you use a server installation use the typical address you use to access it.\nTo stop a running docker container, run “\ndocker kill container_ID”\n.\nThe container_ID can be found by running the command\n“docker ps”\n.\nHow to extend the official JupyterHub for openBIS image\n\nModify a currently running container - From UI (for users)\n\nPlease note that libraries installed in this way are NOT permanently\nsaved. After upgrade of the image, the libraries need to be\nre-installed.\n## Check Available Python 2 Libraries\n\nhelp(\"modules\")\n## Add Python 2 Library\n\nIt can probably be done but we are currently not supporting it.\n## Check Available Octave Libraries\n\npkg\nlist\n## Add Octave Library\n\nIt can probably be done but we are currently not supporting it.\n## Check Available Python 3 Libraries\n\npip\nfreeze\n## Add Python 3 Library\n\nUse pip install as you would normally do. The Python 3 kernel often\ndoesn’t need to be restarted to pick up new libraries, but is\nrecommended to do so.\n## Check Available R Libraries\n\nmy_packages <- library()$results\nhead(my_packages, 1000000)\n## Add R Library\n\nUse the install command as you would normally do. The R kernel needs\nto be restarted to pick up new libraries.\nModify a currently running container - From Console (for admins)\n\nFind the container id of the image currently running.\n$\ndocker\nps\n## CONTAINER\n## ID\n## IMAGE\n## COMMAND\n## CREATED\n## STATUS\n## PORTS\n## NAMES\na2b76d1dd204\njupyterhub-openbis-sis-20180405\n\"./vagrant/initial...\"\n4\nseconds\nago\n## Up\n2\nseconds\n0\n.0.0.0:8000-8002->8000-8002/tcp,\n0\n.0.0.0:8081->8081/tcp\nnervous_leakey\nLog into the container.\n$\ndocker\nexec\n-it\na2b76d1dd204\nshell\n## Add Python Library\n\nAdd a new library to Python 3\n# First we should move to the environment used by JupyterHub\n[\nroot@a2b76d1dd204\n/\n]\n# export PATH=/vagrant_installation/miniconda3/bin:$PATH\n[\nroot@a2b76d1dd204\n/\n]\n# export LC_ALL=en_US.utf8\n[\nroot@a2b76d1dd204\n/\n]\n# export LANG=en_US.utf8\n# Install a new python lib using pip\n[\nroot@a2b76d1dd204\n/\n]\n# python --version\n## Python\n3\n.6.4\n## ::\n## Anaconda,\n## Inc.\n[\nroot@a2b76d1dd204\n/\n]\n# pip install prettytable\nThis type of changes can be validated straightaway in JupyterHub, by\njust starting a Python 3 notebook. Other changes could require to reboot\nJupyterHub.\nPlease note that this approach should only be used for testing. To\npreserve the changes, the running container should be saved as a new\nimage, otherwise when the container is shutdown these changes will be\nlost.\n## Add R Library\n\nAdd a new library to R\n# First we should move to the environment used by JupyterHub\n[\nroot@a2b76d1dd204\n/\n]\n# export PATH=/vagrant_installation/miniconda3/bin:$PATH\n[\nroot@a2b76d1dd204\n/\n]\n# export LC_ALL=en_US.utf8\n[\nroot@a2b76d1dd204\n/\n]\n# export LANG=en_US.utf8\n# Install a new r lib using conda\n[\nroot@a2b76d1dd204\n/\n]\n# sudo conda list r-\n[\nroot@a2b76d1dd204\n/\n]\n# sudo conda install -c r -y r-base64enc\nThis type of changes can be validated straightaway in JupyterHub, by\njust starting a R notebook. Other changes could require to reboot\nJupyterHub.\nSave the state of a running container as a new image\n\nIf you know that you have made significant changes that you want to keep\nuntil you build a new docker recipe, you have the option to save the\nrunning container as a new image.\nbs-mbpr28:jupyterhub_reference_installation\njuanf$\ndocker\nps\n## CONTAINER\n## ID\n## IMAGE\n## COMMAND\n## CREATED\n## STATUS\n## PORTS\n## NAMES\na2b76d1dd204\njupyterhub-openbis-sis-20180405\n\"./vagrant/initial...\"\n37\nminutes\nago\n## Up\n37\nminutes\n0\n.0.0.0:8000-8002->8000-8002/tcp,\n0\n.0.0.0:8081->8081/tcp\nlucid_stonebraker", "timestamp": "2025-09-18T09:38:29.981987Z", "source_priority": 2, "content_type": "concept"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html", "repo": "openbis", "title": "JupyterHub for openBIS", "section": "REPOSITORY", "text": "$\ndocker\ncommit\na2b76d1dd204\njupyterhub-openbis-sis-juanextensions-20180406\nsha256:5dd0036664c75a21d6a62b80bf5780e70fcad345bb12a7ad248d01e29a3caa99\n$\ndocker\nimages\n## REPOSITORY\n## TAG\n## IMAGE\n## ID\n## CREATED\n## SIZE\njupyterhub-openbis-sis-juanextensions-20180406\nlatest\n5dd0036664c7\n4\nseconds\nago\n4\n## .75GB\njupyterhub-openbis-sis-20180405\nlatest\n585a9adf333b\n23\nhours\nago\n4\n## .75GB\nExtend a docker image using a docker recipe (for maintenance)\n\nThe recommended approach for maintenance purposes is to extend the\nlatest official docker image distributed by SIS.\nUsing our last example, let’s create a file called “Dockerfile” and with\nthe content shown below.\n# vim:set ft=dockerfile:\n## FROM\nopenbis/jupyterhub-openbis-sis-20180405\n## Adding Python 3 library\n## RUN\nexport\n## PATH\n=\n## /vagrant_installation/miniconda3/bin:\n## $PATH\n&&\n\\\nexport\n## LC_ALL\n=\nen_US.utf8\n&&\n\\\nexport\n## LANG\n=\nen_US.utf8\n&&\n\\\npip\ninstall\nprettytable\nPlease change the name of the image in the file to the one you are\nusing.\nNow we can create a new image using as a starting point the latest from\nthe official repository.\n## Warning\nIt is best practice to include both the name of the user and the creation date in the image name. This will help when dealing with many versions created by different users at different times.\n$\ndocker\nbuild\n-t\njupyterhub-openbis-sis-juanextensions-recipe-20180406\n.\n## Sending\nbuild\ncontext\nto\n## Docker\ndaemon\n4\n## .957GB\n## Step\n1\n/2\n## :\n## FROM\nopenbis/jupyterhub-openbis-sis-20180405\n....\n## Step\n2\n/2\n## :\n## RUN\nexport\n## PATH\n=\n## /vagrant_installation/miniconda3/bin:\n## $PATH\n&&\nexport\n## LC_ALL\n=\nen_US.utf8\n&&\nexport\n## LANG\n=\nen_US.utf8\n&&\npip\ninstall\nprettytable\n....\n## Successfully\ntagged\njupyterhub-openbis-sis-juanextensions-recipe-20180406:latest\n## The\nnew\nimage\nis\nnow\navailable\nand\ncan\nbe\nstarted\nas\ndescribed\nabove.", "timestamp": "2025-09-18T09:38:29.981987Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:2", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html", "repo": "openbis", "title": "JupyterHub for openBIS", "section": "REPOSITORY", "text": "$\ndocker\nimages\n## REPOSITORY\n## TAG\n## IMAGE\n## ID\n## CREATED\n## SIZE\njupyterhub-openbis-sis-juanextensions-recipe-20180406\nlatest\na0106501b223\n3\nminutes\nago\n4\n## .75GB\nopenbis/jupyterhub-openbis-sis-20180405\nlatest\n585a9adf333b\n23\nhours\nago\n4\n## .75GB\nHow to start a jupyterhub-openbis docker image on a productive JupyterHub server\n\n## Warning\nYou can only have\n## ONE\njupyterhub-openbis image running on a server at one given time, since JupyterHub makes use of certain ports on the machine that are also configured in openBIS.\n1. Find the jupyterhub-openbis-start.sh file in your server (please ask\nyour admin).\nFind the container id of the image that is currently running.\n$\ndocker\nps\n## CONTAINER\n## ID\n## IMAGE\n## COMMAND\n## CREATED\n## STATUS\n## PORTS\n## NAMES\na2b76d1dd204\njupyterhub-openbis-sis-20180405\n\"./vagrant/initial...\"\n4\nseconds\nago\n## Up\n2\nseconds\n0\n.0.0.0:8000-8002->8000-8002/tcp,\n0\n.0.0.0:8081->8081/tcp\nnervous_leakey\nStop the current container.\n$\ndocker\nkill\na2b76d1dd204\na2b76d1dd204\nEdit the  jupyterhub-openbis-start.sh file in your server and update\nthe name of the image it runs to the one of your choice\ndocker\nrun\n-v\n/Users/juanf/Documents/programming/git/jupyter-openbis-integration/jupyterhub_reference_installation/home:/home\n-v\n/Users/juanf/Documents/programming/git/jupyter-openbis-integration/jupyterhub_reference_installation/vagrant/config/certificates:/vagrant/config/certificates\n-e\n## OPENBIS_URL\n=\nhttps://129.132.229.37:8443\n-e\n## JUPYTERHUB_INTEGRATION_SERVICE_PORT\n=\n8002\n-e\n## JUPYTERHUB_PORT\n=\n8000\n-e\n## CERTIFICATE_KEY\n=\n/vagrant/config/certificates/default.key\n-e\n## CERTIFICATE_CRT\n=\n/vagrant/config/certificates/default.crt\n-p\n8000\n:8000\n-p\n8081\n:8081\n-p\n8001\n:8001\n-p\n8002\n:8002\njupyterhub-openbis-sis-20180405\n./vagrant/initialize/start_jupyterhub.sh\nStart the new image.\n$\n./jupyterhub-openbis-start.sh\nOther useful Docker commands\n\nSave an image as a tar file to share it\n\n## Warning\nIt is best practice to include both the name of the user and the creation date in the image name. This will help when dealing with many versions created by different users at different times.\n$\ndocker\nsave\njupyterhub-openbis-sis-20180405\n>\njupyterhub-openbis-sis-20180405.tar\n$\nls\n-lah\ntotal\n9681080\n-rw-r--r--\n1\njuanf\n1029\n4\n## .6G\n## Apr\n5\n15\n:38\njupyterhub-openbis-sis-20180405.tar\nLoad an image from a tar file\n\n$\ndocker\nload\n<\njupyterhub-openbis-sis-20180405.tar\n## 8feeda13d3ce:\n## Loading\nlayer\n[==================================================\n>\n]\n27\n.65kB/27.65kB\n## 622cd2c170f3:\n## Loading\nlayer\n[==================================================\n>\n]\n## 152MB/152MB\n## 633fa40a6caa:\n## Loading\nlayer\n[==================================================\n>\n]\n2\n.048kB/2.048kB\n## 7219a9159e4f:\n## Loading\nlayer\n[==================================================\n>\n]\n223\n## .9MB/223.9MB\n## 678b55e862c7:\n## Loading\nlayer\n[==================================================\n>\n]\n4\n## .377GB/4.377GB\n## Loaded\n## image:\njupyterhub-openbis-sis-20180405:latest\n$\ndocker\nimages\n## REPOSITORY\n## TAG\n## IMAGE\n## ID\n## CREATED\n## SIZE\njupyterhub-openbis-sis-20180405\nlatest\n585a9adf333b\n24\nhours\nago\n4\n## .75GB\nRemove an image\n\n$\ndocker\nrmi\njupyterhub-openbis-sis-juanextensions-recipe-20180406\nRemove all stopped containers\n\n$\ndocker\nrm\n$(\ndocker\nps\n-aq\n)\n### openBIS ELN Integration Configuration\n\nOn the openBIS end, what needs to be done is to append the following\nlines into your ELN instance profile:\nservers/core-plugins/eln-lims/1/as/webapps/eln-lims/html/etc/InstanceProfile.js\n# Ansible yml syntax, replace the variables in the double curly braces by the appropriate values:\nthis.jupyterIntegrationServerEndpoint = \"https://{{ openbis_jupyterhub_hostname }}:{{ openbis_jupyterhub_communication_port }}\";\nthis.jupyterEndpoint = \"https://{{ openbis_jupyterhub_hostname }}/\";", "timestamp": "2025-09-18T09:38:29.981987Z", "source_priority": 2, "content_type": "reference"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:3", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html", "repo": "openbis", "title": "JupyterHub for openBIS", "section": "Example:", "text": "# Example:\nthis.jupyterIntegrationServerEndpoint = \"https://jupyterhub-demo.labnotebook.ch:80\";\nthis.jupyterEndpoint = \"https://jupyterhub-demo.labnotebook.ch/\";\n\nOn the jupyterhub end, the docker command would then look as follows:\n\ndocker run -e OPENBIS_URL=https://{{ openbis_public_hostname }} -e JUPYTERHUB_INTEGRATION_SERVICE_PORT=8002 -e JUPYTERHUB_PORT=8000 -e CERTIFICATE_KEY=/vagrant/config/certificates/default.key -e CERTIFICATE_CRT=/vagrant/config/certificates/default.crt -p 8000:8000 -p 8081:8081 -p 8001:8001 -p {{ openbis_jupyterhub_communication_port }}:8002 585a9adf333b ./vagrant/initialize/start_jupyterhub.sh", "timestamp": "2025-09-18T09:38:29.981987Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:4", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html", "repo": "openbis", "title": "JupyterHub for openBIS", "section": "Example:", "text": "# Example:\nopenbis_public_hostname: openbis-test.ethz.ch\nopenbis_jupyterhub_hostname: jupyterhub-test.ethz.ch\nopenbis_jupyterhub_communication_port: 80\n\nThe only port you need to open on your jupyterhub instance is the one\nmatching {{ openbis\\_jupyterhub\\_communication\\_port }}. Using\nfirewall-cmd this would look as follows:", "timestamp": "2025-09-18T09:38:29.981987Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_jupiterhub-for-openbis:5", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/jupiterhub-for-openbis.html", "repo": "openbis", "title": "JupyterHub for openBIS", "section": "Example:", "text": "# Example:\nopenbis_public_hostname: openbis-test.ethz.ch\nopenbis_jupyterhub_hostname: jupyterhub-test.ethz.ch\nopenbis_jupyterhub_communication_port: 80\n\n\n\nfirewall-cmd --permanent --zone=public --add-rich-rule='rule family=\"ipv4\" source address=\"{{ openbis_jupyterhub_openbis_hostname }}\" port protocol=\"tcp\" port=\"{{ openbis_jupyterhub_communication_port }}\" accept'\nTroubleshooting Connectivity to openBIS\n\nCurrently only connecting to the openBIS server used to validate your\nlog in is supported.\nSession is no longer valid. Please log in again error\n\nThis error can be show due to two reasons:\nThe openBIS server has a self-signed certificates for whatever\nreason, typically this is true for test servers.\nThe session has timeout.\nFor each one of them a different fix needs to be applied.\nSession is no longer valid. The openBIS server has a self-signed certificate\n\nTo fix this issue please allow self signed certificates when connecting\nas on the example shown below using the verify_certificates modifier.\nSession is no longer valid. The session has timeout\n\nThe session token obtained during the login is stored by the Jupiter\nserver during its startup. This session token has most likely timeout\nafter a couple of days without use and needs to be refreshed. If you\njust log in to JupyterHub there is a new session available that needs to\nbe handed over to the Jupyter server. For that just stop and start it\nagain.\nStep 1 : Go to your control panel clicking on the button of the top\nright corner.\n## Step 2: Press Stop My Server\n## Step 3 : Press Start My Server\n## Step 4: Press Launch My Server\nStep 5: Wait for the server to startup, after the startup finishes go\nback to your notebook, it should connect now.", "timestamp": "2025-09-18T09:38:29.981987Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_advance-features_openbis-data-modelling:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/advance-features/openbis-data-modelling.html", "repo": "openbis", "title": "openBIS Data Modelling", "section": "Overview", "text": "openBIS Data Modelling\n\n## Overview\n\nopenBIS has the following data structure:\n## Space\n: entity with\nCode and\n## Description\n## Project\n: entity with\n## Code\nand\n## Description\n## Experiment/Collection:\nentity with\nuser-defined properties\n## Object\n: entity with\nuser-defined properties\n## Dataset\n: folder where data files are stored. A dataset has\nuser-defined properties\n## Space\nis the top level. Below\n## Spaces\nthere are\n## Projects\nand below\n## Projects\nthere are\n## Experiments/Collections\n.\nIn the general openBIS data model,\n## Objects\n## can:\nbe shared across\n## Spaces\n(i.e. they do not belong to any Space)\nbelong to a\n## Space\nbelong to a\n## Project\nbelong to an\n## Experiment/Collection\n## Datasets\ncan be associated only to\n## Experiments/Collections\nor to\n## Objects\n.\nAccess to openBIS is controlled at the\n## Space\nlevel,\n## Project\nlevel or openBIS instance level (see\nopenBIS roles\n).\nData model in openBIS ELN-LIMS\n\nIn the openBIS ELN-LIMS a simplified data model is used, as shown below.\n.\nIn this case,\n## Objects\ncan only belong to\n## Experiments/Collections\n.\n## Inventory\n\nThe inventory is usually conceived to be shared by all lab members. The\ninventory is used to store all materials and protocols (i.e. standard\noperating procedures) used in the lab. It is possible to create\nadditional inventories, for example of instruments and equipment.\nThe picture below shows an example of an Inventory with the different openBIS levels.\n## Lab Notebook\n\nBy default, the lab notebook is organised per user. Each user has a\npersonal folder (=\n## Space\n), where to create\n## Projects\n,\n## Experiments\nand Experimental Steps (=\n## Objects\n). Data files can be uploaded to\n## Datasets\n## . Example structure:\nSome labs prefer to organise their lab notebook using an organization\nper project rather than per user. In this case an openBIS\n## Space\nwould\ncorrespond to a lab Project and an openBIS\n## Project\ncould be a\nsub-project or a user folder (one folder per user working on the same project).\nopenBIS parents and children\n\n## Objects\ncan be linked to other\n## Objects\n,\n## Datasets\nto other\n## Datasets\nwith\nN:N relationship. In openBIS these connections are known as\nparents\nand\nchildren\n.\nExamples of parent-child relationships\n\nOne or more samples are derived from one main sample. This is the\nparent of the other samples:\nOne Experimental step is performed following a protocol stored in the\ninventory, on a sample stored in the inventory, using a given equipment. The protocol, the sample and the equipment are the parents of the Experimental step\nOne Experimental Step is done after another and we want to keep\ntrack of the links between the steps:", "timestamp": "2025-09-18T09:38:29.984989Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_associate-file-types-to-dataset-types:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/associate-file-types-to-dataset-types.html", "repo": "openbis", "title": "Associate File Types to Dataset Types", "section": "Dataset", "text": "Associate File Types to Dataset Types\n\nIt is possible to associate given file types to given\n## Dataset\ntypes\n.\nIn this way when a file of this type is uploaded, the\n## Dataset\ntype\nis automatically selected. This option can be found in\n## Settings\n.\nFor example, a Jupyter notebook, which has extension\n.ipynb\ncan\nalways be associated with the\n## Dataset\ntype\n## Analysis Notebook\n.\nGo to\n## Settings\n## Click\n## Edit\nScroll down to the\nDataset types for filenames\nsection\nEnter the file extension (e.g. ipynb) in the\nFilename extension\nfield\nSelect the D\nataset type\nwith which you always want to associate\nthis file type (e.g. Analysis Notebook)\n## Save\nUpdated on November 30, 2022", "timestamp": "2025-09-18T09:38:29.988009Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_create-templates-for-objects:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/create-templates-for-objects.html", "repo": "openbis", "title": "Create Templates for Objects", "section": "Objects", "text": "Create Templates for Objects\n\nIt is possible to create templates for\n## Objects\n. Templates are useful\nwhen one has to register repetitive\nExperimental steps\nor\nmeasurements\nwhere some fields should always be pre-defined.\nFor each\n## Object\ntype several templates can be created. This can be\ndone by the lab manager, who should have admin rights for editing the\n## Settings\n. It is not necessary to be\nInstance admin\nfor this. In a\nmulti-group set up, this can be done by the\ngroup admin\n.\n## Procedure:\nGo to the\n## Settings\n, under\n## Utilities\nScroll down to the\n## Templates\nsection\nFrom the\n## New Template\ntab select select the\nObject type\nfor\nwhich you want to have a template.\nFill in the fields as desired.\n## Save.\nYour templates will be show in the table in the\n## Templates\nsection,\nas shown below\n## See\nUse template for Experimental Steps\nfor more info on how to use templates.\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:29.990042Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-materials-and-samples:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-inventory-of-materials-and-samples.html", "repo": "openbis", "title": "Customise Inventory Of Materials And Samples", "section": "Collections", "text": "Customise Inventory Of Materials And Samples\n\nCreate Collections of Materials\n\n## Collections\nare folders used to organise\n## Objects\nin the\n## Materials\n## Inventory. Such\n## Objects\ncan be different types of\nsamples and materials (e.g. chemicals, antibodies, batteries,\nenvironmental samples).\n## Collections\nneed to be created inside another folder, called\n## Project\n, in the\n## Materials\ninventory.\nFor example, if we want to create a collection of raw samples, we need\nto adopt the following steps:\nCreate an\n## Object\ntype\ncalled Sample. This can only be done by an\nInstance admin\n, from the admin interface, as explained here:\n## New Entity Type Registration\nCreate a first folder called Samples inside the\n## Materials\nfolder\n## (Project)\nCreate a second folder called\n## Raw Samples (Collection)\nCreate the Project folder\n\nTo create the\n## Project\n## folder:\nClick on the\n## Materials\nfolder\nClick the\n## + New Project\nbutton in the form.\nProvide a description, if wanted. This is not mandatory.\nEnter the\n## Code\n. This will be the name of the folder, in this case SAMPLES. Codes only take alphanumeric characters and no spaces.\nCreate the Collection folder\n\nTo register the\n## Collection\nfolder, inside the\n## Project\n## folder:\nClick on the\n## Project\nfolder, in this case\n## Samples\n.\nClick the\n## + New\nbutton in the main form and choose\n## Collection\nfrom the dropdown.\nReplace the automatically generated\n## Code\nwith something pertinent to the collection (e.g RAW_SAMPLES)\nFill in the\n## Name\nfield (e.g. Raw Samples). Note that by default, the navigation menu on the left shows the name. If the name is not provided, the code is shown.\nSelect the\nDefault object type\nfrom the list of available types. This is the\n## Object\nfor which the\n## Collection\nis used. In this case,\n## Sample\n.\nSelect the\nDefault collection view\n(see\n## Customise Collection View\n)\nAdd the “+Object type” button in the Collection percentage\n\nIf you use a Collection for one Object type, you can display a button to add that type to the Collection, as shown below.\nFor this you need to edit the Collection form and set the Default Object type, as shown below.\n## Delete Collections\n\nTo delete an existing Collection:\n## Select\n## Edit Collection\nunder the\n## More..\ndropdown menu\n## Select\n## Delete\nunder the\n## More..\ndrop down menu\nEnable Storage Widget on Sample Forms\n\nWhen a new\nObject type\nis created by an\nInstance admin\n(see\n## New Entity Type Registration)\n, the storage widget is disabled by default.\nIf we want to track storage positions for this particular\nObject type\nas described in\nAllocate storage positions to samples\n, the\n## Storage\nshould be enabled in the\n## Settings\n, under\n## Utilities\n. This can be done by a\ngroup admin\n.\nFor this, follow the steps below:\nGo to\n## Settings\n, under\n## Utilities\nClick the\n## Edit\nbutton\nScroll to the last section of the Settings:\nObject Type definitions Extension\nOpen the\nObject type\nfor which you want to enable the storage, e.g.\n## Sample\n## Select\n## Enable Storage\n## Save\n## Configure Lab Storage\n\nFridges and freezers can be configured in the\n## Settings\n, under\n## Utilities\n.\nGo to\n## Settings\n## Click\n## Edit\nScroll down to the\n## Storages\nsection\nClick the\n## + New Storage\nbutton above the storage table, as shown below.\nFill in the\n## Storage Form\nas explained below\nHow to fill in Storage Form:\n## Code\n. It is advisable to provide a meaningful code for the storage, rather than using the default, because this information is needed when registering storage positions in Batch mode. For example MINUS80_ROOM_A1\n## Name\n. The name is what is shown in most parts of the ELN. E.g. Minus 80°C in Room A1\nNumber of rows\n. This is the number of shelves.\nNumber of columns\n. This is the number of racks per shelf.\nAllowed number of boxes in a rack\n. This is the maximum number per rack. Enter a very high number if this is not important.\nRack space warning\n. Enter space as percentage. E.g. 80, means that the system will give a warning when 80% of a rack is occupied.\nBox space warning\n. Enter space as percentage. E.g. 80, means that the system will give a warning when 80% of a box is occupied.\nValidation level\n. This is the minimum level of information required about the storage:\nRack validation\n. The position in the shelf and rack needs to be specified.\nBox validation\n. In addition to\na\n, a box name needs to be specified.\nBox position validation\n. In addition to\na\nand\nb\n, the position in the box needs to be specified.\nAdd metadata to Storage Positions\n\nStorage positions by default have the following metadata:\nStorage code\nStorage rack row\nStorage rack column\nBox name\nBox size\nBox position\nUser id\nIt is possible to add additional information. This can be done by an\n## Instance Admin\nby editing the\n## Object\n## Type\n## STORAGE_POSITION\nin\nthe admin interface (see\n## New Entity Type Registration\n).", "timestamp": "2025-09-18T09:38:29.993913Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-inventory-of-protocols:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-inventory-of-protocols.html", "repo": "openbis", "title": "Customise Inventory Of Protocols", "section": "Customise Inventory Of Protocols", "text": "## Customise Inventory Of Protocols\n\nCreate Collections of Protocols\n\n## Collections\nare folders used to organise\n## Objects\nin the\n## Methods\nInventory. In this case,\n## Objects\nare protocols.\n## Collections\nneed to be created inside another folder, called\n## Project\n, in the\n## Methods\n## Space\nin the inventory.\nFor example, if we want to create a collection of lab protocols for\nmicroscopy and Mass spec, we need to adopt the following steps:\nRegister a first\n## Project\nfolder called PROTOCOLS in the\n## Methods\n## Space\n.\nIn the\n## Protocols\nfolder, you can register two additional\nCollections called Microscopy Protocols and MS Protocols\nThe steps for the registration of the folders are the same as explained\nin\nCreate Collections of\n## Materials\nUpdated on April 26, 2023\nEnable Protocols in Settings\n\nIf a new\nObject type\nfor a protocol is created by an\nInstance admin\nin the admin interface, it is advisable to set the\nObject type\nto\n## Protocol\nin the\n## Settings\n, under\n## Utilities\n.\nFor this, follow the steps below:\nGo to\n## Settings\n## Click\n## Edit\nScroll to the last section of the\n## Settings\n## :\n## Object Type\ndefinitions Extension\nOpen the\nObject type\ncorresponding to your protocol, e.g.\n## General Protocol\n## Select\nUse as Protocol\n## Save\nThis is done to be able to create local copies of protocols from the\n## Inventory\ninside an\n## Experiment\nwhen writing\nExperimental steps,\nas\ndescribed in\nHow to use protocols in Experimental\n## Steps\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:29.996232Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-parents-and-children-sections-in-object-forms:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-parents-and-children-sections-in-object-forms.html", "repo": "openbis", "title": "Customise Parents and Children Sections in Object Forms", "section": "The", "text": "Customise Parents and Children Sections in Object Forms\n\n## The\n## Parents\nand\n## Children\nsections are automatically created in\nall\n## Object\nforms. It is possible to customise or remove these\nsections, from the\n## Settings\n, under\n## Utilities\n.\nLet’s consider an example. The default\n## Experimental Step\n, present in\nall openBIS instances, looks like the picture below: in the\n## Parents\nsection,\n## General Protocol\nis predefined. If we want to add a General\nProtocol to the form, we use the\n## Search\nor\n## Paste\noptions next to\nGeneral Protocol. If we want to add another parent, for example a\n## Sample\n, we need to use the\n## Search Any\nor\n## Paste Any\nnext to\nParents. See also\nAdd parents and children to Experimental\n## Steps.\nNow let’s see how the\n## Parents\nand\n## Children\nsections of an\n## Experimental Ste\np can be configured in the\n## Object Types Definition\n## Extension\nin the\n## Settings.\n## Section Name\n. Enter an alternative name for the\n## Parents\nor\n## Children\nsection. If empty the default is used\n## (Parents/Children).\nDisable the section\nfor the\n## Object\ntype. No parents/children\ncan be added to this\n## Object\ntype.\nDisable addition of any object type\n. This removes the\n+\nbutton next to the section name, which enables to add as parent any\n## Object\ntype. In this way only\n## Objects\nof types pre-defined in\nthe form can be added.\nTo define which\n## Object\ntypes should always be shown in the form of\na this\n## Object\ntype, click the\n+\nbutton.\nSelect if this is a\n## Parent\nor\n## Child\nfrom the\ndrop down\n.\nEnter a\n## Label\n, which is what is shown in the\n## Object\nform.\nSelect the\n## Object\ntype from the\ndrop down\n.\nSpecify the\nminimum\nand\nmaximum\nnumber of parents needed as\ninput for this\n## Object\ntype. This can be left empty if parents are\nnot mandatory for this type. If a minimum is specified, this makes\nthe addition of those parents mandatory. As many parents as\nspecified in the minimum field will have to be added in order to be\nable to save the form.\n## Specify A\nnnotations\n(e.g. Comments) for this parent\n## Object\ntype.\nClick the + button on the section to add an annotation field.\nSelect the\n## Annotation\nfield from the list of available fields.\nSpecify if the\n## Annotation\nis mandatory.\nThe figure below shows how the\n## Annotation\nof type\n## Comments\nlooks\nlike in the\n## Experimental Step\nform.\nUpdated on November 30, 2022", "timestamp": "2025-09-18T09:38:29.998773Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_customise-the-main-menu:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/customise-the-main-menu.html", "repo": "openbis", "title": "Customise the Main Menu", "section": "Main Menu Sections", "text": "Customise the Main Menu\n\n## Main Menu Sections\n\nThe main menu can be customised from the\n## Settings\n, under\n## Utilities\n, to hide sections that are not needed by the lab.\nGo to\n## Settings.\n## Click\n## Edit.\nGo to the\n## Main Menu\nsection.\nDisable the parts of the menu which are not needed.\n## Save.\nshowLabNotebook\n: if unselected, the\n## Lab Notebook\nsection of\nthe main menu (\n## Lab\n## Notebook)\nwill be hidden.\nshowInventory\n: if unselected, the\n## Inventory\nsection of the\nmain menu (\nInventory of Materials and\n## Methods\n)\nwill be hidden.\nshowStock\n: if unselected, the\n## Stock\nsection of the main menu\n(\nManaging Lab Stocks and\n## Orders\n)\nwill be hidden.\nshowObjectBrowser\n: if unselected, the\n## Object Browser\nunder\n## Utilities\nin the main menu (\n## Object\n## Browser)\nwill be hidden.\nshowStorageManager\n: if unselected, the\n## Storage Manager\nunder\n## Utilities\nin the main menu\n## (Storage\nmanager\n)\nwill be hidden.\nshowAdvancedSearch\n: if unselected, the\n## Advanced Search\nunder\n## Utilities\nin the main menu (\n## Advanced\n## Search)\nbe hidden.\nshowUnarchivingHelper\n: if unselected, the\n## Unarchiving Helper\nand\n## Archiving Helper\nunder\n## Utilities\nin the main menu (\n## Data\narchiving)\nwill be hidden.\nshowTrashcan\n: if unselected, the\n## Traschcan\nunder\n## Utilities\nin the main menu\n(\n## Trashcan\n)\nwill be hidden.\n## showVocabularyViewer:\nif unselected, the\n## Vocabulary\n## Browser\nunder\n## Utilities\nin the main menu (\n## Vocabulary\nbrowser\n)\nwill be hidden.\nshowUserManager\n: if unselected, the\n## User Manager\nunder\n## Utilities\nin the main menu (\n## User\n## Registration\n)\nwill be hidden.\nshowUserProfile\n: if unselected, the\n## User Profile\nunder\n## Utilities\nin the main menu (\n## User\n## Profile\n)\nwill be hidden.\nshowZenodoExportBuilder\n: if unselected, the\n## Zenodo\n## Export\nunder\n## Utilities -> Exports\nin the main menu\n(\nExport to\n## Zenodo\n)\nwill be hidden.\nshowBarcodes\n: if unselected, the\nBarcodes/QR codes Generator\nunder\n## Utilities\nin the main menu\n(\n## Barcodes)\nwill be hidden.\nLab Notebook menu\n\nIt is also possible to customise which entities should be shown under\n## Experiments/Collections\nin the main menu under the\n## Lab Notebook\nsection.\nBy default, only the\n## Object\ntypes\n## Entry\nand\n## Experimental Step\nare shown (see picture below).\nIf you want to show additional custom\n## Object\ntypes in the lab notebook\nmain menu, they need to be enabled by editing the\n## Settings\n.\nGo to the\nObject Type definitions Extension\nsection in the\n## Settings\n. Open the relevant\n## Object\ntype, which you would like to\nsee in the Main menu of the Lab Notebook and select\nShow in lab\nnotebook main menu\n, as shown below.\nBy default, the Object Types\n## Entry\nand\n## Experimental Step\nhave\nthis option already selected.\nPlease note that this is only valid for the\n## Lab Notebook\nsection. In\nthe\n## Inventory\nsection, entries are never shown in the main menu,\nbecause inventories can potentially have thousands of entries, which are\nbetter visualised in tables, rather than in the main menu.\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.000779Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_database-navigation-in-admin-ui:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/database-navigation-in-admin-ui.html", "repo": "openbis", "title": "Database navigation in admin UI", "section": "Spaces, Projects,", "text": "Database navigation in admin UI\n\nopenBIS version 20.10.5 provides a new database navigation in the admin\nUI, as shown below.\nThis allows to navigate the openBIS hierarchy of\n## Spaces, Projects,\n## Experiments/Collections, Objects, Datasets\n.\nThe same navigation menu will be used in the ELN UI in an upcoming\nopenBIS version.\n## Features\n\n## Filter\n\nIt is possible to filter the menu by code or name of the desired entity.\n## Navigation\n\nTo navigate the menu, the nodes have to be opened individually.\nWhen you select an entry in the node, the corresponding entry page\nopens. Please note that at this stage, this is only intended as a\npreview for the navigation menu and the forms show the information  of\nthe entity in json format.\nIf you open several entry pages and you switch between them, by default\nthe navigation menu will scroll up or down to the corresponding entry in\nthe menu. If you do not want to have this behaviour, you can turn off\nthe scrolling by selecting the button on the top right corner of the\nmenu, as show in the picture below.\n## Sorting\n\nEach level of the hierarchy except for\n## Spaces\n(\n## Projects, Collections,\n## Objects, Datasets\n) can be sorted separately, using the buttons shown in\nthe pictures below.\nBy default, the sorting is done by\n## Code\nin alphabetical order. Other\noptions are: sorting by code in reverse alphabetical order; sorting by\nascending date; sorting by descending date.\nUpdated on December 5, 2022", "timestamp": "2025-09-18T09:38:30.005502Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-archiving-to-long-term-storage:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/enable-archiving-to-long-term-storage.html", "repo": "openbis", "title": "Enable archiving to Long Term Storage", "section": "Datasets", "text": "Enable archiving to Long Term Storage\n\nopenBIS supports archiving of datasets to Strongbox and StronLink\n(\nhttps://www.strongboxdata.com/\n) as\ndescribed in\n## Datasets\n## Archiving\nThis needs to be set up and configured on\nsystem level\n.\nArchiving can be manually triggered from the ELN interface. By default,\nthe archiving buttons are not shown, and they need to be enabled by an\nInstance admin\nor even a\ngroup admin\nin the case of a multi-group\ninstance. This is done in the ELN Settings, as shown below.\nIn addition, the Unarchiving helper tool should also be enabled in the\n## ELN Settings:\nMore information on archiving and unarchiving datasets can be found\n## here:\n## Data\narchiving\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.008548Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-barcodes:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/enable-barcodes.html", "repo": "openbis", "title": "Enable Barcodes and QR codes", "section": "Objects", "text": "Enable Barcodes and QR codes\n\nIn order to be able to add custom barcodes and QR codes to\n## Objects\n, an\n## Instance\n## Admin\nneeds to add the $BARCODE property to the object type for which\nbarcodes/QR codes are needed.\nThe barcode functionality is disabled by default in the ELN UI. This can\nbe enabled by a\nlab manager\nor a\ngroup admin\nwith admin right to\nedit the\n## Settings\n, as shown below.\nAfter enabling the option, please refresh your browser. The\nBarcodes/QR Codes Generator\nwill be shown in the main menu under\n## Utilities\nand a\nbarcode icon will be added above the menu.\nInformation on how to use the Barcode functionality in openBIS can be\nfound\n## here:\n## Barcodes\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.010541Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_enable-transfer-to-data-repositories:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/enable-transfer-to-data-repositories.html", "repo": "openbis", "title": "Enable Transfer to Data Repositories", "section": "Zenodo", "text": "Enable Transfer to Data Repositories\n\nCurrently openBIS offers an integration with the\n## Zenodo\ndata\nrepository (\nhttps://zenodo.org/).\nThis enables data direct data transfer from openBIS to Zenodo.\nThis feature needs to be configured on\nsystem level\nas explained\n## here:\nopenBIS DSS configuration\nfile\n.\nIf this is done, the Zenodo Export needs to be made visible in the ELN\nUI by a lab manager, who has should have admin rights for the\n## Settings\n. This can be done by a\ngroup admin\n, in case of a\nmulti-group instance set up.\n## Procedure:\nEdit the\n## Settings\nunder\n## Utilities.\n## Select\nshowZenodoExportBuilder\nin the\n## Main Menu\nsection.\n## Save.\n## The\nExport to Zenodo\nfunctionality becomes available under the\n## Utilities\nmenu (a refresh of the browser page may be necessary to\nsee the change):\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.012549Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_general-overview:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/general-overview.html", "repo": "openbis", "title": "Login", "section": "Login", "text": "The admin interface of openBIS can can be accessed via a URL of this type: https://openbis-xxx/openbis/webapp/openbis-ng-ui/\nwhere openbis-xxx is the name of the server specified in the openBIS configuration file, during the installation by a system admin.\n## Login\n\nFile based and/or LDAP authentication\n\nWhen file based and/or LDAP authentication are used in openBIS, the login interface is as shown below. Users need to provide their username and password to login.\nOnly registered users with assigned rights can login to openBIS.\nSWITCHaai authentication\n\nWhen SWITCHaai (SSO) authentication is used in addition to file based and/or LDAP authentication, the login interface is as shown below.\nSWITCHaai is selected by default. In this case, users need to click on\n## Login\nand they will be redirected to the SWITCHaai login page.\nIf a user would like to authenticate with a file-based account or LDAP (depending on system configuration), they need to select\n## Default Login Service\nfrom the dropdown and provide username and password.", "timestamp": "2025-09-18T09:38:30.014543Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_history-overview:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/history-overview.html", "repo": "openbis", "title": "History Overview", "section": "History Overview", "text": "## History Overview\n\nHistory of deletions\n\n## When\n## Experiments/Collections\n,\n## Objects\nand\n## Datasets\nin openBIS are\npermanently deleted, i.e. they are removed from the trashcan, the\ninformation of these permanently deleted entries is stored in the\ndatabase and it is visible in the admin UI.\n## Spaces\nand\n## Projects\nare directly permanently deleted, without going\nto the trashcan. Their information is also shown in the table of history\nof deletions in the admin UI.\nThe table of history of deletions is under the\n## Tools\nsection, as\nshown below.\nFor each deleted entry, the table shows:\nEntity type\n: this can be\n## Space\n,\n## Project\n,\n## Collection,\n## Object, Dataset;\nEntity identifier\n: this is the PermID of the entity.\n## Spaces\ndo\nnot have PermID, so the code of the\n## Space\nis shown instead;\n## Entity Space\n: the\n## Space\nto which the entity belonged;\n## Entity Project\n: the\n## Project\nto which the entity belonged;\n## Entity Registrator\n: the user who registered the entity;\n## Entity Registration Date\n: the date of registration of the\nentity;\n## Reason\n: the reason of deletion of the entity;\n## Description\n: the PermID (\n## Collection\n,\n## Object\n), identifier\n(\n## Space\n,\n## Project\n),  dataset path (\n## Dataset\n) of the entity;\n## Content\n: the metadata of the entity when it was deleted. This is\navailable for\n## Projects\n,\n## Collections\n,\n## Objects\n,\n## Datasets\n, but\nnot for\n## Spaces\n;\n## User\n: the user who deleted the entity;\n## Date\n: the date and time of deletion of the entity.\nUpdated on October 9, 2022\nHistory of freezing\n\nIn the admin UI it is possible to have an overview of all frozen entries\nin openBIS. Frozen entries can no longer be modified (see\n## Freeze\n## Entities\n).\nThe table showing the history of freezing can be found under the\n## Tools\nsection in the admin UI, as shown below.\nUpdated on March 4, 2022", "timestamp": "2025-09-18T09:38:30.016543Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_import-openbis-exports:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/import-openbis-exports.html", "repo": "openbis", "title": "Imports of openBIS exports", "section": "folder might contain additional folders:", "text": "Imports of openBIS exports\n\nIt is possible to export metadata and data from one openBIS instance and import them to another openBIS instance.\nThe export process is described\nhere\n.\nMetadata import\n\nThe exported metadata (and related masterdata) can be imported in another openBIS instance by an instance admin via the admin UI, as described in\nmastedata import and export\n.\nExported metadata (and masterdata) are contained in a\nxlsx\nfolder, as shown below.\nMetadata and masterdata are contained in the\nmetadata.xlsx\nfile. In addition to the metadata.xlsx file, the\nxlsx\n## folder might contain additional folders:\na\nscripts\nfolder: contains scripts associated with types in the metadata.xlsx file, if these are present;\na\ndata\nfolder: holds the content of spreadsheet fields and large text fields that exceed the size of an Excel cell (32767 characters) and would not fit in the masterdata.xlsx file;\na\nmiscellaneous\nfolder: contain images embedded in text of exported entries, if present.\nIf a\ndata\n,\nscripts\nand/or\nmiscellaneous\nfolders are present in the exported\nxlsx\nfolder, the\nxlsx\nfolder needs to be zipped and the\nxlsx.zip\nfile can be imported via admin UI.\nIf only the\nmetadata.xlsx\nfile is contained in the\nxlsx\nfolder, the metadata.xlsx file can be directly uploaded via admin UI.\nDatasets import\n\nExported datasets are contained in a\ndata\nfolder in a format ready to be imported via\neln-lims default dropbox\n.\nThe folders contained in the\ndata\nfolder need to be placed in the\neln-lims incoming directory\nand from here will be uploaded to the corresponsing openBIS entities. The metadata of the datasets is read from the\nmetadata.json\nfile contained inside each dataset folder.\nWhen importing both metadata and data in a different openBIS instance, first the metadata need to be imported and afterwards the data.", "timestamp": "2025-09-18T09:38:30.019548Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/index.html", "repo": "openbis", "title": "Admins Documentation", "section": "Admins Documentation", "text": "## Admins Documentation\n\n## Login\nFile based and/or LDAP authentication\nSWITCHaai authentication\nInventory overview\nCustomise Inventory Of Materials And Samples\nCreate Collections of Materials\nCreate the Project folder\nCreate the Collection folder\nAdd the “+Object type” button in the Collection percentage\n## Delete Collections\nEnable Storage Widget on Sample Forms\n## Configure Lab Storage\nAdd metadata to Storage Positions\n## Customise Inventory Of Protocols\nCreate Collections of Protocols\nEnable Protocols in Settings\nMove Collections to a different Project\nCustomise Parents and Children Sections in Object Forms\nCustomise the Main Menu\n## Main Menu Sections\nLab Notebook menu\nAssociate File Types to Dataset Types\n## User Registration\nRegister users in ELN Interface\nDefault roles assigned in ELN\nRegister users from the admin UI\nDeactivate users\nRemove users\nCreate users groups in admin UI\nopenBIS roles\n## Observer\n## Space/Project User\n## Space/Project Power User\n## Space/Project Admin\n## Instance Admin\n## User Profile\nAssign home space to a user\n## New Entity Type Registration\nRegister a new Object Type\nRegistration of Properties\nProperty Data Types\nConsiderations on properties registration\n## Controlled Vocabularies\nRegister a new Experiment/Collection type\nRegister a new Dataset type\nEnable Rich Text Editor or Spreadsheet Widgets\nEnable Objects in dropdowns\nRegister masterdata via Excel\nModifying existing types\nProperties overview\nInternal properties and vocabularies\nMasterdata exports and imports\nMasterdata export\nMasterdata import\nMasterdata version\nImports of openBIS exports\nMetadata import\nDatasets import\nCreate Templates for Objects\nEnable Transfer to Data Repositories\nEnable Barcodes and QR codes\nEnable archiving to Long Term Storage\n## History Overview\nHistory of deletions\nHistory of freezing\n## Space Management\nCreate new Inventory Spaces\nCreate a new Inventory Space from the ELN UI\nMulti-group instances\nCreate a new Inventory Space from the core UI\n## Set Inventory Spaces\nCreate new ELN Spaces\nCreate a new Lab Notebook Space from the ELN UI\nMulti-group instances\nCreate a new Lab Notebook Space from the core UI\n## Delete Spaces\nMove Spaces between Lab Notebook and Inventory\n## Multi Group Set Up\nGeneral ELN Settings\n## Instance Settings\n## Group Settings\nGroup ELN Settings\nDatabase navigation in admin UI\n## Features\n## Filter\n## Navigation\n## Sorting", "timestamp": "2025-09-18T09:38:30.022544Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_inventory-overview:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/inventory-overview.html", "repo": "openbis", "title": "Inventory overview", "section": "folders:", "text": "Inventory overview\n\nThe default Inventory contains two\n## folders:\n## Materials\nand\n## Methods\n.\nThese are used to organise respectively samples and materials of any\ntype and lab protocols.\nSamples, materials and protocols are modelled in openBIS as\n## Objects\n.\nIn the openBIS ELN-LIMS for life sciences, the following Object types\nare preconfigured in the database:\nAntibodies, Chemicals, Enzymes, Media, Solutions and Buffers, Plasmids, Plants, Oligos,\nRNAs, Bacteria, Cell lines, Flies, Yeasts, General protocols, PCR protocols, Western blotting protocols.\n## These\n## Objects\nare organised in\n## Collections\nin the\n## Materials\nand\n## Methods\nsections of the Inventory .\nThe generic openBIS ELN-LIMS only has one predefined\n## Object\ntype for\nthe Inventory:\n## General Protocol\n## Additional\n## Object\ntypes and the corresponding\n## Collections\nmust be\ncreated by the\nInstance admin\n, based on the needs of the lab.\nIt is possible to add additional folders in the Inventory, for example\nfor\n## Equipment\n(see\nCreate new Inventory\n## Spaces\n)\n.\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.025221Z", "source_priority": 2, "content_type": "concept"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_masterdata-exports-and-imports:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/masterdata-exports-and-imports.html", "repo": "openbis", "title": "Masterdata exports and imports", "section": "Object Types, Collection Types, Dataset Types,", "text": "Masterdata exports and imports\n\nSince openBIS version 20.10.5 it is possible to export masterdata from\none openBIS instance and import it in another one via the admin UI.\nMasterdata export\n\nAll types tables (\n## Object Types, Collection Types, Dataset Types,\n## Vocabulary Types, Property Types\n) can be exported as shown below for\nthe\n## Object Types\n.\nWhen you export you have the following options:\n## Import Compatible\n## :\n## Yes\n: in this case some columns which are incompatible\nwith imports (i.e. modification date) are not exported even\nif selected; some columns that are required by openBIS for\nimports are added to the exported file even if not selected.\n## No\n: in this case all columns or selected columns are\nexported.\n## Columns\n## :\nAll (default order)\n. All columns are exported, in accordance\nwith the selection explained above for import compatibility.\nSelected (shown order)\n. Selected columns are exported, in\naccordance with the selection explained above for import\ncompatibility.\n## Rows\n: current page or all pages\nInclude dependencies\n: yes or no. If you include dependencies,\nall property types, vocabularies and associated objects are also\nexported. Default is “yes”.\nIf the types have validation plugins or dynamic script plugins\nassociated with them, a zip file containing the scripts is exported from\nopenBIS.\nMasterdata import\n\nTo import the file with the relevant masterdata that was exported as\n## explained above:\nGo to the\n## Tools\nsection and select\n## Import -> All\nfrom the\nmenu.\nUpload the file you exported before using the\n## CHOOSE FILE\nbutton.\nSelect one of the 3 possible options for the Update mode:\nFail if exists\n: if some entries already exist in openBIS,\nthe upload will fail;\nIgnore if exists\n: if some entries already exist in openBIS,\nthey will be left untouched, even if their definition in the\nfile is different from the existing definition in openBIS;\nUpdate if exists\n: if some entries already exist in openBIS\nand their definition in the file is different from the existing\ndefinition in openBIS, they will be updated;\nSince openBIS 20.10.6, the import of zip files is supported.\nMasterdata version\n\nIn openBIS version 20.10.8, the masterdata version has been removed from the exported masterdata files.", "timestamp": "2025-09-18T09:38:30.028282Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_move-collections-to-a-different-project:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/move-collections-to-a-different-project.html", "repo": "openbis", "title": "Move Collections to a different Project", "section": "Collection", "text": "Move Collections to a different Project\n\nIt is possible to move one\n## Collection\nwith it entire content\n(\n## Objects\n+\n## Datasets\n) from one\n## Project\nto another.\n## If\n## Objects\ncontain parent/child relationships these are preserved.\nThis operation requires\n## Space Power User\nor\n## Admin\nrights.\nTo move\n## Collections\nin the Inventory:\nGo to the\n## Collection\npage you want to move\nClick on\n## Edit Collection\n## 3. Select\n## Move\nfrom the\n## More..\ndropdown\n4. Enter the code of the\n## Project\nwhere you want to move your\n## Collection\n. If you start typing the code, openBIS will prompt you with\na list of available options and you can select the appropriate one from\nthere.\n## 5. Click\n## Accept\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.030286Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_multi-group-set-up:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/multi-group-set-up.html", "repo": "openbis", "title": "Multi Group Set Up", "section": "Multi Group Set Up", "text": "## Multi Group Set Up\n\nopenBIS can be configured to be used by multiple groups, where every group only sees their own group Spaces.\nThis configuration needs to be done on\nsystem level\n, as described\nhere\n.\nIn the example below we see two groups:\n## RDM\nand\n## ETHRDH\n. For each group, in the Inventory, there area n\n## Equipment\n, a\n## Materials\n, a\n## Methods\nand a\n## Publications\nSpaces with the group prefix. In the lab notebook, each group member has a personal\n## Space\nwhere the name is the group prefix and the username of the user.\nIt is possible to configure the user management configuration file (\nsys admin\n) not to create user Spaces for a given group, in case one group prefers to organise their notebook by project, rather than by group members, as described\nhere\n.\nIn a multi group instance users are automatically registered and the roles defined in the user management configuration file on the server are automatically assigned to them. There is a maintenance task that runs in the background at configured frequency. This can be once per day or several times per day. If there are new users, they will be added to openBIS when the maintenance task runs.\n## An\ninstance admin\ncan assign additional roles to users from the admin interface (\n## User Registration\n). Default roles defined in the user management configuration file and automatically assigned cannot be removed, because they will be assigned again automatically by openBIS when the maintenance task runs.\nWe would recommend to assign\n## SPACE_USER\nrights for the Inventory Spaces to every group user and\n## SPACE_ADMIN\nrights for their own lab notebook. This can be specified in the user management configuration file on\nsystem level\n.\nIn the user management configuration file, one or more admins for each group can be designated. The\ngroup admin\nhas by default\n## SPACE_ADMIN\nrights to all the\n## Spaces\nof their group. A\ngroup admin\ncan customise the\nGroup ELN Settings\nfor the group.\nGeneral ELN Settings\n\nIn a multi-group instance an\nInstance admin\ncan customise the General\nELN Settings.\nThe Settings can be access from the main menu, under\n## Utilities\n.\nThe General ELN Settings are Settings that are not specific to any of\nthe defined groups group , as shown below.\nThe General ELN Settings consist of two parts:\n## Instance Settings\n. These settings affect the whole instance, it\nis not possible to customise them on a group level.\n## Group Settings\n. These settings affect all general\n## Spaces\nthat\ndo not belong to any group defined in the configuration file\n(see\nopenBIS set up for multi group\ninstances\n).\nThis is the case, for example, if\n## Spaces\nare manually created and\nthey do not belong to any group (see\nCreate new ELN Spaces\n).\nSpaces that do not belong to any group do not have a group prefix. In\nthe example below\n## Publications\ndo not belong to any group in the\n## Inventory.\nand\n## Horizon\n,\n## Snf\ndo not belong to any group in the Lab notebook.\n## Instance Settings\n\nCustom widget\ns. This section allows to enable the Rich Text\nEditor or Spreadsheet component for a given field, as described\nin\nEnable Rich Text Editor or Spreadsheet\n## Widgets;\n## Forced Monospace Font\n. This section allows to force the use of\nmonospace font (i.e. fixed width) for selected MULTILINE_VARCHAR\nproperties. This is useful for example for plasmid sequences.\nDataset types for filenames\n. This section allows to associate\nfiles with a given extension to a specific dataset type, as\ndescribed in\nAssociate File Types to Dataset\n## Types\n.\n## Group Settings\n\n## Storages\n. In this section the storages for samples to be used in\n## Spaces\nnot belonging to any predefined group (see above), can be\ncreated, as described in\n## Configure Lab\n## Storage;\n## Templates\n. In this section, the templates for a given\n## Object\ntype\nto be used in\n## Spaces\nnot belonging to any predefined group\n(see above) can be created, as described in\nCreate Templates for\n## Objects\n;\nObject types definition extension\n. In this section, it is\n## possible to:\nDefine if one\nObject type\nis a protocol. If an\nObject type\nis defined as a protocol, it is possible to create a local copy\nof it under an Experiment, when linking to it as a parent, as\ndescribed in\nEnable Protocols in\n## Settings;\nEnable the storage widget for an\nObject type,\nas described\nin\nEnable Storage Widget on Sample\n## Forms\nDefine if the\nObject type\nshould be shown in drop downs, as\ndescribed in\nEnable Objects in\ndropdowns\n;\nDefine if the\nObject type\nshould be shown in the main menu\nunder the Lab notebook section. By default objects are not shown\nin the main menu in the Inventory section.\nCustomise the\n## Parents\nand\n## Children\nsections for an\n## Object\ntype\nas described in\nCustomise Parents and Children Sections\nin Object\n## Forms\n;\n## Inventory Spaces\n. It is possible to move\n## Spaces\nfrom the\nInventory section to the Lab notebook section and vice-versa as\ndescribed in\nMove Spaces between Lab Notebook and\n## Inventory\nMain menu\n. The main menu for the\n## Spaces\nthat do not belong to\nany predefined group (see above) can be customised here, as\ndescribed in\nCustomise the Main\n## Menu;\n## Miscellaneous\n. In this section it is possible to:\nShow the dataset archiving buttons in\n## Spaces\nthat do not\nbelong to any predefined group. Please note that this is not\navailable by default, but the infrastructure for\narchiving to\ntapes\n(StrongBox/StrongLink) needs to be put in place (\nMulti data set archiving\n)*.\nHide sections by default in\n## Spaces\nthat not belong to any\npredefined group. By default some sections in some forms are\n## hidden:\nDescription in\n## Spaces\nand\n## Projects\n.\nIdentification info in\n## Spaces\n,\n## Projects\n,\n## Experiments\n,\n## Objects\n,\n## Datasets\n.\nBy unchecking this option, these sections will be shown by default.\nUpdated on April 26, 2023\nGroup ELN Settings\n\nIn a multi group instance a\ngroup admin\nor\nInstance admin\ncan\ncustomise the ELN Settings for the group.\nThe group Settings can be selected from the\n## Settings\nin the main\nmenu.\nThe Settings for the relevant group can be selected from the available\ndropdown, as shown below.\nIn the group settings the following is configurable:\n## Storages\n. In this section the group storages for samples can be\ncreated, as described in\n## Configure Lab\n## Storage;\n## Templates\n. In this section, the templates for a given\n## Object\ntype\ncan be created, as described in\nCreate Templates for\n## Objects\n;\nObject types definition extension\n. In this section, it is\n## possible to:\nDefine if one\nObject type\nis a protocol. If an\nObject type\nis defined as a protocol, it is possible to create a local copy\nof it under an Experiment, when linking to it as a parent, as\ndescribed in\nEnable Protocols in\n## Settings;\nEnable the storage widget for an\nObject type,\nas described\nin\nEnable Storage Widget on Sample\n## Forms\nDefine if the\nObject type\nshould be shown in drop downs, as\ndescribed in\nEnable Objects in\ndropdowns\n;\nDefine if the\nObject type\nshould be shown in the main menu\nunder the Lab notebook section. By default objects are not shown\nin the main menu in the Inventory section.\nCustomise the Parents and Children sections for an\nObject type\nas described in\nCustomise Parents and Children Sections in\n## Object\n## Forms\n;\n## Inventory Spaces\n. It is possible to move Spaces from the\nInventory section to the Lab notebook section and vice-versa as\ndescribed in\nMove Spaces between Lab Notebook and\n## Inventory\nMain menu\n. The main menu for the group can be customised here,\nas described in\nCustomise the Main\n## Menu;\n## Miscellaneous\n. In this section it is possible to:\nShow the dataset archiving buttons for the group. Please note\nthat this is not available by default, but the infrastructure\nfor\narchiving to\ntapes\n(StrongBox/StrongLink) needs to be put in place (\nMulti data set\narchiving\n)*.\nHide sections by default. By default some sections in some forms\n## are hidden:\nDescription in\n## Spaces\nand\n## Projects\n.\nIdentification info in\n## Spaces\n,\n## Projects\n,\n## Experiments\n,\n## Objects\n,\n## Datasets\n.\nBy unchecking this option, these sections will be shown by default.\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.033727Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_new-entity-type-registration:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/new-entity-type-registration.html", "repo": "openbis", "title": "New Entity Type Registration", "section": "New Entity Type Registration", "text": "## New Entity Type Registration\n\nEntity types, i.e.\n## Experiment/Collection\n,\n## Object\nand\n## Dataset\ntypes, as well as\nProperty types\nand\n## Controlled Vocabularies\nconstitute the openBIS\nmasterdata\n. They can be created by someone\nwith\nInstance admin\nrole in the\nnew Admin openBIS UI\n.\n## Note:\n## Material\ntypes are also part of the openBIS masterdata. However,\nthey are no longer supported and will be decommissioned soon. They\nshould NOT be used.\nThe new Admin openBIS UI can be accessed via a URL of this type:\nhttps://openbis-xxx/openbis/webapp/openbis-ng-ui/\nwhere openbis-xxx is the name of the server specified in the openBIS\nconfiguration file, during the installation by a system admin.\nRegister a new Object Type\n\n## Select\n## Types -> Object Types\nfrom the menu.\n## Click\n## Add\nat the bottom of the page.\nEnter a\n## Code\n. E.g.\n## INSTRUMENT\n. This is the name of the\nObject to create and is unique. Please note that Codes should be in\ncapital letters, and they can only contain A-Z, a-z, 0-9 and _, -, .\nProvide a description (not mandatory).\nEntity validation plugin is used when we want to have validation on\nsome data entries. This is done via a custom script (see\n## Entity Validation Scripts\n)\nEnter the\n## Generated Code Prefix\n. As a convention, we recommended\nto use the first 3 letters of the\n## Object\ntype\ncode (e.g.\n## INS\n, in\nthis case). This field is used by openBIS to automatically generate\nObject codes: the codes will be for example INS1, INS2, INS3, etc.\n## Leave\n## Generate Codes\nselected if you want to have codes\nautomatically generated by openBIS.\n## Unique Subcodes\nis used for contained objects, which are not\nused in the ELN. Ignore this option.\n## Click\n## Add Section\nat the bottom of the page. Sections are ways\nof grouping together similar properties. Examples of sections used in\nthe ELN are\nGeneral info\n,\nStorage info\n,\n## Experimental Details\n, etc.\nAdd properties inside the Section, by clicking the\n## Add Property\nbutton at the bottom of the page. To remove a property, use the\n## Remove\nbutton at the bottom of the page.\n## Click\n## Save\nat the bottom of the page.\nPlease note that new\nObject types\ncreated in the admin UI, do not\nautomatically appear in ELN drop downs, but they have to be manually\n## enabled, as described here:\nEnable Objects in dropdowns\nRegistration of Properties\n\nWhen registering new properties, the fields below need to be filled in.\n## Code.\nUnique identifier of the property. Codes can only contain\nA-Z, a-z, 0-9 and _, -, .\nData Type.\nSee below for data types definitions.\n## Label.\nThis is the property/column header that the user can see\nin the ELN.\n## Description\n: The description is shown inside a field, to give\nhints about the field itself. In most cases, label and description\ncan be the same.\n## Dynamic Property Plugin\n: Script for calculated properties.\n## See\nDynamic properties\n## Editable\n: Editable in the ELN interface. In some cases, metadata\nis automatically imported by scripts and this should not be changed\nby users in the interface.\n## Mandatory\n: Field can be set as mandatory.\nProperty Data Types\n\n## BOOLEAN\n: yes or no\n## CONTROLLEDVOCABULARY\n: list of predefined values\n## DATE\n. Date field\n## HYPERLINK\n## : URL\n## INTEGER\n: integer number\n## MATERIAL\n. Not used in ELN. It will be dismissed.\n## MULTILINE_VARCHAR\n: long text. It is possible to enable a Rich\nText Editor for this type of property. This is described\n## here:\nEnable Rich Text Editor or Spreadsheet Widgets\n## REAL\n: decimal number\n## OBJECT\n. 1-1 connection to a specific object type.\n## TIMESTAMP\n: date with timestamp\n## VARCHAR\n: one-line text\n## XML\n: to be used by\n## Managed Properties\n(see\nopenBIS Managed Properties\nand for\nSpreadsheet component\n## s, as described here:\nEnable Rich Text Editor or Spreadsheet Widgets\nConsiderations on properties registration\n\nIf you create a property with code “PROJECT”, you should not use the\nlabel “Project”. This will give an error if you use XLS Batch\nregistration/update, because openBIS considers this to be an openBIS\n## Project\n.\nYou should not assign more than 1 property with same label to the\nsame\n## Object\ntype. If two or more properties with the same label\nare present in the same\n## Object\ntype, this will result in an error\nin the XLS Batch registration/update.\n## Controlled Vocabularies\n\nControlled vocabularies are pre-defined lists of values for a given\nfield.\nExisting Vocabularies can be visualised from the Types ->\nVocabularies Tab. Vocabularies staring with the “\n$\n” symbol are\ninternal: they cannot be deleted and their terms cannot be deleted.\nHowever, it is possible to add new terms to these vocabularies and these\ncan also be deleted.\nNew Vocabularies can be added, by clicking the\n## Add\nbutton at the\nbottom of the page.\nWhen registering a new vocabulary, a Code for the vocabulary needs to be\nentered. This corresponds to the name of the vocabulary, and it is a\nunique identifier. Codes can only contain A-Z, a-z, 0-9 and _, -, .\nTo add terms to the list click\n## Add Term\nat the bottom of the page.\nVocabulary terms always have a code and a label: the code is unique and\ncontain only alpha-numeric characters; labels are not necessarily unique\nand allow also special characters. If the label is not defined, codes\nare shown.\nAfter creating the vocabulary and registering the terms, remember to\n## Save\n.\nRegister a new Experiment/Collection type\n\nThe registration of a new\n## Collection\ntype is very similar to the\nregistration of\n## Object\ntypes. For Collection Types, you only need to\nprovide a Code (which is a unique identifier), Description and add a\nvalidation plugin if you want to have metadata validation (see\n## Entity\n## Validation Scripts\n).\nRegister a new Dataset type\n\nThe registration of a new Dataset types is similar to the registration\nof object types.\nIt is possible to disallow deletion for a given dataset type.\nEnable Rich Text Editor or Spreadsheet Widgets\n\nFor certain fields, it is possible to enable the use of a Rich Text\nEditor (RTE) or a spreadsheet component.\nInstance admin\nrights are\nnecessary for this.\n## The\n## RTE\ncan be enabled for properties of type\n## MULTILINE_VARCHAR\n## . The\nspreadsheet component\ncan be enabled for\nproperties of type\n## XML\n.\n## Procedure:\nProperties are defined when creating new entity types (\n## Datasets\n,\n## Objects\n,\n## Experiments/Collections\n)\nTo set a property as RTE or spreadsheet go to the\n## Settings\n,\nunder\n## Utilities\nSelect /ELN_SETTINGS/GENERAL_ELN_SETTINGS\nEnable editing and scroll down to the\n## Custom Widgets\nsection\nClick the + button on the same line as\n## Property Type\nand\n## Widget\n, as shown below\nA new field will appear where you can select the property type and\nthe widget. Choices are:\n## Word Processor\n(=RTE) or\n## Spreadsheet.\nUpdated on October 19, 2022\nEnable Objects in dropdowns\n\nBy default, no Object shows in dropdown menus. Which object types should\nshow in dropdown menus can be customised from the Settings.\nNavigate to the Object Type definitions Extension\nOpen one Object Type (e.g. Antibody)\nSelect show in drop downs\nSave the Settings\nUpdated on October 19, 2022\nRegister masterdata via Excel\n\nIt is possible to register openBIS masterdata using an Excel template\nfrom the admin UI.\nThis can be done from the Import menu under the Tools sections, as shown\nbelow. Three options can be chosen for the import:\nfail if exists\n: if a type or a property already exists in the\ndatabase, the upload will fail.\nignore if exists\n: if a type or a property already exists in the\ndatabase, the upload will ignore this.\nupdate is exists\n: if a type or a property already exists in the\ndatabase, the upload will update existing values.\nAn example template of an Excel masterdata file can be found here:\nmasterdata-template\nPlease note that in the template we used separate spreadsheets for each\ntype (Sample, Experiment, Dataset), but it is also possible to have\neverything in one single spreadsheet.\nModifying existing types\n\nIf you wish to add a new property to an existing\n## Collection/Object/Dataset\n## type, you need to:\n1. add the property in the file\n2. increase the version number of the\n## Collection/Object/Dataset\ntype\n3. use\nIgnore if exists\nas upload method. In this case, only the\nnew property is added to the type.\nThe import on the admin UI allows to register entities in addition to\nmasterdata. An example template file for this can be found here:\nmasterdata-metadata\nMore extensive documentation on the XLS format for masterdata and\nmetadata registration can be found\nhere\n.\nUpdated on January 13, 2023\nProperties overview\n\nThe overview of all properties registered in openBIS and their\nassignments to types is available under the\n## Types\nsection in the\nadmin UI, as shown below.\nUpdated on March 1, 2022\nInternal properties and vocabularies\n\nSome properties and vocabularies are internally defined in openBIS.\nThese can be identified by the\n$\nsign.\nInternal properties (e.g. $NAME, $BARCODE, etc) cannot be deleted nor\nmodified, not even by an instance admin.\nInternal vocabularies (e.g. $DEFAULT_COLLECTION_VIEWS, etc), cannot be\ndeleted and their existing terms cannot be deleted nor modified, however\nan instance admin can add new terms to an internal vocabulary.\nUpdated on January 5, 2023", "timestamp": "2025-09-18T09:38:30.038801Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_space-management:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/space-management.html", "repo": "openbis", "title": "Space Management", "section": "Space Management", "text": "## Space Management\n\nCreate new Inventory Spaces\n\nThe default Inventory contains two\n## folders:\n## Materials\nand\n## Methods\n. These are openBIS\n## Spaces\n.\n## Additional\n## Spaces\ncan be created by an\nInstance admin\n.\nCreate a new Inventory Space from the ELN UI\n\nFrom openBIS version 20.10.4 it is possible to create\n## Spaces\ndirectly\nfrom the ELN interface.\nTo create a new\n## Space\nunder the Inventory:\n## Select\n## Inventory\nin the main menu\nClick on\n## +New Inventory Space\nin the Inventory page\n3. Enter the\n## Code\nfor the\n## Space\n, e.g. EQUIPMENT. Please note that\ncodes only accept alphanumeric characters, –, . , _.\n4.\n## Save\nMulti-group instances\n\nIn a multi-group instance, the\nInstance admin\ncan choose where to\ncreate a new\n## Space\n## :\nno group\n. The new\n## Space\nwill have no prefix and the Settings\ndefined in General Settings will apply (see\nGeneral ELN\n## Settings\n).\nin one of the existing groups\n. The new\n## Space\nwill have the\ngroup prefix and the Settings of that group will apply (see\n## Group\n## ELN\n## Settings\n).\nCreate a new Inventory Space from the core UI\n\nIn the core UI:\n## Select\n## Admin -> Spaces\n## Click\n## Add Space\nat the bottom of the page\nEnter the\n## Space\n## Code\n, e.g.\n## EQUIPMENT\n## Save\n## Set Inventory Spaces\n\nWhen new\n## Spaces\nare created in the core UI, they are automatically\ndisplayed under the\n## Lab Notebook\npart of the ELN main menu.\nIt is possible to move a new\n## Space\nto the Inventory, by editing the\n## Settings\nunder\n## Utilities\nin the\n## ELN UI:\nGo to the\n## Settings\nand click\n## Edit.\nGo to the\n## Inventory Spaces\nsection in the\n## Settings\nand click\nthe\n+\nbutton as shown below.\nSelect the\n## Space\nyou want to move to the\n## Inventory\nfrom the list\nof available\n## Spaces\n.\n## Save\nthe Settings.\nRefresh the browser.\nUpdated on April 26, 2023\nCreate new ELN Spaces\n\nCreate a new Lab Notebook Space from the ELN UI\n\nFrom openBIS version 20.10.4 it is possible to create\n## Spaces\ndirectly\nfrom the ELN interface. To create a new Space under the Inventory:\n## Select\n## Lab Notebook\nin the main menu\nClick on\n## +New\n## Space\nin the Lab Notebook page\n3. Enter the\n## Code\nfor the\n## Space\n. Please note that codes only\naccept alphanumeric characters, –, . , _.\n4.\n## Save\nMulti-group instances\n\nIn a multi-group instance, the\nInstance admin\ncan choose where to\ncreate a new\n## Space\n## :\nno group\n. The new\n## Space\nwill have no prefix and the Settings\ndefined in General Settings will apply (see\nGeneral ELN\n## Settings\n).\nin one of the existing groups\n. The new\n## Space\nwill have the\ngroup prefix and the Settings of that group will apply (see\n## Group\n## ELN\n## Settings\n).\nUse cases where this could be useful:\nin a multi-group instance with user folders in the Lab Notebook it\nis desired to have in addition some\n## Spaces\nthat are not linked to\na particular user, but maybe rather to some projects.\nin a multi-group instance it is not at all desired to have the lab\nnotebooks organised by users, but rather by projects. A\nsystem\nadmin\ncan configure the user management config file not to create\nusers folders in the lab notebook section (see\nMulti group instances\n).\nThe rights for\n## Spaces\nnot belonging to any group need to be manually\nassigned by an\nInstance admin\n.\nCreate a new Lab Notebook Space from the core UI\n\nIn the core UI:\n## Select\n## Admin -> Spaces\n## Click\n## Add Space\nat the bottom of the page\nEnter the Space\n## Code\n, e.g.\n## EQUIPMENT\n## Save\nBy default all\n## Spaces\ncreated in the core UI are shown under the Lab\nNotebook part of the ELN UI.\nUpdated on April 26, 2023\n## Delete Spaces\n\n## Spaces\ncan be deleted by\nInstance admins\nor by\nSpace admins\n.\nTo delete a\n## Space\n## :\nClick on the\n## Space\nin the main menu\n## Select\n## Delete\nfrom the\n## More..\ndropdown\n3. Provide a\nreason\nfor deletion\n4.\n## Accept\n## Notes:\n## Spaces\nare not moved to the trashcan, but they are permanently deleted\nstraight away.\n## Spaces\ncan only be deleted when they are empty and no entries\npreviously belonging to the\n## Space\nare in the trashcan.\nUpdated on April 26, 2023\nMove Spaces between Lab Notebook and Inventory\n\nIf a\n## Space\nbelongs to the Inventory, this information is stored in the\nELN Settings,\nunder the section\n## Inventory Spaces.\nTo move a\n## Space\nfrom the Lab Notebook to the Inventory, click on the\n+\nbutton on top of the\n## Inventory Spaces\nsection, select the\n## Space\nyou want to move and\n## Save\nthe Settings.\nTo move a\n## Space\nfrom the Inventory to the Lab Notebook, click on the\n–\nbutton next to the\n## Space\nyou want to remove in the\n## Inventory\n## Spaces\nsection and\n## Save\nthe Settings.\nELN Settings can be edited by Instance admins, group admins in\nmulti-group instances and by anyone with admin rights to the\n## ELN_SETTINGS\n## Space\n.\nUpdated on August 2, 2022", "timestamp": "2025-09-18T09:38:30.041951Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_admins-documentation_user-registration:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/admins-documentation/user-registration.html", "repo": "openbis", "title": "User Registration", "section": "User Registration", "text": "## User Registration\n\nRegister users in ELN Interface\n\nUsers can only be registered by someone with\nInstance admin\n## role:\nGo to the\n## User Manager\n, under\n## Utilities\n.\nClick the  +\n## New\n## User\nbutton.\nSelect the\n## Authentication Service\n## :\na.\n## Default Authentication Service\n. This can be LDAP or SSO.\nb.\n## File Authentication Service\n. In this case a username and password need to be created.\nUser ID\n. for LDAP authentication, this is the LDAP username; for SSO authentication this is the email address of the user.\nFor file-based authentication provide username and password. The password can later be changed by the user.\nDefault roles assigned in ELN\n\nWhen a user is registered via the ELN interface, a\n## Space\n(folder) with\nthe name of the user is automatically created under the Lab Notebook\nmain menu. The user is also assigned some default roles:\nSpace admin\nof the\n## Space\ncreated for him/her under the notebook.\nSpace user of\nthe Inventory\n## Spaces\n(MATERIALS, METHODS by\ndefault), the STOCK_CATALOG and the STORAGE\n## Spaces\n.\n## Space Observer\nof the STOCK_ORDERS, ELN_SETTINGS and\n## PUBLICATIONS S\npaces\n.\nModification to default rights can be granted by an\nInstance admin\nfrom the\nadmin UI\n, as explained below.\n## Overview of roles:\nopenBIS Roles\nRegister users from the admin UI\n\nWhen users are registered via the admin UI no default roles are\nassigned.\nTo register new users from the admin UI:\ngo to the\n## Users\ntab. The\n## Users\nand\n## Groups\nwill show in\nthe main menu on left had side.\nClick on\n## Users\nin the menu: the\n## Add\nbutton at the bottom of\nthe menu will become active (blue)\nClick the\n## Add\nbutton\nEnter the U\nser Id\n. This is the LDAP username, when LDAP\nauthentication is used, or the email address if SSO is used. Please\nnote that file-based authentication (where username and password can\nbe created) is not supported by the admin UI.\nHome space\n: this sets the default folder a user sees marked as\n## My Space\nin the Lab Notebook.\nClick the\n## Add Role\nbutton at the bottom of the page to assign a\nrole to the user.\nClick the\n## Add Group\nbutton at the bottom of the page to assign a\nuser to a group of users.\nTo assign a role to a user, first the\n## Level\nneeds to be selected\n## (Instance, Space, Project) .\nIf level is Instance, you can directly select a role (Admin, Observer). If the level is Space or Project, you first need to select the Space or Project and then assign a\n## Role\n.\nMultiple roles can be assigned to a user.\nRoles can be removed from the\n## Remove\nbutton at the bottom of\nthe page.\nAfter making the necessary changes, press the\n## Save\nbutton.\nNote: for using the ELN interface, it is necessary to assign every user\nor user group the OBSERVER role to the space ELN_SETTINGS.\nDeactivate users\n\nUsers can be deactivated in the admin UI:\nSelect the user to deactivate in the left menu of the\n## USERS\ntab\nClick the\n## EDIT\nbutton on the right bottom corner\nUnselect the\n## Active\ncheckbox\nRemove users\n\nUsers can be removed from openBIS only if they have not registered anything in the system. If they have, they can only be deactivated, not removed.\nUsers can be removed in the admin UI, by selecting the user in the left menu of the\n## USERS\ntab and clicking the\n## REMOVE\nbutton at the bottom of the menu, as shown below.\nCreate users groups in admin UI\n\nIt is possible to create groups of users and assign rights to a group:\ngo to the\n## Users\ntab. The\n## Users\nand\n## Groups\nwill show in\nthe main menu on left had side.\nClick on\n## Groups\nin the menu: the\n## Add\nbutton at the bottom of\nthe menu will become active (blue)\nClick the\n## Add\nbutton\nEnter a\n## Code\nfor the group. This is the equivalent of a name,\nbut Codes can only contain numbers, letters and the following\nsymbols: . – _\nYou can now assign registered users to the group and assign Roles as\nexplained above.\nopenBIS roles\n\n## Observer\n\nThis role can be assigned to the whole openBIS instance (\n## Instance\n## Observer\n) or to specific\n## Spaces\nor\n## Projects\n(\n## Space\nor\n## Project\n## Observer\n). Users with this role have read-only access to the whole\nopenBIS (\n## Instance Observer\n), or to a specified\n## Space\nor\n## Project\n(\n## Space\nor\n## Project Observer\n).\nAn Observer can see and search everything in an openBIS instance or the\n## Space/Project\nwhich they have access to. They can also download\ndatasets. They cannot modify nor delete anything.\n## Space/Project User\n\nExtends Observer permissions with some creating and editing\nfunctionality. Permissions are limited to specified\nSpace(s)\nor\nProject(s)\n.\nCan do everything that Observer and additionally:\ncreate\nobjects\ncollections\nedit\nobjects\ncollections\nprojects\n## Space/Project Power User\n\n## Extends\n## Space/Projec\nt User permissions with some deleting, editing and\nprocessing functionality. Permissions are limited to specified\nSpace(s)\nor\nProject(s)\n.\nCan do everything that\n## Space/Project\nUser and additionally:\ncreate projects\ndelete\nprojects\ndata sets\nobjects\ncollections\nPlease note that this role cannot be assigned via the ELN UI, only via\nadmin UI.\n## Space/Project Admin\n\nExtends Space/Project Power User permissions allowing to manage roles\nand projects inside given\nSpace(s)\nor\nProject(s)\n.\nCan do everything that Space/Project Power User and additionally:\nassign and remove Space/Project roles\n## Instance Admin\n\nHas the full access to given openBIS instance.\nCan do everything that Space/Project Admin and additionally:\ncreate\nspace\nmaterial\nperson\nproperty type\nvocabulary\nmaterial type\nobject type\ncollection type\ndata set type\ncreate/delete instance admin role\nedit\nmaterial\nproperty type\nproperty type assignment\nvocabulary\nmaterial type\nobject type\ncollection type\ndata set type\nassign/unassign property type\ndelete\nspace\nvocabulary terms\nmaterial type\nsample type\nexperiment type\ndata set type\nPlease note that this role cannot be assigned via the ELN UI, only via\nadmin UI.\nUpdated on April 26, 2023\n## User Profile\n\nIn the User Profile, a user who is logged in into openBIS can find the\n## following information:\n## First Name\n## Last Name\n## Email\nopenBIS session token\nZenodo API Token\n(\nExport to\n## Zenodo\n)\nFirst name, last name and email are automatically filled in when LDAP or\nSSO are used for authentication.\nIn case of file-based authentication, this information can be entered\nhere directly from the user.\nFor file-based authentication, users can also change their password\nhere, from the\n## Change Password\noption under the\n## More..\ndropdown.\nUpdated on June 28, 2022\nAssign home space to a user\n\nWhen a home space is assigned to a user, this becomes marked as\n## My\n## Space\nfor that user in the lab notebook, as shown below.\nWhen users are registered via the ELN UI, a\n## Space\nwith their username\nis created (see\n## User\n## Registration)\nand this is automatically set as home space for the user.\nThe same happens in multi-group instances where spaces are created for\neach user in the lab notebook section.\n## An\ninstance admin\ncan change the home space of a user or assign one to\na user that does not have a home space assigned from the admin UI, as\nshown below.\nPlease note that when a user is inactivated, the home space assigned to\nthat user is moved in the ELN UI to the folder\nOthers (disabled).\n## If\nthis is not desired, the space should be removed as home space from the\ninactivated user. This can be done by an\ninstance admin\n.\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.046608Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_custom-database-queries:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/custom-database-queries.html", "repo": "openbis", "title": "Custom Database Queries", "section": "Custom Database Queries", "text": "## Custom Database Queries\n\n## Introduction\n\nopenBIS application server can be configured to query any relational\ndatabase server via SQL. There are three ways to use this feature in\n## openBIS Web application:\nRunning arbitrary SELECT statements.\nDefining parametrized queries.\nRunning parametrized queries.\nThe three features correspond to three menu items of the menu\n## Queries\n.\nThe last feature can be used by any user having OBSERVER role whereas\nfor the first two features user needs a\nquery creator\nrole which\nusually is at least POWER_USER role and is\nconfigured\nby administrator of the openBIS server. The idea is that power users\nhaving the knowledge to write SQL queries define a query which can be\nused by everybody without knowing much about SQL.\nMultiple query databases may be configured for any openBIS Web\napplication. Database labels specified in the configuration file will be\nshown in a combo box for database selection while defining new / editing\nexisting queries.\nNote that only the first 100000 rows of the result set of a query are\nshown. This restriction should prevent from running ill-designed queries\nwhich consume all the memory of the server. There is also a time out of\n5 minutes defined after which the query is canceled if it didn’t return\nany result.\nHow it works\n\n## Database:\nis configured as a core-plugin of type “query-databases”\ncan be assigned to a space:\nspace == null : should be used for databases that contain data\nfrom multiple spaces or data which is space unrelated\nspace != null : should be used for databases that contain data\nfrom one specific space only\ncan be assigned a minimal query creator role:\ndatabase with space == null : by default the minimal query\ncreator role is INSTANCE_OBSERVER\ndatabase with space != null : by default the minimal query\ncreator role is POWER_USER\n## Query:\ncan be created/updated/deleted only by a user with a database\nminimal query creator role or stronger (if database space != null\nthen the user role has to be defined for that space or the user has\nto be an instance admin)\ncan be seen by:\nprivate query : a user who created it or an instance admin\npublic query : any user\ncan be executed by:\ndatabase with space == null : by users with at least\nPROJECT_OBSERVER role (results are filtered by a\nexperiment_key/sample_key/data_set_key column values which\nare expected to contain entity perm_id; WARNING: if no such\ncolumn is returned by a query then ALL results are returned)\ndatabase with space != null : by users with at least\nSPACE_OBSERVER role in that space (all results are returned\nwithout any filtering as they all belong to the space a user has\naccess to)\ncan be updated/executed/deleted only by a user who can see the query\ncan contain additional parameters (e.g. ${my_parameter}); values of\nsuch parameters can be set in the UI by a user right before an\nexecution of a query\ncan be GENERIC (accessible only from the “Queries” top menu) or\nEXPERIMENT/SAMPLE/DATA_SET/MATERIAL specific (accessible from the\n“Queries” top menu and from Experiment/Sample/DataSet/Material view\nrespectively)\nentity specific queries should contain ‘${key}’ parameter which will\nbe replaced by a permId of the displayed experiment/sample or by a\ncode of the displayed dataset/material before the query execution\n(MATERIAL queries also have ‘${type}’ parameter which is replaced\nwith a type code of the material)\nentity specific queries may be configured to appear only in the\nviews of entities of chosen types (e.g. only for samples of types\nthat match a given regexp)\n## Arbitrary SQL:\nrunning an arbitrary SQL is treated as a creation of a query which\nis simply not stored for a future use i.e. only a user with a\nminimal query creator role or stronger can do it (if database space\n!= null then the user role has to be defined for that space or the\nuser has to be an instance admin)\n## Setup\n\nTo use the custom database queries, it is necessary to define query databases. See\nInstallation and Administrator Guide of the openBIS Server\nfor an explanation on how to do this.\nRunning a Parametrized Query\n\nChoose menu item\n## Queries -> Run Predefined Query\n. The tab\n## Predefined Query\nopens.\nChoose a query using the query combo box. Queries specified for all\nconfigured databases are selected transparently using the same combo\nbox which displays only query names.\nIf the query has no parameters it will be executed immediately and\nthe result is shown in tabular form. Otherwise text fields for each\nparameter appear right of the query combo box.\nEnter some values into the parameter fields and click on the\n## Execute\nbutton. The query result will be shown as a table.\nFeatures of a query result:\nThe result can be browsed, exported, sorted, and filtered as most\ntables in openBIS.\nValues referring to permIDs of an experiment, sample, or data set\nmight be shown as hyperlinks. A click on such a link opens a new tab\nwith details.\nRunning a SELECT statement\n\nThis feature is only for users with\ncreator role\n. It is useful for\nexploring the database by ad hoc queries.\nChoose menu item\nQueries -> Run Custom SQL Query\n. The tab\nCustom SQL Query\nopens.\nEnter a SELECT statement in the text area, select database and click\non the\n## Execute\nbutton. The result appears below in tabular form.\nDefining and Editing Parametrized Queries\n\nThis feature is only for users with\ncreator role\n.\nDefine a Query\n\nChoose menu item\n## Queries -> Browse Query Definitions\n. The tab\n## Query Definitions\nopens. It shows all definitions where the user\nhas access rights.\nClick on\n## Add Query Definition\nfor defining a new parametrized\nquery. A large dialog pops up.\nEnter a name, database, an optional description, and a SELECT\nstatement.\nClick on button\n## Test Query Definition\nto execute the query. The\nresult will be shown in the same dialog.\nClick on button\n## Save\nto save the definition. The dialog\ndisappears and the new definition appears in the table of query\ndefinitions.\nPublic flag\n\nA query definition can be public or private depending on whether the\ncheck box\npublic\nis checked or not. A private query is visible only\nby its creator. Public queries are visible by everybody. The idea is\nthat a power user first creates query definitions for their own\npurposes. If he or she find it useful for other users they will set the\npublic flag.\n## Specifying Parameters\n\nA SQL query can have parameters which are defined later by the user\nrunning the query. A parameter is of the form\n${<parameter\nname>\n}.\n## Example:\nselect * from my_table where code = ${my table code}\nThe parameter name will appear in the text field when running the query.\nOptionally, you can provide key-value pairs which are “metadata” for the\nparameter name and separated by ‘::’ from the name. These metadata keys\n## are defined:\nMetadata key\n## Explanation\n## Example\ntype\nSets the data type of this parameter. Valid values are VARCHAR (or STRING), CHAR, INTEGER, BIGINT, FLOAT, DOUBLE, BOOLEAN, TIME, DATE or TIMESTAMP.\n${code::type=VARCHAR}\nlist\nComa-separated list of allowed values for the parameter.\n${color::list=red,green,blue}\nquery\nA SQL query which is run to determine the allowed values for the parameter. The query is expected to return exactly one column. You should specify only fast queries here with a reasonably small number of returned rows as the UI will block until this query has returned.\n${name::query=select last_name from users}\nIt is possible to combine multiple keys like\n## this:\n${estimate::type=integer::list=1,3,7,12\n}.\n## Warning\nWhy to provide a data type\nProviding a data type with\ntype=...\nis not mandatory. In a future version of the software we may add additional client-side validation based on this value, but in the current version we don’t do that yet. If you do\nnot\nprovide a data type, openBIS will ask the database for the type of the particular query parameter. This works fine for most databases, but not for all. Oracle is a well-known example that cannot provide this information. So if your query source is an Oracle database and you do not provide a data type, you will get an error saying\n## \"Unsupported\nfeature\n”. To fix this, you have to rovide the data type.\nArray Literals for PostgreSQL data sources\n\nFor PostgreSQL, there exist neat array functions\n## ANY\nand\n## ALL\n(see\nPostgreSQL\ndocumentation\n).\nIn particularly\n## ANY\ncomes in handy in\n## WHERE\nclauses to check whether\na column has one of several values. The official form for providing an\narray literal as a string (which is what you have to do here) is a bit\nclumsy, as you have to write for the query\n\"select\n*\nfrom\ndata\nwhere\ncode\n=\nANY(${codes}::text[])\n” and then the\nuser running the query has to put the parameter value in curly braces\nlike “\n{code1,code2,code3,...}\n”.\nThe custom query engine has a simplification for this construct. You can\n## just write:\n\"select\n*\nfrom\ndata\nwhere\ncode\n=\nANY({${codes}})\n” for the\nquery and then the user running the query will be able to skip the curly\nbracket and write for the parameter value: “\ncode1,code2,code3,...\n## ”. A\nuser who doesn’t know that this is an array will in particular get away\nwith just providing a single value like “\ncode1\n”.\nNote that the most obvious way of specifying a set relationship with\n\"select\n*\nfrom\ndata\nwhere\ncode\nin\n(${codes})\n” does\nnot\nwork as\ncustom queries are not using simple text concatenation but prepared\nqueries to avoid a security problem known as “SQL Injection”.\n## Hyperlinks\n\nIn order to create hyperlinks in the result table the column names in\nthe SQL statement should be one of the following\nmagic\n## words:\nexperiment_key\nsample_key\ndata_set_key\nThey should denote a perm ID of specified type.\n## Example:\nselect\nid\n,\nperm_id\nas\ndata_set_key\nfrom\ndata_sets\n## Warning\nBe careful with this feature\n: The table is shown with the hyperlinks even if the value isn’t a perm ID of specified type.\nEdit a Query\n\nChoose menu item\n## Queries -> Browse Query Definitions\n. The tab\n## Query Definitions\nopens.\nSelect a query and click on button\n## Edit\n. The same dialog as for\ndefining a query pops up.\nEntity Queries (Experiment, Sample, Material, Data Set)\n\nBy default, all custom queries are\n## Generic\n, which means that the user\nwill be able to execute them from the standard Queries menu.\nAdditionally it is possible to create a query containing a special\n‘magic’ parameter, which will be automatically replaced by the entity\nidentifier (perm id in case of experiments and samples, code for data\nsets and a pair (code, type) in case of materials). Those entity\nspecific queries will be visible only in entity details views (e.g.\nexperiment details) in a special\nsection\ncalled\n## Queries\n. One can\nalso limit visibility of a query to a specific entity types (e.g.\nexperiment of type\n## EXP\n).\nHow to create/edit entity custom queries\n\nEntity custom queries can be created and edited in the same way as\n## Generic\nqueries (\n## Queries -> Browse Query Definitions\n), but the\nvalue of\n## Query\n## Type\nfield should be set to Experiment, Sample,\nData Set or Material.\n## Entity\n## Type\n(e.g. Experiment Type) should be changed if one wants\nto limit the visibility of a query to a specific type (default option -\n(all)\n, doesn’t introduce such a restriction). The field accepts not\nonly values selected from the list but also typed text containing a\nregular expression (e.g. Experiment Type\n## 'EXP.*'\nwould mean that the\nquery should be visible in views of experiments of type with code\nstarting with\n## 'EXP'\nprefix).\nFurthermore the sql should contain the ‘magic’ parameter\n‘${key}’\n(will be replaced by perm id (experiment, sample) or code (data set,\nmaterial)). In case of material custom query, additional ‘magic’\n## parameter is required:\n‘${type}’\n(will be replaced by material type\ncode).\n## Examples\n\n## Warning\n## Legacy Syntax:\nOlder versions of openBIS required to put string parameters in ticks, like ‘${param}’. Current versions of openBIS don’t need this anymore, so you can use ${param} without the ticks. However, the syntax with ticks is still accept for backward compatibility.", "timestamp": "2025-09-18T09:38:30.051734Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/index.html", "repo": "openbis", "title": "General Admin Users", "section": "General Admin Users", "text": "## General Admin Users\n\n## Admins Documentation\n## Login\nFile based and/or LDAP authentication\nSWITCHaai authentication\nInventory overview\nCustomise Inventory Of Materials And Samples\nCreate Collections of Materials\nCreate the Project folder\nCreate the Collection folder\nAdd the “+Object type” button in the Collection percentage\n## Delete Collections\nEnable Storage Widget on Sample Forms\n## Configure Lab Storage\nAdd metadata to Storage Positions\n## Customise Inventory Of Protocols\nCreate Collections of Protocols\nEnable Protocols in Settings\nMove Collections to a different Project\nCustomise Parents and Children Sections in Object Forms\nCustomise the Main Menu\n## Main Menu Sections\nLab Notebook menu\nAssociate File Types to Dataset Types\n## User Registration\nRegister users in ELN Interface\nDefault roles assigned in ELN\nRegister users from the admin UI\nDeactivate users\nRemove users\nCreate users groups in admin UI\nopenBIS roles\n## Observer\n## Space/Project User\n## Space/Project Power User\n## Space/Project Admin\n## Instance Admin\n## User Profile\nAssign home space to a user\n## New Entity Type Registration\nRegister a new Object Type\nRegistration of Properties\nProperty Data Types\nConsiderations on properties registration\n## Controlled Vocabularies\nRegister a new Experiment/Collection type\nRegister a new Dataset type\nEnable Rich Text Editor or Spreadsheet Widgets\nEnable Objects in dropdowns\nRegister masterdata via Excel\nModifying existing types\nProperties overview\nInternal properties and vocabularies\nMasterdata exports and imports\nMasterdata export\nMasterdata import\nMasterdata version\nImports of openBIS exports\nMetadata import\nDatasets import\nCreate Templates for Objects\nEnable Transfer to Data Repositories\nEnable Barcodes and QR codes\nEnable archiving to Long Term Storage\n## History Overview\nHistory of deletions\nHistory of freezing\n## Space Management\nCreate new Inventory Spaces\nCreate a new Inventory Space from the ELN UI\nCreate a new Inventory Space from the core UI\nCreate new ELN Spaces\nCreate a new Lab Notebook Space from the ELN UI\nCreate a new Lab Notebook Space from the core UI\n## Delete Spaces\nMove Spaces between Lab Notebook and Inventory\n## Multi Group Set Up\nGeneral ELN Settings\n## Instance Settings\n## Group Settings\nGroup ELN Settings\nDatabase navigation in admin UI\n## Features\n## Filter\n## Navigation\n## Sorting\n## Properties Handled By Scripts\n## Introduction\nTypes of Scripts\nDefining properties\n## Dynamic Properties\n## Introduction\nDefining dynamic properties\nCreating scripts\n## Simple Examples\n## Advanced Examples\nData Types\nCreating and Deploying Java Plugins\nDynamic properties evaluator\nEntity validation scripts\n## Introduction\nDefining a Jython validation script\nScript specification\nTriggering Validation of other Entities\nScript example\nActivating the validation\nCreating and Deploying Java Validation Plugins\nWhen are validations performed\nGood practices\n## Managed Properties\n## Introduction\n## Defining Managed Properties\nCreating scripts\n## Predefined Functions\nJava API\nExamples of user defined functions\nStoring structured content in managed properties\nUnofficial API\n‘Real World’ example\nCreating and Deploying Java Plugins\n## Custom Database Queries\n## Introduction\nHow it works\n## Setup\nRunning a Parametrized Query\nRunning a SELECT statement\nDefining and Editing Parametrized Queries\nDefine a Query\nPublic flag\n## Specifying Parameters\nArray Literals for PostgreSQL data sources\n## Hyperlinks\nEdit a Query\nEntity Queries (Experiment, Sample, Material, Data Set)\nHow to create/edit entity custom queries\n## Examples", "timestamp": "2025-09-18T09:38:30.055303Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-admin-users_properties-handled-by-scripts:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-admin-users/properties-handled-by-scripts.html", "repo": "openbis", "title": "Properties Handled By Scripts", "section": "Properties Handled By Scripts", "text": "## Properties Handled By Scripts\n\n## Introduction\n\nOne of the reasons why openBIS is easily extensible and adjustible is\nthe concept of generic entities like samples, experiments, materials and\ndata sets. By adding domain specific properties to the mentioned\nobjects, instance administrator creates a data model specific to given\nfield of study. Values of configured properties will be defined by the\nuser upon creation or update of the entities (samples etc.).\nIn most cases values of properties must be provided directly by the\nuser. The default way of handling a property in openBIS can be changed\nby instance admin defining a property that should be handled by a script\nwritten in\n## Jython\nor Predeployed Plugin written\nin Java. Jython plugins use Jython version configured by the\nservice.properties property\njython-version\nwhich can be either 2.5 or\n2.7.\nTypes of Scripts\n\nThere are two types of plugins that can be used for handling properties,\nand one script type to perform validations on entities:\n## Dynamic Property Evaluator\n(for properties referred to as\n## Dynamic Properties\n)\nfor properties that\ncan’t be modified by users\n,\nvalues of such properties will be\nevaluated automatically\nusing metadata already stored in openBIS (e.g. values of other\nproperties of the same entity or connected entities),\nthe script defines an expression or a function that returns a\nvalue for a\n## Dynamic Property\nspecified in the script\n## Managed Property Handler\n(for properties referred to as\n## Managed\n## Properties\n)\nfor properties that will be\nindirectly modified by users\n,\nthe script alters default handling of a property by openBIS by\ndefining functions that specify e.g.:\nhow the property should be\ndisplayed\nin entity detail\nview (e.g. as a table),\ninput fields\nfor modifying the property,\ntranslation\nand/or\nvalidation\nof user input.\n## Entity Validation\nperformed after each update or creation of a given entity type.\nthe user provided script performs a validation, which can cancel\nthe operation if the validation fails\nDefining properties\n\nTo create a property that should be handled by a script perform the\nfollowing steps.\nDefine a property type with appropriate name and data type\n## (Administration->Property Types->New).\nDefine a script that will handle the property\n(Administration->Scripts) or deploy a Java plugin. For details\nand examples of usage go to pages:\n## Dynamic Properties\n## Managed Properties\nEntity validation scripts\nAssign the created property type to chosen entity type using the\ncreated script (e.g. for samples: Administration->Property\nTypes->Assign to Sample Type):\nselect Handled By Script checkbox,\nselect the appropriate Script Type\nchoose the Script\nThe validation scripts are assigned to the type in the “Edit Type”\nsection. (e.g Admin->Types->Samples. Select sample and click\nedit.)\nNo labels\n## Dynamic Properties\n\n## Introduction\n\n## Dynamic Properties\nare one of two types of properties that use Jython\nscripts for providing special functionality to OpenBIS. To understand\nthe basic concept read about\n## Properties Handled By Scripts\n.\nDefining dynamic properties\n\nTo create a dynamic property:\nDefine a property type with appropriate name and data type\n(\n## Admin->Plugins→Add\n## Plugin\n)\n## Choose\n## Dynamic\n## Property\n## Evaluator\nfrom Plugin type dropdown list\nin the upper left corner.\nYou may evaluate script on chosen entity in Script Tester section.\nCreating scripts\n\nTo edit existing dynamic property script, edit dynamic\nproperty (\nAdmin→Plugins→(click\non\ndynamic\nproperty)→Edit\nplugin\n)\nThe scripts should be written using standard Jython syntax. Unlike\ncustom columns and filters (which also require Jython syntax), dynamic\nproperties can have more than one line of code. If the Script contains\nonly one line, it will be evaluated and used as the value of appropriate\ndynamic property. If on the other hand a multi line script is needed,\nthe function named\n\"calculate\"\nwill be expected and the the result\nwill be used as property value.\nTo access the entity object from the script, use the following syntax:\nentity.<requested\nmethod>\nCurrently available methods that can be called on all kinds of entities\n## include:\ncode()\nproperty(propertyTypeCode)\npropertyValue(propertyTypeCode)\npropertyRendered(propertyTypeCode)\nproperties()\nFor more details see\nIEntityAdaptor\n(interface implemented by\nentity\n) and\nIEntityPropertyAdaptor\n(interface implemented by each property).\nIt is also possible to acces the complete Java Object by calling\n\"entityPE()\"\nmethod, but this is appropach is not recomended, as the\nreturned value is not a part of a well defined API and may change at any\npoint. You may use it as a workaround, in case some data are not\naccessible via well defined API, but you should contact OpenBIS helpdesk\nand comunicate your needs, so the appropriate methods can be added to\nthe official API.\nYou can test your script on selected entities\n(samples/experiments/materials/data sets) using the testing environment\npart of\n## Script\n## Editor\n.\n## Simple Examples\n\nShow a value of a Sample property which is named ‘Multiplicity’\nentity\n.\npropertyValue\n(\n## 'Multiplicity'\n)\nTakes an existing property and multiplies the value by 1.5\nfloat\n(\nentity\n.\npropertyValue\n(\n## 'CONCENTRATION_ORIGINAL_ILLUMINA'\n))\n*\n1.5\n## Advanced Examples\n\n## Show all entity properties as one dynamic property:\ndef\nget_properties\n(\ne\n## ):\n\"\"\"Automatically creates entity description\"\"\"\nproperties\n=\ne\n.\nproperties\n()\nif\nproperties\nis\n## None\n## :\nreturn\n\"No properties defined\"\nelse\n## :\nresult\n=\n\"\"\nfor\np\nin\nproperties\n## :\nresult\n=\nresult\n+\n\"\n\\n\n\"\n+\np\n.\npropertyTypeCode\n()\n+\n\": \"\n+\np\n.\nrenderedValue\n()\nreturn\nresult\ndef\ncalculate\n## ():\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nreturn\nget_properties\n(\nentity\n)\nCalculate a new float value based some other values\nimport\njava.lang.String\nas\n## String\ndef\ncalculateValue\n## ():\nnM\n=\n0\nuLDNA\n=\n0\nif\nentity\n.\npropertyValue\n(\n## 'CONCENTRATION_PREPARED_ILLUMINA'\n)\n!=\n''\nand\n\\\nentity\n.\npropertyValue\n(\n## 'FRAGMENT_SIZE_PREPARED_ILLUMINA'\n)\n!=\n''\n## :\nnM\n=\nfloat\n(\nentity\n.\npropertyValue\n(\n## 'CONCENTRATION_PREPARED_ILLUMINA'\n))\n/\n\\\nfloat\n(\nentity\n.\npropertyValue\n(\n## 'FRAGMENT_SIZE_PREPARED_ILLUMINA'\n))\n*\n\\\n1000000\n/\n650\nif\nfloat\n(\nentity\n.\npropertyValue\n(\n## 'UL_STOCK'\n))\n!=\n''\n## :\nuLDNA\n=\nfloat\n(\nentity\n.\npropertyValue\n(\n## 'UL_STOCK'\n))\n*\n2\n/\nnM\nuLEB\n=\nfloat\n(\nentity\n.\npropertyValue\n(\n## 'UL_STOCK'\n))\n-\nuLDNA\nreturn\n## String\n.\nformat\n(\n\"\n%16.1f\n\"\n,\nuLEB\n)\nreturn\n0\ndef\ncalculate\n## ():\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nreturn\ncalculateValue\n()\nCalculate a time difference between two time stamps:\nfrom\ndatetime\nimport\ndatetime\ndef\ndateTimeSplitter\n(\nopenbisDate\n## ):\ndateAndTime\n,\ntz\n=\nopenbisDate\n.\nrsplit\n(\n\" \"\n,\n1\n)\npythonDateTime\n=\ndatetime\n.\nstrptime\n(\ndateAndTime\n,\n\"%Y-%m-\n%d\n## %H:%M:%S\"\n)\nreturn\npythonDateTime\ndef\ncalculate\n## ():\ntry\n## :\nstart\n=\nentity\n.\npropertyValue\n(\n## 'FLOW_CELL_SEQUENCED_ON'\n)\nend\n=\nentity\n.\npropertyValue\n(\n## 'SEQUENCER_FINISHED'\n)\ns\n=\ndateTimeSplitter\n(\nstart\n)\ne\n=\ndateTimeSplitter\n(\nend\n)\ndiffTime\n=\ne\n-\ns\nreturn\nstr\n(\ndiffTime\n)\nexcept\n## :\nreturn\n## \"N/A\"\nIllumina NGS Low Plexity Pooling Checker: checks if the complexity of a pooled sample is good enough for a successful run:\ndef\ncheckBarcodes\n## ():\n'''\n'parents' are a HashSet of SamplePropertyPE\n'''\n## VOCABULARY_INDEX1\n=\n## 'BARCODE'\n## VOCABULARY_INDEX2\n=\n## 'INDEX2'\n## RED\n=\nset\n([\n## 'A'\n,\n## 'C'\n])\n## GREEN\n=\nset\n([\n## 'T'\n,\n## 'G'\n])\n## SUCCESS_MESSAGE\n=\n## \"OK\"\n## NO_INDEX\n=\n## \"No Index\"\nlistofIndices\n=\n[]\nboolList\n=\n[]\npositionList\n=\n[]\nreturnString\n=\n\" \"\nfor\ne\nin\nentity\n.\nentityPE\n()\n.\nparents\n## :\nfor\ns\nin\ne\n.\nproperties\n## :\nif\ns\n.\nentityTypePropertyType\n.\npropertyType\n.\nsimpleCode\n==\n## VOCABULARY_INDEX1\n## :\nindex\n=\ns\n.\ngetVocabularyTerm\n()\n.\ncode\nif\nlen\n(\nlistofIndices\n)\n>\n0\n## :\nfor\nn\nin\nrange\n(\n0\n,\nlen\n(\nindex\n)\n-\n1\n## ):\nlistofIndices\n[\nn\n]\n.\nappend\n(\nindex\n[\nn\n])\nelse\n## :\nfor\nn\nin\nrange\n(\n0\n,\nlen\n(\nindex\n)\n-\n1\n## ):\nlistofIndices\n.\nappend\n([\nindex\n[\nn\n]])\n# remove any duplicates\nsetofIndices\n=\n[\nset\n(\nlist\n)\nfor\nlist\nin\nlistofIndices\n]\n# Test whether every element in the set 's' is in the RED set\nboolList\n=\n[\nsetofNuc\n.\nissubset\n(\n## RED\n)\nfor\nsetofNuc\nin\nsetofIndices\n]\nif\nboolList\n## :\nfor\nb\nin\nboolList\n## :\nif\nb\n## :\npositionList\n.\nappend\n(\nboolList\n.\nindex\n(\nb\n)\n+\n1\n)\n# set the value to False, because 'index' returns only the first occurrence\nboolList\n[\nboolList\n.\nindex\n(\nb\n)]\n=\n## False\nelse\n## :\nreturn\n## NO_INDEX\n#  if s.entityTypePropertyType.propertyType.simpleCode == VOCABULARY_INDEX2:\n#   pass\nif\npositionList\n## :\nfor\npos\nin\npositionList\n## :\nreturnString\n+=\n\"WARNING! Base position \"\n+\nstr\n(\npos\n)\n+\n\" of \"\n+\n\\\n## VOCABULARY_INDEX1\n+\n\\\n\" does not contain both color channels\"\n+\n\\\n\"\n\\n\n\"\nelse\n## :\nreturnString\n=\n## SUCCESS_MESSAGE\nreturn\nreturnString\ndef\ncalculate\n## ():\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nreturn\ncheckBarcodes\n()\nData Types\n\nAny data type that can be used by openBIS properties is supported by\ndynamic properties. The script always returns just a string\nrepresentation of a property value. The value is then validated and in\nspecial cases converted before being saved. The string formats and\nvalidation rules are the same as in batch import/update of\nsamples/experiments/data sets/materials.\nThe non-trivial cases are properties with data type:\nCONTROLLED VOCABULARY - use code of vocabulary term as string\nrepresentation,\nMATERIAL - use function\nmaterial(code,\ntypeCode)\nto create the\nstring representation.\nCreating and Deploying Java Plugins\n\nTo create valid Java plugin for Dynamic Properties, one should create a class that is implementing\nch.systemsx.cisd.openbis.generic.server.dataaccess.dynamic_property.calculator.api.IDynamicPropertyCalculatorHotDeployPlugin\ninterface. The class should be annotated with\nch.ethz.cisd.hotdeploy.PluginInfo\nannotation specifying the name of the plugin, and\nch.systemsx.cisd.openbis.generic.server.dataaccess.dynamic_property.calculator.api.IDynamicPropertyCalculatorHotDeployPlugin\nclass as a plugin type.\nSuch a plugin should be exported to a jar file and put\ninto\n<<openBIS\ninstallation\ndirectory>>/servers/entity-related-plugins/dynamic-properties\ndirectory. The plugin will be detected automatically and will be\nautomatically available to openBIS. No restart is needed.\nDynamic properties evaluator\n\nEvaluation of dynamic properties may be very time consuming, therefore\nit is not done automatically after each metadata update. To make sure\nthat the potential inconsistencies are repaired, the maintenance task\ncan be defined (\nservice.properties\n), that runs in specified intervals:\nmaintenance-plugins\n=\ndynamic-property-evaluator\ndynamic-property-evaluator.class = ch.systemsx.cisd.openbis.generic.server.task.DynamicPropertyEvaluationMaintenanceTask\n# run daily at midnight\ndynamic-property-evaluator.interval = 86400\ndynamic-property-evaluator.start = 00:00\nIf the value of a dynamic property has not yet been calculated, it will\nbe shown as\n(pending\nevaluation)\n.\nNo labels\nEntity validation scripts\n\n## Introduction\n\nEntity validation scripts are a mechanism to ensure metadata\nconsistency. For each entity type a user can define a validation\nprocedure, which will be performed at each creation or update of the\nentity of that type. There are two ways to define an entity validation\nprocedure: Jython scripts and Java plugins.\nDefining a Jython validation script\n\nGo to Admin -> Plugins -> Add Plugin.\nSelect “Entity Validator” as the plugin type\nChoose name, entity kind, and description.\nPrepare a script (see paragraph “Script specification” below)\nScript specification\n\nThe script should at least include the validate function, that takes two\nparameters. The first one is the entity being validated, and the second\nis the boolean stating whether it is a new entity (creation) or an\nexisting one(update).\nthe function name should be ‘validate’. It will be called with two\nparameters.\nthe first argument is the entity. It will be the object\nimplementing\nthe\nIEntityAdaptor\ninterface\nthe second argument is the boolean “isNewEntity”. It will be\ntrue if the entity is new.\nThe script should return None (or nothing) if the validation is\nsuccessful and a string with an error message, if the validation\nfails.\nTriggering Validation of other Entities\n\nA plugin can specify that another entity needs to be validated as well.\nFor example, a change to a sample could require validation of its\nchildren. The infrastructure can be informed of this dependency by\ncalling\nrequestValidation\nwith the entity that needs to be validated\nas an argument.\nScript example\n\nHere is the example script that validates, that the newly created entity\n## does not have any properties defined:\n## Basic Example\n## def validate(entity, isNew):\n## if isNew:\n## if not entity.properties() is None:\nreturn \"It is not allowed to attach properties to new sample.\"\n## Triggering Example\n## def validate(entity, isNew):\nfor s in entity.children():\nrequestValidation(s)\nActivating the validation\n\nTo make the validation active per entity type you have to select the\nvalidation script for each type:\n## Admin -> Types ->\nyou selected also in the\nscript definition ->\nSelect a Sample Type and edit it\nYou find a property which is called ‘Validation Script’ (see screen\nshot below). Just select your defined Script and hit save.\nCreating and Deploying Java Validation Plugins\n\nTo create a valid Java plugin for Entity Validation, one should create a\nclass that is implementing\nthe\nch.systemsx.cisd.openbis.generic.server.dataaccess.entity_validation.IEntityValidatorHotDeployPlugin\ninterface.\nThe class should be annotated with\nch.ethz.cisd.hotdeploy.PluginInfo\nannotation specifying the name of\nthe plugin,\nand\nch.systemsx.cisd.openbis.generic.server.dataaccess.entity_validation.IEntityValidatorHotDeployPlugin\nclass\nas a plugin type.\nAll classes needed to run the plugin have to be exported to a jar file\nand put into\nthe\ndirectory\n<<openBIS\ninstallation\ndirectory>>/servers/entity-related-plugins/entity-validation\n.\nThe plugin will be detected and made available automatically to openBIS.\nNo restart is required for that.\nWhen are validations performed\n\nValidations are performed at the end of the transaction, not at the\nmoment of the change. So if during some longer operation there are\nseveral updates to different entities, all of them are evaluated at the\nend, when all changes are available to the validation script.\nTherefor it is possible to write for instance a dropbox that makes some\nupdates that break validation temporarily, and still succeeds as long as\nthe validations succeed after all the updates have been done.\nGood practices\n\nValidation scripts should be read-only.\nIn theory it is possible to edit the entity during the\nvalidation. This is a bad practice. Consider using\n## Dynamic\n## Properties\nif you\nwant calculations being performed after the entity updates.\nThink about performance\nThe plugins will be executed for every creation or update of\nentities of that type. This can affect the performance\ndrastically if the plugin will be too heavy.\nNo labels\n## Managed Properties\n\n## Introduction\n\n## Managed Properties\nare one of two types of properties that use Jython\nscripts for providing special functionality to openBIS. To understand\nthe basic concept read about\n## Properties Handled By\n## Scripts\n.\nThe feature is especially useful when a complex data structure should be\nstored in a single property. Properties holding XML documents are a good\nexample. The problem with XML documents is that users may find them\ndifficult to work with (read, create or even modify without mistakes).\nThis is where managed properties come in handy. Users don’t have to be\naware of the complex format of data stored in a property. It can be a\ntask for instance administrator to specify how the values should be\npresented to a user and how is he going to modify them using a custom\n## UI.\n## Defining Managed Properties\n\nTo create a Managed Property:\nDefine a property type with appropriate name and data type\n(\n## Administration->Property\n## Types->New\n)\nDefine the formula which should be used to manage your property\n(\n## Administration->Scripts\n).\nAssign created property type to chosen entity (sample etc.) type\n(\n## Administration->Property\n## Types->Assign\nto\n## Sample\n## Type\n). In the\nassignment form perform the following steps specific to managed\n## properties:\nselect\n## Handled\n## By\n## Script\ncheckbox,\nselect\n## Managed\n## Property\n## Handler\nradio option\nchoose the appropriate\n## Script\nthat will be responsible\noptionally set the\nShown in Edit Views\ncheckbox to true. This\nhas the following effects:\nIn registration/editing form the raw values backing the\nmanaged properties is show.\nIn registration form additional input fields are provided if\nthe script has defined the function\nbatchColumnNames\nor\ninputWidgets\n.\nCreating scripts\n\nTo browse and edit existing scripts or add new ones, select\nAdministration->Scripts from the top menu.\nThe scripts should be written in standard Jython syntax. The following\n## functions are invoked by openBIS, some of them are mandatory:\n## Function\n## Mandatory/Optional\n## Description\nupdateFromUI(action)\noptional\nUpdates the property value from an input form constructed by the action defined in\nconfigureUI()\n.This function has an access to a variable named ‘person’ which holds an object of type  ch.systemsx.cisd.openbis.generic.shared.basic.dto.api.IPerson. This object contains information about a user that is performing the update.\nupdateFromRegistrationForm(bindings)\noptional\nCreates the property value from input data provided by a registration forms. This function is mandatory if function  inputWidgets  are defined. The argument is list of a  java.util.Map  objects which has the values taken from the registration form. Here\n<code>\nis either a batch column name in upper case or a input widget code. The bound value is a string accessed with  get(\n<code>\n).    This function has an access to a variable named ‘person’ which holds an object of type  ch.systemsx.cisd.openbis.generic.shared.basic.dto.api.IPerson. This object contains information about a user that is performing the update.\nupdateFromBatchInput(bindings)\noptional\nCreates the property value from input data provided by a batch input file (for batch import and batch update operations). This function is mandatory if function batchColumnNames is defined. The argument is a java.util.Map which has the values of the columns with headers of the form\n<property\ntype\ncode>:<code>\nin the input file. Here\n<code>\nis either a batch column name in upper case or a input widget code. The bound value is a string accessed with\nget(<code>\n). If no batchColumnNames is defined in the script the input value from the file with column\n<property\ntype\ncode>\nis accessed by\nget('')\n.This function has an access to a variable named ‘person’ which holds an object of type  ch.systemsx.cisd.openbis.generic.shared.basic.dto.api.IPerson. This object contains information about a user that is performing the update.\nBatch update has currently only limited support. There is no way to retrieve old value of managed property (property.getValue() always returns null). The\nupdateFromBatchInput(bindings)\nfunction can set the new value though.\ninputWidgets()\noptional\nA function which returns a list of IManagedInputWidgetDescription instances which will be used for batch update and/or input widgets in registration forms. The function\nupdateFromBatchInput(bindings)\nassumes that values are bound to the upper-case version of the code of the widget description which is by default the upper-case version of its label.\nconfigureUI()\nmandatory\nIt defines output UI (e.g. table for tabular data) to be used in the detailed view of the entity owning the property. Multiple actions (of type IManagedUIAction) for update of the property value can be defined in this function. Every action specifies input UI for displaying forms with input fields and transferring the values provided by users.\nbatchColumnNames()\noptional\nA function which returns a list of column names to be used for batch update and/or input widgets in registration forms. The function\nupdateFromBatchInput(bindings)\nassumes that values are bound to the upper-case version of the column names. The names will be the labels of non-mandatory text input widgets.\nAll functions (except\nbatchColumnNames\nand\ninputWidgets\n)  have\naccess to a variable named\nproperty\nwhich holds an object of type\nIManagedProperty\n. Methods of this class are explained below. To\naccess the property object from the script, use the following syntax:\nproperty\n.<\nrequested\nmethod\n>\n## Predefined Functions\n\nThe following functions are predefined and can be used everywhere in the\n## script:\nISimpleTableModelBuilderAdaptor\ncreateTableBuilder()\n: Creates a\ntable model builder. It will be used in\nconfigureUI\nto create\ntabular data to be shown in openBIS GUI.\nValidationException\nValidationException(String\nmessage)\n: Creates a\nValidation Exception with specified message which should be raised\nin functions\nupdateFromUI\nand\nupdateFromBatchInput\nin case of\ninvalid input.\nIManagedInputWidgetDescriptionFactory\ninputWidgetFactory()\n## :\nreturns a factory that can be used to create descriptions of input\nfields used for modification of managed property value (see\nexample\n).\nIElementFactory\nelementFactory()\n: returns a factory that can be\nused to create\nIElement\n-s.\n## See\n#Storing structured content in managed\nproperties\n.\nIStructuredPropertyConverter\nxmlPropertyConverter()\n: returns a\nconverter that can translate\nIElement\n-s\nto XML Strings and from XML or JSON Strings. See\n#Storing\nstructured content in managed\nproperties\n.\nIStructuredPropertyConverter\njsonPropertyConverter()\n: returns a\nconverter that can translate\nIElement\n-s\nto JSON Strings and from XML or JSON Strings. See\n#Storing\nstructured content in managed\nproperties\n.\nJava API\n\nJava objects used and created in the scripts are implementing Java\n## interfaces from two packages:\nch.systemsx.cisd.openbis.generic.shared.basic.dto.api\nDTOs (Data Transfer Objects) that will be transferred between server\nand client (web browser)\nch.systemsx.cisd.openbis.generic.shared.managed_property.api\nutilities (e.g. for managing DTOs)\nExamples of user defined functions\n\nThe following examples show how to implement particular script functions\nthat will be invoked by openBIS.\nconfigureUI\n\n## Example 1\n\nThis example shows how to configure a fixed table (without using value\nstored in the property at all) that will be shown in detail view of an\nentity.\ndef\nconfigureUI\n## ():\n\"\"\"create table builder and add 3 columns\"\"\"\ntableBuilder\n=\ncreateTableBuilder\n()\ntableBuilder\n.\naddHeader\n(\n\"column 1\"\n)\ntableBuilder\n.\naddHeader\n(\n\"column 2\"\n)\ntableBuilder\n.\naddHeader\n(\n\"column 3\"\n)\n\"\"\"add two rows with values of types: string, integer, real\"\"\"\nrow1\n=\ntableBuilder\n.\naddRow\n()\nrow1\n.\nsetCell\n(\n\"column 1\"\n,\n\"v1\"\n)\nrow1\n.\nsetCell\n(\n\"column 2\"\n,\n1\n)\nrow1\n.\nsetCell\n(\n\"column 3\"\n,\n1.5\n)\nrow2\n=\ntableBuilder\n.\naddRow\n()\nrow2\n.\nsetCell\n(\n\"column 1\"\n,\n\"v2\"\n)\nrow2\n.\nsetCell\n(\n\"column 2\"\n,\n2\n)\nrow2\n.\nsetCell\n(\n\"column 3\"\n,\n2.5\n)\n\"\"\"add a row with only value for the first column specified (two other columns will be empty)\"\"\"\nrow3\n=\ntableBuilder\n.\naddRow\n()\nrow3\n.\nsetCell\n(\n\"column 1\"\n,\n\"v3\"\n)\n\"\"\"specify that the property should be shown in a tab and set the table output\"\"\"\nproperty\n.\nsetOwnTab\n(\n## True\n)\nuiDesc\n=\nproperty\n.\ngetUiDescription\n()\nuiDesc\n.\nuseTableOutput\n(\ntableBuilder\n.\ngetTableModel\n())\nLet’s assume, that a property type with label\n## Fixed Table\nwas assigned\nto sample type CELL_PLATE as a managed property using the example\nscript.\nThe picture below shows that in detail view of CELL_PLATE sample S1\nthere will be a tab titled\n## Fixed Table\ncontaining a table defined by\nthe script. The table has the same functionality as all other openBIS\ntables like sorting, filtering, exporting etc.\n## Example 2\n\nThis is another example of showing how to configure a fixed table, but\nthis time values in the table will be displayed as clickable links to\nopenBIS entities (see\nLinking to openBIS entities\nfor more details):\ndef\nconfigureUI\n## ():\n\"\"\"create table builder with 4 columns (any column names can be used)\"\"\"\ntableBuilder\n=\ncreateTableBuilder\n()\ntableBuilder\n.\naddHeader\n(\n\"sample\"\n)\ntableBuilder\n.\naddHeader\n(\n\"experiment\"\n)\ntableBuilder\n.\naddHeader\n(\n\"data set\"\n)\ntableBuilder\n.\naddHeader\n(\n\"material\"\n)\n\"\"\"\nAdd rows with values of type entity link.\nUse element link factory to create link cells and.\n\"\"\"\nfactory\n=\nelementFactory\n()\nrow\n=\ntableBuilder\n.\naddRow\n()\n\"\"\" for links to samples, experiments and datasets provide the permId \"\"\"\nrow\n.\nsetCell\n(\n\"sample\"\n,\nfactory\n.\ncreateSampleLink\n(\n\"samplePermId\"\n))\nrow\n.\nsetCell\n(\n\"experiment\"\n,\nfactory\n.\ncreateExperimentLink\n(\n\"experimentPermId\"\n))\nrow\n.\nsetCell\n(\n\"data set\"\n,\nfactory\n.\ncreateDataSetLink\n(\n\"dataSetPermId\"\n))\n\"\"\" for material links material code and material type code are needed \"\"\"\nrow\n.\nsetCell\n(\n\"material\"\n,\nfactory\n.\ncreateMaterialLink\n(\n\"materialCode\"\n,\n\"materialTypeCode\"\n))\n\"\"\"specify that the property should be shown in a tab and set the table output\"\"\"\nproperty\n.\nsetOwnTab\n(\n## True\n)\nuiDesc\n=\nproperty\n.\ngetUiDescription\n()\nuiDesc\n.\nuseTableOutput\n(\ntableBuilder\n.\ngetTableModel\n())\nIf linked entity doesn’t exist in the database the perm id\n((\ncode\n(type)\nfor materials) will be shown as plain text (not\nclickable).\nOtherwise clickable links will be displayed with link text equal to:\nidentifier of samples or experiments,\ncode of a data set (the same as perm id),\ncode\n(type)\nof a material\n## Example 3\n\nThis example shows how to configure a table representation of a property\nvalue holding a CSV document (many lines with comma separated values):\ndef\nconfigureUI\n## ():\n\"\"\"get the property value as String and split it using newline character\"\"\"\nvalue\n=\nproperty\n.\ngetValue\n()\nlines\n=\n[]\nif\nvalue\n!=\n## None\n## :\nlines\n=\nvalue\n.\nsplit\n(\n\"\n\\n\n\"\n)\ntableBuilder\n=\ncreateTableBuilder\n()\nif\nlen\n(\nlines\n)\n>\n0\n## :\n\"\"\"treat first line as header - split using comma character to get column titles\"\"\"\nheader\n=\nlines\n[\n0\n]\n.\nsplit\n(\n\",\"\n)\ntableBuilder\n.\naddFullHeader\n(\nheader\n)\n\"\"\"iterate over rest of lines and add them to the table as rows\"\"\"\nfor\ni\nin\nrange\n(\n1\n,\nlen\n(\nlines\n## )):\nrow\n=\nlines\n[\ni\n]\n.\nsplit\n(\n\",\"\n)\ntableBuilder\n.\naddFullRow\n(\nrow\n)\n\"\"\"specify that the property should be shown in a tab and set the table output\"\"\"\nproperty\n.\nsetOwnTab\n(\n## True\n)\nuiDesc\n=\nproperty\n.\ngetUiDescription\n()\nuiDesc\n.\nuseTableOutput\n(\ntableBuilder\n.\ngetTableModel\n())\n## Let’s assume, that:\na multiline property type with label CSV was assigned to sample type\nCELL_PLATE as managed property using the example script,\nvalue of CSV property for CELL_PLATE sample S1 is:\ncol 1,col 2\nr1 v1,r1 v2\nr2 v1,r2 v2\nr3 v1,r3 v2\nThe picture below shows that in detail view of sample S1 there will be a\ntab titled CSV containing a table defined by the script.\nManaged property value will be visible as text in the left panel\n(\n## Sample Properties\n) only if user had enabled debugging mode in openBIS\n(\n## User\n## Menu->Settings->Enable\n## Debugging\n## Mode\n).\n## Example 4\n\nThis is an extension of the previous example showing how to specify user\ninput for actions like add, edit and delete:\ndef\nconfigureUI\n## ():\n\"\"\"code from previous example is not repeated here\"\"\"\nfactory\n=\ninputWidgetFactory\n()\nif\nlen\n(\nlines\n)\n>\n0\n## :\nheader\n=\nlines\n[\n0\n]\n.\nsplit\n(\n\",\"\n)\n\"\"\"define an action labelled 'Add' for adding a new row to the table\"\"\"\naddAction\n=\nuiDesc\n.\naddTableAction\n(\n## 'Add'\n)\n.\nsetDescription\n(\n'Add new row to the table'\n)\n\"\"\"for every header column add a text input field with the same label as column title\"\"\"\nwidgets\n=\n[]\nfor\ni\nin\nrange\n(\n0\n,\nlen\n(\nheader\n## )):\nwidgets\n.\nappend\n(\nfactory\n.\ncreateTextInputField\n(\nheader\n[\ni\n]))\naddAction\n.\naddInputWidgets\n(\nwidgets\n)\n\"\"\"define an action labelled 'Edit' for editing a selected row of the table\"\"\"\neditAction\n=\nuiDesc\n.\naddTableAction\n(\n## 'Edit'\n)\n.\nsetDescription\n(\n'Edit selected table row'\n)\neditAction\n.\nsetRowSelectionRequiredSingle\n()\n\"\"\"for every header column add a text input field that is bounded with a column\"\"\"\nwidgets\n=\n[]\nfor\ni\nin\nrange\n(\n0\n,\nlen\n(\nheader\n## )):\ncolumnName\n=\nheader\n[\ni\n]\nwidgets\n.\nappend\n(\nfactory\n.\ncreateTextInputField\n(\ncolumnName\n))\neditAction\n.\naddBinding\n(\ncolumnName\n,\ncolumnName\n)\neditAction\n.\naddInputWidgets\n(\nwidgets\n)\n\"\"\"define an action labelled \"Delete\" for deleting selected rows from the table - no input fields are needed\"\"\"\ndeleteAction\n=\nuiDesc\n.\naddTableAction\n(\n## 'Delete'\n)\n\\\n.\nsetDescription\n(\n'Are you sure you want to delete selected rows from the table?'\n)\ndeleteAction\n.\nsetRowSelectionRequired\n()\nThe picture below shows updated detail view of sample S1. For every\naction defined in the script there is a button in bottom toolbar of the\ntable.\nThe screenshot was taken after a user clicked on the first table row and\nthen clicked on\n## Edit\nbutton. This resulted in showing a dialog with\ninput fields defined in the script. Every field has default value set\nautomatically to a value from the selected row.\nWhenever an action is defined in\nconfigureUI\nthere should be\nupdateFromUI()\nfunction defined that handles the actions (see\nnext\nexample\n). Otherwise clicking on action\nbuttons will cause an error.\nupdateFromUI()\n\nThis function should update the value of the managed property in\nresponse to user’s action.\n## Example 5\n\nThis is an extension of the previous example showing how to specify\nbehaviour of actions defined in\nconfigureUI()\n## function:\ndef\nconfigureUI\n## ():\n\"\"\"code from previous example is not repeated here\"\"\"\ndef\nupdateFromUI\n(\naction\n## ):\n\"\"\"get the property value as String and split it using newline character\"\"\"\nvalue\n=\nproperty\n.\ngetValue\n()\nlines\n=\n[]\nif\nvalue\n!=\n## None\n## :\nlines\n=\nvalue\n.\nsplit\n(\n\"\n\\n\n\"\n)\n\"\"\"for 'Add' action add a new line with values from input fields\"\"\"\nif\naction\n.\ngetName\n()\n==\n## 'Add'\n## :\nnewLine\n=\nextractNewLineFromActionInput\n(\naction\n)\nlines\n.\nappend\n(\nnewLine\n)\nelif\naction\n.\ngetName\n()\n==\n## 'Edit'\n## :\n\"\"\"\nFor 'Edit' action find the line corresponding to selected row\nand replace it with a line with values from input fields.\nNOTE: line index is one bigger than selected row index because of header.\n\"\"\"\nlineIndex\n=\naction\n.\ngetSelectedRows\n()[\n0\n]\n+\n1\nlines\n.\npop\n(\nlineIndex\n)\nnewLine\n=\nextractNewLineFromActionInput\n(\naction\n)\nlines\n.\ninsert\n(\nlineIndex\n,\nnewLine\n)\nelif\naction\n.\ngetName\n()\n==\n## 'Delete'\n## :\n\"\"\"\nFor 'Delete' action delete the lines corresponding to selected rows.\nNOTE: deletion of rows is implemented here in reversed order\n\"\"\"\nrowIds\n=\nlist\n(\naction\n.\ngetSelectedRows\n())\nrowIds\n.\nreverse\n()\nfor\nrowId\nin\nrowIds\n## :\nlines\n.\npop\n(\nrowId\n+\n1\n)\n\"\"\"in the end update the property value concatenating all the lines\"\"\"\nvalue\n=\n\"\n\\n\n\"\n.\njoin\n(\nlines\n)\nproperty\n.\nsetValue\n(\nvalue\n)\ndef\nextractNewLineFromActionInput\n(\naction\n## ):\ninputValues\n=\n[]\nfor\ninput\nin\naction\n.\ngetInputWidgetDescriptions\n## ():\ninputValue\n=\n\"\"\nif\ninput\n.\ngetValue\n## ():\ninputValue\n=\ninput\n.\ngetValue\n()\ninputValues\n.\nappend\n(\ninputValue\n)\nreturn\n\",\"\n.\njoin\n(\ninputValues\n)\nupdateFromBatchInput(), batchColumNames() and inputWidgets()\n\nAll examples assume batch upload of a CSV/TSV file in openBIS for an\nentity type which has managed properties.\n## Example 6\n\nThis example assumes one column in the file for the managed property.\ndef\nupdateFromBatchInput\n(\nbindings\n## ):\nproperty\n.\nsetValue\n(\n'hello '\n+\nbindings\n.\nget\n(\n''\n))\ndef\nconfigureUI\n## ():\nbuilder\n=\ncreateTableBuilder\n()\nbuilder\n.\naddHeader\n(\n## 'Greetings'\n)\nrow\n=\nbuilder\n.\naddRow\n()\nrow\n.\nsetCell\n(\n## 'Greetings'\n,\nproperty\n.\ngetValue\n())\nproperty\n.\ngetUiDescription\n()\n.\nuseTableOutput\n(\nbuilder\n.\ngetTableModel\n())\nThe following input file for a batch upload for samples of a type where\nproperty\n## MANGED-TEXT\n## MANGED-TEXT\nworld\nuniverse\nwould create in sample detailed view\n## Example 7\n\nThis example takes two columns from the batch input file for creation of\none managed property.\ndef\nbatchColumnNames\n## ():\nreturn\n[\n## 'Unit'\n,\n## 'Value'\n]\ndef\nupdateFromBatchInput\n(\nbindings\n## ):\nproperty\n.\nsetValue\n(\nbindings\n.\nget\n(\n## 'VALUE'\n)\n+\n' ['\n+\nbindings\n.\nget\n(\n## 'UNIT'\n)\n+\n']'\n)\ndef\nconfigureUI\n## ():\nbuilder\n=\ncreateTableBuilder\n()\nbuilder\n.\naddHeader\n(\n## 'Value'\n)\nbuilder\n.\naddRow\n()\n.\nsetCell\n(\n## 'Value'\n,\nproperty\n.\ngetValue\n())\nproperty\n.\ngetUiDescription\n()\n.\nuseTableOutput\n(\nbuilder\n.\ngetTableModel\n())\nAssuming a sample type is assigned to the property\n## MANAGED-TEXT\nwith\nthis script. On the batch upload form a click on\nDownload file\ntemplate\nwould return a template file like the following one:\n# The \"container\" and \"parents\" columns are optional, only one should be specified.\n# \"container\" should contain a sample identifier, e.g. /SPACE/SAMPLE_1, while \"parents\" should contain comma separated list of sample identifiers.\n# If \"container\" sample is provided, the registered sample will become a \"component\" of it.\n# If \"parents\" are provided, the registered sample will become a \"child\" of all specified samples.\n# The \"experiment\" column is optional, cannot be specified for shared samples and should contain experiment identifier, e.g. /SPACE/PROJECT/EXP_1\nidentifier  container   parents experiment  MANAGED-TEXT:UNIT   MANAGED-TEXT:VALUE\nInstead of one property column\n## MANAGED-TEXT\nthere are two columns for\neach value of the list returned by the script function\nbatchColumnNames\n.\nUploading the following file for such a sample type\nidentifier container   parents experiment  MANAGED-TEXT:UNIT   MANAGED-TEXT:VALUE\n/test/sample-with-managed-property-1                mm  1.56\n/test/sample-with-managed-property-2                sec 47.11\nwould lead to a detailed view as in the following screenshot:\nIf the flag\nShown in Edit Views\nis set and flag\nShow Raw Value in\n## Forms\nis not set, the registration form would have a field called\n‘Managed Text’ with initially one section with input field ‘Unit’ and\n## ‘Value’:\nWith ‘+’ and ‘Add More’ button additional sections can be created.\nExisting sections can be deleted by the ‘-’ button. The section fields\nare all non-mandatory single-line fields with labels specified by the\nbatch column names. More is possible if the function\nbatchColumnsNames\nis replaced by function\ninputWidgets\nas in the following example:\ndef\ninputWidgets\n## ():\nfactory\n=\ninputWidgetFactory\n()\nunit\n=\nfactory\n.\ncreateComboBoxInputField\n(\n## 'Unit'\n,\n[\n'cm'\n,\n'mm'\n])\n.\nsetMandatory\n(\n## True\n)\nvalue\n=\nfactory\n.\ncreateTextInputField\n(\n## 'Value'\n)\n.\nsetMandatory\n(\n## True\n)\nreturn\n[\nunit\n,\nvalue\n]\nThe field ‘Managed Text’ in the registration form will be as shown in\nthe following screen shot:\nBoth fields are mandatory and the first field is a combo box with the\ntwo elements ‘cm’ and ‘mm’.\nHTML Output\n\nIn addition to table output, a managed property may produce HTML output.\nHere is a (overly) simple example:\n## Example 8\n\ndef\nconfigureUI\n## ():\nproperty\n.\ngetUiDescription\n()\n.\nuseHtmlOutput\n(\n\"<p>hello<br>foo</p>\"\n)\nAccessing information about a person that performs an update operation\n\n## Example 9\n\nThis example shows how information about a person that performs an\nupdate operation can be access in a managed property script. The\ninformation is stored in the ‘person’ variable that is available in both\n‘updateFromUI’ and ‘updateFromBatchInput’ functions.\ndef\nupdateFromBatchInput\n(\nbindings\n## ):\nproperty\n.\nsetValue\n(\n'userId: '\n+\nperson\n.\ngetUserId\n()\n+\n', userName: '\n+\nperson\n.\ngetUserName\n())\nStoring structured content in managed properties\n\nBy “structured” properties we understand properties holding complex data\nstructures in their values. Typically, the Jython scripts handling such\nproperties need to implement a conversion strategy transforming the data\nstructure held into a property value and vice versa. To facilitate this\ntask, openBIS offers an API allowing the users to easily create an\nabstract data structure and persist it as a single property.\nSupported structure elements\n\nThe abstract data structure supported by the API is basically a list of\nIElement\n-s.\nIElements are named objects optionally having associated key-value\nattributes and optionally containing other IElements. Additionally,\nIElements can also be used as containers for larger chunks of raw data.\nTo construct concrete instances of IElement one has to use\nIElementFactory\navailable via the predefined function\nelementFactory()\n.\nLinking to openBIS entities\n\nAstute readers may have already noticed that the\nIElementFactory\nalso offers methods that create\nIEntityLinkElement\ninstances. An IEntityLinkElement denotes a link to another object in\nopenBIS.\nCurrently the entity links can lead to entities that don’t exist in the\ndatabase. Such links are displayed as text instead of html links in\ngrids defined for managed properties.\nConverting to property value\n\nOnce you a have created the desired data structure in form of\nIElement\n-s,\nyou can use an\nIStructuredPropertyConverter\nto convert it to a property value. An instance of\nIStructuredPropertyConverter\ncan be created from the\n#Predefined\n## Functions\n.\nManaged properties can be stored either as XML Strings or as JSON\nStrings. The script writer makes the decision for a serialization type\nby either calling\nxmlPropertyConverter()\nor\njsonPropertyConverter()\n.\nNote that both converters can convert from XML or JSON Strings\nto\nIElement\nlists, detecting automatically which type of String they\nget. The two converters only differ in what type of serialization they\nuse when converting from\nList<IElement>\nto a String. By this mechanism\nit is even possible to change the serialization type after values of the\nmanaged property have been created and stored in the database without\nbreaking the functionality of managed properties. To maintain this\ntransparency it is recommended that API users avoid parsing the XML or\nJSON Strings themselves and let the converter do the job.\nJython example script\n\nSometimes a few lines of code are worth a thousand words. Have a look at\nthe example code below. It is extracted from a Jython script and\ndemonstrates the basics of constructing and serializing structured\ncontent within a managed property.\nfactory\n=\nelementFactory\n()\nconverter\n=\nxmlPropertyConverter\n()\ndef\ninitialCreationOfPropertyValue\n## ():\n\"\"\"\nCreate an element data structure containing\n1) A link to Sample with specified perm id\n2) A link to Material with specified type and typCode\n3) An application specific element \"testElement\"\n\"\"\"\nelements\n=\n[\nfactory\n.\ncreateSampleLink\n(\n\"samplePermId\"\n),\nfactory\n.\ncreateMaterialLink\n(\n\"type\"\n,\n\"typeCode\"\n),\nfactory\n.\ncreateElement\n(\n\"testElement\"\n)\n.\naddAttribute\n(\n\"key1\"\n,\n\"value1\"\n)\n.\naddAttribute\n(\n\"key2\"\n,\n\"value2\"\n)\n]\n# save the created data structure into the property value\nproperty\n.\nvalue\n=\nconverter\n.\nconvertToString\n(\nelements\n)\ndef\nupdateDataStructure\n## ():\n\"\"\"\nThis function imitates an update procedure. The content of the property\nis parsed to a list of elements, several modifications are made on the elements\nand these are then saved back in the property.\n\"\"\"\n# read the stored data structure\nelements\n=\nlist\n(\nconverter\n.\nconvertToElements\n(\nproperty\n))\n# we assume, the contents from the \"create...\" method above\nelements\n[\n0\n]\n=\nfactory\n.\ncreateSampleLink\n(\n\"modifiedLink\"\n)\nelements\n[\n1\n]\n.\nchildren\n=\n[\nfactory\n.\ncreateElement\n(\n\"nested1\"\n)\n.\naddAttribute\n(\n\"na1\"\n,\n\"na2\"\n)\n]\n# replaces the old value of the \"key2\" attribute\nelements\n[\n2\n]\n.\naddAttribute\n(\n\"key2\"\n,\n\"modifiedvalue\"\n)\n# update the property value to reflect the modified data structure\nproperty\n.\nvalue\n=\nconverter\n.\nconvertToString\n(\nelements\n)\nAt the end of the function\ninitialCreationOfPropertyValue()\n, the\nvariable\nproperty.value\nwill contain an XML representation of the\ncreated data structure, which will look like\n<root>\n## <Sample\npermId=\n\"samplePermId\"\n/>\n## <Material\npermId=\n\"type (typeCode)\"\n/>\n<testElement\nkey1=\n\"value1\"\nkey2=\n\"value2\"\n/>\n</root>\nThe function\nupdateDataStructure()\nassumes that the\ninitialCreationOfPropertyValue()\nhas already been called and modifies\nthe data structure to what would translate to the following XML snippet:\n<root>\n## <Sample\npermId=\n\"modifiedLink\"\n/>\n## <Material\npermId=\n\"type (typeCode)\"\n>\n<nested1\nna1=\n\"na2\"\n/>\n## </Material>\n<testElement\nkey1=\n\"value1\"\nkey2=\n\"modifiedvalue\"\n/>\n</root>\nUnofficial API\n\nIn addition to the variable\nproperty\n, the variable\npropertyPE\nis\nalso available to managed property scripts. Its use is not officially\nsupported and code that uses it is not guaranteed to work after an\nupgrade of openBIS, but it can be used to get access to useful\ninformation that is not available through the official API.\n‘Real World’ example\n\nThe following example shows a complete implementation of a managed\nproperty script for handling list of log entries. The property value is\nstored as an XML document.\nfrom\njava.util\nimport\n## Date\n\"\"\"\nExample XML property value handled by this script:\n<root>\n<logEntry date=\"2011-02-20 14:15:28 GMT+01:00\" person=\"buczekp\" logType=\"INFO\">Here is the 1st log entry text.<logEntry>\n<logEntry date=\"2011-02-20 14:16:28 GMT+01:00\" person=\"kohleman\" logType=\"WARN\">Here is the 2nd log entry text - a warning!<logEntry>\n<logEntry date=\"2011-02-20 14:17:28 GMT+01:00\" person=\"tpylak\" logType=\"ERROR\">Here is the 3rd log entry text - an error!!!<logEntry>\n<logEntry date=\"2011-02-20 14:18:28 GMT+01:00\" person=\"brinn\" logType=\"ERROR\">Here is the 4th log entry text - an error!!!<logEntry>\n<logEntry date=\"2011-02-20 14:19:28 GMT+01:00\" person=\"felmer\" logType=\"WARN\">Here is the 5th log entry text - a warning!<logEntry>\n</root>\n\"\"\"\n## LOG_ENTRY_ELEMENT_LABEL\n=\n'logEntry'\n## LOG_TYPES\n=\n[\n## 'INFO'\n,\n## 'WARN'\n,\n## 'ERROR'\n]\n\"\"\" labels of table columns and corresponding input fields \"\"\"\n## DATE_LABEL\n=\n## 'Date'\n## PERSON_LABEL\n=\n## 'Person'\n## LOG_TYPE_LABEL\n=\n## 'Log Type'\n## LOG_TEXT_LABEL\n=\n## 'Log Text'\n\"\"\" names of attributes of XML elements for log entries \"\"\"\n## DATE_ATTRIBUTE\n=\n'date'\n## PERSON_ATTRIBUTE\n=\n'person'\n## LOG_TYPE_ATTRIBUTE\n=\n'logType'\n\"\"\" action labels (shown as button labels in UI) \"\"\"\n## ADD_ACTION_LABEL\n=\n## 'Add Log Entry'\n## EDIT_ACTION_LABEL\n=\n## 'Edit'\n## DELETE_ACTION_LABEL\n=\n## 'Delete'\ndef\nconfigureUI\n## ():\n\"\"\"Create table builder and add headers of columns.\"\"\"\nbuilder\n=\ncreateTableBuilder\n()\nbuilder\n.\naddHeader\n(\n## DATE_LABEL\n,\n250\n)\n# date and log text values are long, override default width (150)\nbuilder\n.\naddHeader\n(\n## PERSON_LABEL\n)\nbuilder\n.\naddHeader\n(\n## LOG_TYPE_LABEL\n)\nbuilder\n.\naddHeader\n(\n## LOG_TEXT_LABEL\n,\n300\n)\n\"\"\"\nExtract XML elements from property value to a Python list.\nFor each element (log entry) add add a row to the table.\n\"\"\"\nelements\n=\nlist\n(\nxmlPropertyConverter\n()\n.\nconvertToElements\n(\nproperty\n))\nfor\nlogEntry\nin\nelements\n## :\nrow\n=\nbuilder\n.\naddRow\n()\nrow\n.\nsetCell\n(\n## DATE_LABEL\n,\n## Date\n(\nlong\n(\nlogEntry\n.\ngetAttribute\n(\n## DATE_ATTRIBUTE\n))))\nrow\n.\nsetCell\n(\n## PERSON_LABEL\n,\nlogEntry\n.\ngetAttribute\n(\n## PERSON_ATTRIBUTE\n))\nrow\n.\nsetCell\n(\n## LOG_TYPE_LABEL\n,\nlogEntry\n.\ngetAttribute\n(\n## LOG_TYPE_ATTRIBUTE\n))\nrow\n.\nsetCell\n(\n## LOG_TEXT_LABEL\n,\nlogEntry\n.\ngetData\n())\n\"\"\"Specify that the property should be shown in a tab and set the table output.\"\"\"\nproperty\n.\nsetOwnTab\n(\n## True\n)\nuiDescription\n=\nproperty\n.\ngetUiDescription\n()\nuiDescription\n.\nuseTableOutput\n(\nbuilder\n.\ngetTableModel\n())\n\"\"\"\nDefine and add actions with input fields used to:\n1. specify attributes of new log entry,\n\"\"\"\naddAction\n=\nuiDescription\n.\naddTableAction\n(\n## ADD_ACTION_LABEL\n)\n\\\n.\nsetDescription\n(\n'Add a new log entry:'\n)\nwidgets\n=\n[\ninputWidgetFactory\n()\n.\ncreateComboBoxInputField\n(\n## LOG_TYPE_LABEL\n,\n## LOG_TYPES\n)\n\\\n.\nsetMandatory\n(\n## True\n)\n\\\n.\nsetValue\n(\n## 'INFO'\n),\ninputWidgetFactory\n()\n.\ncreateMultilineTextInputField\n(\n## LOG_TEXT_LABEL\n)\n\\\n.\nsetMandatory\n(\n## True\n)\n]\naddAction\n.\naddInputWidgets\n(\nwidgets\n)\n\"\"\"\n2. modify attributes of a selected log entry,\n\"\"\"\neditAction\n=\nuiDescription\n.\naddTableAction\n(\n## EDIT_ACTION_LABEL\n)\n\\\n.\nsetDescription\n(\n'Edit selected log entry:'\n)\n# Exactly 1 row needs to be selected to enable action.\neditAction\n.\nsetRowSelectionRequiredSingle\n()\nwidgets\n=\n[\ninputWidgetFactory\n()\n.\ncreateMultilineTextInputField\n(\n## LOG_TEXT_LABEL\n)\n.\nsetMandatory\n(\n## True\n)\n]\neditAction\n.\naddInputWidgets\n(\nwidgets\n)\n# Bind field name with column name.\neditAction\n.\naddBinding\n(\n## LOG_TEXT_LABEL\n,\n## LOG_TEXT_LABEL\n)\n\"\"\"\n3. delete selected log entries.\n\"\"\"\ndeleteAction\n=\nuiDescription\n.\naddTableAction\n(\n## DELETE_ACTION_LABEL\n)\n\\\n.\nsetDescription\n(\n'Are you sure you want to delete selected log entries?'\n)\n# Delete is enabled when at least 1 row is selected.\ndeleteAction\n.\nsetRowSelectionRequired\n()\ndef\nupdateFromUI\n(\naction\n## ):\n\"\"\"Extract list of elements from old value of the property.\"\"\"\nconverter\n=\nxmlPropertyConverter\n()\nelements\n=\nlist\n(\nconverter\n.\nconvertToElements\n(\nproperty\n))\n\"\"\"Implement behaviour of user actions.\"\"\"\nif\naction\n.\nname\n==\n## ADD_ACTION_LABEL\n## :\n\"\"\"\nFor 'add' action create new log entry element with values from input fields\nand add it to existing elements.\n\"\"\"\nelement\n=\nelementFactory\n()\n.\ncreateElement\n(\n## LOG_ENTRY_ELEMENT_LABEL\n)\n\"\"\"Fill element attributes with appropriate values.\"\"\"\nelement\n.\naddAttribute\n(\n## DATE_ATTRIBUTE\n,\nstr\n(\n## Date\n()\n.\ngetTime\n()))\n# current date\nelement\n.\naddAttribute\n(\n## PERSON_ATTRIBUTE\n,\naction\n.\ngetPerson\n()\n.\ngetUserId\n())\n# invoker the action\n\"\"\"Retrieve values from input fields filled by user on the client side.\"\"\"\nelement\n.\naddAttribute\n(\n## LOG_TYPE_ATTRIBUTE\n,\naction\n.\ngetInputValue\n(\n## LOG_TYPE_LABEL\n))\n\"\"\"Set log text as a text element, not an attribute.\"\"\"\nelement\n.\nsetData\n(\naction\n.\ngetInputValue\n(\n## LOG_TEXT_LABEL\n))\n\"\"\"Add the new entry to the end of the element list.\"\"\"\nelements\n.\nappend\n(\nelement\n)\nelif\naction\n.\nname\n==\n## EDIT_ACTION_LABEL\n## :\n\"\"\"\nFor 'edit' action find the log entry element corresponding to selected row\nand replace it with an element with values from input fields.\n\"\"\"\nselectedRowId\n=\naction\n.\ngetSelectedRows\n()[\n0\n]\nelements\n[\nselectedRowId\n]\n.\nsetData\n(\naction\n.\ngetInputValue\n(\n## LOG_TEXT_LABEL\n))\nelif\naction\n.\nname\n==\n## DELETE_ACTION_LABEL\n## :\n\"\"\"\nFor 'delete' action delete the entries that correspond to selected rows.\nNOTE: As many rows can be deleted at once it is easier to delete them in reversed order.\n\"\"\"\nrowIds\n=\nlist\n(\naction\n.\ngetSelectedRows\n())\nrowIds\n.\nreverse\n()\nfor\nrowId\nin\nrowIds\n## :\nelements\n.\npop\n(\nrowId\n)\nelse\n## :\nraise\nValidationException\n(\n'action not supported'\n)\n\"\"\"Update value of the managed property to XML string created from modified list of elements.\"\"\"\nproperty\n.\nvalue\n=\nconverter\n.\nconvertToString\n(\nelements\n)\nCreating and Deploying Java Plugins\n\nTo create valid Java plugin for Managed Properties, one should create a\nclass that is\nimplementing\nch.systemsx.cisd.openbis.generic.shared.managed_property.api.IManagedPropertyHotDeployEvaluator\ninterface.\nThe class should be annotated\nwith\nch.ethz.cisd.hotdeploy.PluginInfo\nannotation specifying the name\nof the plugin,\nand\nch.systemsx.cisd.openbis.generic.shared.managed_property.api.IManagedPropertyHotDeployEvaluator\nclass\nas a plugin type.\nSuch a plugin should be exported to a jar file and put\ninto\n<<openBIS\ninstallation\ndirectory>>/servers/entity-related-plugins/managed-properties\ndirectory.\nThe plugin will be detected automatically and will be automatically\navailable to openBIS. No restart is needed.\nNo labels", "timestamp": "2025-09-18T09:38:30.066323Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_additional-functionalities:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/additional-functionalities.html", "repo": "openbis", "title": "Additional Functionalities", "section": "Additional Functionalities", "text": "## Additional Functionalities\n\nPrint PDF\n\nFor every entity in openBIS it is possible to generate a pdf using the\nPrint PDF\noption from the\n## More..\ndropdown menu.\nThe generated pdf file can be printed or downloaded from the browser.\nAn example for a Space is shown in the picture below.\n## Visualise Relationships\n\nParent-child relationships between\n## Objects\ncan be visualised as trees\nor tables in the ELN.\nTo see the genealogical tree, select the\n## Hierarchy Graph\noption from the\n## More…\ndropdown in an entity form.\nLarge trees can be pruned, by selecting how many levels of parents\nand/or children and which types to show.\nTo view the genealogy of an\n## Object\nin a tabular format, select the\n## Hierarchy Table\noption from the\n## More…\ndropdown.\n## Tables\n\nAll tables in the ELN have a similar format and functionalities. The\ntables have been redesigned for the 20.10.3 release of openBIS.\nHere we give an overview of the main functionalities of the tables.\n## Filters\n\nTwo filter options are available form the\n## Filters\n## button:\n## Filter\n## Per Column\nand\n## Global Filter\n. The first allows to filter on\nindividual columns, or multiple columns, whereas the second filters\nterms across the entire table using the\n## AND\nor\n## OR\noperator.\n## Sorting\n\nIt is possible to sort individual columns or also multiple columns. For\nmulti-column sorting, you should click on the column header and press\nthe\n## Cmd\nkeyboard key. The order of sorting is shown by a number in\neach column, as shown below.\n## Exports\n\nTables can be exported in different ways, using the export button shown\nbelow.\n## Import Compatible\n## :\n## Yes\n: in this case some columns which are incompatible\nwith imports (i.e. registration date, registrator,\nmodification date, modifier) are not exported even if\nselected; some columns that are required by openBIS for\nimports are added to the exported file even if not\nselected (i.e. code, identifier, $ column). Moreover text\nfields are exported in HTML, to keep the formatting upon\nimport.\n## No\n: in this case all columns or selected columns are\nexported.\n2.\n## Columns\n## :\nAll (default order)\n. All columns are exported, in\naccordance with the selection explained above for import\ncompatibility.\nSelected (shown order)\n. Selected columns are exported,\nin accordance with the selection explained above for\nimport compatibility.\n3.\n## Rows\n## :\n## All Pages\n. All pages of the table are exported.\n## Current Page\n. Only the currently visible page of\nthe table is exported.\n## Selected Rows\n. Only selected rows in the table are\nexported.\n4.\n## Value\n## :\n## Plain Text\n. Text fields are exported in plain text,\nwithout any formatting. This option is not available if\nthe export is import-compatible.\n## Rich Text\n. Text fields are exported in HTML format.\nTables are exported to\n## XLS\nformat. Exported tables can be used for\nupdates via the\nXLS Batch Update Objects\n.\nNote: Excel has a character limit of 32767 characters in each cell. If you export entries where a field exceeds this limit, you get a warning and the exported Excel file will not contain the content of the cell which is above this limit and the cell is highlighted in red, as shown below.\n## Columns\n\nUsers can select which properties to display in the table clicking on\nthe\n## Columns\nbutton. It is also possible to show all properties or\nhide all properties. The position of the columns can also be changed by\nplacing the cursor next to the = sign in the list and moving the fields.\nThis information is stored in the database for each user.\n## Spreadsheets\n\nIf a table contains\n## Objects\nwhich have a spreadsheet field which is\nfilled in, a spreadsheet icon is displayed in the table. Upon clicking\non the icon, the content of the spreadsheet can be expanded.\nText fields\n\nIf a table contains Objects which have long text fields, only the\nbeginning of the text is shown and can be expanded. If the text contains\na picture or a table, an icon is shown in the table and the content of\nthe text becomes visible by clicking on the icon.\nSelection of entries in table\n\nSingle entries in a table can be selected using the checkbox in the row.\nBy clicking the checkbox in the table header, all entries of the table\n## are selected. After selection of entries, some actions become available:\n## Delete\n: allows to move the selected entries to the trashcan.\n## Move\n: allows to move the selected entries to a different existing\n## Collection/Experiment\nor to a new one.\nGenerate barcodes\n: allows to generate custom barcodes for the\nselected entries.\nUpdate custom barcodes/QR codes\n: allows to update existing custom\nbarcodes of the selected entries.\nClear selection\n: allows to clear the selection made.\n## In\n## Object\ntables inside\n## Experiments/Collections\nthere is an\n## Operations\ncolumn, which allow users to perform certain tasks on an\n## Object\n## :\nUpload a file to the\n## Object\nMove the\n## Object\nto another exiting\n## Experiment/Collection\n.\nUpdate Barcode/QR code.\nOpen the hierarchy graph. This is the graph showing parent/child\nconnections of the\n## Object\n.\nOpen the hierarchy table. This is the table showing parent/child\nconnections of the\n## Object\n.\nBrowse Entries by Type\n\n## The\n## Object Browser\nunder the\n## Utilities\nmain menu allows to see\nall entries of the same type and all\n## Experimental Steps\n, which may be\ncontained in different\n## Experiments/Collections\nand\n## Projects\n.\nThis is useful when there are entries of a certain type that belong to\ndifferent\n## Collections\n(e.g. protocols of the same type stored in two\ndifferent protocol collections), or to have an overview of all\n## Experimental Steps\n, independently of the\n## Experiment\nthey belong to.\nFrom the\n## Object Browser\npage, it is also possible to\n## Batch\nregister\nor\nBatch update\n## Objects\nusing an XLS or TSV template.\n## Trashcan\n\n## When\n## Experiments\n,\n## Objects\nand\n## Datasets\nare deleted, they are moved\nto the openBIS\ntrashcan\n, under the\n## Utilities\nmain menu. Items\ncan be removed from the trashcan only by someone with\nSpace admin\nor\nInstance admin\nrole. Deletion from the trashcan is\n## IRREVERSIBLE\n.\n## Note:\n## Spaces\nand\n## Projects\nare directly permanently deleted, they are\nnot moved to the trashcan first.\nTo empty the whole trashcan, click the blue\n## Empty Trash\nbutton above the table.\nTo delete permanently single entries choose one of two options from the\n## Operations dropdown:\ndelete permanently\n: deletes permanently only the selected entry.\ndelete permanently (including dependent deletions)\n: if the\nselected entry had children which are also in the trashcan, this\noption allows to permanently delete both the entry and its children.\nIf one entity was unintentionally deleted, the operation can be reverted\nat this stage by choosing the\n## Revert Deletions\noption from\nthe\n## Operations\ndrop down in the table.\n## Visualize Available Storage Space\n\nThe storage space available in an openBIS instance can be visualized by navigating to\n## Other Tools\nin the navigation menu and clicking on the\nShow available storage space\nbutton.\nBefore uploading large datasets, the available storage space should always be checked.\n## Vocabulary Browser\n\n## The\nVocabulary browser\nis accessible from the\n## Utilities\nmain\nmenu. This shows all controlled vocabularies registered in openBIS and\nthe terms they contain. Vocabularies are predefined lists of values to\nchoose from in given fields. Vocabularies can be created/modified by an\nopenBIS\nInstance admin\n(see\n## New Entity Type\n## Registration\n).\nThis information is needed for filling the forms for\n## Batch\n## Upload\nor\n## Batch Update\nof\n## Objects\nvia TSV file. If an\n## Object\nhas a property of type\n## Controlled Vocabulary\n, the codes of the\nvocabulary have to be entered in the .tsv template file. This is not the\ncase for XLS Batch registration or update, where labels can be used.\n## Freeze Entities\n\nEach level of the openBIS hierarchy (Space, Project,\nExperiment/Collection, Object, Dataset) can be frozen, so it can be no\nlonger edited and/or deleted.\nAt every level, everything contained underneath is selected by default\nto be frozen. E.g. if I choose to freeze a Space, everything contained\nin the Space is automatically selected to be frozen. Single entities can\nbe manually unselected.\nA Space admin role is necessary to freeze entities in a given Space.\nIMPORTANT: the freezing is IRREVERSIBLE!\nThis operation cannot be undone from any UI, not even by an\nInstance admin.\nPlease freeze entities only when you are absolutely sure that\nthey should not be further modified!\nHow to freeze an entity\n\nAt each level of the openBIS hierarchy (\n## Space, Project,\n## Experiment/Collection, Object, Dataset\n) the\n## Freeze Entity\noption is\navailable under the\n## More..\ndropdown menu. See the example for a\n## Space\nbelow.\nIf you select this, a list of entities contained or connected to the one\nselected will be presented to you, as shown below. By default everything\nis selected, so you need to unselect entries that you do not want to\nfreeze.\nTo freeze one or several entities, you need to provide your login\npassword and save.\nRules for freezing\nFreeze Space only\n## Allowed\nNot allowed\nCreate new Project\nx\nCreate new Experiment/Collection\nx\nCreate new Object\nx\nCreate new Dataset in existing Experiment/Collection\nx\nCreate new Dataset in existing Object\nx\nEdit existing Project\nx\nEdit existing Experiment/Collection\nx\nEdit existing Object\nx\nEdit existing Dataset\nx\n## Delete Space\nx\n## Delete Project\nx\n## Delete Experiment/Collection\nx\n## Delete Object\nx\n## Delete Dataset\nx\n## Move Experiment/Collection\nx\n## Move Object\nx\n## Copy Object\nx\n## Export\nx\nFreeze Project only\n## Allowed\nNot allowed\nCreate new Experiment/Collection\nx\nCreate new Object\nx\nCreate new Dataset in existing Experiment/Collection\nx\nCreate new Dataset in existing Object\nx\n## Edit Project\nx\nEdit existing Experiment/Collection\nx\nEdit existing Object\nx\nEdit existing Dataset\nx\n## Delete Project\nx\n## Delete Experiment/Collection\nx\n## Delete Object\nx\n## Delete Dataset\nx\n## Move Experiment/Collection\nx\n## Move Object\nx\n## Copy Object\nx\n## Export\nx\n3. Freeze Experiment/Collection only\n## Allowed\nNot allowed\nCreate new Object\nx\nCreate new Dataset in existing Experiment/Collection\nx\nCreate new Dataset in existing Object\nx\nEdit existing Experiment/Collection\nx\nEdit existing Object\nx\nEdit existing Dataset\nx\n## Delete Experiment/Collection\nx\n## Delete Object\nx\n## Delete Dataset\nx\n## Move Experiment/Collection\nx\n## Move Object\nx\n## Copy Object\nx\n## Export\nx\n4. Freeze Object only\n## Allowed\nNot allowed\nCreate new Dataset in existing Object\nx\nEdit existing Object\nx\nEdit existing Dataset in Object\nx\n## Delete Object\nx\n## Delete Dataset\nx\n## Move Object\nx\n## Copy Object\nx (only if the Experiment is not frozen)\n## Export\nx\n5. Freeze Dataset only\n## Allowed\nNot allowed\nEdit existing Dataset\nx\n## Delete Dataset\nx\n## Move Dataset\nx\n## Export\nx\nNavigation menu\n\nopenBIS 20.10.6 features a new navigation menu.\nThis has the following functionalities:\n## 1.Filter\n. You can filter the menu by names or codes.\n2. Root nodes\n. If you do not want to navigate the full menu, but\nonly a section of it, you can set the section you want to navigate as\nroot node, by clicking the icon shown in the picture below.\nThis now becomes the root node, as shown below. To restore the full menu\nview, you can click on the root node icon shown below.\n## 3. Sorting\n. The default sorting of the menu is in alphabetical. It\nis now possible to sort separately individual sections of the menu\n(\nELN, Inventory, Stock\n) and individual nodes inside those sections. It\nis possible to do a custom sorting by moving around (drag&drop) entities\nin the menu. Please note that this is only possible inside a given\nlevel, i.e. you can re-organise\n## Objects\ninside a\n## Collection/Experiment\n;\n## Collections/Experiments\ninside a\n## Project\n;\n## Projects\ninside a\n## Space\n. However, you cannot move entities from one\nlevel to another, i.e. you cannot move an\n## Object\nto a different\n## Collection/Experiment\n; a\n## Collection/Experimen\nt to a different\n## Project\n; a\n## Project\nto a different\n## Space\n. This can only be done\nfrom the\n## Move\noption under the\n## More..\ndropdown menu in the\nforms.\n## 4. Collapse/Expand.\nThe full menu or individual nodes can be\nexpanded or collapsed, with the button shown below.\n5. Scroll to selected node\n. In some cases, the view in the main ELN\npage does not correspond to an entry selected in the menu. You can\nscroll to the selected node in the menu, using the button shown below.\nThe state of the menu is saved. Every time you change something in the\nmenu, this change will be saved and when you login next time you will\nsee the menu in the state you last saved it.\n## Custom Imports\n\nFrom openBIS version 20.10.4, Custom Imports, previously available only\nin the core UI, are available in the ELN UI.\nCustom imports allow users to import metadata in a custom way, by using\na dropbox script in the background. You can use this if you want to\nparse a file in a given format and import the information from this file\nas metadata in openBIS.\nCustom imports are not available by default, but need to be enabled on\nthe server side by a\nsystem admin\n, and a dropbox script needs to be\nassociated with an import (see\n## Custom\n## Imports\n).\nIf one or more custom imports are configured in openBIS, the\n## Custom\n## Import\noption is available under the\n## Utilities\nin the\nmain\nmenu\n.\nThe available custom imports can be selected from the\n## Custom Import\n## Service\ndrop down menu in the Custom Import page (see below).\nIf the available custom import provides a template that can be used as\ninput for the import, the template will be available to download from\nthe Custom Import page.\nIf the custom import is not configured to provide a template, no\ndownload link is shown in the Custom Import page.\nEntity history\n\nWhenever an entity of type\n## Collection/Experiment\n,\n## Object\nor\n## Dataset\nis modified in openBIS, the changes are stored in the\ndatabase. The stored changes are modifications to property fields,\naddition and deletion of parents/children for\n## Objects\nand\n## Datasets\n,\nchanges of\n## Space/Project/Experiment/Object\nownership if an entity is\nmoved.\n## The\n## History\nof changes of each entity is now available in the ELN\nUI. In versions prior to openBIS 20.10.3 this was only available in the\ncore UI.\nHistory table for Collections\n\nIn a\n## Collection\npage, the\n## History\ncan be accessed from the\n## More..\ndropdown list.\n## The\n## History\ntable shows the version number of the changes, the\nauthor of the changes, the changes made (with the values before- in red,\nand after the change – in green), and the timestamp, i.e. the time when\nthe changes were made.\nFor a\n## Collection\n, the\nPermID\n(Permanent Identifier) of the\n## Project\nit belongs to is shown. If a\n## Collection\nis moved from one\n## Project\nto another, the PermID of the old and new\n## Projects\nare shown\nin the history table.\n## The\nshow\noption in\n## Full Document\nshows the full metadata of the\nentry (not only the changed fields) when changes were applied. This is\ndisplayed in JSON format.\nHistory table for Objects\n\nFor every\n## Object\n, the history of changes can be accessed from the\n## More..\ndropdown on the\n## Object\npage.\nFor an\n## Object\n, the\nPermID\n(Permanent Identifier) of the\n## Collection\nit belongs to is shown. If an\n## Object\nis moved from one\n## Collection\nto another, the PermID of the old and new\n## Collections\nare\nshown in the history table.\nHistory table for Datasets\n\nFor every dataset, the history of changes can be accessed from the\n## More..\ndropdown on the\n## Dataset\npage.\nFor a\n## Dataset\n, the\nPermID\n(Permanent Identifier) of the\n## Object\n/\n## Collection\nit belongs to is shown. If a\n## Dataset\nis moved\nfrom one\n## Object\n/\n## Collection\nto another, the PermID of the old and new\n## Objects\n/\n## Collections\nare shown in the history table.\n## Spreadsheet\n\nThe spreadsheet component needs to be enabled by a group admin or lab manager who can edit the ELN Settings, as described here:\nEnable Rich Text Editor or Spreadsheet Widgets\nThe spreadsheet supports some basic Excel functionalities, such as mathematical formulas (e.g. =SUM(A1+A2)).\nIt is possible to import an openBIS Object into the spreadsheet, with the\nimport\nbutton, on the spreadsheet itself:\nPlease note that if the Object is updated in openBIS, it will NOT be automatically updated in the spreadsheet.\n## Session Token\n\nWhen users log in to openBIS, a session token is generated. The session token is visible in the ELN UI, under the\n## User Profile\n, in the\nnavigation menu\n.\nThe session token is needed to connect to openBIS via pyBIS or obis, in cases where SSO (e.g. SWITCHaai) is used for authentication. See\npyBIS\n.", "timestamp": "2025-09-18T09:38:30.075220Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_barcodes:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/barcodes.html", "repo": "openbis", "title": "Barcodes and QR codes", "section": ":", "text": "Barcodes and QR codes\n\nBarcodes and QR codes\n\nThe barcode functionality must be enabled in openBIS by a\nlab manager\nor\ngroup admin\n## :\n## Enable\nBarcodes and QR codes\n.\nBarcodes for individual samples\n\nWhen a sample is registered, a barcode is automatically generated by\nopenBIS. This is found in the\nIdentification info\nsection, as shown\nbelow.\nThis barcode can be printed and the label can be added to the vial\ncontaining the sample. The option to print the barcode is under the\n## More..\nmenu\nIf a sample already has its own barcode or QR code, it is possible to scan this with\na scanner or the camera of a mobile device and assign it to the sample.\nThis can be done after registration of a sample, with the\n## Custom\nBarcode/QR Code Update\noption under the\n## More..\ndrop down.\nThe custom barcode will appear in the\n## Identification Info\n. If a custom\nbarcode/QR code is registered, the print function shown above will print the\ncustom barcode /QR code, instead of the default one.\nGenerate batches of barcodes / QR codes\n\nIn some cases there is the need to generate several barcodes/QR codes that can be\nlater on assigned to samples registered in openBIS.\nTo generate new barcodes, go to the\nBarcodes/QR codes Generator\nin the main\nmenu under\n## Utilities\n.\nUsers can select:\nThe type of barcode to generate:\n## Code 128\nQR Code\nMicro QR code\nThe number of barcodes to generate\nThe layout:\n## Split\n: one barcode per page\n## Continuous\n: several barcodes in one page\nThe width of the barcode\nThe length of the barcode\nAfter selecting the desired parameters, click the\n## Generate Custom\n## Barcodes\nbutton.\nTo print the barcodes use the\nprint icon\non the form, next to\nGenerate Custom Barcodes/QR Codes\n. These barcodes can be printed on labels to\nbe attached to vials. When the samples are registered in openBIS, these\nbarcodes can be scanned and assigned to the samples as explained above.\nScan barcodes from mobile devices\n\nIt is also possible to scan barcodes and QR codes using the scan button\non top of the main menu, as shown below. In this way, you can scan a\nbarcode or QR code already associated with an entry and this will open\nthe entry page in openBIS. You can use a scanner or the camera of a\nmobile device. The selection you make is saved.\nUpdated on July 5, 2023\nPrinter and Barcode Scanner Requirements\n\n## Printers\n\nThere are several manufacturers of printers and different kinds of\nbarcodes and paper to adapt to different use cases. Most manufacturers\nhave their own proprietary printer driver and language for labels.\nTo allow freedom of choice for the barcode printer, the openBIS ELN-LIMS\nallows to configure both the type of barcodes and the layout and size of\nthe labels. openBIS uses this information to produce a PDF document,\nthus having as single requirement that the printer driver used allows to\nprint PDF documents using applications such as Adobe Acrobat Reader or\n## Preview (Mac).\n### Printer Configuration\n\nThere are different types of printer drivers. The two types we can\ndefine as generic are\n## PS\n(PostScript) (recommended) and\n## PCL\n(Printer Command Language). Printers with these drivers are likely to\nprint PDF documents and other types of documents with embedded fonts,\nimages, etc…\nThe printer paper type needs to be configured for each printer. Two\n## layouts are supported:\n## Split\n: The PDF will contain separate pages with each barcode.\n## Continuous\n: The PDF will contain a continuous layout with the\nbarcodes. More uncommon for this applications.\nThe printer paper size needs to be configured for each printer. It is\npossible to indicate the size of the barcode, so it can fit.\nPrinter testing\n\nWe provide two example documents that can be used to test the printer.\n## Split barcodes example PDF:\nprinter-test-code128-split-50-15\n## Continuous barcodes example PDF:\nprinter-test-code128-continuous-50-15\nPlease consider that these examples likely do not correspond to the\nparticular paper size of the printer being evaluated and as such the\nbarcodes may look squashed. In order to obtain optimal results, the\npaper size would need to be configured. However, for the test it is\nenough to verify that the printer can print those files.\nPrinter Advice before purchasing\n\nBefore purchasing a printer, we recommend to check with the manufacturer\nthat the barcode printer provides a general driver and that it can print\none of the documents provided as example above.\n## Tested Printers\n\nZebra ZD420\n## Scanners\n\nThere are several manufacturers of barcode scanners. In most cases\nscanners act as a keyboard for the computer, so when the barcode scanner\nscans a barcode it will type whatever has been scanned.\n### Scanner Configuration\n\nThe scanner keyboard layout should be the same as the computer used. If\nnot this could cause problems if there are any special characters.\nScanner testing\n\nOpen a notepad and scan the barcodes provided in the examples below. The\nscanner should read them and type the correct output.\n## Barcode Code 128.\nscanner-test-code128-50-15\n.\nThis should give as output “20210720122856003-454071” without\nquotes.\nBarcode QR Code.\nscanner-test-qrcode-50-50\n.\nThis should give as output “20210720122856003-454071” without\nquotes.\nBarcode Micro QR Code.\nscanner-test-microqrcode-30-30\n.\nThis should give as output “20210720122856003-454071” without\nquotes.\nScanner Advice before purchasing\n\nBefore purchasing a scanner, ensure that the barcode scanner provides a\nkeyboard driver and ask the manufacturer’s support to scan the examples\nabove.\n## Tested Scanners\n\nHoneywell 1902G-BF\nUpdated on July 27, 2022", "timestamp": "2025-09-18T09:38:30.080245Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_data-archiving:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-archiving.html", "repo": "openbis", "title": "Data archiving", "section": "Datasets", "text": "Data archiving\n\nDataset archiving\n\nopenBIS supports archiving of datasets to Strongbox\n(\nhttps://www.strongboxdata.com/\n) as\ndescribed in\n## Datasets\n## Archiving\nThis needs to be set up and configured on\nsystem level\n.\nTo trigger archiving manually from the ELN, navigate to a dataset and\nuse the\nRequest or disallow archiving\nbutton, as shown below.\nPlease note that the strongbox has a minimum size requirement of\n## 10GB\n. If a single dataset is below this threshold it will be queued\nfor archiving and it will be archived only when additional datasets in\nthe same\n## Space/Project/Experiment\nare selected for archiving and the\nminimum size is reached. All datasets are bundled together and archived\ntogether. This implies that if unarchiving is requested for one dataset\nin a bundle, all other datasets will also be unarchived.\nDataset archiving helper tool\n\nIf you wish to archive multiple datasets, you can use the\n## Archiving\n## Helper\ntool under\n## Utilities\nin the main menu. You can search for\ndatasets and select multiple ones to be archived, by clicking the\n## Request Archiving\nbutton on the top of the page.\nIt is possible to search datasets by size, by selecting\n## Property\nin\nthe\n## Field Type\n,\nSize (bytes)[ATTR.SIZE]\nin the\n## Field Name\nand the desired\nComparator Operator\n, as shown below.\nDataset unarchiving\n\nOnce the dataset is archived on tapes, the button on the dataset page\nchanges to\n## Unarchive\n, as shown below. Datasets can be unarchived by\nusing this button.\nDataset unarchiving helper tool\n\nTo unarchive several datasets it is possible to use the\n## Unarchiving\n## Helper\ntool, under\n## Utilities\nin the main menu, as shown below. You\ncan search for datasets and select multiple ones to be unarchived, using\nthe\n## Unarchive\nbutton on tope of the page.\nUpdated on April 25, 2023", "timestamp": "2025-09-18T09:38:30.082425Z", "source_priority": 2, "content_type": "procedure"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_data-export:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-export.html", "repo": "openbis", "title": "Export", "section": "Export", "text": "## Export\n\nExport to File\n\n## Export Lab Notebooks & Inventory Spaces\n\nAll levels of the\n## Lab Notebook\nand\n## Inventory\ncan be exported, using\nthe\n## Export\noption in the\n## More..\ndrop down, as shown below.\n## Space\n## Project\n## Experiment/Collection\n## Object\n## Dataset\nIn each case, the following export options are available:\nMake import compatible\n. If selected, datasets are exported in a\ndata\nfolder and are in a format ready to be uploaded in openBIS using the default eln-lims dropbox; the metadata are exported in a\nxlsx\nfolder which contains information in a format ready to be uploaded via the openBIS admin UI.\nExport metadata as PDF\n. Metadata are exported in a\nhierarchy\nfolder that keeps the folder structure of the ELN. At each level, one pdf file for each exported entity is generated.\nExport metadata as XLSX\n. Metadata are exported in one\nxlsx\nfolder. The folder contains the metadata of all exported entities and the corresponding masterdata in a\nmetadata.xlsx\nfile. If\nMake import compatible\nis selected, this file is suitable for re-import in openBIS. If not, the file contains some fields which are not compatible with re-imports. These fields are: PermId of entities, registrator, registration date, modifier, modification date. In addition to the metadata.xlsx file, the\nxlsx\n## folder might contain additional folders:\na\nscripts\nfolder, which contains scripts associated with types in the metadata.xlsx file, if these are present;\na\ndata\nfolder which holds the content of spreadsheet fields and large text fields that exceed the size of an Excel cell;\na\nmiscellaneous\nfolder which contain images embedded in text of exported entries, if present.\nExport data\n. The default maximum size of all datasets to be exported is 10GB. This can be configured by a system admin in the\nAS service.properties file\n. We recommend to use\nsftp\nto download large datasets.\n## If\nMake import compatible\nis selected, datasets are exported in a\ndata\nfolder in a format ready to be uploaded in openBIS using the default eln-lims dropbox. If not, the datasets are exported in a\nhiearchy\nfolder that matches the ELN hierarchy.\nInclude levels below from same space\n. If selected, all hierachy levels below the selected entity and belonging to the same Space are exported.\nInclude Object and Dataset parents from same space\n. If selected, Object parents and Dataset parents from the same Space are exported. Example: I export Object A, in Experiment A, in Space 1. Object B in Experiment B also in Space 1 is parent of Object A. When this option is selected, Object B is also exported, otherwise it is not.\nInclude Objects and Datasets parents and children from different spaces\n. This allows to export Object and Dataset parents and children that belong to a different Space than the Space from where Objects and Datasets are being exported. Example: I export Object A in Space 1, which has parents in Space 2. If this option is selected, the parents in Space 2 are also exported, otherwise they are not.\nWait for download to complete in browser\n. This is suitable when exporting only metadata or small datasets. When the dowload is ready, a zip file will be available to download from the browser.\nNote: ensure that pop-ups are not disabled in your browser\n.\nReceive results by email\n. If this option is selected, when the export is ready, you will receive an email notification with a download link.  Email notification needs to be configured on\nsystem level\nduring or after installation, as explained in\nConfigure Data Store\n## Server\nWe provide below a couple of examples of the export, to clarify how it works.\n1. Import-compatible export of a Space selecting all options\n\nWe select all options from the export widget, as shown below.\nWe export a Space called CATERINA in the Lab Notebook with all its sublevels (see below).\nOne Object in this Space has a parent in a Space called METHODS (see below).\nThe exported zip file contains 3 folders:\n## A.\ndata\nfolder\nThis contains the datasets in the correct format to be uploaded via eln-lims dropbox, as shown below.\n## B.\nhiearchy\nfolder\nThis contains folders that match the openBIS hierarchy (Space/Project/Experiment/Object).\nIn this case 2 Space folders are present:\n## CATERINA\n: is the exported space.\n## METHODS\n: contains an Object which is parent of an Object in the Space CATERINA. This was exported because the option\nInclude Objects and Datasets parents and children from different spaces\nwas selected for export.\nInside each folder, there is a pdf of the corresponding entity. Example:\nin the Space folder\n## CATERINA\nthere is a\nCATERINA.pdf\nfile that contains the metadata of the Space;\nin the Project folder\n## PROJECT_1\nthere is a\nPROJECT_1.pdf\nfile that contains the metadata of the Project;\nin the Experiment folder\nMy second experiment (PROJECT_1_EXP_1)\nthere is a\nMy second experiment (PROJECT_1_EXP_1).pdf\nfile with the metadata of the Experiment;\nin the Object folder\nStep A (EXP4)\nthere is a\nStep A(EXP4).pdf\nfile with the metadata of the Object and a\n20240726094631217-68.pdf\nfile that contains the metadata of the dataset that belongs to this Object.\n## C.\nxlsx\nfolder.\nThis contains:\na\nmetadata.xlsx\nfile which has the metadata of the exported entities and the corresponding masterdata (types and properties) in the correct format to be re-imported in another openBIS instance;\na\nscripts\nfolder that contains evaluation plugins associated to two types defined in the metadata.xlsx file. This folder is present only if the exported types have plugins associated with them.\na\ndata\nfolder that contains the information stored in the spreadsheet field of one of the Objects in this Space. This folder is present only if the exported entities contain information in spreadsheet or if there are text fields with more than 32,767 characters (this is the limit of the Excel cells).\na\nmiscellaneous\nfolder that contains images that are embedded in text fields of the exported entities. This folder is present only if exported entities contain images embedded in text.\n2. Non import-compatible export of a Space selecting all options\n\nWe export the same Space as described in Example 1, with all options selected, but the export this time is not import-compatible, as shown below.\nIn this case the exported zip file contains only 2 folders:\nhierarchy\nand\nxlsx\n. Data are exported inside the\nhierachy\nfolder, instead of being in a separate\ndata\nfolder.\n## A.\nhierarchy\nfolder\nThis contains the same folder structure as described above. In addition, in this case, inside the Object\nStep A (EXP4)\nfolder there is a\ndata\nfolder that contains the dataset belonging to this Object, as shown below. The metadata of the dataset is provided as a metadata.json file inside the data folder and as pdf file inside the Object folder (\nStep A (EXP4)\n).\n## B.\nxlsx\nfolder\nThis contains the same files and folders as described in Example 1 (see below). The only difference in this case is that the metadata.xlsx is not import-compatible. It contains some fields which are not compatible with openBIS re-import, as explained above.\nExport to Zenodo\n\nopenBIS provides an integration with the\n## Zenodo\ndata\nrepository (\nhttps://zenodo.org/).\nThis enables data direct data transfer from openBIS to Zenodo. First of\nall the connection to Zenodo needs to be configured on\nsystem level\nin the DSS service.properties (see\nHow to configure the openBIS\n## DSS)\nIf this is configured, a lab manager, who has admin rights for the\n## Settings,\nneeds to enable it in the ELN, as explained in\n## Enable\nTransfer to Data\n## Repositories\n.\n## Create Zenodo Personal Access Token\n\nIn order to be able to export data to Zenodo, you need a valid Zenodo\naccount. You also need to create a\npersonal access token.\nThis can\nbe done from the\n## Applications\nunder\n## Settings\nin Zenodo, as shown\n## below:\nSave Zenodo Personal Access Token in openBIS\n\nAfter creating the personal access token in Zenodo, this needs to be\nstored in openBIS, with the following procedure:\nGo to\n## User Profile\nunder\n## Utilities\nin the main menu.\nEnable editing.\nAdd the personal access token from Zenodo.\n## Save.\nExport data to Zenodo\n\nTo export data to Zenodo:\nGo to\n## Exports\n->\nExport to Zenodo\nunder\n## Utilities\nin\nthe main menu.\nSelect the data you want to export from the menu.\nEnter a\n## Submission\n## Title.\n## Click\n## Export Selected\non top of the export form.\nThe selected data are transferred as a zip file to Zenodo. You are\nnow redirected to Zenodo, where you should fill in additional\nmetadata information.\nPublish the entry in Zenodo.\nThe data exported to Zenodo is a .zip file that contains the metadata of the exported entries in 4 formats (.txt, .html, .doc, .json) and the data. The hiearchy (i.e.folder structure) used in the ELN is preserved in the exported .zip file.\nAfter you hit the\n## Publish\nbutton in Zenodo, a new entry with the\ndetails of this submission will be created in the\n## Publications\nfolder in the\n## Inventory\n. Please note that this may take a few\nminutes.\nExport data to Zenodo in a multi-group instance\n\nIf you export data from a multi-group instance where you have access to more than one group, you need to select the group under which the new publication entry should be created.\nIn the example below we see 3 group names: GENERAL, DEMO, TEST.\nIf you select GENERAL, the publication entry will be created under the PUBLICATION Space (if present).\nIf you select DEMO, the publication entry will be created under the DEMO_PUBLICATION Space.\nIf you select TEST, the publication entry will be created under the TEST_PUBLICATION Space.\nExport to ETH Research Collection\n\n## The\nETH Research Collection\nis a FAIR repository for publications and research data provided by ETH\nZurich to its scientists.\nData can be uploaded to the ETH Research Collection\nonly by members of\nETH Zurich\n. This export feature is only available to ETHZ members.\nTo export data to the ETH Research Collection:\nGo to\n## Utilities\n->\n## Exports\n->\nExport to Research\n## Collection\n.\nSelect what to export from the tree.\nSelect the\n## Submission Type\nfrom the available list:\n## Data\ncollection, Dataset, Image, Model, Sound, Video, Other Research\n## Data\n.\nSelect the\n## Retention Period\nthat will be used in the ETH\n## Research Collection:\n10 years, 15 years, indefinite.\nThis is time\nfor which the data will be preserved in the Research Collection.\nClick the\n## Export Selected\nbutton on top of the page.\nThe selected data are transferred as zip file to the ETH Research\nCollection. You will be redirected to the ETH Research Collection\nand will need to complete the submission process there.\nThe data exported to the Research Collection is a .zip file that contains the metadata of the exported entries in 4 formats (.txt, .html, .doc, .json) and the data. The hiearchy (i.e.folder structure) used in the ELN is preserved in the exported .zip file.\nA new entry with the details of this submission will be created in the\n## Publications\nfolder in the\n## Inventory\nafter the submission\nprocess in complete. This may take a few minutes.\nThe size limit for one single export to the ETH Research Collection is\n## 10GB.\nExport data to the ETH Research Collection in a multi-group instance\n\nIf you export data from a multi-group instance where you have access to more than one group, you need to select the group under which the new publication entry should be created. See explanation in section\nExport data to Zenodo in a multi-group instance\nabove.", "timestamp": "2025-09-18T09:38:30.086089Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_data-upload:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-upload.html", "repo": "openbis", "title": "Data Upload", "section": "Experiments", "text": "Data Upload\n\nData can be uploaded to Datasets in openBIS to\n## Experiments\nand\n## Objects\n(e.g.,\n## Experimental Steps\n). openBIS is agnostic of file formats and types.\nSmall data files can be uploaded via the web user interface, larger data files can be uploaded via dropbox mechanism.\nData upload via web UI\n\nTo upload data via the web interface:\n1.Click the\n## Upload\nbutton in the form, as shown below.\n2. Select the dataset type (e.g. Attachment).\n3. Fill in the relevant fields in the form. It is advisable to always\nenter a\n## Name\n, because this is shown in the menu. If the name is not\nprovided, the dataset code is shown.\n4. Drag and drop files in the\n## Files\n## Uploader\narea or browse for\nfiles.\n5. When uploading a zip file, the option to\nuncompress before\nimport\nwill be presented in the form.\n6.\n## Save.\nNote for MacOS users:\nthe default MacOS archiver generates hidden\nfolders that become visible in openBIS upon unarchive. To avoid this\n## there are two options:\nZip using  the following command on the command-line:\nzip\n-r folder-name.zip\nfolder-name/\\*\n-x\n“\\.DS\\_Store”\nUse an external archiver (e.g. Stuffit Deluxe).\nUpdated on March 23, 2023\nData upload via dropbox\n\nWeb upload of data files is only suitable for files of limited size (few GB). To upload larger data, openBIS uses dropbox scripts that run in the background (see\n## Dropboxes\n). A default dropbox script is provided with the openBIS ELN-LIMS plugin, and the dropbox folder needs to be set up by a\nsystem admin\n.\nIf this is available, users need to organise their data in a specific way:\n## Folder 1\n## Data\n(can be single files or folders)\n## Folder 1\nneeds to have a specific name that encodes the information\nof where the data should be uploaded to openBIS.\nThe name of\n## Folder 1\ncan be generated from the ELN interface:\nFrom the page where you want to upload data, select\nDataset upload\nhelper tool for eln-lims dropbox\nfrom the\n## More…\ndropdown and\nfollow the instructions on screen.\n## Select:\nThe dataset type from the list of available types (mandatory);\nEnter the name of your dataset (optional, but recommended);\nCopy the generated name of the folder using the copy to clipboard icon.\n3. In your finder/explorer, create a new folder and paste the name you\ncopied from openBIS. Place your data in this folder.\n4. Place this folder containing your data inside the\neln-lims-dropbox\nfolder. openBIS continuously monitors this folder\nand when data are placed here, they are\nmoved\nto the final storage.\nThe move happens after a predefined (and customisable) inactivity period\non the eln-lims-dropbox folder.\nDropbox with markerfile\n\nIn case of uploads of data >100GB we recommend to configure the\neln-lims-dropbox-marker\n. The set up and configuration need to be\ndone by a\nsystem admin\n. The process of data preparation is the same as\ndescribed above, however in this case the data move to the openBIS final\nstorage only starts when a markerfile is placed in the\neln-lims-dropbox-marker folder. The marker file is an empty file with\nthis name:\n.MARKER_is_finished_\n.\nPlease note the “.” at the start of the name, which indicates that this is a hidden file. This file should also not have any extension. For example, if the folder to be uploaded has the following name:\n## O\n+\n## BARILLAC\n+\n## PROJECT\n\\\n_1\n+\n## EXP1\n+\n## RAW\n\\\n## _DATA\n+\ntest\nThe marker file should be named:\n.MARKER_is_finished_O+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test\nHow to create the Marker file in Windows\n\nYou can create the Marker file in Windows using a text editor such as\n## Editor\n. Any other text editor will  also work.\nopen\n## Editor.\nSave the file with a name such as\n.\nMARKER_is_finished_O+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test.\nThe file is automatically saved with a “.txt” extension. This needs\nto be removed.\nUse the\n## Rename\noption to remove the extension from the file.\nHow to create the Marker file on Mac\n\nIf you are not familiar with the command line, you can create an empty\ntext file using for example the\nTextEdit\napplication in a Mac. Any\nother text editor will also work.\nOpen the\nTextEdit\napplication and save an empty file with a name\nsuch as\n.MARKER_is_finished_O+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test\n.\nSave to any format.\nYou will get a message to say that files starting with “.” are\nreserved for the system and will be hidden. Confirm that you want to\nuse “.”\nTo show these hidden files, open the Finder and press\nCommand +\nShift + . (period)\n.\nThe file you saved before has an extension, that needs to be\nremoved. If the extension is not shown in your Finder, go to Finder\nPreferences menu, select the Advanced tab, and check the “Show\nall filename extensions” box.\nRemove the extension from the file.\nDropbox monitor\n\nIt is possible to check the status of the upload via dropbox using the\n## Dropbox Monitor\nunder\n## Utilities\nin the main menu.\nThe Dropbox Monitor shows a table with all available dropboxes for a\ngiven openBIS instance. By default,\ndefault-dropbox, eln-lims-dropbox\nand eln-lims-dropbox-marker\nare shown.\nIf data are uploaded in a dropbox folder, users can see the status of\nthe data upload in the table. A red face in the column\n## Last Status\nindicates a failure of data import, a green face indicates successful\ndata import.\nIf you click on the row of the table above, you can see the details of\nevery upload attempt for a given dropbox, as shown below. For failures,\nthe log with the error is shown.\nRegistration of metadata for datasets via dropbox\n\nStarting from openBIS version 20.10.2, the default eln-lims dropbox\nsupports the registration of metadata for datasets. The metadata needs\nto be provided in a file called\nmetadata.json.\nThis file should be\nplaced inside the folder with the openBIS-generated name described\nabove, together with the data. This is shown in the example below.\nO+BARILLAC+PROJECT_1+EXP1+RAW_DATA+test\nis the folder with the openBIS-generated name. Inside this folder there\nis the metadata.json file, and the data, which consists of a few files\nand 2 folders.\nFor example, the metadata.json file for the default RAW_DATA dataset\n## type would be:\n## { “properties” :", "timestamp": "2025-09-18T09:38:30.090360Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_data-upload:1", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/data-upload.html", "repo": "openbis", "title": "Data Upload", "section": "Other Tools", "text": "{ “$NAME” : “my raw data”,\n\n“NOTES” : “This is a test for metadata upload via dropbox” }\n\n}\nIt is possible to download the template metadata.json file for each\ndataset type from the\n## Other Tools\nsection under the\n## Utilities\nin\nthe main menu.\n## In\n## Other Tools\n, there is also the\nShow available storage space\nbutton, which shows the available storage space on the openBIS instance.\nThis is helpful in calculating how much space one might require for\nfuture data upload, especially large data.\nUpdated on April 26, 2023", "timestamp": "2025-09-18T09:38:30.090360Z", "source_priority": 2, "content_type": "general"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_ELN-types:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/ELN-types.html", "repo": "openbis", "title": "ELN types", "section": "Storage", "text": "ELN types\n\nStandard types\n\nWhen the eln-lims plugin is enabled the following types are installed by default.\nObject types\n\nGeneral protocol\n## Storage\nStorage position\n## Product\n## Supplier\n## Order\n## Request\n## Publication\nCollection types\n\n## Collection\nDataset types\n\nELN preview\nRaw data\nProcessed data\nAnalyzed data\n## Attachment\nOther data\nSource code\nAnalysis notebook\nPublication data\nBasic default types\n\nThe following Object types are created if the\neln-lims-template-types\nis enabled in core plugins. This can be enabled by a\nsystem admin\nwhen openBIS is first installed (see\ninstallation steps\n) or at any time afterwards.\n## Entry\n## Experimental Step\n## Default Experiment\nLife science types\n\nThe following Object types are provided with the\neln-lims-life-science\ndata model which can be downloaded from\nCommunity data model\n. An openBIS\ninstance admin\ncan upload these types from the admin UI, as explained\nhere\n.\n## Antibodies\n## Chemicals\n## Enzymes\n## Media\nSolutions and Buffers\n## Plasmids\n## Plants\n## Oligos\n## RNA\n## Bacteria\nCell lines\n## Flies\n## Yeasts\nGeneral protocols\nPCR protocol\nWestern blotting protocols", "timestamp": "2025-09-18T09:38:30.093366Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_general-overview:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/general-overview.html", "repo": "openbis", "title": "General Overview", "section": "General Overview", "text": "## General Overview\n\nThe openBIS platform has three primary functionalities:\nInventory management\nof laboratory samples, materials,\nprotocols, equipment.\nLaboratory notebook\n, to document lab experiments.\nData management\n, to store all data related to lab experiments\n(raw, processed, analysed data, scripts, Jupyter notebooks, etc.).\nIt is possible to use all functionalities or only selected ones.\nIn the most general use-case, the\n## Inventory\nis shared by all lab\nmembers, so everyone can access information about available lab\nmaterials and regularly used protocols.\nIn addition, every lab member has a personal folder in the\n## Lab\nnotebook\n, where to organise projects and experiments. This folder can\nbe shared with other lab members or collaborators with openBIS access.\nExperimental steps described in the lab notebook can be linked to\nprotocols and samples stored in the inventory. Experimental steps can\nalso be linked to each other.\n## Data\nof any sort can be attached to the corresponding Experimental\nstep in different ways, depending on the size.\nData can be exported to data repositories, such as\n## Zenodo\nor the\nETH Research\n## Collection\n(for ETHZ users\nonly).\nThis allows to have the complete overview of workflows and information,\nfrom initial data generation to data analysis and publication.\nThe openBIS ELN interface can be accessed via a URL of this type:\nhttps://openbis-xxx/openbis/webapp/eln-lims/\nwhere\nopenbis-xxx\nis the name of the server specified in the openBIS\nconfiguration file, during the installation by a system admin.\n## Login\n\nFile based and/or LDAP authentication\n\nWhen file based and/or LDAP authentication are used in openBIS, the login interface is as shown below. Users need to provide their username and password to login.\nOnly registered users with assigned rights can login to openBIS.\nSWITCHaai authentication\n\nWhen SWITCHaai (SSO) authentication is used in addition to file based and/or LDAP authentication, the login interface is as shown below.\nSWITCHaai is selected by default. In this case, users need to click on\n## Login\nand they will be redirected to the SWITCHaai login page.\nIf a user would like to authenticate with a file-based account or LDAP (depending on system configuration), they need to select\n## Default Login Service\nfrom the dropdown and provide username and password.\nopenBIS also supports SWITCH edu-id authentication and the login process is the same as described in this section.", "timestamp": "2025-09-18T09:38:30.095681Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/index.html", "repo": "openbis", "title": "General Users", "section": "General Users", "text": "## General Users\n\n## General Overview\n## Login\nFile based and/or LDAP authentication\nSWITCHaai authentication\nELN types\nStandard types\nObject types\nCollection types\nDataset types\nBasic default types\nLife science types\nInventory Of Materials And Methods\n## Customise Collection View\nRegister single entries in a Collection\nBatch register entries in a Collection\nBatch registration via Excel template file\n## Codes\nControlled vocabularies\nAssign parents\nDate format\nRegister storage positions and samples in the same XLS file\nBatch registration via TSV template file\nRules to follow to fill in the template .tsv file\nAdvantages of XLS batch registration vs the old batch registration\nBatch register entries in several Collections\nXLS Batch Register Objects\nTSV Batch Register Objects\nBatch update entries in a Collection\nXLS Batch Update Objects\nTSV Batch Update Objects\nBatch update entries in several Collections\nXLS Batch Update Objects\nTSV Batch Update Objects\nCopy entries\nMove entries to a different Collection\nMove from entry form\nMove from Collection Table\nRegister Protocols in the Methods Inventory\nLINKS TO SAMPLES, MATERIALS, OTHER PROTOCOLS\n## Managing Storage Of Samples\nAllocate storage positions to samples\nRegister storage position for a single sample\nAdd additional metadata to storage positions\nBatch register storage positions\nXLS Batch Registration\nBatch Registration with TSV file\nBatch update storage positions\nDelete storage positions\nDelete single storage positions\nRemove one of multiple positions in the same box\nDelete multiple storage positions\n## Overview of lab storages\n## Overview of lab Storages\nChange storage position of samples\nBarcodes and QR codes\nBarcodes and QR codes\nBarcodes for individual samples\nGenerate batches of barcodes / QR codes\nScan barcodes from mobile devices\nPrinter and Barcode Scanner Requirements\n## Printers\n### Printer Configuration\nPrinter testing\nPrinter Advice before purchasing\n## Tested Printers\n## Scanners\n### Scanner Configuration\nScanner testing\nScanner Advice before purchasing\n## Tested Scanners\n## Lab Notebook\n## Register Projects\n## Register Experiments\nRegister a Default Experiment:\nRegister a Collection:\n## Register Experimental Steps\n## Comments Log\nAdd parents and children to Experimental Steps\nAdding a parent\nAdding a parent of a predefined type in the form\nAdding parent of any available type\nAdding parent via barcodes\nRemoving a parent\nAdding and Removing Children\n## Children Generator\nParent-child relationships between entries in lab notebook\nHow to use protocols in Experimental Steps\n## Move Experimental Steps\n## Copy Experimental Steps\nUse templates for Experimental Steps\nDatasets tables\nData Access\n## Example of SFTP Net Drive connection:\nExample of Cyber Duck configuration\nExample of  Dolphin File Manager configuration\nSFTP access via session token\n## Move Datasets\nMove one Experiment to a different Project\n## Project Overview\nEdit and Delete Projects, Experiments, Experimental Steps\nShare Lab Notebooks and Projects\n## Rich Text Editor\n## EMBED IMAGES IN TEXT FIELDS\nData Upload\nData upload via web UI\nData upload via dropbox\nDropbox with markerfile\nHow to create the Marker file in Windows\nHow to create the Marker file on Mac\nDropbox monitor\nRegistration of metadata for datasets via dropbox\n## Export\nExport to File\n## Export Lab Notebooks & Inventory Spaces\n1. Import-compatible export of a Space selecting all options\n2. Non import-compatible export of a Space selecting all options\nExport to Zenodo\n## Create Zenodo Personal Access Token\nSave Zenodo Personal Access Token in openBIS\nExport data to Zenodo\nExport data to Zenodo in a multi-group instance\nExport to ETH Research Collection\nExport data to the ETH Research Collection in a multi-group instance\nData archiving\nDataset archiving\nDataset archiving helper tool\nDataset unarchiving\nDataset unarchiving helper tool\n## Search\nAdvanced search\nSearch for: All\nSearch for: Experiment/Collection\nSearch for: Object\nSearch for: Dataset\nSearch for: specific Object Type (e.g Experimental Step)\n## Search Collection\n## Search\nGlobal search\nBLAST search\nData Set File search\nSave and reuse searches\n## Additional Functionalities\nPrint PDF\n## Visualise Relationships\n## Tables\n## Filters\n## Sorting\n## Exports\n## Columns\n## Spreadsheets\nText fields\nSelection of entries in table\nBrowse Entries by Type\n## Trashcan\n## Visualize Available Storage Space\n## Vocabulary Browser\n## Freeze Entities\nHow to freeze an entity\nNavigation menu\n## Custom Imports\nEntity history\nHistory table for Collections\nHistory table for Objects\nHistory table for Datasets\n## Spreadsheet\n## Session Token\nManaging Lab Stocks and Orders\n## STOCK CATALOG\nBuilding the catalog of products and suppliers\nCatalog of suppliers\nCatalog of products\nCreating requests for products to order\n## STOCK ORDERS\nProcessing product orders from requests\nTools For Analysis Of Data Stored In Openbis\n## Jupyter Notebooks\nHow to use Jupyter notebooks from openBIS\n## Overview of Jupyter notebook opened from openBIS.\nWhat to do in case of invalid session token\nUsing a local Jupyter installation with openBIS\nMATLAB toolbox", "timestamp": "2025-09-18T09:38:30.099174Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_inventory-of-materials-and-methods:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/inventory-of-materials-and-methods.html", "repo": "openbis", "title": "Inventory Of Materials And Methods", "section": "Materials", "text": "Inventory Of Materials And Methods\n\nThe default Inventory contains two folders:\n## Materials\nand\n## Methods\n.\nThese are used to organise respectively samples and materials of any type and lab protocols.\nSamples, materials and protocols are modelled in openBIS as\n## Objects.\nIn the\nopenBIS ELN-LIMS for life sciences\n, the following Object types are provided:\nAntibodies, Chemicals, Enzymes, Media, Solutions and Buffers, Plasmids, Plants, Oligos, RNAs, Bacteria, Cell lines, Flies, Yeasts, General protocols, PCR protocols, Western blotting protocols.\nThese Objects are organised in\n## Collections\nin the\n## Materials\nand\n## Methods\nsections of the Inventory.\nThe generic openBIS ELN-LIMS only has one predefined Object type for the Inventory,\n## General Protocol\n, in the General Protocols Collection in the Methods folder. The Material folder is empty. Additional Object types and Collections must be created by an openBIS instance admin, based on the needs of the lab(s).\n## Customise Collection View\n\nIt is possible customise the view of\n## Collections\nin the ELN.\nThe default\n## Collection\ncan have a\n## Form View\nor a\n## List View\n.\nDepending on this selection, the collection view will be different.\n## Form View:\nThis shows the metadata of the\n## Collection\nalong with\nthe table of objects. This view is useful when a user wants to see\nspecific metadata for a\n## Collection\n.\nIf you do not see the table with the Objects in the form, you need to\nenable this by selecting\n## Show Objects\nfrom the\n## More..\ndropdown\n## List View:\nThe metadata of the\n## Collection\nis not shown in this\nview, but only the table of objects is shown.\nIn this case a user would need to click on\n## More..\n, and\n## Edit\n## Collection\nin order to see the metadata and be able to edit the\n## Collection\n.\nUpdated on April 25, 2023\nRegister single entries in a Collection\n\nIn this example, we will see how to register one\n## Object\nof type\n## Sample\nin the\n## Raw Samples\n## Collection.\nThe same procedure\nshould be followed to register any other\n## Object\nin other\n## Collections\n.\nClick on the\n## Raw Samples\n## Collection\nfolder in the main menu.\nClick the\n## New Sample\nin the main page\nFill in the form\n## Save\nPlease note that the\nObject type\nshown in the\n## +New\nbutton (in this\ncase\n## Sample\n), is what is defined as\ndefault object type\nfor the\n## Collection\n. If this is missing in the\n## Collection,\nthe button will\nnot be present.\nTo register a different object type in the Collection:\n## Select\n## New Object\nfrom the\n## More\ndrop down menu (as shown\nbelow)\nSelect the relevant\nObject type\nfrom the list\n## (Sample,\nin this case).\nFill in the form\n## Save\nUpdated on April 25, 2023\nBatch register entries in a Collection\n\nIt is possible to register several samples at once via file upload. Two\n## methods are currently available:\nBatch registration via Excel template file (XLS Batch Register\n## Objects)\nBatch registration via TSV template file (TSV Batch Register\n## Objects)\n## Warning\nIn openBIS versions prior to 20.10.6, the XLS batch registration is not\nrecommended to register several hundreds of entries. The use of the TSV\nbatch upload to register several hundreds of entries is recommended in\nthose cases.\nBatch registration via Excel template file\n\nTo register several entries of the same type with an Excel file:\nNavigate to the relevant collection (e.g.\n## Samples\n).\n## Select\nXLS Batch Register Objects\nfrom the\n## More\ndrop-down menu (see figure above)\nDownload the\ntemplate\nfile and fill in the relevant information.\n## (Example file:\nSAMPLE-COLLECTION-REGISTRATION-SAMPLE-STORAGE_POSITION-template\n)\nUpload the file.\n## Codes\n\nIn most cases,\n## Object\ntypes have the option to auto-generate codes set\nto true in the admin UI. In this case, openBIS automatically generates\ncodes and identifiers when\n## Objects\nare registered. If that is not the\ncase, the code needs to be manually entered by the users in the Excel\ntemplate. The current template does not have a\n## Code\ncolumn. This can\nhowever be manually added if codes should be provided by the user and\nnot automatically generated by openBIS.  If codes should be manually\nentered and are missing, openBIS will show the error message\n“\nUserFailureExceptionmessage: Code cannot be empty for a non auto\ngenerated code.\n”\nControlled vocabularies\n\nFor Controlled Vocabularies fields, i.e. fields with a drop down menu,\nyou can enter either the\ncode\nor the\nlabel\nof the terms in the\nExcel file.\nPlease note that codes are not case-sensitive, but labels are.\nCodes and labels of vocabulary terms can be seen under\n## Utilities -> Vocabulary Browser\n.\nAssign parents\n\nAssign already existing parents\nIf the parents you want to assign to your Objects are already registered\nin openBIS, in the\n## Parents\ncolumn of the Excel file, you can assign\nthe relationship, by providing the identifier of the parent (i.e. /SPACE\ncode/PROJECT code/OBJECT code). If you want to add multiple parents to\none Object, every identifier should be in a new line in the\ncorresponding Excel cell. A new line in an Excel cell is entered with\nthe keyboard shortcuts\n## Alt\n+\n## Enter.\n## Example file:\nSAMPLE-COLLECTION-REGISTRATION-ANTIBODY-STORAGE_POSITION-template\n## Note:\nno other separators (e.g “,” or  “;”) should be used,\notherwise an error will be thrown.\nRegister Objects and assign parents in the same batch registration\nprocess.\nIf you want to register a few\n## Objects\nand at the same time establish a\nparent-child relationship between some of them, you can do so by using\nthe\n$\nand\n## Parents\ncolumns. In the example below we want to\nregister 2\n## Objects\n, antibody 1 and antibody 2. We want to assign\nantibody 1 as parent of antibody 2. In the\n$ column\ncorresponding to\nantibody 1 we need to enter numbers or letters proceeded by the $ symbol\n(i.e. $1, or $parent1). In the\n## Parents\ncolumn of antibody 2, we need\nto use the same value used in the\n$ column\nfor antibody 1.\nDate format\n\nFor date fields, the expected format is YYYY-MM-DD.\nRegister storage positions and samples in the same XLS file\n\n## A\nsample\nand its\nstorage\nposition\ncan be registered\ntogether, as shown in the template provided above:\nThe info in the\n$\ncolumn of the\nsample\nspreadsheet should\nmatch the\n## Parents\ncolumn in\n## Storage Positions\nspreadsheet.\nIn the $ column you can enter numbers or letters proceeded by the $\nsymbol (i.e. $1, $2 or $parent1, $parent2)\n.\nBatch registration via TSV template file\n\n## Select\n## TSV\n## Batch Register Objects\nfrom the\n## More\ndrop-down menu\nSelect the\n## Object\ntype (E.g. Sample or Storage)\nDownload the\ntemplate\nfile and fill in the relevant information\nUpload the file\nRules to follow to fill in the template .tsv file\n\n## Identifiers\n## :\nIdentifiers are given by\n/SPACE code/PROJECT code/OBJECT\ncode\n, e.g\n## /MATERIALS/EQUIPMENT/INS1\n. Users can provide\ntheir own identifiers, or these can be automatically generated\nby openBIS.\nTo have identifiers automatically generated by openBIS,\ncompletely remove the\nidentifier\ncolumn from the file.\n## Lists\n. In fields that have lists to choose from (called\n## Controlled Vocabularies\n), the code of the term needs to be\nentered. Term codes can be seen under\n## Utilities -> Vocabulary\n## Browser\n.\n## Parents\n. Use the following syntax to enter parents:\nidentifier1, identifier2, identifier3.\nParents annotations\n. Use the following syntax to annotate\n## parents:\nidentifier:xxx;COMMENTS:xxxx\\identifier:yyy;COMMENTS:yyyy\n## . Where\n## COMMENTS\nis the property used for the annotation in this case\n(to be replaced with the actual property used).\nDate fields\n. The expected syntax for dates is YYYY-MM-DD.\nAdvantages of XLS batch registration vs the old batch registration\n\nXLS batch registration uses labels instead of codes in the column\nheaders in the template file.\nFields which are Controlled Vocabularies can use labels instead of\ncodes.\nThe template can be used as it is, and no modifications are\nnecessary by removing the identifier column, as it was in case of\nthe old batch registration.\nUpload of samples and storage positions can now be performed using\nsingle template file.\n## The\nold\nbatch register mode is being maintained for backward\ncompatibility and will be phased out.\nUpdated on April 25, 2023\nBatch register entries in several Collections\n\nIt is possible to batch register\n## Objects\nthat belong to different\n## Collections\n.\nThis can be done from the\n## Object Browser\npage, under\n## Utilities\n.\n## Two options are available:\nXLS Batch Register Objects\n: batch registration via Excel\ntemplate file.\nTSV Batch Register Objects\n: batch registration via .tsv template\nfile.\nXLS Batch Register Objects\n\nThis option for batch registration is available since openBIS version\n20.10.3. It allows to register\n## Objects\nof different types to multiple\n## Collections\n.\nYou can select which types you want to register from the list of\navailable types.\nYou can then download the template that will allow you to register\n## Objects\nof the selected types to single or multiple\n## Collections\n## . The\n## Space, Project, Collection\nneed to be entered in the file. The\ncomplete path for\n## Projects\nand\n## Collections\nneed to be used, as shown\nin this example file:\nSAMPLE-GENERAL-REGISTRATION-EXPERIMENTAL_STEP-MASS_MEASUREMENT-SAMPLE-template\nTSV Batch Register Objects\n\nThe batch registration via .tsv file allows to batch register only one\ntype of\n## Object\nat a time.\n## Objects\nhowever can be registered to\nseveral\n## Collections\n.\nThis batch upload method is kept for backward compatibility, but it will\nbe phased out.\nIn this case, if\n## Objects\nare to be registered to multiple\n## Collections\n, an\nidentifier\nfor the\n## Objects\nneeds to be provided,\nas shown below. This is not the case with the XLS batch registration,\nwhere identifiers can be automatically generated by openBIS.\nUpdated on April 25, 2023\nBatch update entries in a Collection\n\nIt is possible to modify the values of one or more fields in several\nobjects simultaneously via batch update. This can be done in two ways:\nXLS Batch Update Objects\nTSV Batch Update Objects\nXLS Batch Update Objects\n\nNavigate to the relevant collection (e.g.\n## Raw Samples\n).\nIn the Collection table, from the\n## Columns,\nselect\n## Identifier\nand the field(s) you want to update (e.g.\n## Source\n), as shown\nbelow\n3. If you have several entries you can filter the table\n(see\n## Tables\n)\n4.\n## Export\nthe table choosing the options\nImport Compatible= YES;\nSelected Columns; All pages/Current page/Selected rows\n(depending on\nwhat you want to export)\n.\n5. Modify the file you just exported and save it.\n## 6. Select\nXLS Batch Update Objects\nfrom the\n## More..\ndropdown\n6. Upload the file you saved before and click\n## Accept\n. Your entries\nwill be updated.\n## Note\n## :\nIf a column is removed from the file or a cell in a column is left empty\nthe corresponding values of updated samples will be preserved.\nTo delete a value or a parent/child connection from openBIS one needs to\nenter\ninto the corresponding cell in the XLS file.\nTSV Batch Update Objects\n\nNavigate to the relevant collection (e.g.\n## Raw Samples\n).\n## Select\n## TSV\n## Batch Update Objects\nfrom the\n## More…\ndropdown.\nSelect the relevant\n## Object\ntype\n, e.g.\n## Sample\nDownload the available\ntemplate\nFill in the\nidentifiers\nof the objects you want to update\n(identifiers are unique in openBIS. This is how openBIS knows what to\nupdate). You can copy the identifiers from the identifier column in the\ntable and paste them in the file. Identifiers have this format:\n## /MATERIALS/SAMPLES/SAMPLE1.\nFill in the values in the columns you want to update\nSave the file and upload it via the\nXLS Batch Update\n## Objects\nfrom the\n## More..\ndropdown\n## Note\n## :\nIf a column is removed from the file or a cell in a column is left empty\nthe corresponding values of updated samples will be preserved.\nTo delete a value/connection from openBIS one needs to enter\n## _ _DELETE_ _\ninto the corresponding cell in the file.\nUpdated on April 25, 2023\nBatch update entries in several Collections\n\nIt is possible to batch update\n## Objects\nthat belong to different\n## Collections\n.\nThis can be done from the\n## Object Browser\npage, under\n## Utilities\n.\n## Two options are available:\nXLS Batch Update Objects\n: batch update via Excel template file.\nTSV Batch Update Objects\n: batch update via .tsv template file.\nXLS Batch Update Objects\n\nThis option for batch update is available since openBIS version 20.10.3.\nIt allows to update\n## Objects\nof different types that belong to\ndifferent\n## Collections\n.\nYou can select which types you want to update from the list of available\ntypes.\nYou can then download the template that will allow you to update\n## Objects\nof the selected types to single or multiple\n## Collections\n## . The\n## Space, Project, Collection\nneed to be entered in the file. The\ncomplete path for\n## Projects\nand\n## Collections\nneed to be used. In\naddition, identifiers for the\n## Objects\nneed to be provided: identifiers\nare unique in openBIS, by providing them openBIS will know which\n## Objects\nhave to be updated. Example file:\nSAMPLE-GENERAL-REGISTRATION-EXPERIMENTAL_STEP-MASS_MEASUREMENT-SAMPLE-template\nTSV Batch Update Objects\n\nThe batch update via .tsv file allows to batch update only one type of\n## Object\nat a time. However, it is possible to update\n## Objects\nthat\nbelong to several\n## Collections\n.\nThis batch update method is kept for backward compatibility, but it will\nbe phased out.\n## The\n## Space, Project, Collection\nneed to be entered in the file. The\ncomplete path for\n## Projects\nand\n## Collections\nneed to be used. In\naddition, identifiers for the\n## Objects\nneed to be provided: identifiers\nare unique in openBIS, by providing them openBIS will know which\n## Objects\nhave to be updated.\nUpdated on April 25, 2023\nCopy entries\n\nTo create a copy of an existing entry, select\n## Copy\nfrom the\n## More..\ndrop down menu in the\n## Collection\npage.\nWhen an entry is copied, the user has the option to\nlink parents\n,\ncopy children into the Parents’ collection\nand\ncopy the comments\nlog\n.\nAll these options are disabled by default.\nUpdated on July 27, 2022\nMove entries to a different Collection\n\nYou can move entries to a different\n## Collection\neither from the e\nntry\nform or from a\n## Collection\ntable.\nMove from entry form\n\nTo move entries to a different\n## Collection\n, select\n## Move\nfrom the\n## More…\ndrop down menu in the entry form.\nYou have the option to move to an existing\n## Collection\nor to create a\nnew\n## Collection\n.\nMove from Collection Table\n\nIt is also possible to move objects from\n## Collection\ntables. You can\nselect one or multiple entries from a table and click on the\n## Move\nbutton.\nAlso in this case you can move to an existing\n## Collection\nor create a\nnew one.\nUpdated on July 27, 2022\nRegister Protocols in the Methods Inventory\n\nProtocols are standard operating procedures (SOPs) used in the lab. If such procedures are in place, they should be organised in folders in the Methods Inventory which, by default, is accessible by all lab members.\nopenBIS provides a General Protocol Object type that can be used. If different specific metadata is needed for protocols, new Object types can be created by an Instance admin in the admin UI and the corresponding Collections can be created in the ELN UI.\nTo register a new General Protocol in the General Protocols folder, follow these steps:\nGo to the General Protocols Collection in the Methods folder.\nClick the + New General Protocol button in the main page.\nFill in the relevant fields in the form or choose from available templates.\n## Save\nLINKS TO SAMPLES, MATERIALS, OTHER PROTOCOLS\n\nWhen writing a protocol, it is possible to create links to samples, materials or other protocols stored in the Inventory. These are parent-child relationships in openBIS.\nEverything that is used in the protocol can be added as Parent of the protocol itself. This can be done as described fo Experimental Steps:\nAdd parents and children to Experimental Steps", "timestamp": "2025-09-18T09:38:30.104193Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_lab-notebook:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/lab-notebook.html", "repo": "openbis", "title": "Lab Notebook", "section": "Lab Notebook", "text": "## Lab Notebook\n\nIn the most common use-cases, the\n## Lab Notebook\npart of the openBIS ELN-LIMS contains a personal Space (i.e. folder) for each scientist. Within this\n## Space\n, scientists can organise their work using the openBIS\nProjects, Experiments and Objects.\nAn openBIS\n## Experiment\nis defined as a specific scientific question. The individual attempts to answer this question, are\n## Objects\nof type\n## Experimental Step\n. At this level, the user can create links to\nmaterials\nand\nmethods\nregistered in the Inventory that were used to perform the\n## Experimental Step\n. These are entered as\n## Parents\nof the\n## Experimental Step\n. All data produced in the\n## Experimental Step\nand further processed and analysed can be added at this level.\nIt is also possible to organise the Lab Notebook on\n## Projects\n, rather than on personal Spaces. This should be configured by an\nInstance admin\n.\n## Register Projects\n\nIn a personal folder, users can register one or more\n## Projects\nthey\ncurrently work on.\n## Projects\nin openBIS only have a\n## Description\nfield, no additional fields can be added.\nNavigate to the relevant\n## Space\nin the\n## Lab Notebook\nmenu and click the\n## + New Project\nShould you have an empty page, select\n## Show Identification Info\nand\n## Show Description\nfrom the\n## More…\ndropdown\n## Projects\ndo not have a\n## Name\nfield, but only\n## Code\n. Codes can only take alphanumeric characters and no spaces. Codes are prettified in the Main Menu.\nEnter a\n## Description\nfor the project.\n## Click\n## Save\non top of the form.\nIn the\n## More…\ndropdown you have additional options on what you can do\nin the Project folder, as shown below.\n## Register Experiments\n\nInside one\n## Project\n, a user can register several\n## Experiments\n, which can in turn be divided into single\n## Experimental Steps.\nopenBIS provides by default 2 options for registering Experiments:\n## Default Experiment\n: The form of the Default Experment contains several metedata fields that can be filled in by the user.\n## Collection\n: This form has limited metadata fields. It should be considered as a folder, to be used in cases where a user only needs to group subsequent steps, and does not need any relevant information at this folder level.\nRegister a Default Experiment:\n\nNavigate to the relevant\n## Project\nin the\n## Lab Notebook\nmenu\n## Select\n## Default Experiment\nfrom the\n## +New\ndropdown, as shown below.\nPlease note\nthat your openBIS instance might have different types of Experiments, depending on how it has been configured by the Instance admin.\nFill in the relevant fields in the form.\n## Select\nShow in project overview = true\nif the\n## Experiment\nis\nimportant and should be shown in the\n## Project\nform.\n## Click\n## Save\non top of the form.\nRegister a Collection:\n\nNavigate to the relevant\n## Project\nin the\n## Lab Notebook\nmenu\n## Select\n## Collection\nfrom the\n## +New\ndropdown, as shown below.\nPlease note\nthat your openBIS instance might have different types of Experiments, depending on how it has been configured by the Instance admin.\nFill in the\n## Name\nof the Collection and choose the\n## Default Object Type\nand\nDefault collection view\n. For more info about Collections, see\n## Customize Collection View\nand\nCollections of Materials\n## Click\n## Save\non top of the form.\n## Register Experimental Steps\n\nAs mentioned above, the various steps executed when performing an\nExperiment in the lab can be registered in openBIS as\n## Experimental\nSteps or Entries.\nThe default\n## Experimental Step\n## has pre-defined fields, as shown below:\n## An\n## Entry\n, is a blank page, with no pre-defined fields:\nTo register a default\n## Experimental Step\nor\n## Entry\n## :\nNavigate to the relevant\n## Experiment\nin the\n## Lab Notebook\nmenu and click the\n## + New\nbutton, as shown below.\n## Select\n## Experimental Step\nor\n## Entry\nFill in the relevant information or select an available template from the list (see below).\n## If\nShow in project overview\nis set to true, this\n## Experimental Step\nor\n## Entry\nwill be displayed on the\n## Project\npage.\n## Click\n## Save\non top of the form.\n## Comments Log\n\nSeveral comments can be added by different users who have write-access to a given user Space:\nClick the button in the\n## Comments\nsection.\nEnter the\ncomment\n.\n## Click\n## Save.\nAdd parents and children to Experimental Steps\n\nIn the default\n## Experimental Step\nand in the\n## Entry\n, there is a\n## Parents\nsection where it is possible to specify links to materials\nand methods from the\n## Inventory\nor to any other\n## Object\n, e.g. another\n## Experimental Step\nor\n## Entry\n.\n## Parents\nare all samples/materials used in an experimental procedure,\nstandard protocols from the inventory followed in the experimental\nprocedure, the equipment used. It is also possible to set one\n## Experimental Step/Entry\nas parent of a second\n## Experimental\n## Step/Entry,\nto keep the connection between the two.\nThe name of this section and which parents should be shown in the form,\nis customisable by the\nlab manager\nor\ngroup admin\nas described in\nCustomise Parents and Children Sections in Object\n## Forms\nAdding a parent\n\nAdding a parent of a predefined type in the form\n\nIn the screenshot above,\nGeneral protocol\nis predefined as parent\ntype in the form. We have two options to add a parent of this predefined\n## type:\n## 1. Search\n\nClick on the\n## Search\nbutton.\nEnter the\nname\nor\ncode\nof the entry you want to add as\nparent.\nSelect the entry you want to add from the list presented to you.\nThe parent will be added only when you\nsave\nthe entity.\n\n## 2. Paste\n\nYou may copy the identifier of an entry you want to add as\nparent from a file, or from an advanced search or from another\nELN page. You can paste the identifier(s) in the\n## Paste\ntext\nfield.\nClick the\n## +Add\nbutton\nAdding parent of any available type\n\nIf you want to add a parent that is not specified in the\n## Experimental\n## Step\nform, you can use the\n## Search Any\nor\n## Paste Any\noptions next\nto\n## Parents.\n## 1. Search Any\n\n## Click\n## Search Any\nSelect the\n## Object\ntype for which you want to add a parent\nSearch by\ncode\nor\nname\nas explained above\nClick the\n## +Add\nbutton\n## 2. Paste Any\n\nThere are cases where you may want to add several parents of the same\ntype or also of different types. In this case, we recommend to use the\n## Advanced Search\nto find the entries you want to add. You can select\nthe desired entries from the table and the\n## Copy Identifiers\nbutton\nwill become visible. You can copy the identifiers and paste them in the\n## Paste Any\nfield in the\n## Experimental Step\npage, as shown below.\nAdding parent via barcodes\n\nIf you want to add a parent that is registered in openBIS and has a\nbarcode associated with it by scanning the barcode:\n1.Click on the\nbarcode\nicon in the Parents section\n## A\nBarcode/QR code reader\nwindow opens\nScan the barcode/QR code of the entry you want to add as parent with\na scanner or with the camera of a mobile device\nClick on the\n## Add Objects\nbutton\n## Close\nRemoving a parent\n\nTo remove a parent, choose\n## Remove\nfrom the\n## Operations\ndrop down in the parent table, as shown below.\nAdding and Removing Children\n\nChildren of\n## Experimental Steps\nare usually derivative\n## Experimental\n## Steps,\nor products of the\n## Experimental Step.\nAs for the\n## Parents\nsection, this section can also be customised by a\ngroup admin\nor\nlab\nmanager\nin the\nELN Settings\n(\nCustomise Parents and Children Sections in Object Forms)\n.\nThe procedure for adding and removing children is the same as explained\nfor parents.\n## Children Generator\n\n## The\n## Children Generator\ncreates a matrix of all the parents entered\nin the\n## Experimental Step\n, as shown below. Combinations of parents\nneeded to generate children can then be selected by the user. The\n## Object\ntype to assign to the children and the number of replicas need\nto be specified. The children will then be automatically generated by\nopenBIS upon registration of the\n## Experimental Step\n.\nParent-child relationships between entries in lab notebook\n\nIn the Lab Notebook section, if you create a new\n## Object\nfrom an\nexisting\n## Object\n, independently of the type, this will be automatically\nset as parent of the new Object. For example, if you create a new\nExperimental Step (measurement 4) from an existing Experimental Step\n(measurement 3), this will be automatically set as child of measurement\n3, as shown below.\nIf you do not wish to have this relationship established, you need to\ncreate the new Object starting from the Experiment level, as shown\nbelow.\nHow to use protocols in Experimental Steps\n\nWhen adding protocols to an\n## Experimental Step\n, two options are\n## available:\nLink to a\n## Protocol\nstored in the\n## Inventory\n. This can be used\nif the protocol was followed exactly in all steps as described.\nCreate a\nlocal copy of the Protocol\nfrom the\n## Inventory\nin the\ncurrent\n## Experiment\n. This should be done if some steps of the main\nprotocol were modified. These modifications can be edited in the\nlocal copy of the protocol, while the template is left untouched.\nTo create a local copy under the current Experiment of a template protocol stored in the\n## Inventory\n## :\nAdd a protocol as parent.\nFrom the\n## Operations\ndropdown in the parents table select\nCopy to Experiment.\nProvide the\nObject code\nfor the new protocol.\nA copy of the protocol is created under the current\n## Experiment\n, where the user can modify it. This copy has the original protocol set as parent, so that connection between the two is clear.\n## Move Experimental Steps\n\nTo move an\n## Experimental Step\nto a different\n## Experiment\n, choose\n## Move\nfrom the\n## More..\ndrop down, as shown in the picture above.\nIt is possible to move\n## Experimental Steps\nfrom the\n## Object\ntable\nwhich is presented on an\n## Experiment\nor\n## Collection\npage.\nSelect the entries to move and use the\n## Move\nbutton on the table. You\ncan move to an existing\n## Experiment\n/\n## Collection\nor create a new one.\n## Copy Experimental Steps\n\nTo copy an\n## Experimental Step\n, select\n## Copy\nfrom the\n## More…\ndrop\ndown menu, as shown below.\nWhen an\n## Experimental Step\nis copied, the user has the option to\nlink\nparents, copy children to the current Experiment\nand\ncopy the\ncomments log.\n## The\n## Experimental Step\nis copied inside the same\n## Experiment\n.\nUse templates for Experimental Steps\n\nTemplates need to be defined by the lab manager in the\nELN Settings\n. If templates have been created for a given\n## Experimental Step\n, you can choose from the list of available templates by clicking the\n## Template\nbutton on the\n## Object\nform, as shown below.\nA template of an\n## Experimental Step\nis an\n## Experimental Step\nwith\npre-filled values. Templates are useful when you need to repeat an\n## Experimental Step\nwith the same parameters several times and you wold\nlike to have default values for those parameters.\nDatasets tables\n\nSince openBIS version 20.10.7, a dataset table has been added to the Experiment/Collection and Object pages.\nThis table shows the metadata of the datasets. The content of the datasets can be navigated through the main menu.\nData Access\n\n## Datasets\nare displayed on the left hand-side of the\n## Experiment/Object\nform, as shown below.\nTo navigate and open data registered in openBIS via Finder or Explorer, open the\n## Dataset\nfolder and click on the drive icon next to the Dataset type name (see above).\n## If\nSFTP has been configured on system level, you will be provided with a link to copy/paste in an application such as\n## Cyberduck\nor other.\nPlease check our documentation for SFTP server configuration:\nInstallation and Administrators Guide of the openBIS Data Store Server\nFor native access through Windows Explorer or Mac Finder we recommend\nthe following:\n## Windows\n## 10:\nhttps://www.nsoftware.com/sftp/netdrive/\nMac OS X Yosemite and\n## higher:\nhttps://mountainduck.io\nKubuntu: Default Dolphin File Manager with SFTP support\n## Example of SFTP Net Drive connection:\n\n1. open SFTP Net Drive and click on\n## New\n## :\n2. Edit the drive with the following info, as shown below:\na.\nDrive name\n: choose any name you want. Can be the same as\nyour openBIS server, but does not have to be.\nb.\n## Remote Host\n: the name of your openBIS. For example, if the\nurl of your openBIS is https://openbis-\ndemo.ethz.ch/openbis/webapp/eln-lims, then openbis-demo.ethz.ch is the\nname you want to enter.\nc.\nRemote por\nt: enter 2222.\nd.\nAuthentication type\n: Password (this is selected by default).\ne.\n## Username\n: the username you use to login to openBIS.\nf.\n## Password\n: the password you use to login to openBIS.\ng.\nRoot folder on server\n: you can leave the default, User’s home\nfolder.\nh. Press\n## OK\nafter filling in all the information above.\n3. After saving the drive, select it in the drivers’ window and click\n## Connect\n.\n3. openBIS will now appear as a drive in your Explorer window. Click on\nthe\n## ELN-LIMS\nfolder and navigate to the folder containing the data\nyou want to access.\nNote: if you encounter the error message “\nSSH connection failed: Could\nnot find a part of the path\n.” you can fix this by disabling the cache\n(Drives -> Advanced -> Enable Caching), and disabling log files.\nThe error is caused by an attempt to create files in a folder not\navailable to Windows.\nExample of Cyber Duck configuration\n\nCreate a new connection in cyberduck:\nselect\nSFTP (SSH File Transfer Protocol)\n## Nickname\n: the name you want to use for the server\n## Server\n: the name of the server you want to connect to. In the\nexample below openbis-training.ethz.ch. Replace this with the name\nof your own openBIS server.\n## Port\n: 2222\n## Username\n: this is the username with which you connect to your\nopenBIS\n## Password\n: this is the password you use to connect to your\nopenBIS\n## SSH\nprivate Key: none\nSave the specifications and connect to the server.\nYou will see the folders of your own openBIS in the Cyberduck window and\nyou can navigate to your data from there.\nExample of  Dolphin File Manager configuration\n\nTo access the Dataset form and edit the Dataset metadata, click on the\nDataset code or Name (if provided).\nSFTP access via session token\n\nTo access via session token (for example when using SSO authentication)\nyou need to provide the following credentials:\n## Username: ?\nPassword: session token\n.\nThe session token can be copied from the\n## User Profile\nunder\n## Utilities\nin the main menu, as shown below.\n## Move Datasets\n\nIt is possible to move a\n## Dataset\nfrom one\n## Experiment/Object\nto\nanother\n## Experiment/Object\n.\nClick on the\n## Dataset\nin the main menu\nIn the\n## Dataset\npage select\n## Move\nfrom the\n## More..\ndropdown\nEnter the name or code of the\n## Experiment\nor\n## Object\nwhere you\nwant to move the\n## Dataset\nto. If you start typing, openBIS will\nshow you a list of possible entries that match what you entered.\nPress the\n## Accept\nbutton.\nMove one Experiment to a different Project\n\nIt is possible to move one Experiment and all contained Objects and\nDatasets from one Project to another.\nIf Objects contain parent/child relationships these are preserved.\nTo move one Experiment from one Project to another:\nSelect the Experiment you want to move from the main menu\n## Select\n## Move\nfrom the\n## More…\ndropdown\n3. Enter the code of the Project where you want to move your\nExperiment. If you start typing the code, openBIS will prompt you with a\nlist of available options and you can select the appropriate one from\nthere.\n## 4. Click\n## Accept\n## Project Overview\n\nIn the Project page you have the options to see:\nDefault Experiments and Experimental Steps with the field\nShow in project overview = true\n. This is a way to mark the most    relevant Experiments and Experimental steps and see them at a glance on the project page (\n## Show Overview\n).\nAll experiments belonging to the project (\n## Show Experiments/Collections\n).\nThe two options are available from the\n## More..\ndropdown on the Project\npage.\nBelow you see an example of an overview in a Project page.\nBelow you see an example of the visualisation of Experiments and\nCollections in a Project page.\nEdit and Delete Projects, Experiments, Experimental Steps\n\n## Projects\n,\n## Experiments\nand\n## Experimental Steps\ncan be edited at any\ntime, by selecting the\n## Edit\nicon from the toolbar of the relevant\npage.\n## Projects\n,\n## Experiments\nand\n## Experimental Steps\ncan be deleted using\nthe\n## Delete\noption under\n## More\ntab in the toolbar.\n## Experiments\nand\n## Experimental Steps\nare moved to the\ntrashcan\n,\nfrom where they need to be removed in order to be permanently deleted\nfrom the database.\n## Projects\nare directly deleted, they are not moved\nto the trashcan first.\n## Projects\ncan be deleted only after deleting all\nthe\n## Experiments\nthey contain.\nPlease be aware that, by default, only users with\nSpace Admin and\n## Instance Admin\nrole have permission to delete. Default permissions can\nbe modified on\nsystem level\n(see\nChanging the openBIS\ncapability role\nmap\n)\nShare Lab Notebooks and Projects\n\nIt is possible to share either a complete lab notebook or single\n## Projects\n, using the\n## Manage Access\noption in the\n## More..\ndropdown of a\n## Space\nor\n## Project\npage, as shown below.\n## Available roles are:\n## Observer\n: read-only access to Space or Project\n## User\n: can create and modify entities in Space or Project\n## Admin\n: can create, modify and delete entities in Space or\n## Project\nThe roles can be granted to:\n## User\n: the user needs to be already registered in openBIS. The\nusername of the user needs to be entered.\n## Group\n: the name of a user group existing in openBIS needs to be\nentered.\n## Rich Text Editor\n\n## EMBED IMAGES IN TEXT FIELDS\n\nTo embed an image in the a text field with the Rich Text Editor (RTE) enabled, you can simply drag & drop a .png or .jpg file and resize the image by clicking on and dragging the corners.", "timestamp": "2025-09-18T09:38:30.110210Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_managing-lab-stocks-and-orders-2:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/managing-lab-stocks-and-orders-2.html", "repo": "openbis", "title": "Managing Lab Stocks and Orders", "section": "Stock Catalog", "text": "Managing Lab Stocks and Orders\n\nIt is possible to use openBIS to manage stocks of products and create\norders of products to buy for the lab.\nEvery lab member can register products and place requests of products to\nbuy. The requests can be converted into orders by the lab manager or the\nperson responsible for purchases in the lab. The orders created with\nopenBIS contain the information that can be sent to the suppliers.\nIn the\n## Stock Catalog\nfolder, a lab can create one collection of all\nproducts purchased in the lab and one collection of all suppliers used\nfor purchasing. Each product must be linked to 1 supplier.\nEvery lab member by default has\n## Space User\naccess rights to the\n## Stock Catalog\nfolder and is able to register products, suppliers and\nplace requests for products to buy.\n## The\n## Stock Orders\nfolder is visible to all lab members, who have by\ndefault\n## Space Observer\nrights to it.  The lab manager, or person\nresponsible for purchases, has\n## Space Admin\nrights to this Space.\nOrders can be created based on the requests placed in the\n## Stock\n## Catalog\n.\n## STOCK CATALOG\n\nBuilding the catalog of products and suppliers\n\nCatalog of suppliers\n\nTo build the catalog of all suppliers used for purchasing products by\nthe lab:\nGo to the\n## Supplier Collection\nfolder under\n## Stock\n->\n## Stock Catalog\n->\n## Suppliers\nin the main menu.\nClick on the\n## + New Supplier\nbutton in the\n## Collection\npage.\nFollow the steps explained in the\n## Register Entries\ndocumentation page.\nTo register several suppliers at once, follow the steps described in\nBatch register entries in a Collection.\nCatalog of products\n\nTo build the catalog of all products purchased in the lab:\nGo to the\n## Product Collection\nfolder under\n## Stock\n->\n## Stock Catalog\n->\n## Products\nin the main menu.\nClick the\n## + New Product\nbutton in the\n## Collection\npage.\nFor each product it is necessary to register one supplier as parent. Select the correct supplier from the list of suppliers registered in the\n## Supplier Collection.\nThe process for adding parents is the same as described for Experimental Steps:\nAdd parents\n.\nTo register several suppliers at once, follow the steps described in\nBatch register entries in a Collection.\nCreating requests for products to order\n\nEvery lab member can create requests for products that need to be\n## ordered:\nGo to the\n## Request Collection\nfolder under\n## Stock\n->\n## Stock Catalog\n->\n## Requests\nin the main menu.\nClick the\n## + New Request\nbutton in the\n## Collection\npage.\nWhen you fill in the form the following information needs to be provided:\n## Order Status\n. Options  are\n## Delivered\n,\n## Paid\n,\n## Ordered\n,\nNot yet ordered\n. When you create a request set this field to\nNot yet ordered.\nOnly requests with this\n## Order Status\ncan be processed to orders.\nAdd the product you for which you want to place a request for order. This can be done in two ways:\nadd a product that is already present in the catalog. This process is the same as described for adding parents in Experimental steps:\nAdd parents\n. The quantity, i.e. how many units of the product are requested, needs to be specified.\nadd a product that is not yet registered in the Catalog. In this case the information shown in the picture below needs to be provided. After creating the request, the product entered here is automatically registered in the Product Catalog.\nPlease note that only 1 product can be added to 1 request.\n## Click\n## Save\non top of the form.\n## STOCK ORDERS\n\nThis section is accessible by default by every lab member. However, by\ndefault, only the person in charge of lab purchases can process orders\nbased on the requests created in the Stock Catalog by every lab member.\nProcessing product orders from requests\n\nTo create orders of products from requests created in the Stock Catalog:\nGo to the\n## Order Collection\nfolder under\n## Stock\n->\n## Stock Orders\n->\n## Orders\nin the main menu.\nClick the\n## + New Order\nbutton in the\n## Collection\npage.\nIf you do not see the\n## Code\nin the form, select\n## Show Identification Info\nfrom the\n## More..\ndropdown\nEnter a\n## Code\nfor the order\nIf an\norder\ntemplate\nform is available (see\nCreate Templates for Objects\n), this template can be used and most fields will be automatically filled (see\nUse templates for Experimental Steps\n). If no template is available, the relevant fields in the form need to be filled in with the relevant information.\nEnter the\n## Order Status.\nThis field is mandatory. Available options are\n## Delivered\n,\n## Paid\n,\n## Ordered\n,\nNot yet ordered\n. When you first create the order, you should set the status to\nNot yet ordered\n.\nAdd one or more requests to the Order. Only requests with Order Status set to\nNot yet ordered\nwill be displayed and can be selected.\n## Click\n## Save\non top of the form.\nIf the price information is available in the products, the total cost of\nthe order is calculated by openBIS and displayed in the order form, as\nshown above.\nBy using the\n## Print Order\nbutton in the order form, the order can be\nprinted as text file that can be sent to the suppliers for purchasing\nthe products.\nTo simplify the process of ordering multiple products from the same\nsupplier, all information related to the same supplier is grouped in one\nsingle text file.\nIn the example presented in the picture above, there are 2 products to\nbuy from fluka and 1 product to buy from Sigma-Aldrich. In this case the\ntwo attached files have been printed from the Order form in openBIS,\nusing the\n## Print Order\n## button:\norder_ORD1_p0;\norder_ORD1_p1\nOnce the order is processed, you should change the\n## Order Status\nto\n## Ordered\n. This will automatically change the\n## Order Status\nin all\nconnected requests. Requests with this oder status cannot be added to\nadditional orders.\nUpdated on April 25, 2023", "timestamp": "2025-09-18T09:38:30.115348Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_managing-storage-of-samples:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/managing-storage-of-samples.html", "repo": "openbis", "title": "Managing Storage Of Samples", "section": "Managing Storage Of Samples", "text": "## Managing Storage Of Samples\n\nAllocate storage positions to samples\n\nIf we want to track the storage position of samples, openBIS provides a\ngraphical overview of lab storages.\nLab storages need to be configured by a\nlab manager\nor\ngroup admin\n,\n## as explained here:\n## Configure Lab\n## Storage\nThis can be done in two ways:\nadd storage information on the sample form during (or after) sample\nregistration\nbatch register storage positions for several samples\nRegister storage position for a single sample\n\n1. Navigate to the\n## Storage\nsection, at the bottom of the sample\nform. Click the\n## + New Storage Positions\nabove the table, as shown\n## below:\nIn the widget that opens, select the appropriate\n## Storage\nfrom the\ndropdown menu. Storage must be configured by a lab manager or group\nadmin as explained in\n## Configure Lab\n## Storages\n3. Select the\nposition\nin the storage (shelf and rack).\n4. If the sample is in a box, provide a\n## Box Name.\n5. Select the\n## Box Size\nform the list of configured sizes (the list\ncan be configured by an\n## Instance Admin)\n.\n6. Select the\n## Position\nin the box.\n## 7. Click\n## Accept.\nAdd additional metadata to storage positions\n\nBy default, the storage only keeps track of locations. If the\n## Storage\n## Position\nhas been configured by an\nInstance admin\nto have additional\nmetadata (e.g. freezing date), these can be added by clicking on the\nlink in the storage table, as shown below. The link becomes available\nafter saving the sample.\nThe additional information can be entered in the\n## Storage Position\n## Object\nform.\nBatch register storage positions\n\nXLS Batch Registration\n\nWith the new XLS batch registration, samples and their storage positions\ncan be registered in one transaction using the XLS template file, as\nexplained in\nBatch register entries in a\n## Collection\n.\nBatch Registration with TSV file\n\nStorage positions are modelled in openBIS as children of other entries.\nTo register the positions for several samples with the Batch\nRegistration using the .tsv template, first the parent samples need to\nbe registered in openBIS. In a second step, the positions are assigned.\nTo assign storage positions in batch mode follow the steps below:\n## Select\nStorage positions\nfrom the\n## Batch Registration\ndrop\ndown menu.\nDownload the\ntemplate file\n.\nRemove the\nidentifier\ncolumn from the file (identifiers need\nto be automatically generated by openBIS).\nFill in the\nparents\ncolumn. These are the identifiers of the\nsamples for which we want to register the storage\npositions(/MATERIALS/PROJECT/OBJECT_CODE).\nFill the remaining information about the storage positions.\nSave the file and upload with the\n## Batch Registration\n.\nAn example file can be found\n## here:\nSAMPLE-STORAGE_POSITION-template\nUpdated on April 26, 2023\nBatch update storage positions\n\nTo update several storage positions, we can use the batch update option\nfrom the Object Browser:\nGo to the\n## Object Browser\nunder\n## Utilities\nin the main menu\nSelect the object type\n## Storage Position\nfrom the dropdown menu\n(see picture)\nUse the table\n## Filter\nto select the storage positions you want to\nupdate\n(see\n## Tables\n)\nExport the table (see\n## Tables\n)\nEdit the file to make the changes needed (e.g. change the name of a\nbox, change the storage, change a box position, change box size etc)\n## Select\nXLS Batch Update Objects\nfrom the\n## More..\ndropdown.\n7. Import the file you modified before and update the storage\npositions.\nUpdated on April 25, 2023\nDelete storage positions\n\nDelete single storage positions\n\nTo delete a single storage position from a sample:\nEdit the sample for which you want to deleted the storage position\nNavigate to the\n## Storage\nsection at the end of the page\nUse the “\n–\n” button in the\n## Storage Position\ntable, as shown\nin the picture\nSave the sample\nPlease note that the storage position deleted in this way is moved to\nthe trashcan. To delete the position permanently, this has to be deleted\nfrom the trashcan (see\n## Trashcan\n).\nRemove one of multiple positions in the same box\n\nIf one sample has been assigned to multiple positions in the same box\nand you need to remove only one or some of them, you can follow these\n## steps:\n## Edit\nthe sample for which you need to remove the storage\nposition in the box\nNavigate to the\n## Storage\nsection at the end of the page\nClick on the\ntable row\n(see picture below)\n## Unselect\nthe position you want to remove (eg. A5 in the example below)\n## Click\n## Accept\n## Save\nthe sample\nDelete multiple storage positions\n\nTo delete multiple storage positions from multiple samples we can use\nthe\n## Object Browser\n.\nGo to the\n## Object Browser\nunder\n## Utilities\nin the main menu\n## Select\n## Storage Position\nfrom the\n## Object Type\ndropdown\n3.\n## Filter\nthe table to find the storage positions you want to\ndelete\n(see\n## Tables\n)\n4. Select the positions you want to delete from the table and click the\n## Delete\nbutton (see picture below)\n5. You will be asked to provide a reason for deletion\n6. The deleted storage positions will be moved to the trashcan and\nshould be removed from there to be permanently deleted (see\n## Trashcan)\nUpdated on May 2, 2023\n## Overview of lab storages\n\n## The\n## Storage Manager\n, under\n## Utilities\n, provides an overview of\neach single storage configured for the lab, by the lab admin.\nSelect the storage containing the samples to visualise from the\n## Storage\ndrop down menu.\nClick on a box to view its content.\nWhen hovering with the mouse over a sample inside a box, the info\nabout the sample is shown.\n## Overview of lab Storages\n\nChange storage position of samples\n\n## The\n## Storage Manager\ncan also be used to move samples from one\nstorage position to another, if the location of the sample is changed:\nClick on\n## Toggle Storage B\n(see figure above).\nSelect the destination storage, from the\n## Storage\ndrop down\nmenu.\nDrag and drop the box or sample to move from\n## Storage A\nto the\ndesired position in\n## Storage B\n. Please note that the move\noperation for samples with multiple positions in the same box or\nin different boxes is not supported.\nChanges are visualised at the bottom of the page. To save them,\nclick\n## Save Changes\non top of the\n## Storage Manager\nform.\nUpdated on April 25, 2023", "timestamp": "2025-09-18T09:38:30.118362Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_general-users_tools-for-analysis-of-data-stored-in-openbis:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/general-users/tools-for-analysis-of-data-stored-in-openbis.html", "repo": "openbis", "title": "Tools For Analysis Of Data Stored In Openbis", "section": "Jupyter Notebooks", "text": "Tools For Analysis Of Data Stored In Openbis\n\n## Jupyter Notebooks\n\nJupyter notebooks are web applications that combine text, code and\noutput (\nhttps://jupyter.org/\n). Jupyter supports\nover 40 programming languages.\nJupyter notebooks can be used to analyze data stored in openBIS.\nIt is possible to connect to a JupyterHub server and launch Jupyter\nnotebooks directly from the openBIS interface. This feature is not\navailable by default, but needs to be enabled and configured by a\nsystem admin\n. JupyterHub docker containers are available from our\n## download page:\nopenBIS\ndownload.\nFurther documentation can be found here:\nJupyterHub for\nopenBIS\nHow to use Jupyter notebooks from openBIS\n\nJupyter notebooks can be opened at every level of the openBIS hierarchy\n(\n## Space, Project, Experiment/Collection, Object, Dataset\n) from the\n## More…\ndropdown menu, as shown below.\nIf you get a similar error as the one shown below when you try to launch\na notebook from an entity, you need to start the JupyterHub server by\ngoing to the main menu\n## Utilities\n->\n## Jupyter Workspace\n## . This\nerror appears when the JupyterHub server is restarted (e.g. after an\nupgrade), because the user profile needs to be recreated.\nIf you go to the Jupyter workspace, the user profile is re-created on\nthe server. After this, you can open a notebook from any entity of the\nopenBIS hierarchy as explained above (\n## Space, Project,\n## Experiment/Collection, Object, Dataset\n).\nJupyter notebooks can also be launched from the main menu, under\n## Utilities\n, as shown below.\n## Note\n: if you use SSO for authentication (eg. Switch aai), the first\ntime you want to work with a Jupyter notebook, you first need to open\nthe\n## Jupyter Workspace\nand then launch a notebook from wherever you\nwant to open it.\nWhen you launch a notebook from the\n## New Jupyter Notebook\nin the main\nmenu under\n## Utilities\n, it is necessary to enter:\n## The\ndataset(s)\nneeded for the analysis.\n## The\nowner\nof the Jupyter notebook. Jupyter notebooks are saved\nback to openBIS as datasets, and these belong either to an\n## Experiment/Collection\nor to an\n## Object\n. The owner is the\n## Experiment/Collection\nor\n## Object\nwhere the notebook should be\nstored.\n## The\ndirectory name\n. This is the name of the folder that will be\ncreated on the JupyterHub server.\nNotebook name\n. This is the name of the Jupyter notebook.\nJupyter notebooks can also be opened from a\n## Project\n,\n## Experiment\n,\n## Experimental Step\nchoosing the corresponding option in the\n## More\ndrop down menu. When opening notebooks from an\n## Experiment\nor\n## Experimental Step\n, all connected datasets are automatically selected.\nIf some are not needed, they can be deselected.\n## Overview of Jupyter notebook opened from openBIS.\n\nThe Jupyter notebooks running on the JupyterHub server for openBIS\nsupport the following kernels:\n## Bash, Octave, Python 2, Python 3, R,\nSoS\n(\nScript of Scripts).\nWhen you open a Jupyter notebook from openBIS, the default kernel used\nis Python 3, but you can change to another language as shown below.\nThe Jupyter notebook opened from the openBIS interface contains some\npre-filled cells. All cells need to be run. The information of two cells\n## should be modified:\nName of the dataset\nwhere the notebook will be\nstored and\n## Notes\n(in red below).\nIf you are running a JupyterHub version released after July 2021\n(available at\nhttps://hub.docker.com/u/openbis\n)\nyou do not need to enter username and password, as authentication uses\nthe openBIS session token.\nWhat to do in case of invalid session token\n\nIf your session token is not automatically renewed you will see a long\nerror message when you try to retrieve information of a dataset. At the\nbottom of the  error message you can see:\nIn such case, the session token can be manually entered in the cell as\n## shown below:\nThe session token can be copied from the\n## User Profile\nunder the\n## Utilities\nMain Menu in the ELN.\nEnter the session token, run the cell above and then move to the next\ncell to get the dataset(s) information.\nAlternatively you can go to the Jupyter Workspace under\n## Utilities\nand restart the server.\nYour script should be written in the section named\nProcess your data\nhere\n, that contains one empty cell (see below). You can, of course, add\nadditional cells.\nAfter the analysis is done, the notebook can be saved back to openBIS,\nby running the last few cells which contain the information about where\nthe notebook will be stored (as shown below).\nThe last pre-filled cell in the notebook, contains the information on\nwhere to upload the Jupyter notebook in openBIS. After you run this\ncell, you can go back to the ELN interface, refresh the webpage and you\nwill see your Jupyter notebook uploaded to the Object or Experiment you\nspecified. By default the Jupyter notebook are save to datasets of type\nANALYSIS_NOTEBOOK. If you prefer to use a different type, you can edit\nthe pre-filled cell shown above.\nUsing a local Jupyter installation with openBIS\n\nIt is also possible to use a local Jupyter installation with openBIS. In\nthis case, it is possible to download an extension for JupyterLab that\nadds 3 buttons to a default notebook:\nconnect to an openBIS instance;\ndownload datasets from the openBIS instance;\nupload the notebook to openBIS.\nThe JupyterLab openBIS extension is available from:\nJupyterLab openBIS\nextension\nUpdated on April 25, 2023\nMATLAB toolbox\n\nThe MATLAB toolbox for openBIS allows to access data stored in openBIS\ndirectly from MATALB. Full documentation can be found here:\n## MATLAB\n## API\nUpdated on April 17, 2023", "timestamp": "2025-09-18T09:38:30.121611Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_legacy-advance-features_index:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/legacy-advance-features/index.html", "repo": "openbis", "title": "Legacy Advance Features", "section": "Legacy Advance Features", "text": "## Legacy Advance Features\n\nopenBIS KNIME Nodes\n## Introduction\n### Installation\n### Usage\n## Nodes\nDefinining openBIS URLs\nDefining User Credentials for Authentication\nopenBIS Query Reader\nopenBIS Report Reader\nopenBIS Data Set File Importer\nopenBIS Data Set Registration (Flow Variable Port)\n### Usage\nopenBIS Data Set Registration (URI Port)\nopenBIS Aggregation Service Report Reader\nopenBIS Aggregated Data File Importer\nKNIME Aggregation Service Specifications\nKNIME Aggregation Service Helper API\nExample for an Aggregation Service Report Reader\nExample for an Aggregated Data File Importer", "timestamp": "2025-09-18T09:38:30.123612Z", "source_priority": 2, "content_type": "concept"}
{"id": "docs:openbis:en_20.10.0-11_user-documentation_legacy-advance-features_openbis-kinme-nodes:0", "source": "https://openbis.readthedocs.io/en/20.10.0-11/user-documentation/legacy-advance-features/openbis-kinme-nodes.html", "repo": "openbis", "title": "openBIS KNIME Nodes", "section": "Introduction", "text": "openBIS KNIME Nodes\n\n## Introduction\n\n## KNIME\nis a powerful workflow system. It allows\nto import data from some sources and process them in a workflow\ngraphically designed by the user.\nThere are special openBIS KNIME nodes for importing/exporting data\nfrom/to openBIS. KNIME version 2.7.2 or higher is required.\n### Installation\n\nStart KNIME application.\nClick on menu item ‘Install New Software…’ of menu ‘Help’. An\ninstallation dialog pops up.\nClick on the add button. A dialog titled ‘Add Repository’ pops up.\nEnter a name like ‘KNIME Community Nodes’ and the URL\nhttp://update.knime.org/community-contributions/3.1\nCheck the check box of ‘openBIS Knime Nodes’ in section ‘Community\nContributions - Bioinformatics & NGS’ and click twice the next\nbutton.\nAccept the license agreements.\nClick the finish button.\nIgnore the security warning and restart KNIME application.\n### Usage\n\nAll openBIS KNIME nodes can be found in Node Repository under Community\n## Nodes -> openBIS:\nDrag and drop a node into the project and double-click on the node. A\nnode setting dialog opens for entering parameters.\n## Nodes\n\nAll nodes need\nURL of openBIS server, like\nhttps://sprint-openbis.ethz.ch/openbis\n.\nUser credentials\nWhen configuring a node in the node setting dialog the user is asked for\nthese parameters in the section ‘Connection Parameters’:\nAfter pressing the button\nconnect\na connecting to the openBIS server\nwill be established. This is needed for editing additional node\nparameters. For example, the combo boxes of the reader nodes have to be\npopulated.\n## Warning\nFor a data set registration node the credentials combo box is only filled if all nodes of the upstream part of the workflow are successfully configured.\nThe OK button closes the node setting dialog. The connection parameters\nand all other parameters will be stored and used when executing a\nworkflow.\nDefinining openBIS URLs\n\nContrary to the previous version of openBIS KNIME nodes (Version 13.04.0\nand earlier) the URL field in the node setting dialog is no longer a\ntext field but a combo box with URLs. This list of predefined URLs is\ninitially empty. It has to be created by the following preference page:\nDefining User Credentials for Authentication\n\nFor security reasons it is not recommended to specify user ID and\npassword directly for each openBIS node. Instead named credentials\nshould be used. This has the advantage to enter user ID and password\nonly once for a workflow with several openBIS nodes.\nNamed credentials are defined for a particular workflow. They are called\nworkflow credentials and can be specified via the context menu of the\n## workflow:\nEach set of credentials has a name (which is used in the combo box), a\nuser ID (called ‘Login’) and a password:\nThe credentials are saved with the workflow except of the passwords. The\nuser will be asked for the passwords after loading a workflow.\n## Warning\nIf user ID and password are entered directly in the node setting dialog the KNIME master key on the preferences page\nKNIME -> Master Key\nshould be activated. Otherwise passwords will be stored unencrypted!\nopenBIS Query Reader\n\nThis node allows to run parametrized SQL queries on openBIS. The combo\nbox shows a list of available queries. After choosing one additional\nparameters have to be entered.\nopenBIS Report Reader\n\nThis node allows to get a report for a specified data set. The combo box\nshows a list of available report. After choosing a report a data set\nshould be entered. The button with three dots lets pop up a dialog for\nconvenient way to choose a data set.\nopenBIS Data Set File Importer\n\nThis nodes allows to download a particular file from a specified data\nset. Data set code, file path and a local folder for downloads have to\nbe specified. The output of the node is not a table put an object of\ntype\norg.knime.core.data.uri.URIPortObject\n. Other nodes with input\nports of this type can access the downloaded file. Such nodes exist in\nGenericKnimeNodes of the Community Nodes (which are a part of openMS\nKNIME Nodes). Also ‘openBIS Data Set Registration (URI Port)’ is such a\nnode.\nThe absolute path of the downloaded file is also available as a flow\nvariable\nabsolute-file-path\n. This allows to connect a openBIS Data Set\nFile Importer with a file reader which supports absolute file paths in\nflow variables like the CSV Reader node. The mechanism of connecting\nboth nodes via flow variable ports is explained in the next section\nwhere a CSV Writer node is connected with an openBIS Data Set\nRegistration node.\nThis importer node also creates the following KNIME flow variables:\nopenbis.DATA_SET\n,\nopenbis.EXPERIMENT\n, and\noptionally\nopenbis.SAMPLE\n. These variables contain data set code,\nexperiment identifier, and sample identifier, respectively. The flow\nvariable\nopenbis.SAMPLE\nidentifier only appears if the data set is\ndirectly link to a sample. KNIME flow variables are available to other\nnodes downstream.\nopenBIS Data Set Registration (Flow Variable Port)\n\nThis node allows to register a file as a data set. The path of the file\nto be registered is the value a flow variable specified in the node\nsettings dialog. In addition the user has to specify owner type and data\nset type.\nThe owner identifier (which is either a data set code, an experiment\nidentifier, or a sample identifier depending on the chosen owner type)\ncan be chosen by a chooser dialog. If the owner field is empty one of\nthe flow variables s\nopenbis.DATA_SET\n,\nopenbis.EXPERIMENT\n, or\nopenbis.SAMPLE\nwill be used.\n### Usage\n\nThis node is usually used in combination with a writer node which stores\ndata (e.g. data table) in a file. Writer nodes are end nodes of a\nworkflow. But it is possible to append another node downstream by using\nthe flow variable port. Normally the flow variable ports are not\nvisible. To make them visible choose item ‘\n## Show Flow Variable Ports\n’\nof the context menu of the node. Two red circle will appear at the upper\ncorners of the node symbol:\nadd a node of type ‘openBIS Data Set Registration (Flow Variable Port)’\nand connect the upper right circle of the writer node with the input\nnode of the registration node. A click on ‘Hide Flow Variable Ports’ of\nthe context menu of the writer node hides the upper left circle:\nyou need to tell the registration node which flow variable has the path\nto the file to be registered. This needs two steps:\nThe configuration parameter of the writer has to be made available\nas a flow variable. This can be done in tab ‘Flow Variables’ of the\nnode settings dialog. It lists all configuration parameters. If a\nname is specified in the text field of a certain parameter its value\nwill be available as a flow variable of specified name for the\ndownstream nodes. Here is the example for CSV Writer:\nThis works for all writers. There is an easier way for CSV Writer:\nOn the Settings tab there is small button named ‘\nv=?\n’. Clicking on\nthis button opens a dialog where the flow variable for the file name\ncan directly be specified by using ‘Create Variable’:\nIn the registration node the flow variable specified in the first\nstep has to be chosen as the file variable:\nopenBIS Data Set Registration (URI Port)\n\nThis nodes allows to register a file as a data set. The file to be\nregistered is the first one in the list of URIs of the port object of\ntype\norg.knime.core.data.uri.URIPortObject\n. The user has to specify\nowner type and data set type in the node settings dialog.\nThe owner identifier (which is either a data set code, an experiment\nidentifier, or a sample identifier depending on the chosen owner type)\ncan be chosen by a chooser dialog. If the owner field is empty one of\nthe flow variables\nopenbis.DATA_SET\n,\nopenbis.EXPERIMENT\n, or\nopenbis.SAMPLE\nwill be used.\nopenBIS Aggregation Service Report Reader\n\nThis nodes allows to get an\naggregation\nservice\nreport. Only\naggregation services where the service key starts with\nknime-\ncan be\nchosen by the user in the node settings dialog.  After the service has\nbeen chosen the aggregation service will be invoked with the parameter\n## _REQUEST_\nset to\ngetParameterDescriptions\n. The service has to return\na table where each row defines the name of the parameter and optionally\nits type. This is used to created an appropriated form in the node\nsettings dialog. The values specified by the user will be used to invoke\nthe aggregation service when the node is executed. The result will be\navailable as a KNIME table. See also section\nKNIME Aggregation Service\n## Specifications\n.\nopenBIS Aggregated Data File Importer\n\nThis nodes allows to invoke an\naggregation\nservice\nwhich returns a name\nof a file in the session workspace which will be downloaded and made\navailable for nodes with input ports of type\norg.knime.core.data.uri.URIPortObject\n. Such nodes exist in\nGenericKnimeNodes of the Community Nodes. Also ‘openBIS Data Set\nRegistration (URI Port)’ is such a node.\nOnly aggregation services where the service key starts\nwith\nknime-file-\ncan be chosen by the user in the node settings\ndialog.  The communication protocol between this node and openBIS is as\nfor nodes of type ‘openBIS Aggregation Service Report Reader’. The only\ndifference is that the returned table has only one row with one cell\nwhich contains the file name.\nKNIME Aggregation Service Specifications\n\nNodes of type ‘openBIS Aggregation Service Report Reader’ and ‘openBIS\nAggregated Data File Importer’ rely on\naggregation\nservices\nwhich follow a\ncertain protocol. In order to distinguish these services from other\naggregation services the service key (i.e.\ncore\nplugins\nID) has to start\nwith\nknime-\n. The specifications of such services are the following:\nIf there is a parameter\n## _REQUEST_\nwith\nvalue\ngetParameterDescriptions\ndescriptions of all parameters will\nbe returned in the form specified as follows:\nThe table has the columns\nname\nand\ntype\n.\nEach row has a non-empty unique value of column\nname\n## . It\nspecifies the name of the parameter. It is also shown in node\nsettings dialog.\nThe type columns contains either an empty string or\n## VARCHAR\n,\n## VOCABULARY\n,\n## EXPERIMENT\n,\n## SAMPLE\n, or\n## DATA_SET.\nThe default\ntype is\n## VARCHAR\nwhich is represented in the node settings\ndialog by a single-line text field. The types\n## EXPERIMENT\n,\n## SAMPLE\n, and\n## DATA_SET\nare also single line text field with an\nadditional button to open an appropriate chooser.\nThe type\n## VOCABULARY\nisn’t useful without a list of terms in\nthe following form:\nVOCABULRY:<term\n1>,\n<term\n2>,\n...\n.\n## Example:\nVOCABULARY:Strong,\n## Medium,\n## Weak\nIf there is no parameter\n## _REQUEST_\nor its value\nisn’t\ngetParameterDescriptions\nthe aggregation service can assume\nthat all parameters as defined by the parameters description are\npresent. Some of them might have empty strings as values.\nAn exception should be returned as a table with five columns where\nthe first column is\n## _EXCEPTION_\n. If such a table is returned an\nexception with stack trace will be created and thrown in KNIME. It\nwill appear in KNIME log. For each row either the first cell isn’t\nempty or the five other cells are not empty. In the first case the\nvalue of the first column is of the form\n## :\n. If the first column is empty\nthe row represents a stack trace entry where the other columns are\ninterpreted as class name, method name, file name, and line number.\nIn order to simplify KNIME aggregation services a Helper API in Java is\navailable\nopenbis-knime-server.jar\n.\nIt should be added to openBIS installation in\nfolder\n<installation\nfolder>/servers/datastore_server/ext-lib\n.\nKNIME Aggregation Service Helper API\n\nThe helper API contains the two\nclasses\nch.systemsx.cisd.openbis.knime.server.AggregationCommand\nand\nch.systemsx.cisd.openbis.knime.server.AggregationFileCommand\nwhich\nshould be extend when writing an aggregation service for nodes of type\n‘openBIS Aggregation Service Report Reader’ and ‘openBIS Aggregated Data\nFile Importer’, respectively.\nThe subclasses should override the method\ndefineParameters()\n## . Its\nargument is a\nParameterDescriptionsBuilder\nwhich simplifies creation\nof parameter descriptions.\n## If\nAggregationCommand\n/\nAggregationFileCommand\nis subclassed the\nmethod\naggregate()/createFile()\nshould be overridden. The\naggregate()\nmethods gets the original arguments which are the\nparameters binding map and the ISimpleTableModelBuilderAdaptor. The\ncreateFile()\nmethods gets only the parameters binding map. It returns\nthe name of the file in the session workspace.\nThe aggregation service should instanciate the subclass and\ninvoke\nhandleRequest()\nwith the parameters binding map and the table\nmodel builder adaptor.\n## The\nParameterDescriptionsBuilder\nhas the method\nparameter()\n## . It\ncreates a\nParameterDescriptionBuilder\nbased on the specified parameter\nname. The\nParameterDescriptionBuilder\nhas the\nmethods\ntext()\n,\nvocabulary()\n,\nexperiment()\n,\nsample()\n,\ndataSet()\nwhich specify the parameter type. Only\nvocabulary()\nhas an\nargument: The string array of vocabulary terms.\nExample for an Aggregation Service Report Reader\n\nfrom\nch.systemsx.cisd.openbis.knime.server\nimport\nAggregationCommand\nfrom\nch.systemsx.cisd.openbis.generic.shared.api.v1.dto\nimport\nSearchCriteria\nfrom\nch.systemsx.cisd.openbis.generic.shared.api.v1.dto\nimport\nSearchSubCriteria\nfrom\nch.systemsx.cisd.openbis.generic.shared.api.v1.dto.SearchCriteria\nimport\nMatchClause\nfrom\nch.systemsx.cisd.openbis.generic.shared.api.v1.dto.SearchCriteria\nimport\nMatchClauseAttribute\n## EXPERIMENT\n=\n## 'Experiment'\n## DATA_SET_COLUMN\n=\n'Data Set'\n## PATH_COLUMN\n=\n## 'Path'\n## SIZE_COLUMN\n=\n## 'Size'\ndef\nscan\n(\ntableBuilder\n,\ndataSetCode\n,\nnode\n## ):\nif\nnode\n.\nisDirectory\n## ():\nfor\nchild\nin\nnode\n.\nchildNodes\n## :\nscan\n(\ntableBuilder\n,\ndataSetCode\n,\nchild\n)\nelse\n## :\nrow\n=\ntableBuilder\n.\naddRow\n()\nrow\n.\nsetCell\n(\n## DATA_SET_COLUMN\n,\ndataSetCode\n)\nrow\n.\nsetCell\n(\n## PATH_COLUMN\n,\nnode\n.\nrelativePath\n)\nrow\n.\nsetCell\n(\n## SIZE_COLUMN\n,\nnode\n.\nfileLength\n)\nclass\nMyAggregationCommand\n(\nAggregationCommand\n## ):\ndef\ndefineParameters\n(\nself\n,\nbuilder\n## ):\nbuilder\n.\nparameter\n(\n## EXPERIMENT\n)\n.\nexperiment\n()\ndef\naggregate\n(\nself\n,\nparameters\n,\ntableBuilder\n## ):\nexperiment\n=\nsearchService\n.\ngetExperiment\n(\nparameters\n.\nget\n(\n## EXPERIMENT\n))\nsearchCriteria\n=\nSearchCriteria\n()\nsubCriteria\n=\nSearchCriteria\n()\nsubCriteria\n.\naddMatchClause\n(\nMatchClause\n.\ncreateAttributeMatch\n(\nMatchClauseAttribute\n.\n## PERM_ID\n,\nexperiment\n.\npermId\n))\nsearchCriteria\n.\naddSubCriteria\n(\nSearchSubCriteria\n.\ncreateExperimentCriteria\n(\nsubCriteria\n))\ndataSets\n=\nsearchService\n.\nsearchForDataSets\n(\nsearchCriteria\n)\ntableBuilder\n.\naddHeader\n(\n## DATA_SET_COLUMN\n)\ntableBuilder\n.\naddHeader\n(\n## PATH_COLUMN\n)\ntableBuilder\n.\naddHeader\n(\n## SIZE_COLUMN\n)\nfor\ndataSet\nin\ndataSets\n## :\ndataSetCode\n=\ndataSet\n.\ndataSetCode\ntry\n## :\ncontent\n=\ncontentProvider\n.\ngetContent\n(\ndataSetCode\n)\nscan\n(\ntableBuilder\n,\ndataSetCode\n,\ncontent\n.\nrootNode\n)\nfinally\n## :\nif\ncontent\n!=\n## None\n## :\ncontent\n.\nclose\n()\ndef\naggregate\n(\nparameters\n,\ntableBuilder\n## ):\nMyAggregationCommand\n()\n.\nhandleRequest\n(\nparameters\n,\ntableBuilder\n)\nExample for an Aggregated Data File Importer\n\nimport\nos.path\nfrom\njava.util\nimport\n## Date\nfrom\nch.systemsx.cisd.openbis.knime.server\nimport\nAggregationFileCommand\nclass\nMyAggregationFileCommand\n(\nAggregationFileCommand\n## ):\ndef\ndefineParameters\n(\nself\n,\nbuilder\n## ):\nbuilder\n.\nparameter\n(\n## 'Greeting Type'\n)\n.\nvocabulary\n([\n## 'Hi'\n,\n## 'Hello'\n])\nbuilder\n.\nparameter\n(\n## 'Name'\n)\nbuilder\n.\nparameter\n(\n## 'Sample'\n)\n.\nsample\n()\ndef\ncreateFile\n(\nself\n,\nparameters\n## ):\nsessionWorkspace\n=\nsessionWorkspaceProvider\n.\ngetSessionWorkspace\n()\nfilename\n=\n\"report.txt\"\noutput\n=\nopen\n(\nos\n.\npath\n.\njoin\n(\nsessionWorkspace\n.\ngetAbsolutePath\n(),\nfilename\n),\n\"w\"\n)\nname\n=\nparameters\n.\nget\n(\n## 'Name'\n)\nsample\n=\nsearchService\n.\ngetSample\n(\nparameters\n.\nget\n(\n## 'Sample'\n))\noutput\n.\nwrite\n(\nstr\n(\nparameters\n.\nget\n(\n## 'Greeting Type'\n))\n+\n\" \"\n+\nstr\n(\nname\n)\n+\n\"!\n\\n\\n\n\"\n+\n## Date\n()\n.\ntoString\n()\n+\n\"\n\\n\n\"\n)\noutput\n.\nwrite\n(\nsample\n.\ngetSampleType\n())\noutput\n.\nclose\n()\nreturn\nfilename\ndef\naggregate\n(\nparameters\n,\ntableBuilder\n## ):\nMyAggregationFileCommand\n()\n.\nhandleRequest\n(\nparameters\n,\ntableBuilder\n)", "timestamp": "2025-09-18T09:38:30.128676Z", "source_priority": 2, "content_type": "code"}
{"id": "docs:datastore:en_concepts:0", "source": "https://datastore.bam.de/en/concepts", "repo": "datastore", "title": "Concepts of Data Store and openBIS", "section": "Concepts of Data Store and openBIS", "text": "concepts\n# Concepts of Data Store and openBIS\n## Page Contents\n## Collection\n## Controlled Vocabulary\n## Dataset\nData Structure\n## Dropbox\nEntity and Entity Types\n## Inventory\nIntances in the Data Store\n## Jupyter Notebook\n## Lab Notebook\n## Masterdata\n## Metadata\n## Object\n## Parent-Child Relationship\n## Project\n## Property\npyBIS\nRoles defined in openBIS\nRoles and Rights\nRoles defined per default in the Data Store\nRoles Management: Access to Spaces and Projects\n## Space\n## Demidova, Caroline\nLast Friday at 11:54 AM\n¶\n## Collection\nIn openBIS, a\n## Collection\nis a folder with user-defined\n## Properties\nlocated on the third level of the hierarchical data structure (Space/Project/\n## Collection\n## /Object). A\n## Collection\nis always part of a\n## Project\n.\n## Collections\nof the same type are described by the same set of\n## Properties\n.\n## Collection\ntypes are defined as part of the openBIS\nmasterdata\n.\n## Datasets\ncan be attached to\n## Collections\n.\n## A\n## Collection\ncan logically group an unlimited number of\n## Object\nof one or more\n## Object\ntypes. For instance, a\n## Collection\nof the type \"Measurement Devices\" can be used to organize\n## Objects\nof the type \"Instrument\" in the\n## Inventory\n## . A\n## Collection\nof the type \"Default Experiment\" can be used to organize\n## Objects\nof the type \"Experimental Step\" in the\n## Lab Notebook\n.\n¶\n## Controlled Vocabulary\nA controlled vocabulary is an established list of terms to provide consistency and uniqueness in the description of a given domain, e.g., a list of room labels, SI units or purity grades. In openBIS, controlled vocabularies are a possible data type for metadata\n## Properties\n. Each term in a controlled vocabulary has a code, a label, and a description. All existing controlled vocabularies and their terms are listed in the Vocabulary Browser in the Utilities.\n¶\n## Dataset\nIn openBIS, a\n## Dataset\nis a folder with user-defined\n## Properties\nthat can contain individual files of arbitrary formats (e.g., images, csv files, xml files, etc.) as well as complex folder structures with subfolders and many files (of potentially different formats). The content of\n## Datasets\n(but not their metadata) is immutable, i.e., it cannot be edited after creation.\n## Datasets\nof the same type are described by the same set of\n## Properties\n.\n## Dataset\ntypes are defined as part of the openBIS\nmasterdata\n.\n## A\n## Dataset\nhas to be attached to either an\n## Object\nor to a\n## Collection\n## . Different\n## Datasets\ncan be connected via\nparent-child relationships\n.\n¶\nData Structure\nThe openBIS data structure is hierarchically organized in five (sub)folders called\n## Space\n,\n## Project\n,\n## Collection\n,\n## Object\nand\n## Dataset\n. Folder names can be customized by users.\nTo digitally represent an\n## Object\n, a\n## Space\n,\n## Project\nand\n## Collection\nneed to be created first.\n¶\n## Dropbox\nThe Dropbox is a core openBIS plugin that allows the upload of (large) data files to an openBIS instance. Instead of using the user interface for\n## Dataset\nregistration, users move their data files (+ information about the\n## Object\nor\n## Collection\nthey should be attached to) to a dedicated Dropbox folder in a file-service at BAM that is continously monitored. Once new data files are detected inside the Dropbox folder, they are automatically uploaded as\n## Datasets\nto the openBIS instance.\nIt is possible to control the\n## Dataset\nregistration process via Dropbox scripts written in Python. The script can register new\n## Datasets\n,\n## Objects\n,\n## Properties\nand\nparent-child relationships\nas part of its processing. The Dropbox framework also provides tools to track file operations and, if necessary, revert them, ensuring that the incoming file or directory is returned to its original state in the event of an error.\nThe Dropbox is not related to the commercial file hosting service.\n¶\nEntity and Entity Types\nAn Entity is an item of the \"real world\" (tangible/non tangible) that is uniquely identified by attributes (\n## Properties\n). An Entity Type is a collection of Entities with similar properties. An Entity Type is an object in a data model. In openBIS relevant entity types are\n## Collection\n,\n## Object\nand\n## Dataset\ntypes. Entity types can only be created by someone with the Instance admin role.\n¶\n## Inventory\nThe Inventory is one of the two main components of openBIS. It is used for the digital representation of shared laboratory inventory and the storage of related data files such as measuring instruments, chemical substances, and samples, but can also be user for storing protocols, standard operating procedures (SOPs) and publications. The Inventory is organized into\n## Spaces\n,\n## Projects\nand\n## Collections\naccording to the hierarchical\ndata structure\nof openBIS.\nBy default, each BAM division gets\nprivate\n## Inventory\n## Spaces\n(Equipment, Materials, Methods, Publications) that are only accessible to division members\npublic\n## Inventory\n## Projects\n(Equipment, Materials, Methods) that are accessible to every user of the Data Store.\n¶\nIntances in the Data Store\nIn the Data Store, various instances are available to support users during different stages of the onboarding and data management process. Below are two key instances and their roles:\n## Instance\n## Users\n### Configuration\nAccess time\nTraining Data Store\nDSSt during the onbodring\nMulti-group instance\nOnly during onboarding\nMain Data Store\nOnboarded divisions\nMulti-group instance\nAfter onboarding continously\n¶\n## Jupyter Notebook\nJupyter Notebook is a web-based interactive computing platform that combines live code, equations, narrative text, visualizations, interactive dashboards and other media. Jupyter Notebooks can be used to analyze data stored in an openBIS instance.\n¶\n## Lab Notebook\nThe Lab Notebook is one of the two main components of openBIS. It is the digital version of a paper lab notebook and can be used for the digital representation and documentation of experimental procedures and analyses and the storage of related data files according to good scientific practice.\nBy default, each user gets their own personal\n## Space\nin the Lab Notebook where they can represent multiple research\n## Projects\n. Within a given\n## Project\n,\n## Collections\ncan be used to represent comprehensive experiments which comprise individual\n## Objects\n, e.g., of the type \"Experimental Step\". Access to the personal Lab Notebook\n## Space\nor individual\n## Projects\ncan be shared with colleagues.\n¶\n## Masterdata\nThe term \"Masterdata\" describes all information structures and plugins that are used to define metadata in openBIS (i.e., masterdata = \"meta-metadata\"). Masterdata is comprised of Entity types, i.e.,\n## Collection\n,\n## Object\nand\n## Dataset\ntypes, as well as\n## Property\ntypes,\ncontrolled vocabularies\nand related scripts (e.g., dynamic property plugins and entity validation scripts). Domain-specific masterdata have to be defined by the Data Store Stewards of the BAM divisions, but can only be imported to the openBIS instance (and edited) by Instance Admins.\n¶\n## Metadata\nMetadata is \"data about data\" that provides the information needed to find, interpret and understand research data. This includes general\n## Properties\nsuch as Code, Name, Description and more specific\n## Properties\ndefined by users within Entity Types (for the Data Store starting from rollout phase IV, the only Entity types that are defined are\n## Object\n## Types\nwhich might include controlled vocabularies).\nThe following table provides overview on the metadata generated along the openBIS data structure (\n## Space\n,\n## Projects\n,\n## Collection\n,\n## Object\nand\n## Dataset\n).\n¶\n## Object\nIn openBIS, an\n## Object\nis an entity with user-defined\n## Properties\nlocated on the fourth level of the hierarchical data structure (Space/Project/Collection/\n## Object\n).\n## An\n## Object\nis always part of a\n## Collection\n.\n## Objects\nof the same type are described by the same set of\n## Properties\n.\n## Object\ntypes are defined as part of the openBIS\nmasterdata\n.\n## Datasets\ncan be attached to\n## Objects\n.\n## An\n## Object\ncan be used to represent any kind of physical or intangible entity. For instance, an\n## Object\nof the type \"Chemical\" can be used to represent a batch of ethanol in the\n## Inventory\n## . An\n## Object\nof the type \"Experimental Step\" can be used to represent a measurement or an analysis in the\n## Lab Notebook\n.\n¶\n## Parent-Child Relationship\nA parent-child relationship is a directed link (or \"directed edge\" in graph theory) between two\n## Objects\n(Object1 --> Object2) or between two\n## Datasets\n(Dataset1 --> Dataset2) in an openBIS Instance. For a given relationship between two\n## Objects\n(or\n## Datasets\n), the\n## Object\nwith the outgoing edge is called the \"parent\" and the\n## Object\nwith the incoming edge is called the \"child\". It is not possible to have parent-child relationships between\n## Objects\nand\n## Datasets\nor between other entity types (e.g.,\n## Collection\n).\nParent-child relationships can be used to represent different kinds of logical connections between\n## Objects\n(or\n## Datasets\n## ), e.g.:\na partition of an entity:\n## Object\n\"Sample 1\" is parent of\n## Objects\n\"Sample 1A\" and \"Sample 1B\" because the original sample was broken up into two smaller sub-samples,\ncontext in a research process, e.g., Object \"Experimental Step 1\" is child of the Objects \"Sample 1A\" and \"Measurement Device\" because during the experimental step, the measurement device was used to measure some properties of the sub-sample,\na temporal sequence of different steps in a workflow: Object \"Experimental_Step_1\" is parent of \"Experimental Step 2\" because the first experimental step was conducted prior to the second.\nWhen all of these\n## Objects\nand their connections to each other are combined, we get a hierarchy tree (or a \"directed acyclic graph\" (DAG) in graph theory):\nParent-child relationships are not allowed to form cycles within the graph (e.g., an\n## Object\ncannot be both parent and child of another\n## Object\n), otherwise an error will be reported.\nParent-child relationship can also be used to represent relations between\n## Datasets\n, e.g., \"Dataset_v2\" being the parent of \"Dataset_v1\" because the second\n## Dataset\nis a newer version of the first one.\nParent-child relationships between\n## Objects\n(or\n## Datasets\n) are independent of the folder hierarchy, i.e.,\n## Objects\n(or\n## Datasets\n) can be connected across different\n## Spaces\nand\n## Projects\nand irrespective of whether they are located in the\n## Inventory\nor the\n## Lab Notebook\n.\nBy default, every\n## Object\n(or\n## Dataset\n) can have a unlimited number of parents and/or children or none (N:N relationships with N being any number from 0 to N). For a given\n## Object\ntype, group admins can set a minimum and maximum number of children and parents of a certain type in the settings.\n¶\n## Project\nIn openBIS, a\n## Project\nis a folder located on the second level of the hierarchical data structure (Space/\n## Project\n## /Collection/Object). A\n## Project\nis always part of a\n## Space\n## . A\n## Project\ncan logically group an unlimited number of\n## Collections\n.\nFor instance, a\n## Project\n\"Reagents\" can be used to organize\n## Collections\nof the type \"Chemicals\" in the\n## Inventory\n. A Project \"Master Thesis\" can be used to organize\n## Collections\nof the type \"Experiment\" in the\n## Lab Notebook\n.\nApart from a code (PermId) and a description,\n## Projects\nhave no metadata. User access rights can be defined at the\n## Project\n-level.\n¶\n## Property\nIn openBIS, a\n## Property\nis a metadata field that can be used to describe a\n## Collection\n, an\n## Object\nor a\n## Dataset\n.\n## Properties\ncan be of different\ndata types\n, e.g., numbers (Boolean, real, integer), text, hyperlink, date,\ncontrolled vocabularies\nbut also tabular data.\n¶\npyBIS\npyBIS is a Python module for interacting with openBIS. Most actions that can be carried out in the openBIS graphical user interface (GUI) can also be done via pyBIS. pyBIS is designed to be most useful in a\n## Jupyter Notebook\nor IPython environment.\n¶\nRoles defined in openBIS\nThe openBIS roles defined the rights that users get assigned to manage the research data stored in the BAM Data Store. In openBIS, there are four different types of roles, in descending order of rights:\n## Admin\n## User\n## Observer\nopenBIS roles are assigned to users at different levels:\nfor the complete openBIS instance (only Admin or Observer role)\nfor a\n## Space\nor a\n## Project\n## : Adin, User, Observer\nRoles assigned to\n## Spaces\nand\n## Projects\nalso apply to the corresponding subfolders (\n## Collections\n,\n## Objects\nand\n## Datasets\n).\nSince users with the role of Instance Admin  have full access to the entire instance and all\n## Spaces\ncontained therein, this role is the sole responsibility of the core team of the Data Store team. Only Instance Admins can make changes to the Masterdata, create new\n## Spaces\n, and edit the settings of the ELN-LIMS User Interface (UI).\n¶\nRoles and Rights\nThe corresponding rights to openBIS User roles are summarized in the table below.\nFor additional information on roles and permissions, please refer to the official openBIS docs\nhere\n.\n## Role\n## Rights\nInstance Admin (Data Store Team)\n- Full access to the complete openBIS Instance\n-\n## Space\n/\n## Project\nAdmin rights\n- Create and edit Masterdata\n- Create and edit\n## Spaces\n- Create/manage\n## Space\nAdmin role\nGroup Admin (Division Head, DSSt)\n-\n## Space\n/\n## Project\nAdmin rights\n- Customise the group‘s ELN Settings\n- Revert deletions\n## Space/Project Admin\n-\n## Space\n/\n## Project\nUser rights\n- Assign and remove\n## Space\n/\nProject roles\n## -Create\n## Projects\n## -Delete\n## Project\ns,\n## Collections\n,\n## Objects\n,\n## Datasets\n- Save searches\n## Space\n/\n## Project\n## User\n- Observer rights\n## - Create\n## Collections\nand\n## Objects\n## - Edit\n## Projects\n,\n## Collections\nand\n## Objects\n## Observer\n- Read-only access\n## - Download\n## Datasets\n¶\nRoles defined per default in the Data Store\nDefault roles are based on the\n## Lab Notebook\nand\n## Inventory\nstructure and the multi-groups set up of the Data Store. By default, all division members / users (who are not DSSt or division leads) have the following roles:\n## Space/Project Admin\nin their personal Lab Notebook\n## Space\n(e.g., X.1 Amueller)\n## Observer\nin the personal Lab Notebook of colleagues from the same division\n## Space/Project User\nin the private Inventory of their devision (e.g., X.1 EQUIPMENT, X.1 MATERIALS, X.1 METHODS, X.1 PUBLICATIONS)\n## Observer\nin the public Inventory of the other divisions (BAM EQUIPMENT, BAM MATERIALS, BAM METHODS, BAM PUBLICATIONS)\n## Space/Project User\nin the public Inventory of their group (X.1 EQUIPMENT OPEN, X.1 MATERIALS OPEN, X.1 METHODS OPEN, X.1 PUBLICATIONS OPEN)\nBy default, all Data Store Stewards (DSSt) and division leads/Group Admins have following roles:\n## Space/Project Group Admin\nin their personal Lab Notebook\n## Space\nand the Lab Notebook of all division members.\n## Space/Project Group Admin\nin all Inventory Spaces of their division (X.1 EQUIPMENT, X.1 MATERIALS, X.1 METHODS, X.1 PUBLICATIONS).\n## Space/Project User\nin all public Inventory BAM Spaces (X.1 EQUIPMENT OPEN, X.1 MATERIALS OPEN, X.1 METHODS OPEN, X.1 PUBLICATIONS OPEN)\n¶\nRoles Management: Access to Spaces and Projects\nopenBIS roles can be assigned to individual users or groups. The main instance of the BAM Data Store is organized as a multi-group instance, with each BAM division (“Fachbereich”) corresponding to a group.\nGroup Admins in openBIS are assigned by default to division heads, their deputies, and Data Store Stewards (DSSt) for all Spaces within their division (group). They are responsible for managing user roles within their group(division), specifically for the Spaces and Projects where they hold Admin rights.\nPlease note that\naccess can only be managed at the\n## Space\nand\n## Project\nlevel\nand NOT on the level of individual\n## Collections\n,\n## Objects\nand/or\n## Datasets\n. The rights granted for a\n## Space\napply to all subfolders/entities in the openBIS hierarchical data structure (\n## Project\n,\n## Collection\n,\n## Object\n,\n## Dataset\n), while rights granted at the Project level apply to all subfolders of the same (\n## Collection\n,\n## Object\n,\n## Dataset\n).\nTo manage access rights at the\n## Space\n/\n## Project\nlevel in the ELN-LIMS UI, click on the “More” button and select “Manage access”. Note that roles can only be assigned to users or groups that have been granted access by the Instance Admins (Data Store Team).\n¶\n## Space\nIn openBIS, a\n## Space\nis a folder located on the first level of the hierarchical data structure (\n## Space\n## /Project/Collection/Object). A\n## Space\nis either located in the\n## Inventory\nor in the\n## Lab Notebook\n## . A\n## Space\ncan logically group an unlimited number of\n## Projects\n.\nFor instance, a\n## Space\n\"Materials\" can include the\n## Project\n\"Reagents\" in the Inventory. A\n## Space\n\"Master Students\" can include the\n## Project\n\"Master Thesis\" in the Lab Notebook.\nApart from the permanent ID (PermId) and a description,\n## Spaces\nhave no metadata. User access rights can be defined at the\n## Space\n-level.", "timestamp": "2025-09-18T09:38:30.139644Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_faq:0", "source": "https://datastore.bam.de/en/faq", "repo": "datastore", "title": "Frequently Asked Questions (FAQ)", "section": "Frequently Asked Questions (FAQ)", "text": "faq\n# Frequently Asked Questions (FAQ)\n## Page Contents\n## General\nIT Infrastructure\nData Formats\n## Metadata\n## Traceability\nData Analysis\nData Import/Export\n## Interfaces\n## Development\n## Rollout\n## Pilot Phase\n## Meindl, Kristina\n¶\n## General\nWho can use the Data Store? Is the use of the Data Store mandatory?\nIn the future, all BAM employees who work with research data will be able to use the Data Store. Divisions that have registered for the rollout commit themselves to using the Data Store after the end of the onboarding phase. It is the responsibility of the division head to ensure that the Data Store is used appropriately.\nWho is responsible for maintaining the Data Store?\nThe Data Store is operated as a central service by the central IT (VP.2). Training and consulting is provided by the eScience section (VP.1). The divisions themselves are responsible for the content of their group and maintenance of metadata and data.\nWhat kind of data should be stored and what data should NOT be stored in the Data Store?\nThe Data Store is primarily a system for the internal storage of research data produced at BAM. According to the BAM Data Policy, research data include all digital data that are the basis, object, work steps or result of research processes as well as data that serve to describe the former. Typical examples of research data are measurement and observation data, experimental data obtain in the laboratory, audiovisual information, methodological test procedures and protocols. Simulations, software source code, algorithms, and derivations are also research data that can be stored in the Data Store. However, since the Data Store does not include a version management system, it is recommended to use a more suitable service for software development projects, for example the\nBAM GitHub\n(more information can be found\nhere\nin the Infoportal). It is not allowed to store private data, in particular no personal data, in the Data Store. If you are not sure whether your data may be stored in the Data Store, please contact\ndatastore@bam.de\n.\nHow is the Data Store organized?\nThe Data Store is divided into groups that correspond to the BAM divisions. By default, each group receives its own Inventory for the digital representation of laboratory inventory such as measuring instruments, chemical substances, samples, protocols, etc., as well as associated documents (e.g. technical data sheets). By default, each user additionally receives their own Lab Notebook with personal folders for the documentation of research activities and related data. If required, project-based folders (for several project members) can also be created in the Lab Notebook in addition to the personal folders.\nCan closely collaborating divisions have common Spaces in the Data Store Inventory?\nEach division is assigned open Spaces (readable by all Data Store users) and closed Spaces (readable and editable only by division members) in the Data Store Inventory. Group Admins can grant read/write access to the Spaces to members of other divisions to enable joint work. If required, dedicated shared Spaces can be created as well.\nHow are access rights defined within the Data Store?\nThe Data Store is divided into groups, which correspond to BAM divisions. Within a group, roles can be assigned and access rights can be further defined. Users with the appropriate rights can also grant specific access to Spaces and Projects to non-group members. More information about the rights and role system in the Data Store can be found\nhere\n.\nCan changes in the BAM organization (e.g. merging of divisions) be represented in the Data Store?\nThe user accounts in the Data Store as well as the organizational affiliation of the users are derived from the central user directory of BAM (LDAP). If your division is affected by upcoming organizational changes (e.g. renaming or merging) and if the division is  already using the Data Store, please contact\ndatastore@bam.de\n.\nIs the Data Store/the underlying software openBIS multilingual?\nNo, the user interface of openBIS as well as the official documentation of the ETHZ are only available in English. Training materials for the Data Store are also provided in English. If you need a translation of certain documents/articles, please contact us via datastore@bam.de. Except for justified exceptions, we also recommend documenting user-defined metadata in openBIS exclusively in English to ensure better interoperability.\nIs the Data Store also suitable for the management of data generated in the context of testing tasks/scientific and technical services (Wissenschaftlich-Technisches Dienstleistungen, WTD)?\nIn principle, any type of data can be stored in the Data Store and described with metadata. Whether the Data Store is also suitable for managing test data will be investigated during the rollout.\nMy data files are very large, can the Data Store handle them?\nThe actual data files are stored in the Data Store as files on disk storage, only the metadata is stored in the database. This means that even very large data volumes are possible, as long as the corresponding storage resources are available. In addition, there is the possibility via git-annex in openBIS to manage only references to datasets when they become too large.\n¶\nIT Infrastructure\nWhat happens to the existing file services after the introduction of the Data Store?\nThe personal file services (drive \"M:\") will still be provided. The necessity of shared file services by divisons, project groups, or departments will be reviewed. If required, they will be provided as a supplement to the Data Store.\nWhere is the data stored in the Data Store physically located? Is there be a backup of the Data Store?\nAll data of the Data Store are exclusively stored on servers in the computing center of BAM (UE). The datasets/metadata are backed up regularly/continuously in a multi-stage process.\nWill the Data Store be permanently available?\nAs a central RDM system, the Data Store is designed for permanent operation as far as this is technically possible. However, it will not be possible to avoid interruptions  for maintenance work and updates.\nWill there be access to the Data Store from the lab networks (\"Labornetze\")?\nYes, access from laboratory networks will be enabled where necessary.\nWill there be access for external parties to the Data Store?\nNo, the Data Store is designed as a system for internal research data management at BAM and requires access from internal BAM networks (including VPN). Therefore, external users cannot access the Data Store. However, it is possible to export data from the Data Store, e.g., to the public repository\n## Zenodo\n.\n¶\nData Formats\nWhich file formats can be stored in the Data Store?\nopenBIS works independently of file formats: The data are stored as files in the file system, the associated metadata in a database.\nCan I read/work with proprietary file formats in the Data Store?\nThe Data Store is not a live environment for reading or editing files, independent of whether their format is open or proprietary. To read or edit files stored in the Data Store using specialized software, they must be downloaded locally. The modified files can then be re-saved in the Data Store and linked to the original dataset. If the underlying file format allows programmatic access, files stored in the Data Store can also be read and analyzed via APIs (for example, with the Python module\npyBIS\nand Jupyter Notebook), but not modified.\n¶\n## Metadata\nWhat kind of metadata standards are used in the Data Store?\nIn the Data Store, individual metadata standards can be defined depending on the research domain. Some basic standards are already offered (e.g. for the description of instruments, chemicals and experimental steps). The definition of additional (domain) metadata standards is an important part of the onboarding process. The divisions are supported in this task by the Data Store team.\nCan ontologies be represented in the Data Store?\nThere is currently no function to import ontologies to openBIS. However, it is possible to add so-called semantic annotations when defining metadata Object and Property Types. However, these are currently not visible in the UI and can only be accessed via API.\n¶\n## Traceability\nCan I edit data once it is stored in the Data Store?\nIn the Data Store, there is a clear technical seperation between data (in the form of files) and descriptive metadata. Files are stored in folders, so-called Datasets, which can contain one or more file(s) and whose content is immutable and stored on disk storage. However, Datasets be deleted in their entirety. Metadata entities, on the other hand, are stored in a separate database and can be both edited and deleted.\nCan edits of (meta)data be tracked in the Data Store?\nAll edits to the metadata in the Data Store are saved (which change was made when and by whom), so that the entire history of a metadata entry can be traced back if necessary. Files stored in the Data Store, on the other hand, are immutable: They cannot be edited after they have been saved, but they can be deleted.\nCan metadata/data stored in the Data Store be deleted and can deleted metadata/data be recovered?\nUsers with appropriate rights can edit entities as well as delete metadata Objects and associated files (Datasets). Deleted Objects and Datasets are first moved to the openBIS trashcan from where they can be restored if necessary. If Objects/Datasets are removed from the trashcan, they are permanently deleted from the underlying database and cannot be restored. By default, only users with Space Admin and Instance Admin role have permission to delete. If you want to completely prevent editing/deleting an entity, you can irreversibly \"freeze\" individual Objects or Datasets,  as well as entire folders in the Data Store.\nCan chronological relationships be represented in the Lab Notebook of the Data Store?\nYes, by means of directed links between Objects, so-called \"parent-child relationships\", the chronological sequence of, e.g., several experimental steps can be represented in a hierarchy tree. You can find more information about parent-child relationships\nhere\n.\n¶\nData Analysis\nWhat are Jupyter Notebooks and how can they be used to analyze data in the Data Store?\n## Jupyter Notebook\nis a web-based interactive computing platform that combines live code, equations, narrative text, visualizations etc. The Jupyter system currently supports over 100 programming languages including Python, Java, R, Julia, Matlab, Octave and many more. Jupyter Notebooks can be used to analyze data stored in an openBIS instance, e.g., by connecting a local Jupyter installation with the Data Store. The output of the analysis and the notebooks themselves can then be saved in the Data Store and connected to the dataset they are based on. It is possible to download an\nextension for JupyterLab\nthat adds three buttons to a default notebook to\nconnect to an openBIS instance,\ndownload datasets from the openBIS instance,\nupload the notebook to the openBIS instance. For researchers using Python for data analysis, we recommend to use\npyBIS\n, a Python module for interacting with openBIS, which is designed to be most useful in a Jupyter Notebook.\n¶\nData Import/Export\nWhat are the upload options to the Data Store?\nFiles can be uploaded to the Data Store in several ways:\nvia the graphical user interface (GUI),\nvia script, e.g., via the Python module\npyBIS\n,\nvia the Dropbox mechanism, where files are copied to a specially created Dropbox folder.\nYou can find more information about the upload via GUI\nhere\n.\nWill there be support for implementing Dropbox scripts for automated data import?\nIt is planned to develop a \"meta dropbox script\" that offers a number of core functionalities (e.g. search for metadata Objects in the Data Store to which the datasets are to be attached; create new Objects and set metadata properties) as well as a template for entering the required metadata. The division must fill this template with the appropriate metadata: either manually or automatically via a parser script that is tailored to a specific data format.\nCan warnings or events be triggered when importing data via Dropbox?\nYes, the Dropbox scripts can validate incoming data and act accordingly.\nCan continuous data streams from measuring devices be included in the Data Store?\nNo, openBIS works with files only. Continuous data streams must be split into individual data files (e.g. per week, day, hour) which can be saved in openBIS as immutable data sets. The integration of openBIS and measuring devices is possible via the so-called Dropbox mechanism. For this, the data files must be saved in a dedicated Dropbox folder; from there they are uploaded to the Data Store. The Dropbox can be controlled via scripts that contain the logic for the data upload. You can find more information\nhere\n.\nCan data be exported/published from the Data Store?\nYes, files and descriptive metadata can be exported, both locally as well as to the public research data repository\n## Zenodo\n.\n¶\n## Interfaces\nWhat interfaces (to devices and software) does the Data Store offer?\nThe APIs of openBIS are described\nhere\nin the openBIS documentation of the ETHZ. There are programming interfaces to Java, Javascript and Python. For data analysis, there is the possibility to connect a local Jupyter installation to the Data Store (Jupyter Notebook, Hub and Lab). Jupyter itself provides numerous kernels to support a variety of programming languages for analyzing data in openBIS.\nWill there be an interface between the Data Store to the hazardous substances database sigmaBAM in order to avoid duplicate work when entering hazardous substances?\nThere is currently no interface between the Data Store and sigmaBAM. Although both systems are intended for the digital representation of chemicals or hazardous substances, the perspective differs: sigmaBAM is used for the documentation of handling permits for hazardous substances as well as for a yearly updated total quantity of a hazardous substance. In the Data Store, on the other hand, it is recommended to represent the specific batch of a chemical and link it to experimental steps to ensure traceability. However, the metadata format for chemicals in the Data Store is based on the metadata format of hazardous substances in sigmaBAM. For divisions that have already entered hazardous substances in sigmaBAM, it is therefore possible to export these as Excel lists and import them (after some adjustments) into the Data Store.\nWill there be an interface from the Data Store to the E-Akte (or vice versa)?\nAt the moment, there are no direct interfaces between the Data Store and the E-Akte, because the two systems are used to store different kinds of data (research data in the Data Store vs. (other) record-relevant documents in the E-Akte). If necessary, files in the E-Akte can be hyperlinked in the Data Store.\nCan data stored in the Data Store be linked to publications in Publica?\nThere are currently no interfaces between the Data Store and Publica. However, it is possible to reference publications in the Data Store, e.g. via hyperlinks to their DOIs.\n¶\n## Development\nI have an idea for a new/improved openBIS feature or plugin for the Data Store. Where can I submit this?\nPlease send your ideas for improved and/or additional features to datastore@bam.de. Depending on whether your idea is a BAM-specific or a general openBIS feature, we will include your suggestion in our feature request list or forward it to the openBIS development team at ETHZ. Please note, however, that due to limited development resources we cannot implement every feature request and therefore prioritize them according to effort/added value. Since openBIS is an open source software, the development team at ETHZ also handles openBIS feature requests at its own discretion.\nCan I develop my own plugins for the Data Store?\nIf you would like to participate in the development of plugins for the Data Store, please contact\ndatastore@bam.de\n.\n¶\n## Rollout\nWhen will the rollout of the Data Store begin?\nThe rollout of the Data Store takes place in phases. In each phase, several divisions will be onboarded to the Data Store at the same time. The first rollout phase started in May 2023.\nHow can I register my division for the rollout?\nAll division heads were contacted by the project team in December 2022 as part of a survey to gauge their interest in rollout participation. Based on the responses, the rollout sequence for the first three phases was determined. In February 2025, another survey was conducted, which will now serve as the basis for further phase planning. The divisions involved in phase 4 have already been contacted. If you are a head of division and did not receive the survey, or if you wish to change your response, please contact us at\ndatastore@bam.de\n.\nWhat is a Data Store Steward/Group Admin and what are the responsibilities associated with the role?\nBefore the Data Store can be used operationally, research group-specific data structures and metadata schemas have be developed. To this end, at least one Data Store Steward must be appointed from each division to take on this task in consultation with colleagues and the division head. The Data Store Stewards are trained and supported in their work by the Data Store project team. Once the Data Store has been implemented for a division, the Data Store Steward acts as Group Admin. Group Admins can access all data and metadata within their own group and (in the case of metadata) edit it, as well as adjust access rights and other settings to meet the requirements of the division. Data Store Stewards are also the point of contact for research data management (RDM) issues within the division and are responsible for introducing new employees to the Data Store once onboarding has been completed. Data Store Stewards should be familiar with methodology and processes within the division and its research domain. Prior IT and RDM experience is helpful, but not required. Ideally, Data Store Stewards should have permanent positions in the division to reduce the risk of knowledge loss upon departure. For larger divisions with subgroups, it is recommended that at least two Data Store Stewards be assigned.\nI would like to take on the role of Data Store Steward/Group Admin for my division, where can I apply for this?\nTo do so, please talk first with the head of your division and then contact us at\ndatastore@bam.de\n.\nWill there be training and/or manual on how to use the Data Store?\nYes, during onboarding, target group-specific training (for Data Store Stewards and for normal users) will take place. In addition to the openBIS documentation of the ETHZ, accompanying training material will be provided here in the Data Store Wiki.\nHow much time should a division invest for the rollout?\nA rollout phase lasts approx. 2-3 months. During this time, onboarding events are held for the Data Store Steward(s) and for the members of the division. The effort required for adjustments to the Data Store and, if necessary, the setup of interfaces depends on the research subject, the equipment, the working methods, and the FDM requirements of the department and cannot be specified in general terms. The obligations of DSSts include carrying out the onboarding process (approx. 5 working days) and scheduling additional time for system adjustments, as well as integration into daily workflows after onboarding.\nHow can I/my division prepare for the rollout?\nIn preparation for the rollout (and for good research data management in general), it is recommended to consider the following points:\nHow to define uniform rules for naming files within the division (e.g., YYYYMMDD_someMeasurement_Initials_v1.csv)?\nWhat kind of entities should be represented in the Inventory of the Data Store? Can the properties of these entities be structured as standardized metadata in lists? These lists can later be customized and imported into the Data Store.\nWhat kind of information is needed in a typical experiment description? Can this also be standardized in the form of metadata? Can templates for experiment descriptions be created?\nWhat are the links between experimental steps and inventory elements (e.g. an experimental step should be linked with the measuring instrument that was used)?\nWhat kind of output data formats are generated by measurements? Are they open or proprietary (can only be read with special software from the manufacturer)? If open, what kind of measurement metadata are included? Is it possible to write a parser in the form of a short script that automatically reads this metadata?\nWill we get additional staff for the rollout?\nUnfortunately not, the subject-specific implementation and customization of research data management in the Data Store is to be conducted by the divisions themselves. Each division must appoint one or more Data Store Steward(s) for this task. However, the Data Store team from the sections VP.1 (eScience) and VP.2 (IT) will closely accompany the onboarding process and support the Data Store Stewards and the users by offering webinars, Q&A sessions and training materials.\nCan I test openBIS before my division is onboarded to the Data Store?\nThree demo openBIS instances of ETHZ and EMPA are available\nhere\n.\nIn future, we will provide a demo instance of the Data Store. If you are interested in testing it, please contact the Data Store team at\ndatastore@bam.de\n.\nThe openBIS installer as well as a pre-installed virtual machine image is available\nhere\n. For the download, a registration at the SIS helpdesk is necessary. A\ncontainer image\nis also provided. It should be noted, however, that the entire functionality only becomes visible after a complex configuration process and not all features can be easily grasped in a demo installation.\nCan I participate in the Data Store rollout as an individual employee?\nNo, the rollout of the Data Store is designed for divisions and not for individual employees.\n¶\n## Pilot Phase\nMy research group was part of the pilot project. What happens to the pilot instance during the rollout?\nAs agreed at the beginning of the pilot project, the pilot instances will continue to be supported, provided with security updates and kept operational, but not provided with new features (which go beyond the regular openBIS updates) or further customised. In the medium term, it is planned to transfer the data of the pilot instances to the BAM-wide system.\nWill more pilot groups be onboarded as test cases for the Data Store?\nNo, the pilot project was completed in February 2022 and no additional pilot groups will be included. All further groups will be included in the rollout, i.e. the central implementation of the Data Store, which started in May 2023.", "timestamp": "2025-09-18T09:38:30.149066Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_Feedback:0", "source": "https://datastore.bam.de/en/Feedback", "repo": "datastore", "title": "Feedback", "section": "Feedback", "text": "# Feedback\n# Feedback\n## Demidova, Caroline\nPlease leave your feedback in this\n## Onboarding\nMsTeams channel.", "timestamp": "2025-09-18T09:38:30.154280Z", "source_priority": 1, "content_type": "general"}
{"id": "docs:datastore:en_home:0", "source": "https://datastore.bam.de/en/home", "repo": "datastore", "title": "Welcome to BAM Data Store", "section": "Welcome to BAM Data Store", "text": "# Welcome to BAM Data Store\n## Page Contents\nWelcome to the Data Store Wiki\n## Wiki Structure:\n## 💡 Concepts\n📖 How-to guides\n## ❓ FAQ\n👥 Use cases\nWhat is the Data Store?\nWhat is openBIS?\nWhat is the Data Store Project?\nWhat is the Data Store rollout process?\n## Demidova, Caroline\nYesterday at 11:44 AM\n¶\nWelcome to the Data Store Wiki\nThis Wiki provides information on the BAM Data Store - the central system for research data management at the Bundesanstalt für Materialforschung und -prüfung (BAM).\nThe Wiki is not intended to replace the openBIS documentation by the ETHZ (\nUser docs\n,\nAdmin docs\n). It provides conscise guidance and should serve as an additional source of openBIS and Data Store documentation for BAM employees.\nSome articles of this Wiki are currently under construction. If you have further questions that are not yet answered here, please contact\ndatastore@bam.de\n.\n¶\n## Wiki Structure:\n¶\n## 💡 Concepts\nExplanation about terms and concepts.\n## Explore Concepts\n¶\n📖 How-to guides\nStep-by-step instructions for openBIS functions.\nGo to Guides\n¶\n## ❓ FAQ\nFrequently asked questions about Data Store and openBIS.\nView FAQ\n¶\n👥 Use cases\nDiscover Use cases of the Data Store.\nDiscover Use cases\n¶\nWhat is the Data Store?\nThe Data Store is the central system for research data management (RDM) at BAM.\nThe Data Store is the central system for research data management (RDM) at BAM.\nIt enables divisions to digitally organize and describe laboratory inventory -such as instruments, samples, standard operating procedures (SOPs), using customizable metadata and linked documentation..\nIntegrated with electronic lab notebook (ELN), the Data Store allows experimental steps to be connected with inventory items, ensuring centralized and traceable documentation of research processes.  This structure linkage supports the FAIR principles\n[1]\n(Findable, Accessible, Interoperable, Reusable), facilitating both internal and external reuse of research data in line with funding bodies.\nBy storing data and metadata in a unified system, the Data Store enhances interdisciplinary collaboration and lays the foundation for advanced data analysis, including big data and Artificial Intelligence (AI) applications.\n¶\nWhat is openBIS?\nopenBIS (open Biology Information System) is the underlying platform of the Data Store.\nIt is an open-source software solution for Research Data Management (RDM) and Electronic Lab Notebook (ELN).\nDeveloped and maintained by the Scientific IT Services (SIS) at ETHZ Zurich (ETHZ), openBIS was originally designed for life sciences\n[2]\n[3]\n, it is now increasingly used materials science and other research domains.\nopenBIS provides a browser-based graphical user interface (GUI) for the managing digital laboratory inventories and documenting experiments in a standardized way.  Data files can be imported into via the GUI or through programming interfaces and linked to inventory items and experimental steps.\nopenBIS also supports integration with external tools and services, such as exporting data to\n## Zenodo\nrepository and analyzing scientific data in\n## Jupyter Notebook\n.\nFor more information on openBIS visit the official website (\nhttps://openbis.ch/\n).\n¶\nWhat is the Data Store Project?\nThe introduction of a RDM system does not happen overnight.\nTo evaluate openBIS suitability, a pilot phase was conducted from 01.12.2020 to 28.02.2022. During this period, five BAM research groups from diverse domains successfully implemented openBIS, confirming its effectiveness for managing data in various materials science domains.\nFollowing the pilot’s success, the Data Store project was approved to establish the system based on openBIS as a central RDM system across all BAM divisions. The project, led by VP.1 eScience and VP.2 Information Technology, began in October 2022 and is scheduled to run for 3.5 years.\nThe initial phase focused on developing the necessary IT infrastructure and preparing for the software rollout, included an analysis of RDM needs across BAM. The rollout of the Data Store began in 2023.\nFor more information about the Data Store project visit the BAM infoportal (\nAbout the Project\n)\n¶\nWhat is the Data Store rollout process?\nThroughout all phases, project management and communication play a crucial role in supporting a smooth and efficient implementation.\nThe actual Data Store rollout began in May 2023 and is being carried out in successive phases, with several divisions being trained at the same time. Lessons learned are collected at the end of each rollout phase to implement improvements in subsequent rollout phases. The order of the rollout is determined based on the interest expressed by division heads in surveys done in December 2022 and February 2025.\nThe current onboarding concept lasts 2 to 3 months for assigned Data Store Stewards (DSSt), including 1 day for division heads and all employees working with research data.\nThe Data Store Stewards are one or two members of the division appointed by the division head. They are ideally permanently employed for sustainability and are familiar with inventory, experimental processes and the (digital) workflows of the division. DSSts are trained to use main functions and customize the group settings.\nAll other division members who handle research data receive introductory training in the use of the system. The head of division takes part in information events and coordinates the completion of the rollout phase together with the DSSts.\nAfter the rollout, the heads of the division and the DSSts, together with the users, are responsible for storing newly generated research data and the continuous implementation of the Data Store within their division.\nMark D. Wilkinson et al. (2016). \"The FAIR Guiding Principles for scientific data management and stewardship\". Scientific Data. 3 (1): 160018. doi:\n## 10.1038/SDATA.2016.18\n.\n↩︎\nAngela Bauch et al. openBIS: a flexible framework for managing and analyzing complex data in biology research. 12, 468 (2011). doi:\n10.1186/1471-2105-12-468\n.\n↩︎\nCaterina Barillari et al. openBIS ELN-LIMS: an open-source database for academic laboratories. Bioinformatics. 32 (4), Feb 2016, 638–640. doi:[10.1093/bioinformatics/btv606].(\nhttps://doi.org/10.1093/bioinformatics/btv606\n).\n↩︎", "timestamp": "2025-09-18T09:38:30.157283Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides:0", "source": "https://datastore.bam.de/en/How_to_guides", "repo": "datastore", "title": "How-to Guides", "section": "How-to Guides", "text": "How_to_guides\n# How-to Guides\n## Page Contents\nHow to use the Data Store - main functions for Users\nHow to start\nRegister data in the Lab Notebook\nConnect Experimental Steps in the Lab Notebook\nEdit, Delete and Move functions\nRegister data in the Inventory\n## Connect Inventory Objects\nManage Storage of Objects\n## Barcode\n## Additional Function\nExport data\nCustomize Group Settings - Admin/Data Store Stewards\nGroup settings\n## Templates\nObject types\nParents and Children sections\nStorage for Objects\nOnboarding Data Store Stewards\nLogin training instance\nShare Code in BAM research GitHub\n## Demidova, Caroline\nHere you find an answer to \"How do I …?\" related questions on how to use the BAM Data Store and its  underlying software - openBIS. These goal-oriented instructions should help you accomplish specific tasks.  If the function you are looking for is missing, please contact the Data Store Team at\ndatastore@bam.de\n.\n¶\nHow to use the Data Store - main functions for Users\n¶\nHow to start\nLog in to the BAM Data Store - main instance\nRepresent research data - Conceptual data model\nManage Access to Spaces and Projects\n¶\nRegister data in the\n## Lab Notebook\nRegister a Project\nRegister Collection of the type Default Experiment\nRegister non-sequential Experimental Steps\nRegister sequential Experimental Steps\nUpload data\n¶\nConnect Experimental Steps in the\n## Lab Notebook\nDefine Parents and Children of Experimental Steps\nDisplay connections of Experimental Steps - Hierarchy Graph\nAdd multiple Children to an Experimental Steps - Children generator\n¶\nEdit, Delete and Move functions\n## Project Overview\nEdit Projects, Collections, Objects and Datasets\nDelete Projects, Collections, Objects and Datasets\nRevert deletions of Collection, Objects and Datasets\nMove Projects, Collections, Objects and Datasets\nMove Experimental Step - Object with descendants\nHistory of Changes\n¶\nRegister data in the\n## Inventory\n## Register Projects\n## Register Collections\n## Register Objects\nBatch Registration of Objects\nBatch Update of Objects\n¶\n## Connect\n## Inventory\n## Objects\nConnect Inventory Objects with Experimental Steps from Lab Notebook\nDefine Parents and Children of Inventory Objects\n¶\nManage Storage of Objects\nAllocate Storage position to an Object\nBatch registration/update of storage position(s)\n## Verify Storage Position\n## Delete Storage Position\n¶\n## Barcode\nUse Barcodes and QR Codes\nPrint Barcodes or QR codes\nScan Barcodes and QR Codes\n¶\n## Additional Function\nEmbedding Images in Text Fields\nFilter Objects within a Collection\nSearch - Global and Advanced search across all fields of all Entities\nSearch for: Objects in the Inventory\nSearch for: Objects - Experimental Steps in the Lab Notebook and save search queries\nUse saved search queries\nSearch for Datasets\n¶\nExport data\nExport (meta)data to file\nExport data to Zenodo\n¶\nCustomize Group Settings - Admin/Data Store Stewards\n¶\nGroup settings\nCustomize the Main menu\n## Enable Barcodes\n¶\n## Templates\nCreate Templates for Experimental Steps and other Objects\n¶\nObject types\nEnable Object Types in drop-downs\n¶\nParents and Children sections\nCustomize Parents and Children sections in Object Forms\nAdd a hint in Parents and Children sections\n¶\nStorage for Objects\nEnable Storage Widget on Object Form\nConfigure Storage of Objects\n¶\nOnboarding Data Store Stewards\n¶\nLogin training instance\nLog in to the BAM Data Store-training instance\nChecklist Group Settings customization\nChecklist Use Case implementation\n¶\nShare Code in BAM research GitHub\nShare Code in BAM research GitHub", "timestamp": "2025-09-18T09:38:30.164377Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Add_hint_parents_and_children_sections:0", "source": "https://datastore.bam.de/en/How_to_guides/Add_hint_parents_and_children_sections", "repo": "datastore", "title": "Add a hint in Parents and Children sections", "section": "Add a hint in Parents and Children sections", "text": "How_to_guides\n/\nAdd_hint_parents_and_children_sections\n# Add a hint in Parents and Children sections\n## Demidova, Caroline\nIn the left main menu, under\n## Utilities\nselect\n## Settings\n. The Select Group Settings drop-down menu will appear, select your\ndivision number\nto open your group settings. Click on the\n## Edit\ntab, navigate to the Object type definitions Extension section, options to modify those sections within an Object type will be displayed. In the\nHints for\npart of settings there is an option to set a particular Object type(s) as Parent(s) or Child(ren) and limit the number of them. By pressing the\n+\ntab on the right corner of the\nHints for\nrow, you can extend the number of the Parents and Children. In case, if minimal number of Parents and/or Children is specified, the required number of those is mandatory to enter and the form cannot be saved until this condition is satisfied. Annotation to these connections can be supported by using the Properties. To add Property press\n+\nleft to the Parent(s)/Child(ren) field. Review the entries and\n## Save\n.\n## Under Utilities\n## Select Settings\nSelect division number\nClick on Edit tab\nNavigate to Object type definitions Extension section\nSelect an Object type\nIn the Hints for part set Parent(s) and/or Child(ren) to the particular Object type\nSpecify minimal and maximal number of Parents and Children\nPress + tab to add annotations\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.170911Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Batch_registration_Inventory:0", "source": "https://datastore.bam.de/en/How_to_guides/Batch_registration_Inventory", "repo": "datastore", "title": "Batch register Objects in a Collection", "section": "Batch register Objects in a Collection", "text": "How_to_guides\n/\nBatch_registration_Inventory\n# Batch register Objects in a Collection\nAriza de Schellenberger, Angela\nMultiple Objects can be registered in a\n## Collection\nvia import\n## Excel Template\n. Navigate to the relevant\n## Collection\n, open the\n## More\ndrop-down menu, and select\nXLS Batch Register Objects\n.\n## Download\nthe\n## Template\n, fill out the form and enter values for all properties, upload it and click\n## Accept\n. To confirm batch registration, navigate to updated\n## Collection\n.\nTo assign\n## Parents\nto objects during registration, first assign numbers  to the Parents in the\n$\ncolumn (e.g., $1). Then assign the number of a Parent ($1) to an Object in the\n## Parents\ncolumn to establish the relationship.\n## Select Collection\nOpen More drop-down menu\nSelect XLS Batch Register Objects\nClick on Template Download, update the file\nand enter values for all mandatory (*) Properties\nUpload the file\nReview the entries and Accept.", "timestamp": "2025-09-18T09:38:30.176340Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Batch_registration_storage_position:0", "source": "https://datastore.bam.de/en/How_to_guides/Batch_registration_storage_position", "repo": "datastore", "title": "Batch registration/update of storage position(s)", "section": "Batch registration/update of storage position(s)", "text": "How_to_guides\n/\nBatch_registration_storage_position\n# Batch registration/update of storage position(s)\n## Demidova, Caroline\nTo allocate storage position to multiple Objects, the Data Store Steward(s) for your group must customize the\nstorage for Objects\nand\nEnable object types in drop-downs\nso that\nAllowed object types\nare displayed in Excel spreadsheet for batch registration.\nTo assign a storage location to multiple objects, navigate to the relevant\n## Collection\nand select\nXLS Batch Register Objects\nfrom the\n## More\ndrop-down menu. Ensure that the required Object Types are displayed in the\n## Register Objects\nwindow and click on\n## Download\n. The Excel Template should contain at least two sheets (the SAMPLE and STORAGE_POSITION metadata). To link these sheets, the information in the $ column of the SAMPLE spreadsheet must match the\n## Parents\ncolumn in STORAGE_POSITION spreadsheet. Enter the numbers or letters proceeded by the $ symbol (i.e., $1, $2) in the $ column. Upload the updated file and click\n## Accept\n.\nTo find the\nstorage Code\nrequired in the STORAGE_POSITION spreadsheet, navigate to\n## Utilities\n,\n## Settings\n(choose your division’s number), open the\n## Storages\nsection and select relevant storage to view its metadata. Enter the required information and save the file locally on your device.\n## Select Collection\nOpen More drop-down menu\nSelect XLS Batch Register Objects\nClick on Template Download\n(The Excel Template should contain at least two sheets,\nthe SAMPLE and STORAGE_POSITION metadata.)\nUpload the file\nReview the entries and click Accept.", "timestamp": "2025-09-18T09:38:30.184116Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Batch_update_Inventory:0", "source": "https://datastore.bam.de/en/How_to_guides/Batch_update_Inventory", "repo": "datastore", "title": "Batch Update of Objects", "section": "Batch Update of Objects", "text": "How_to_guides\n/\nBatch_update_Inventory\n# Batch Update of Objects\nAriza de Schellenberger, Angela\n## Multiple\n## Objects\ncan be updated in a\n## Collection\nusing an Excel Template. Navigate to the relevant\n## Collection\n(in the Inventory or Lab Notebook) and click on the\n## COLUMNS\ntab in the Collection form. Select the\n## Identifier\nof the Properties you want to update. If you have multiple Objects, you can filter the table. To export the Excel table for the selected Properties, click on the\n## EXPORTS\ntab and select\n## Import Compatible\n## (Yes);\n## Columns\n(All) (default order);\n## Rows\n(All) pages/Current page/Selected rows (depending on the Objects you want to export and update). Click\n## EXPORT\nto download the table. Modify the file and save it. In the Object form, click the\n## More\ndrop-down menu and select\nXLS Batch Update\n. Upload the file and press\n## Accept\n. To confirm batch update, navigate to updated\n## Collection\n.\n## Select Collection\nClick on Columns tab\nPress show all\n## Click on Exports tab:\n## Select Import Compatible (Yes),\nColumns(All) (default order),\n## Rows(All) Pages\nClick on Export\nUpdate the Excel Template file and save it\nClick on More drop-down menu (Object form)\nSelect XLS Batch Update Objects\nUpload the Excel Template file\nReview the entries and Accept.", "timestamp": "2025-09-18T09:38:30.191143Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Checklist_group_settings:0", "source": "https://datastore.bam.de/en/How_to_guides/Checklist_group_settings", "repo": "datastore", "title": "Checklist Group Settings Customization", "section": "Checklist Group Settings Customization", "text": "How_to_guides\n/\nChecklist_group_settings\n# Checklist Group Settings Customization\n## Page Contents\n## ✅ Group Settings Customization Checklist\nObjects registration\nParent-child relationships\n## Barcodes\n## Lab Storage\nGroup ELN Settings\n## Demidova, Caroline\n¶\n## ✅ Group Settings Customization Checklist\nThis checklist helps you keep track of all group settings and configurations for your department that can be customized by the Data Store Steward.\n¶\nObjects registration\nEnable Object Types in drop-down menus\nCreate Templates for Experimental Steps and other Objects\n¶\nParent-child relationships\nCustomize Parents and Children sections in Object Forms\nAdd a hint in Parents and Children sections\n¶\n## Barcodes\n## Enable Barcodes\n¶\n## Lab Storage\nEnable Storage for an Object Type\nConfigure Lab storage\n¶\nGroup ELN Settings\n## Storages\n## Templates\nObject Type definition extension\n## Inventory Spaces\nCustomize the Main menu\n## Miscellaneous\n## Optional:\nManage Access to Spaces and Projects - FB Inventory folders\nRoles defined per default in the Data Store\nGroup settings can be updated adopt to the data policies of projects, when required.\nDefine naming conventions for entities (Project, Collections, Object, Data sets); templates, etc. to ensure consistency and facilitate findability.", "timestamp": "2025-09-18T09:38:30.197853Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Checklist_implementation:0", "source": "https://datastore.bam.de/en/How_to_guides/Checklist_implementation", "repo": "datastore", "title": "Checklist Use Case implementation", "section": "Checklist Use Case implementation", "text": "How_to_guides\n/\nChecklist_implementation\n# Checklist Use Case implementation\n## Page Contents\n## Example 1\n## 🎯 Define Realistic Goals\n📊 Select Data to Represent\n🗂️ Represent research data in the Data Store - Create a Conceptual Model\n📝 Create Templates for Experimental Steps\n## 🚀 Use Case Implementation\n## 🧪 Define Lab Storage\n## 🔍 Define Search Queries\nAriza de Schellenberger, Angela\n¶\n## Example 1\nThis checklist contains implementation steps that DSSt should consider in order to implement a use case for the division with the goal to improve the discoverability and reusability of research data and simplify the use of Data Store for users.\n¶\n## 🎯 Define Realistic Goals\nEstablish clear and realistic objectives for using the Data Store.\nThis check list focuses on improving findability and reusability of research data.\n¶\n📊 Select Data to Represent\nChoose new research data.\nChoose structured, tabular data that is easily accessible.\n¶\n🗂️ Represent research data in the Data Store - Create a Conceptual Model\nUse the\nRepresent_research_data\n.  Use the\ntemplate\nin (\ndraw.io\n) to Identify key steps and data points in the workflow; map data in the folder structure for the Data Store and define parent-child relationships between objects.\nManage roles and rights at the Space and Project level to comply with agreements and contracts of related research projects.\n¶\n📝 Create Templates for Experimental Steps\nDevelop templates that include sufficient information to describe datasets attached to experimental steps.\n¶\n## 🚀 Use Case Implementation\nAutomate metadata import from instruments (e.g., generate barcodes).\nAutomate metadata import from measurements (e.g., structured tabular data, reuse templates for experimental steps).\nBatch register objects such as inventory items and samples.\n¶\n## 🧪 Define Lab Storage\nSet up lab storage for samples and batch register/update samples.\nEnsure storage locations are well-documented and are easily found.\n¶\n## 🔍 Define Search Queries\nCreate and save search queries to improve data traceability.\nEnsure queries are easy to use and provide relevant results.", "timestamp": "2025-09-18T09:38:30.205195Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Children_generator:0", "source": "https://datastore.bam.de/en/How_to_guides/Children_generator", "repo": "datastore", "title": "Add multiple Children to an Experimental Steps - Children generator", "section": "Add multiple Children to an Experimental Steps - Children generator", "text": "How_to_guides\n/\nChildren_generator\n# Add multiple Children to an Experimental Steps - Children generator\n## Demidova, Caroline\nThe children generator function allows to register multiple children simultaneously during the registration of the Object or while editing it. At first, you need to register an Object of Type Experimental Step and defined at least one parent for it (e.g. instruments, chemicals, a sample, etc.).  The child generator will allow you to generate Children (e.g., subsequent Experimental Steps) with a defined combination of Parents.\nTo do this, select the\n## Experimental Step\nwith registered Parents. Click on the\n## Edit\ntab, scroll down to the\n## Parents\nand\n## Children\nsection in the Experimental Step form, and click on the\n## Generate Children\ntab. A matrix of Parents is displayed in the\n## Children Generator\nform. Select all Parents, choose the\n## Experimental Step\nin the\nObject type\ndrop-down menu and enter the number of replicates of newly generated Children, and click on\n## Generate\n. Review the entries and\n## Save\n.\nNote, to ensure the traceability of the data in the Data Store, a minimum set of mandatory Properties has been defined for some Object Types. The child generator only allows the registration of Objects with\nnon-mandatory\nProperties, as the Object form cannot be edited. It is therefore only possible to register multiple Objects of various Object Types with the child-parents relationship simultaneously using the\nBatch registration\nfunction.\nSelect Experimental Step with registered Parents\nClick on Edit tab\nScroll down to the Parents and Children section\nClick on Generate Children tab\nChoose Experimental Step in the Object type drop-down menu\nEnter the number of new child(ren)\nReview and click on Generate", "timestamp": "2025-09-18T09:38:30.210690Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Connect_Inventory_Lab_Notebook:0", "source": "https://datastore.bam.de/en/How_to_guides/Connect_Inventory_Lab_Notebook", "repo": "datastore", "title": "Connect Objects of Inventory with Experimental Steps from  Lab Notebook", "section": "Connect_Inventory_Lab_Notebook", "text": "How_to_guides\n/\n## Connect_Inventory_Lab_Notebook\n# Connect Objects of Inventory with Experimental Steps from  Lab Notebook\nAriza de Schellenberger, Angela\nTo connect an Inventory Object to an Experimental Step, select the relevant inventory Object and click on the\n## Edit\ntab. In the\n## Parent\nand\n## Children\nsection, click\n## Search Any\nand select\nExperimental step\nfrom the\nSelect an object type\ndrop-down menu. You will be returned to the\n## Update-Object\nform. Enter the\nCode or Name of the Object\nin the text box that will appear below the Parents section. Start typing the Code or Name of the Parent-Object to display available Objects for your group, select the appropriate Object and\n## Save\n.\nTo display linked Objects in a hierarchy graph, navigate to the edited Inventory item, click the\n## More\ndrop-down menu, and select\n## Hierarchy Graph\n.\nIt is also possible to define multiple Parents and Children to  Objects at the same time. To do this, use the\n## Paste Any\noption, enter the Code or Name of respective Objects, review the entries and\n## Save\n. You can copy the Code or Name of Objects from another ELN page (Log in to the BAM Data Store in another/private browser window). Paste the Codes(s) or name(s) in the text fields, review and\n## Save\n.\nSelect Object in the Inventory\nClick on Edit tab\nNavigate to the Parent and Children sections\n## Click Search Any\n## Select Object Type - Experimental Step\nEnter Code or Name of Experimental Step to connect\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.215162Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Customize_Main_menu:0", "source": "https://datastore.bam.de/en/How_to_guides/Customize_Main_menu", "repo": "datastore", "title": "Main menu", "section": "Main menu", "text": "How_to_guides\n/\nCustomize_Main_menu\n# Main menu\n## Page Contents\n# Main menu\n## ⚙️ Step 1: Access Group Settings\n## 🖱️ Step 2: Edit Main Menu Sections\n## 💾 Step 3: Save Changes\n🔄 Step 4: Reload the Interface\nAriza de Schellenberger, Angela\n¶\n# Main menu\n¶\n## ⚙️ Step 1: Access Group Settings\nIn the\nleft menu\n, go to\n## Utilities\n.\nClick on\n## Settings\n.\nIn the\n## Select Group Settings\ndrop-down, choose your\ndivision number\n.\n¶\n## 🖱️ Step 2: Edit Main Menu Sections\nClick the\n## Edit\ntab.\nNavigate to the\n## Main Menu\nsection.\nModify the sections by checking or unchecking the checkboxes to add or hide sections.\n¶\nAvailable Sections to Show/Hide:\nshowLabNotebook\nshowInventory\nshowStock\nshowObjectBrowser\nshowExports\nshowStorageManager\nshowAdvancedSearch\nshowArchivingHelper\nshowUnarchivingHelper\nshowTrashcan\nshowVocabularyViewer\nshowUserManager\nshowUserProfile\nshowZenodoExportBuilder\nshowBarcodes\nshowDatasets\n¶\n## 💾 Step 3: Save Changes\nReview your changes.\n## Click\n## Save\n.\n¶\n🔄 Step 4: Reload the Interface\nRefresh the openBIS webpage to see the updated main menu sections.", "timestamp": "2025-09-18T09:38:30.221245Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Customize_parents_and_children_sections:0", "source": "https://datastore.bam.de/en/How_to_guides/Customize_parents_and_children_sections", "repo": "datastore", "title": "Customize Parents and Children sections in Object Forms", "section": "Customize Parents and Children sections in Object Forms", "text": "How_to_guides\n/\nCustomize_parents_and_children_sections\n# Customize Parents and Children sections in Object Forms\n## Demidova, Caroline\nIn the left main menu, under\n## Utilities\nselect\n## Settings\n. The Select Group Settings drop-down menu will appear. Select your\ndivision number\nto open your group settings. Click on the\n## Edit\ntab, navigate to the\nObject type definitions\nExtension section and select the\n## Object Type\nyou want to customize. To deselect sections for Parents or Children, select the\nDisable section\ncheckbox, to disable the addition of any Object type as Parent or Child, select the disable addition of any object type) checkbox.\nTo make adding Parents and Children more convenient to users, enter a name under Section name (e.g., Parents - Section name: Instruments for Experimental Step).  Review the changes and Save.\n## Under Utilities\n## Select Settings\nSelect division number\nClick on Edit tab\nNavigate to Object type definitions Extension section\nSelect an Object type\nIf required: Select Disable section, Disable addition of any object type checkboxes\nEnter Section name for Parents and Children\nReview changes and Save.", "timestamp": "2025-09-18T09:38:30.226546Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Delete_ELN:0", "source": "https://datastore.bam.de/en/How_to_guides/Delete_ELN", "repo": "datastore", "title": "Delete Projects, Collections, Objects and Datasets", "section": "Delete Projects, Collections, Objects and Datasets", "text": "How_to_guides\n/\nDelete_ELN\n# Delete Projects, Collections, Objects and Datasets\n## Demidova, Caroline\nUsers can delete Projects, Default Experiments, Experimental Steps and Datasets in their\n## Lab Notebook\n-\n## My Space\n. In the division’s private\n## Inventory\n, users do not have\nsufficient rights\n(Space/Project User ) to delete contents and need to contact Data Store Steward(s) (DSSt(s)). The DSSt(s) manage the division’s private\n## Inventory\nand assign roles to users that support Inventory management.\nTo delete contents, navigate to the relevant folder, click on the\n## More\ndrop-down menu, and select\n## Delete\n. In the\n## Confirm Delete\nwindow, enter the *\n## Reason (\nmandatory)\nfor deletion. Click\n## Accept\nto confirm.\nDeletion of Default Experiments, Experimental Steps and Datasets can be\nreverted\n. Deleting Projects, on the other hand is\nirreversible\n.\nNote that deletions from the Trashcan are\nirreversible\n.\n## Select Project, Default Experiment,\nExperimental Steps or Dataset\nClick on More drop-down menu\n## Select Delete\nEnter Reason (*mandatory) for deletion\n## Click Accept.", "timestamp": "2025-09-18T09:38:30.231852Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Delete_storage_position:0", "source": "https://datastore.bam.de/en/How_to_guides/Delete_storage_position", "repo": "datastore", "title": "Delete Storage Positions", "section": "Delete Storage Positions", "text": "How_to_guides\n/\nDelete_storage_position\n# Delete Storage Positions\nAriza de Schellenberger, Angela\nTo delete storage information from an Object, navigate to the relevant\n## Object\n, click\n## Edit\nand navigate to the\n## Storage\nsection in the Object form.  Scroll to the right in the Storage table and click on the\n-\nicon to delete the storage.  The deleted Storage will be moved to the trashcan. To delete it permanently, delete it from the\ntrashcan\n.\n## Select Object\nClick on Edit tab\nNavigate to Storage sections\nClick - icon", "timestamp": "2025-09-18T09:38:30.235750Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Edit_ELN:0", "source": "https://datastore.bam.de/en/How_to_guides/Edit_ELN", "repo": "datastore", "title": "Edit – Projects, Collections, Objects and Datasets", "section": "Edit – Projects, Collections, Objects and Datasets", "text": "How_to_guides\n/\nEdit_ELN\n# Edit – Projects, Collections, Objects and Datasets\n## Demidova, Caroline\nTo edit Projects, Default Experiments - Collections and Experimental Steps - Objects , navigate to the relevant folder, click on the\n## Edit\ntab, enter the changes, review and\n## Save\n.\nSelect relevant folder (Default Experiment or Experimental Steps)\nClick on Edit tab\nEnter the changes\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.241150Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Embedding_Images:0", "source": "https://datastore.bam.de/en/How_to_guides/Embedding_Images", "repo": "datastore", "title": "Embedding Images in Text Fields", "section": "Embedding_Images", "text": "How_to_guides\n/\n## Embedding_Images\n# Embedding Images in Text Fields\n## Demidova, Caroline\nTo embed an image (.jpeg, .png formats) in the description of the Entity during the editing, drag an image in the description field. Another way is to click on the picture icon (\n) in the rich text editor field. Alternatively, the image (.jpeg, .png, .pdf, .svg formats) could be embedded as an\nELN Preview Dataset\n. For that, select relevant\n## Default Experiment (Collections)\nor\n## Object\n, and\nupload\nthe Dataset. In the\n## Create Dataset\nform, in the Data Set Type (*) drop-down menu select\nELN Preview\n. Fill out the relevant information and click on select files to upload, review the files and\n## Save\n.\nSelect Default Experiment or Object\nClick Upload button\nSelect ELN Preview in the Dataset Type (*)\nSelect files to upload\nReview the files and Save.", "timestamp": "2025-09-18T09:38:30.246273Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Enable_barcodes:0", "source": "https://datastore.bam.de/en/How_to_guides/Enable_barcodes", "repo": "datastore", "title": "Enable Barcode / QR-Code Functionality", "section": "Enable Barcode / QR-Code Functionality", "text": "How_to_guides\n/\nEnable_barcodes\n# Enable Barcode / QR-Code Functionality\n## Page Contents\n🧩 Enable Barcode / QR-Code Functionality\n## ✅ Prerequisites\n🪪 Step 1: Understand Default Barcode Behavior\n⚙️ Step 2: Enable Barcode Display for Your Group\n🔄 Step 3: Reload the Interface\n## Demidova, Caroline\n¶\n🧩 Enable Barcode / QR-Code Functionality\nTo make barcodes and QR codes visible and usable for your group in openBIS.\n¶\n## ✅ Prerequisites\nYou must have Group Admin role (e.g., be Data Store Steward (DSSt)\nYour group must be registered in the Data Store.\nAt least one object (e.g., sample, instrument, chemical) must be registered.\n¶\n🪪 Step 1: Understand Default Barcode Behavior\nWhen an object is registered in openBIS, a\ndefault barcode\nis automatically generated.\nThis barcode is visible in the\n## Identification Info\nsection.\n## If it's not visible:\nClick the\n## More\ndrop-down menu.\n## Select\n## Show Identification Info\n.\n¶\n⚙️ Step 2: Enable Barcode Display for Your Group\nIn the\nleft main menu\n, go to\n## Utilities\n.\nClick on\n## Settings\n.\nIn the\n## Select Group Settings\ndrop-down, choose your\ndivision number\n.\nClick the\n## Edit\ntab.\nScroll down to the\nMain menu\nsection.\nCheck the box labeled\n## Show Barcodes\n.\nReview your changes and click\n## Save\n.\n¶\n🔄 Step 3: Reload the Interface\nRefresh the openBIS webpage.", "timestamp": "2025-09-18T09:38:30.251387Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Export_to_file:0", "source": "https://datastore.bam.de/en/How_to_guides/Export_to_file", "repo": "datastore", "title": "Export (meta)data to file", "section": "Export (meta)data to file", "text": "How_to_guides\n/\nExport_to_file\n# Export (meta)data to file\n## Demidova, Caroline\nAll (meta)data from the Lab Notebook and Inventory for which you have observer rights can be exported. Navigate to the relevant file, open the\n## More\ndrop-down menu, and select\n## Export\n. The export window will open. Select the appropriate export options and\nReceive results by email\ninstead of downloading in the browser to enable more efficient software performance. You will receive an email notification from\ndatastore@bam.de\nwith a download link.\nNavigate to the relevant file\nOpen the More drop-down menu\n## Select Export\nSelect the appropriate export options\nSelect to receive results by email\nDownload file via link in email from datastore@bam.de", "timestamp": "2025-09-18T09:38:30.257460Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Export_to_zenodo:0", "source": "https://datastore.bam.de/en/How_to_guides/Export_to_zenodo", "repo": "datastore", "title": "Export data to Zenodo", "section": "Export data to Zenodo", "text": "How_to_guides\n/\nExport_to_zenodo\n# Export data to Zenodo\n## Demidova, Caroline\nTo export data to Zenodo, you need a Zenodo account to generate a personal access token. To do this, log in to your Zenodo account, select\n## Settings\n,\n## Applications\nand copy the Zenodo token.  Return to openBIS, select\n## Utilities\nand then\n## User Profile\nand copy the\nZenodo API token\n.\nSelect then\n## Exports\nand\nExport to Zenodo\nfrom\n## Utilities\n, enter the\ntitle of the submission (*)\n, select the files to be exported, and click\n## Export Selected\n. The selected data will then be transferred to Zenodo as a ZIP file. The files exported to Zenodo can be private or public, before publishing, check the entries and ensure that no personal data has been exported, authors are listed correctly, and the project's data policy is being followed.\n## Select Utilities\n## Select Export\nSelect Export to Zenodo\nEnter title of the submission\nClick on Export Selected", "timestamp": "2025-09-18T09:38:30.262471Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Filter:0", "source": "https://datastore.bam.de/en/How_to_guides/Filter", "repo": "datastore", "title": "Filter Objects within a Collection", "section": "Filter", "text": "How_to_guides\n/\n## Filter\n# Filter Objects within a Collection\n## Demidova, Caroline\nSelect relevant\n## Collection\n, click on the\n## FILTERS\ntab and select\n## Filter Per Column\n. In the appeared line of the Properties fields enter a specific\n## Property\nvalue(s) to narrow the filtering.\n## Select Collection\nClick the FILTERS button\n## Choose Filter Per Column\nEnter the Property value in the Property Filter fields.", "timestamp": "2025-09-18T09:38:30.266470Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Global_advanced_search:0", "source": "https://datastore.bam.de/en/How_to_guides/Global_advanced_search", "repo": "datastore", "title": "Search - Global and Advanced search across all fields of all Entities", "section": "Search - Global and Advanced search across all fields of all Entities", "text": "How_to_guides\n/\nGlobal_advanced_search\n# Search - Global and Advanced search across all fields of all Entities\n## Demidova, Caroline\nTo search across all database fields, enter the search term in the\n## Global Search\nwindow in the top left-hand corner of the main menu. The\n## Advanced Search\nform opens and displays the first results. To continue an advanced search across all fields of all entities (Experiments/Collections, Objects, Datasets), select an option from\n## Search For\ndrop-down menu and the\n## AND\nor\n## OR\noperator, then click the\n## Search\nicon. Narrow the search by selecting an option from the\n## Field Type\ndrop-down menu (the options displayed will vary depending on the search). Select\n## Property\nfrom the Field Type drop-down menu and enter the values required by the system in\n## Field Name\n,\nComparator Operator\nand\n## Field Value\n. To narrow down the search further, click on the\n+\nicon and enter values. Click on the\n## Search\nicon to activate the search.\nEnter the search term in Global Search\n## Advanced Search\nAdd Filters and Conditions; Search For drop-down menu\nand the AND or OR operator\nSelect an option from the Field Type menu\nFill out additional search parameters\nClick the + icon to narrow down the search\nClick on search icon.", "timestamp": "2025-09-18T09:38:30.273170Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Hierarchy_graph:0", "source": "https://datastore.bam.de/en/How_to_guides/Hierarchy_graph", "repo": "datastore", "title": "Display connections of Experimental Steps (Hierarchy Graph)", "section": "Display connections of Experimental Steps (Hierarchy Graph)", "text": "How_to_guides\n/\nHierarchy_graph\n# Display connections of Experimental Steps (Hierarchy Graph)\nAriza de Schellenberger, Angela\nTo display Parent-Child connections in a hierarchy graph, navigate to one of the Experimental steps, click on the\n## More\ndrop-down menu, select\n## Hierarchy Graph\n. The number of displayed Objects (Parents and Children) and\n## Types\ncan be adjusted for better visualization.\nIn the hierarchy graph, the\neye\nicon shows the Objects, the\narrow\nshows the metadata, and the\nplus\nicon can be used to insert Parents and/or Children directly into the hierarchy graph.\n## Select Object-Experimental Step\nClick on More drop-down menu\nSelect Hierarchy Graph and adjust the number of displayed Objects.", "timestamp": "2025-09-18T09:38:30.279515Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_History_ELN:0", "source": "https://datastore.bam.de/en/How_to_guides/History_ELN", "repo": "datastore", "title": "History of Changes", "section": "History of Changes", "text": "How_to_guides\n/\nHistory_ELN\n# History of Changes\n## Demidova, Caroline\nTo access the history of Projects, Default Experiment - Collections, Experimental Steps - Objects or Datasets, navigate to the relevant folder, open the\n## More\ndrop-down menu, and select\n## History\n## . The\n## History\nform shows previous values in\nred\n, updated values in\ngreen\n.\nNote that\n## Samples\nwas the former openBIS term for\n## Objects\n, and\n## Experiment\nfor Default Experiment - Collections.\nSelect relevant folder\nOpen More drop-down menu\n## History", "timestamp": "2025-09-18T09:38:30.283796Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_How_to_log_in:0", "source": "https://datastore.bam.de/en/How_to_guides/How_to_log_in", "repo": "datastore", "title": "Log in to the BAM Data Store", "section": "Log in to the BAM Data Store", "text": "How_to_guides\n/\nHow_to_log_in\n# Log in to the BAM Data Store\n## Demidova, Caroline\nLog in to the\nmain BAM Data Store\n.\nBAM employees are granted access to the main BAM Data Store instance, after their onboarding. To access the main instance, open\nmain BAM Data Store\nin the browser, click on the\nELN (Electronic Lab Notebook)\nicon and you will be redirected to the login page. Log in with your\nBAM username\nand\npassword\n. If you encounter any problems, check your internet connection or contact us at\ndatastore@bam.de\n.\nmain BAM Data Store Instance\nClick on the ELN (Electronic Lab Notebook) icon\nEnter BAM username and password", "timestamp": "2025-09-18T09:38:30.290221Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_How_to_log_in_train:0", "source": "https://datastore.bam.de/en/How_to_guides/How_to_log_in_train", "repo": "datastore", "title": "Log in to the Bam Data Store", "section": "Log in to the Bam Data Store", "text": "How_to_guides\n/\nHow_to_log_in_train\n# Log in to the Bam Data Store\n## Demidova, Caroline\nLog in to the\ntraining BAM Data Store\n.\nBAM employees who are assigned as Data Store Stewards for their division receive access to a training instance of the BAM Data Store for the duration of the\nrollout\n. To access the training instance, open\ntraining BAM Data Store\nin the browser, click on the\nELN (Electronic Lab Notebook)\nicon and you will be redirected to the login page. Log in with your\nBAM username\nand\npassword\n. If you encounter any problems, check your internet connection or contact us at\ndatastore@bam.de\n.\ntraining BAM Data Store Instance\nClick on the ELN (Electronic Lab Notebook) icon\nEnter BAM username and password", "timestamp": "2025-09-18T09:38:30.295430Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Manage_Access:0", "source": "https://datastore.bam.de/en/How_to_guides/Manage_Access", "repo": "datastore", "title": "Manage Access to Spaces and Projects", "section": "Manage_Access", "text": "How_to_guides\n/\n## Manage_Access\n# Manage Access to Spaces and Projects\n## Page Contents\n## Lab Notebook\n## Inventory\n## Demidova, Caroline\n¶\n## Lab Notebook\nUsers can manage access in their Lab Notebook -\n## My Space\nat the Space and Project level (access applies to the underlying content: Collections, Objects, Datasets). To grant access to a Space or a Project, navigate to relevant folder, click on the\n## More\ndrop-down menu, select\nManage access\nto open the form. Select\n## Role\nand set\ngrant to\nUser or Group, enter the\nusername\n(BAM username – lower case only) or\ndivision number\n(e.g. 1.0). Click Grant access to apply settings.\nTo get an overview of who has access in your Lab Notebook, Space or Projects, navigate to the relevant folder, click on the\n## More\ndrop-down menu, select\nManage access\n. In the\nManage access to\nwindow, an overview (User, Group, Role) of the assigned access rights is displayed.\n¶\n## Inventory\nDSSt(s) of the division can also manage access in the division’s private Inventory, but not for the BAM public Inventory. By providing the Users with the\nsufficient rights\nDSSt(s) can gain help from the division members to maintain the Inventory.\nSelect Space or Project\nClick on More drop-down menu\nSelect Manage access\nSelect Role and set grant to options\nEnter BAM username or division number\nClick on Grant access", "timestamp": "2025-09-18T09:38:30.300417Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Move_ELN:0", "source": "https://datastore.bam.de/en/How_to_guides/Move_ELN", "repo": "datastore", "title": "Move Projects, Collections, Objects and Datasets", "section": "Move Projects, Collections, Objects and Datasets", "text": "How_to_guides\n/\nMove_ELN\n# Move Projects, Collections, Objects and Datasets\n## Demidova, Caroline\nTo move content, navigate to the relevant folder, open the\n## More\ndrop-down menu, and select\n## Move\n. Start typing the\n## Name\nor\n## Code\nof the folder you want to move the content to and select it from the available options. Click on\n## Accept\n.\nTo move an Object to a another Collection, navigate to relevant Collection and select the Collection List view. Select the Object to be moved and click on the\n## Move\ntab that appears. In the\n## Move Object\nwindow, select to move to an Existing Collection and enter the\n## Code\nor\n## Name\nof the Collection in which you want to move the Object. Otherwise, select New Collection, fill in the mandatory(*) fields, review them and click\n## Accept\n.\nNavigate to relevant folder\nOpen More drop-down menu\n## Select Move\nEnter Code or Name of the target Collection\nin the search field\n## Click Accept.", "timestamp": "2025-09-18T09:38:30.305638Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Move_with_descendants_ELN:0", "source": "https://datastore.bam.de/en/How_to_guides/Move_with_descendants_ELN", "repo": "datastore", "title": "Move Experimental Step - Object with descendants", "section": "Move Experimental Step - Object with descendants", "text": "How_to_guides\n/\nMove_with_descendants_ELN\n# Move Experimental Step - Object with descendants\n## Demidova, Caroline\nAn Object can also be moved together with its descendant Objects (i.e. children, grandchildren, etc.) if all Objects (Parent and Children) belong to one and the same Collection of the Object to be moved. To do this, click on the Object in the Collection, the\n## Object\nForm opens, open the\n## More\ndrop-down menu, and select\n## Move\n. Select the option\nmove the Object with all descendants\nand start typing the\n## Name\nof the (existing) Collection to which you want to move the Objects, review, and click\n## Accept\n.\nSelect Experimental Step – Object with descendants\nOpen More drop-down menu\n## Select Move\nSelect option (to move all descendants)\nEnter Code or Name of the target Collection in the search field\nCheck the box\n## Click Accept.", "timestamp": "2025-09-18T09:38:30.311647Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_New_storage:0", "source": "https://datastore.bam.de/en/How_to_guides/New_storage", "repo": "datastore", "title": "Configure Storage of Objects", "section": "Configure Storage of Objects", "text": "How_to_guides\n/\nNew_storage\n# Configure Storage of Objects\nAriza de Schellenberger, Angela\nTo configure Fridges and freezers, navigate to the left main menu, under\n## Utilities\nselect\n## Settings\n. The Select Group Settings drop-down menu will appear, select your\ndivision number\nto open your group settings. Click on the\n## Edit\ntab, and navigate to the\n## Storages\nsection, click on\n## + New Storage\ntab. Fill in the\n## New Storage\nform. The Code in the\n## Identification Info\n(if hidden, open the\n## More\ndrop-down menu and select\n## Show Identification Info\n) and Name should be meaningful and descriptive to represent the storage location. The number of rows (shelves), columns (racks per shelf) and boxes allowed in a shelf must be specified. The validation level is the minimum information required about the storage (a. Rack validation. The position in the shelf and rack needs to be specified; b. Box validation. In addition to a., a box name needs to be specified; c. Box position validation. In addition to a. and b, the position in the box needs to be specified). Review the entries and\n## Save\n.\n## Under Utilities\n## Select Settings\nSelect division number\nClick on Edit tab\nNavigate to Storages section\nClick on + New Storage tab\nFill out the New Storage form\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.317125Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Object_types_in_drop-downs:0", "source": "https://datastore.bam.de/en/How_to_guides/Object_types_in_drop-downs", "repo": "datastore", "title": "Enable Object Types in drop-downs menus", "section": "Enable Object Types in drop-downs menus", "text": "How_to_guides\n/\nObject_types_in_drop-downs\n# Enable Object Types in drop-downs menus\n## Demidova, Caroline\nIn the left main menu, under\n## Utilities\nselect\n## Settings\n. The Select Group Settings drop-down menu will appear, select your division number to open your group settings. Click on the\n## Edit\ntab and scroll down to the\nObject type definitions Extension\nsection, open the corresponding object type, and enable the\nShow in drop-downs\noption. You can edit several object types at the same time, review the changes and click on the Save.\n## Under Utilities\n## Select Settings\nSelect division number\nClick on Edit tab\nScroll down to the section: Object type definitions Extension\nSelect an Object type\nEnable Show in drop downs\nReview the changes and Save.", "timestamp": "2025-09-18T09:38:30.322373Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Parents_and_Children_Experimental_Steps:0", "source": "https://datastore.bam.de/en/How_to_guides/Parents_and_Children_Experimental_Steps", "repo": "datastore", "title": "Define Parents and Children of Experimental Steps", "section": "Define Parents and Children of Experimental Steps", "text": "How_to_guides\n/\nParents_and_Children_Experimental_Steps\n# Define Parents and Children of Experimental Steps\n## Demidova, Caroline\nTo assign\nParent and Children\nto Objects of the type - Experimental Step, select relevant Object, click on the\n## Edit\ntab. In the\n## Parent\nand\n## Children\nsection, click\n## Search Any\nand select the\n## Type\nof the\n## Object\nyou want to add from the drop-down menu. Enter the\n## Code\nor\n## Name\nof the Object in the field and start typing to display available options for your group, select accordingly and\n## Save\n.\nTo define multiple Parents and Children at the same time, select the\n## Paste Any\noption, add the Code or Name of respective Objects, review the entries, and\n## Save\n. You can copy the Code/Name of Objects from another ELN page (Log in in the BAM Data Store in another browser window). Paste the Codes(s) or name(s) in the text fields, review and\n## Save\n.\n## Select Object\nClick on Edit tab\nNavigate to the Parent and Children sections\n## Click Search Any\n## Select Object Type\nEnter Code or Name of the Objects to connect\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.328908Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Parents_and_Children_Inventory:0", "source": "https://datastore.bam.de/en/How_to_guides/Parents_and_Children_Inventory", "repo": "datastore", "title": "Define Parents and Children of Inventory Objects", "section": "Define Parents and Children of Inventory Objects", "text": "How_to_guides\n/\nParents_and_Children_Inventory\n# Define Parents and Children of Inventory Objects\nAriza de Schellenberger, Angela\nTo assign Parent and Children to Objects of a division’s private Inventory, select relevant\n## Object\n, click on the\n## Edit\ntab. In the\n## Parent\nand\n## Children\nsection, click\n## Search Any\nand select the\nType of the Object\n(e.g., Instrument of the public BAM inventory) you want to add from the drop-down menu. Enter the\n## Code\nor\n## Name\nof the Object in the field, start typing to display available options for your group. Select and\n## Save\n.\nTo define multiple Parents and Children at the same time, select the\n## Paste Any\noption, add the\n## Code\nor\n## Name\nof respective Objects, review the entries and\n## Save\n. You can copy the Code or Name of Objects from another ELN page (Log in to the BAM Data Store in another/private browser window). Paste the Codes(s) or name(s) in the text fields, review and\n## Save\n.\nParent-Children connections are displayed in the\n## Hierarchy Graph\navailable under the\n## More\ndrop-down menu of each Object.\n## Select Object\nClick on Edit tab\nNavigate to the Parent and Children sections\n## Click Search Any\n## Select Object Type - Instrument\nEnter Code or Name of the Objects to connect\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.334881Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Print_barcode:0", "source": "https://datastore.bam.de/en/How_to_guides/Print_barcode", "repo": "datastore", "title": "Print the Barcode", "section": "Print the Barcode", "text": "How_to_guides\n/\nPrint_barcode\n# Print the Barcode\nAriza de Schellenberger, Angela\nSelect an Object and open its\n## More\ndrop-down menu, select B\narcode/QR Code Print\n. In the Print Barcode/QR Code window, select the code type and size and click on the\n## Download\ntab. The code will be saved on your computer as a PDF file that you can print.\n## Select Object\nOpen More drop-down menu\nSelect Barcode/QR Code Print\nSelect the code type and size\nDownload the code\n## Print.", "timestamp": "2025-09-18T09:38:30.340035Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Project_overview:0", "source": "https://datastore.bam.de/en/How_to_guides/Project_overview", "repo": "datastore", "title": "Project Overview", "section": "Project Overview", "text": "How_to_guides\n/\nProject_overview\n# Project Overview\n## Demidova, Caroline\nTo generate a Project overview navigate to Project, open the More drop-down menu and click on Show Overview.\nNavigate to Project\nOpen the More drop-down menu\n## Click on Show Overview.", "timestamp": "2025-09-18T09:38:30.344031Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Register_collection_Inventory:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_collection_Inventory", "repo": "datastore", "title": "Register Collections in the Inventory", "section": "Register Collections in the Inventory", "text": "How_to_guides\n/\nRegister_collection_Inventory\n# Register Collections in the Inventory\n## Demidova, Caroline\nTo register a\n## Collection\nin the Inventory, navigate to relevant Project, click on\n## + New\ntab and select\n## Collection\nfrom the Experiment/Collection type drop-down menu. The\n## Collection\nform opens. Fill out the\n## Identification Info\n(if hidden, open the\n## More\ndrop-down menu and select Show Identification Info). Enter the\n## Code\n, a meaningful\n## Name\n, select\n## Empty\nin\nDefault object type\nand\n## List\nview in\nDefault collection view\ndrop-down menu.\nNote that Collections can contain Objects of one or many types. Arrange Objects in the Inventory in a meaningful way for the group.\nObjects can be moved together with descendants, only if they are in the same Collection (\nMove Objects to a different Collection\n).\n## Select Project\nClick on + New tab\nSelect Collection from the Experiment/Collection type drop-down menu\nEnter Code and Name\nSelect in Default Object Type – (empty)\nSelect in Default Collection view – List view\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.350303Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Register_default_Experiment:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_default_Experiment", "repo": "datastore", "title": "Register Collection of type Default Experiment", "section": "Register Collection of type Default Experiment", "text": "How_to_guides\n/\nRegister_default_Experiment\n# Register Collection of type Default Experiment\n## Demidova, Caroline\nThe Default Experiment is a type of\n## Collection\nto group sequential or non-sequential Experimental Steps. To register an Experimental Step, navigate to the relevant\n## Project\n, click on\n## + New\ntab, select Default Experiment from the the window Select Experiment/Collection type. Fill out the Create Default Experiment form, review the entries and\n## Save\n.\n## Select Project\nClick on + New tab\n## Select Default Experiment\nFill out Create Default Experiment form\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.356616Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Register_non-seq_Experimental_Step:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_non-seq_Experimental_Step", "repo": "datastore", "title": "Register non-sequential Experimental Steps", "section": "Register non-sequential Experimental Steps", "text": "How_to_guides\n/\nRegister_non-seq_Experimental_Step\n# Register non-sequential Experimental Steps\n## Demidova, Caroline\nTo register non-sequential Experimental steps, register a new\n## Object\nat the Collection level (Default Experiment).\nNavigate to relevant - Default Experiment, click the\n## + New\ntab, select\n## Experimental Step\nin the Select an object type drop-down menu. Fill out the New Experimental Step form, display Identification Info (if hidden, open the More drop-down menu and select Show Identification Info).  The Code is automatically generated for Objects and can only be changed during registration. Give the Experimental Step a meaningful\n## Name\n, as this will be displayed to the users, review the entries and\n## Save\n.\nNote that the new Experimental Step is organized at the same hierarchical level of Objects (\n) in the Lab Notebook left-hand  menu.\nTo fill out the Experimental Step form with a\n## Template\npredefined for the group by the Data Store Steward, click on the\n## Templates\ntab in the New Experimental Step form, select\n## Template\n, add information to the form as required, review the entries and\n## Save\n. Reload the web page to see the changes.\n## Select Default Experiment\nClick the + New tab\nSelect an Object Type - Experimental Step\nFill out the Experimental Step form\nAlternatively, select relevant Template\nclick on Templates tab, select Template and complete relevant information\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.361089Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Register_objects_Inventory:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_objects_Inventory", "repo": "datastore", "title": "Register Objects in the Inventory", "section": "Register Objects in the Inventory", "text": "How_to_guides\n/\nRegister_objects_Inventory\n# Register Objects in the Inventory\nAriza de Schellenberger, Angela\nTo register\n## Objects\nin the Inventory, navigate to the relevant\n## Collection\n, click on the\n## More\ndrop-down menu, select\n## New Object\nand\n## Object Type\nfrom drop-down menu.  Fill out the Object\n## Identification Info\n(if hidden, open the\n## More\ndrop-down menu and select Show Identification Info). The\n## Code\nis generated automatically for Objects and can\nonly\nbe changed during registration. Give the Object a meaningful\n## Name\n, as this will be displayed to the users, review the entries and\n## Save\n.\nNote that you can register an object of type ‘Entry’ in the object form to quickly visualise information. A preview of text, images or tables is displayed in the\n## Document\nicon of the Collection form. To see the icon, select relevant Collection from the left-hand menu.\n## Select Collection\nClick on More drop-down menu\n## Select New Object\nSelect an Object Type\nFill out the Object form\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.367647Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Register_project:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_project", "repo": "datastore", "title": "Register a Project", "section": "Register a Project", "text": "How_to_guides\n/\nRegister_project\n# Register a Project\nAriza de Schellenberger, Angela\nTo register a\n## Project\n, navigate to the Lab Notebook in the left-hand menu, open the drop-down menu, select My Space and click on\n## + New Project\n. The Project form will open. Fill out the Identification Info (if hidden, open the\n## More\ndrop-down menu and select Show Identification Info).  Enter the Code and Description, review the entries and\n## Save\n.\n## Code*\n### Requirements\n## :\nMandatory for openBIS (*)\nAllowed characters: A-Z (uppercase), 0-9, '_' (underscore), '-' (hyphen), and '.' (dot)\nSeparate words with underscores\nShould be meaningful, in English, and between 3-30 characters\nCannot be modified or reused.\n## Description\n### Requirements\n## :\nMandatory for BAM Data Store\nShould contain enough detail to be understandable to people outside of the group\nShould be in the following format: \"English//German\"\nShould contain 2-50 words.\n## Select My Space\nClick on + New Project\nEnter Code and Description\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.369642Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Register_project_Inventory:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_project_Inventory", "repo": "datastore", "title": "Register Projects in the Inventory", "section": "Register Projects in the Inventory", "text": "How_to_guides\n/\nRegister_project_Inventory\n# Register Projects in the Inventory\n## Demidova, Caroline\nNew Projects can be registered by Data Store Stewards (DSSt(s)) with Group Admin\nroles\nin division’s\nprivate\n## Inventory\n## Spaces\n. Projects in\npublic\n## Inventory\n## Projects\nneed to be registered by the Data Store team (Instance Admins), please contact at\ndatastore@bam.de\n.", "timestamp": "2025-09-18T09:38:30.375099Z", "source_priority": 1, "content_type": "general"}
{"id": "docs:datastore:en_How_to_guides_Register_seq_Experimental_Step:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_seq_Experimental_Step", "repo": "datastore", "title": "Register sequential Experimental Steps", "section": "Register sequential Experimental Steps", "text": "How_to_guides\n/\nRegister_seq_Experimental_Step\n# Register sequential Experimental Steps\n## Demidova, Caroline\nTo register sequential Experimental Steps, register a new\n## Object\nat the Object level. To do this, select relevant Experimental Step - Object, click on the\n## + New\ntab, fill out relevant information, review and\n## Save\n.\nNote that the new Experimental Step is organized under a Children drop-down in the left-hand main menu.\n## Select Experimental Step\nClick on + New tab\nSelect an Object Type - Experimental Step\nFill out the Experimental Step form\nAlternatively, select relevant Template\nClick on Templates tab, select Template,\nmodify Object name and complete relevant information\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.380071Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Register_storage_position:0", "source": "https://datastore.bam.de/en/How_to_guides/Register_storage_position", "repo": "datastore", "title": "Allocate storage position to a single Object", "section": "Allocate storage position to a single Object", "text": "How_to_guides\n/\nRegister_storage_position\n# Allocate storage position to a single Object\nAriza de Schellenberger, Angela\nThe digital representation of laboratory storage must be configured by the Data Store Stewards (DSSt(s)) in your division. To add storage information to an\n## Object\nduring registration, navigate to the\n## Storage\nsection in the\n## Object\nform.  If the Object is already registered, navigate to the relevant Object, click\n## Edit\nand scroll down to the\n## Storage\nsection.  Click on the\n## + New Storage Position\n## Tab. The\n## Physical Storage\nform opens. Select\n## Storage\nfrom the drop-down menu, specify the\nposition\nof the Object (e.g., Rack, Box name to display Box position) mark the position of the Object within the Box, click on the\n## Accept\ntab and then on the\n## Save\ntab.\n## Select Object\nClick on Edit tab\nNavigate to Storage sections\n## Click + New Storage Position\nSpecify Object Position (e.g., Rack, Box)\nMark the Object(s) position(s) within the Box\nClick on Accept tab\nSave Object form.", "timestamp": "2025-09-18T09:38:30.384893Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Represent_research_data:0", "source": "https://datastore.bam.de/en/How_to_guides/Represent_research_data", "repo": "datastore", "title": "Represent research data", "section": "Represent research data", "text": "How_to_guides\n/\nRepresent_research_data\n# Represent research data\n## Page Contents\nConceptual data Model\n1. Draw the research data workflow\n2. Identify Entities and Entity types\n3. Map Entities in the Data Store-openBIS data structure\n## 4. Connect Entities\n## Demidova, Caroline\n¶\nConceptual data Model\nTo map research data in the Data Store – openBIS Research Data Management system, data are stored together with all experimental steps, tangible and intangible objects (things you do, generate and have) that are relevant to generate traceable and reusable data.\nIt is advisable that divisions create a conceptual data model to have a visual tool that supports the discussions and development of the data model.  In BAM, the DSSt leads the design of the data model and coordinates the feedback rounds with the division members.  An understandable data model for the division supports effective data management, analysis, and collaboration. It is recommended to understand the concepts from the beginning to improve and expand the data model according to the needs of the department.\nThe following steps serve as a guide for creating a conceptual data model. Any visualization tool can be used, the\ntemplate\nin\ndraw.io\ncan be downloaded and reused to implement the following steps.\n¶\n1. Draw the research data workflow\nIdentify the data (of any format, single or multiple files, datasets) to be stored in the Data Store.\nAdd the\nthings you do\n(e.g., synthesis, measurements, analysis, etc.) to generate the data and specify them as Experimental Steps.  Attach data to Experimental Steps.\nConnect the Experimental Steps with unidirectional arrows to indicate the logically occurrence and dependency.\nAdd the things you generate,\ntangible\nor\nintangible\n(Samples, Materials, etc.).\nAdd the\nthings you have\nor\nneed\nto complete all Experimental Steps and that are relevant to generate traceable and reusable data. Use general categories such as Chemicals to simplify visualization rather than listing individual chemicals.\nTo simplify the flowchart, make sure all Experimental Steps have at least one dataset attached (otherwise, check whether the Experimental step is part of another).\n¶\n2. Identify Entities and Entity types\nIdentify all\n## Entities\nused (Chemical 1, Chemical 2) or generated (Samples) in each Experimental Step.\nGroup similar Entities under common\n## Entity Types\n(chemicals 1, 2 in Chemicals and Nanoparticles in Samples). Several Entity types have already been defined by BAM users, try to reuse them if possible. Use the\nMASTERDATA CHECKER\nto identify existing Entity Types and their properties in the Data Store. To add Properties to an Entity Type or to define new Entity Types, the DSSt(s) can contact the Data Store team at\ndatastore@bam.de\n.\nIf an Entity cannot be grouped with others, list it in the table and leave the Entity Type name blank. Contact the Data Store team to find out how to represent this Entity in the Data Store.\nNote that generated things in an Experimental Step such as Code and Data, can be uploaded to the system as datasets.  These datasets can be uniquely described by defining an Entity Type or be uploaded with minimal metadata as generic Datasets with default properties (e.g., dataset name) defined by the system. If no Entity Type is defined for Code or Data, all relevant Information should be stored within the Experimental Step used linked to these items.\n¶\n3. Map Entities in the Data Store-openBIS data structure\nTo map Entities in the openBIS organize Entities at Object level within the hierarchical\ndata structure\nof openBIS: Space → Project → Collection → Object → Dataset. Consider the pre-defined organization of the BAM (public) and FB (private) -Inventory and Lab Notebook Spaces.\nMap Entities in the openBIS data structure.\nList all Objects at the Object level to represent the things you do and the things you generate in the ELN. Organize all the things you have in the Inventory.\nAssign meaningful names to Collections. Collections can contain one or more Objects of one or more Types (Entity Types).\n¶\n## 4. Connect Entities\nTo connect research data in openBIS, define unidirectional\nparent–child relationships\n. These connections can be defined Object to Object or Dataset to Dataset across Spaces, regardless of whether they are in the Inventory or in the Lab Notebook. For the Data Store, it is recommended to connect Objects to Objects:  openBIS provides an automatic visualization of Objects. To visuaize and understand data, all Objects should be connected to each other.\nVisualize the connections between Objects\n.\nTo define the connections between Objects, draw unidirectional arrows between Objects. By default, each Object can have an unlimited number of parents and/or children or none (N:N relationships with N being any number from 0 to N). For a specific Object type, the DSSt (group admins) can define a maximum number of children and parents.", "timestamp": "2025-09-18T09:38:30.391589Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Revert_deletions_ELN:0", "source": "https://datastore.bam.de/en/How_to_guides/Revert_deletions_ELN", "repo": "datastore", "title": "Revert deletions of Collection, Objects and Datasets", "section": "Revert deletions of Collection, Objects and Datasets", "text": "How_to_guides\n/\nRevert_deletions_ELN\n# Revert deletions of Collection, Objects and Datasets\n## Demidova, Caroline\nTo revert deletions, navigate to\n## Utilities\nin the main menu and select the\n## Trashcan\n. Open the\n## Operations\ndrop-down menu for the relevant content and click on\n## Revert Deletion\n, a green confirmation message will appear. Reload the webpage and navigate to relevant folders to see the reverted deletions.\nNote that deletions from the Trashcan are\nirreversible\n.\nNavigate to Utilities\n## Select Trashcan\nOpen Operations drop-down menu\nClick on Revert Deletion\nReload the webpage", "timestamp": "2025-09-18T09:38:30.396599Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Scan_barcodes:0", "source": "https://datastore.bam.de/en/How_to_guides/Scan_barcodes", "repo": "datastore", "title": "Scan Barcodes", "section": "Scan Barcodes", "text": "How_to_guides\n/\nScan_barcodes\n# Scan Barcodes\n## Demidova, Caroline\nIn the top\nMain menu\nnext to the\n## Global Search\nfield, click on the\n## Barcode\nicon.  Use a\nbarcode scanner\nor the\ncamera of a mobile device\nto scan the code.  The associated entry is opened directly in openBIS.  Your scanned selection will be saved for future reference.\nClick the Barcode icon\nSelect scanning device\nScan the barcode.", "timestamp": "2025-09-18T09:38:30.400620Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Search_Dataset_Lab_Notebook:0", "source": "https://datastore.bam.de/en/How_to_guides/Search_Dataset_Lab_Notebook", "repo": "datastore", "title": "Search for Datasets in the ELN", "section": "Search_Dataset_Lab_Notebook", "text": "How_to_guides\n/\n## Search_Dataset_Lab_Notebook\n# Search for Datasets in the ELN\n## Demidova, Caroline\nTo search through Datasets, navigate to main menu on the left side, open the\n## Utilities\ndrop-down menu and select\n## Advanced Search\n. In the\n## Search For\ndrop-down menu, select the\n## Dataset\noption and select the\n## AND\noperator to add additional search parameters. For example,  to search for a Dataset by the name of the Registrator, select the option\n## Property\nin the drop-down menu under\n## Field Type\n; the option\nRegistrator [ATTR.REGISTRATOR]\nunder\n## Field Name\n, and the option\nthatEqualsUserId (UserId)\nfrom the\nComparator Operator\n. To define the search value, start typing the Name of the Registrator in the\n## Field Value\nand click on the Search icon, next to the operator field Using (e.g., AND) to activate the search.\n## Open Utilities\n## Select Advanced Search\nSelect Dataset in the Search For drop-down menu\nSelect AND operator in the Using drop-down menu\nSelect Property in the drop-down menu under Field Type\nSelect  Registrator [ATTR.REGISTRATOR] under Field Name\nSelect thatEqualsUserId (UserId) from the Comparator Operator\nStart typing the Name of the Registrator in the Field Value\nClick on Search icon.", "timestamp": "2025-09-18T09:38:30.406375Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Search_Experimental_Steps:0", "source": "https://datastore.bam.de/en/How_to_guides/Search_Experimental_Steps", "repo": "datastore", "title": "Search for: Objects - Experimental Steps in the ELN & save search queries", "section": "Search_Experimental_Steps", "text": "How_to_guides\n/\n## Search_Experimental_Steps\n# Search for: Objects - Experimental Steps in the ELN & save search queries\n## Demidova, Caroline\nOpen the\n## Utilities\ndrop-down menu and select\n## Advanced Search\n. Click on\n## Search For\ndrop-down menu and select Experiment/Collection, select an operator from the\n## Using\ndrop-down menu (e.g., AND), select\n## Field Type\noption (e.g., All), enter\n## Field Value\n(e.g., your BAM username to search for all Collections you have registered). Click the\n+\nicon to narrow the search further, select\n## Field Type\noption (e.g., Property) and\n## Field Name\noption (e.g., Modification Date. A list of all available properties becomes available), enter option for a\nComparator Operator\n(e.g., thatISLaterThan (Date)) and select the\ndate\nin\ncalendar\nicon. Click on the\n## Search\nicon to activate the search. Search values can also be excluded from the search by selection the NOT checkbox.\nTo save a search query in your own\nLab Notebook's\nSpace. Click on the\n## Save\nicon displayed in the upper part of the\n## Advance Search\nform, the\nSave Search query\nwindow will open, fill out search\n## Name\nand start typing in the\nsearch entity to store query\nto find the name of a\n## Collection\n. To save the search, click Save. Saved searchers are available in the drop-down menu displayed at the top of the Advanced Search page.\n## Open Utilities\n## Select Advanced Search\nClick on Search For and select Experiment/Collection\nSelect operator from Using drop-down menu (e.g., AND)\nSelect Field Type option (e.g., All)\nEnter Field Value (e.g., your BAM username)\nClick on the + icon; select Field Type option (e.g., Property; Field Name option (i.e., Modification Date);\nenter option for a Comparator Operator (e.g., thatISLaterThan (Date)) and select the date in calendar icon\nClick on search icon\nClick on the Save icon\nEnter search Name and entity to store query (Collection Name)", "timestamp": "2025-09-18T09:38:30.412824Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Search_Inventory:0", "source": "https://datastore.bam.de/en/How_to_guides/Search_Inventory", "repo": "datastore", "title": "Search for: Objects in the Inventory", "section": "Search_Inventory", "text": "How_to_guides\n/\n## Search_Inventory\n# Search for: Objects in the Inventory\n## Demidova, Caroline\nTo search for Objects, navigate to the left menu, open the\n## Utilities\ndrop-down menu and select\n## Advanced Search\n. Click on\n## Search For\ndrop-down menu and select Experiment/Collection; select an operator (e.g., AND) from the\n## Using\ndrop-down menu. Select the\n## Field Type\noption (e.g., All), enter a\n## Field Value\n(e.g., Instrument).  Click the\n+\nicon to narrow the search further, select\n## Field Type\noption (e.g., Property). Select one of the displayed values in the\n## Field Name\nand enter a value in the\n## Field Value\n. Click on the Search icon to activate the search.  Search values can also be excluded by selecting the NOT checkbox.\n## Open Utilities\n## Select Advanced Search\nClick on Search For\nSelect operator from Using drop-down menu (e.g., AND)\nSelect Field Value option (e.g., Instrument)\nClick on the + icon\nSelect Field Type option (e.g., Property)\nEnter Field Name and Field Value if available\nClick on the search icon.", "timestamp": "2025-09-18T09:38:30.418262Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Share_code:0", "source": "https://datastore.bam.de/en/How_to_guides/Share_code", "repo": "datastore", "title": "Share Code in BAM research GitHub", "section": "Share Code in BAM research GitHub", "text": "How_to_guides\n/\nShare_code\n# Share Code in BAM research GitHub\n## Page Contents\n## ✅ Prerequisites\nTwo options to Share Your Code\nOption A: Create a New Repository\nOption B: Copy an Existing Repository\n## 🧩 Need Help?\n## Demidova, Caroline\nThis guide walks you through the steps to share your code (scripts, parsers, tools, etc.) in the\nBAM research GitHub organization\n, following\nopen-source best practices\n.\n¶\n## ✅ Prerequisites\nYou have a GitHub account.\nYou are a member of the\nBAM research GitHub organization\n.\nFor access please contact\njose.pizarro-blanco@bam.de\nvia Microsoft Teams with your GitHub username to request an invitation.\n¶\nTwo options to Share Your Code\n¶\nOption A: Create a New Repository\nGo to the\nBAM research GitHub organization\n.\n## Click\n\"New repository\"\n.\nFill in the repository details:\n## Name\n: Choose a clear, descriptive name.\n## Description\n: Briefly explain what the code does.\n## Visibility\n## : Choose\n## Public\n(preferred for OSS) or\n## Private\n.\n## Click\nCreate repository\n.\n¶\nOption B: Copy an Existing Repository\n## If your code already exists elsewhere:\nFork it\n## :\nIf the original repository is public and has a compatible license:\nGo to the original repo and click\n## Fork\n## → Choose\nBAM research\nas the destination.\nDuplicate it\n## :\n## If forking isn’t suitable:\nClone the original repo locally.\nCreate a new repo under BAM research.\nPush the code to the new repo.\n¶\n## 🧩 Need Help?\nIf you’re unsure about licensing, repository setup, or GitHub workflows, reach out to Data Store Team at\ndatastore@bam.de\n.", "timestamp": "2025-09-18T09:38:30.424578Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Storage_position:0", "source": "https://datastore.bam.de/en/How_to_guides/Storage_position", "repo": "datastore", "title": "Enable Storage Widget on Object Form", "section": "Enable Storage Widget on Object Form", "text": "How_to_guides\n/\nStorage_position\n# Enable Storage Widget on Object Form\nAriza de Schellenberger, Angela\nThe Storage Widget is disabled by default. To track storage positions for a particular Object Type, the storage must be enabled by a group Admin (Data Store Steward). To do this, navigate to the left main menu, under\n## Utilities\nselect\n## Settings\nand your\ndivision number\ndrop-down menu. Click on the\n## Edit\ntab and scroll down to the\nObject Type definitions Extension\nsection, choose the Object type, and check the\n## Enable Storage\ncheckbox. Review the entries and\n## Save\n.\n## Under Utilities\n## Select Settings\nSelect division number\nClick on Edit tab\nScroll down to Object Type definitions Extension\nSelect an Object type\n## Enable Storage\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.428775Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Templates:0", "source": "https://datastore.bam.de/en/How_to_guides/Templates", "repo": "datastore", "title": "Create Templates for Experimental Steps and other Objects", "section": "Templates", "text": "How_to_guides\n/\n## Templates\n# Create Templates for Experimental Steps and other Objects\n## Demidova, Caroline\nTo create a Template for an Object type, navigate to left main menu and select\n## Utilities\n, and then\n## Settings\n. A Select Group Settings drop-down menu will appear, select your\ndivision number\nto open your group settings. Click on the\n## Edit\ntab, open the Templates section, click on the\n## + New Template\ntab. Select an Object Type for which the template is intended, e.g. Experimental Step. Fill out the predefined values as required, review the entries and\n## Save\n.  All available templates to your group are displayed in the Templates section.\nNote, using a Template will overwrite the existing parent(s) and child(ren) defined in the registration form of the\n## Object\n. When registering\nsequential Experimental Steps\nusing a Template, you must\nexplicitly specify the parent(s) and/or child(ren) during the registration\nprocess. If the parent(s) and child(ren) are fixed for a particular Template, they can be defined at the time of the Template's creation. In such cases, they will be automatically applied whenever the Template is used.\nNote that to quickly visualise information and preview text, images or tables in the Object form, you can embed this content when creating a Template for Experimental Steps and other Objects. To display the preview, select the corresponding Project or Collection from the menu on the left and click on the icon in the Document column in the Collection form.\n## Under Utilities\n## Select Settings\nSelect division number\nClick on Edit tab\nOpen Templates section\nClick on + New Template tab\nSelect an Object type- Experimental Step\nFill out the form with the pre-defined values\nReview the entries and Save.", "timestamp": "2025-09-18T09:38:30.434103Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Upload_data:0", "source": "https://datastore.bam.de/en/How_to_guides/Upload_data", "repo": "datastore", "title": "Upload data", "section": "Upload data", "text": "How_to_guides\n/\nUpload_data\n# Upload data\n## Demidova, Caroline\nThe data uploaded in openBIS can be of any type and format, individual files, or data sets. Data uploaded is\nimmutable\n, it cannot be changed, if necessary, different versions of the data must be uploaded. In the Data Store, it is suggested to upload data to the Experimental Steps - Objects. To upload data, navigate to relevant Experimental Step and click on the\n## Upload\ntab. In the\n## Create Dataset\nform, enter the Identification Info (if hidden, open the More drop-down menu and select Show Identification Info).  Select\n## Dataset Type\nif not defined, e.g. Attachment. Fill out the relevant information and click on select\nfiles to upload\n, drag and drop or browse files or upload a zip file (select Uncompress-checkbox before import), review the files and\n## Save\n.  Datasets are displayed in the left-lower corner of the main menu.\nUploaded Datasets are displayed in the left-hand menu. To view the contents of Datasets, click on the Dataset and download it. Please note that there is no preview function for Datasets in openBIS.\nAlternatively, you can preview text, images, or tables by registering an Object of type\n## Entry\nor registering an Object using\n## Templates\n.\n## Select Experimental Step\nClick Upload tab\nSelect Dataset Type (*), e.g. Attachment\nSelect files to upload\n(drag and drop, browse or upload zip file\nand select Uncompress-checkbox  before import)\nReview the files and Save.\n## Select Experimental Step\nClick Upload button\nSelect Entry or Dataset Type (*), e.g. Attachment\nSelect files to upload\n(drag and drop, browse or upload zip file\nand select Uncompress-checkbox  before import)\nReview the files and Save.", "timestamp": "2025-09-18T09:38:30.438045Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Use_barcodes_qr_codes:0", "source": "https://datastore.bam.de/en/How_to_guides/Use_barcodes_qr_codes", "repo": "datastore", "title": "Use Barcodes and QR Codes", "section": "Use Barcodes and QR Codes", "text": "How_to_guides\n/\nUse_barcodes_qr_codes\n# Use Barcodes and QR Codes\n## Page Contents\n🔧 Use Barcodes and QR Codes\n## ✅ Prerequisites\n🪪 Step 1: Choose the Code Content\n🖨️ Step 2: Generate the Codes\n## 🧾 Step 3: Print Code\n📥 Step 4: Print and Apply Stickers\n📡 Step 5: Scan and Use in openBIS\nAriza de Schellenberger, Angela\n¶\n🔧 Use Barcodes and QR Codes\nTo enable fast and secure referencing of physical objects (e.g., samples, instruments) in openBIS using barcodes or QR codes.\n¶\n## ✅ Prerequisites\nAccess to an openBIS instance\nPhysical objects to label (e.g., samples, devices)\nBarcode/QR code reader (USB or Bluetooth)\nSticker printer or external code generator (optional)\n¶\n🪪 Step 1: Choose the Code Content\nWhen an Object is registered, a Default Barcode is automatically generated by openBIS. This is found in Identification Info.\nIt is also possible to use the PermId to generate a Barcode/QR code.\n¶\n## Option A:\nPermId\n## Pros\n## :\nAlways available and unique\nCompact (suitable for Micro-QR)\n## Cons\n## :\nTied to one openBIS instance\nChanges on export/import\nOnly available after object registration\n¶\n## Option B:\n$BARCODE Property\n## Pros\n## :\nCan be pre-assigned and batch imported\nCompatible across systems\n## Cons\n## :\nUniqueness not enforced\nRequires additional data management\n¶\n🖨️ Step 2: Generate the Codes\n¶\n## Option A:\nWithin openBIS ELN\nDirect integration\nLimited formatting\n¶\n## Option B:\n## External Tools\n## Linux\n## :\nqrencode\n(CLI, scriptable)\n## Windows\n## :\n## Zint\n(GUI, flexible)\n## Label Printer Software\n## :\nOften supports Excel import\nGood formatting and printer integration\n¶\n## 🧾 Step 3: Print Code\n¶\nSelect the Code Format\nChoose based on your hardware and space constraints:\n## Code Type\n## Format\n## Pros\n## Notes\n## Code 128\n## 1D\nWidely supported\nMinimum standard\nQR Code\n## 2D\nCompact, robust\n## Recommended\nMicro-QR Code\n## 2D\nVery small (5x5mm)\nIdeal for PermIds\n💡\n## Tip\n: Always prefer 2D codes unless you have a specific reason to use 1D.\n¶\n📥 Step 4: Print and Apply Stickers\nUse durable stickers compatible with your printer\nInclude optional metadata (e.g., contact, organizational unit)\nApply to physical objects clearly and accessibly\n¶\nHow to print Code from openBIS(tbL)\nPrint Barcode/QR code generated in openBIS\n¶\n📡 Step 5: Scan and Use in openBIS\nUse barcode/QR readers (USB-HID or Bluetooth)\n## Scanned codes will:\nDisplay object info\nLink samples/devices in experiments\n🧪 Example Use Case: Scan a sample’s QR code during an experiment to auto-link it to the experiment record in openBIS.", "timestamp": "2025-09-18T09:38:30.445139Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_How_to_guides_Use_saved_search:0", "source": "https://datastore.bam.de/en/How_to_guides/Use_saved_search", "repo": "datastore", "title": "Use saved search queries", "section": "Use saved search queries", "text": "How_to_guides\n/\nUse_saved_search\n# Use saved search queries\n## Demidova, Caroline\nTo use a saved search query in the ELN, navigate to the left main menu, open the\n## Utilities\ndrop-down menu and select\n## Advanced Search\n. Open the\ntop drop-down menu\nof the\n## Advanced Search\nform, select stored search and click on the\n## Search\nicon to activate the search.\n## Open Utilities\n## Select Advanced Search\nOpen the top drop-down menu of the Advanced Search form\nSelect stored search\nClick on Search icon.", "timestamp": "2025-09-18T09:38:30.450139Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_How_to_guides_Verify_storage_position:0", "source": "https://datastore.bam.de/en/How_to_guides/Verify_storage_position", "repo": "datastore", "title": "Verify storage position", "section": "Verify storage position", "text": "How_to_guides\n/\nVerify_storage_position\n# Verify storage position\nAriza de Schellenberger, Angela\nTo verify whether the storage position was registered correctly, select\n## Storage Manager\nfrom the\n## Utilities\ndrop-down menu on the left. Select the Storage where the Object was registered from the\n## Storage\ndrop-down menu. The contents of the Storage (e.g.,rack and  boxes) are displayed. Double click on the box to display the registered Objects. Place the mouse over the Object of interest to display the corresponding metadata fields and click on the Object name displayed in blue to open the Object form.\n## Select Utilities\n## Select Storage Manager\nFrom the Storage drop-down menu select the Storage\nDouble click on the box\nPlace mouse cursor on the Object\nClick on the Object's name", "timestamp": "2025-09-18T09:38:30.455222Z", "source_priority": 1, "content_type": "procedure"}
{"id": "docs:datastore:en_masterdata_definition:0", "source": "https://datastore.bam.de/en/masterdata_definition", "repo": "datastore", "title": "Masterdata definition", "section": "Masterdata definition", "text": "masterdata_definition\n# Masterdata definition\n## Demidova, Caroline\nLast Thursday at 1:12 PM\nMasterdata defintion in the BAM Data Store:\nThe Process of Masterdata Definition\nBest Practices for Masterdata Definition\nMasterdata checker\nHow to use Masterdata checker\nThe Process of Masterdata Definition for phase 4", "timestamp": "2025-09-18T09:38:30.459426Z", "source_priority": 1, "content_type": "concept"}
{"id": "docs:datastore:en_masterdata_definition_best_practices:0", "source": "https://datastore.bam.de/en/masterdata_definition/best_practices", "repo": "datastore", "title": "Best Practices for Masterdata Definition", "section": "Best Practices for Masterdata Definition", "text": "masterdata_definition\n/\nbest_practices\n# Best Practices for Masterdata Definition\n## Page Contents\n# Best Practices for Masterdata Definition\nNaming Spaces and Projects\n## Defining Entity Types\n\"Inheritance\" of Entity Types\nMaking Changes to existing Entity Types\nAriza de Schellenberger, Angela\nLast Thursday at 11:44 AM\n¶\n# Best Practices for Masterdata Definition\nTo ensure a consistent set of\n## Masterdata\nin the BAM Data Store, we are introducing some rules, naming conventions, and recommendations when creating\n## Spaces\n,\n## Projects\n, entity types (\n## Collection\n,\n## Object\n,\n## Dataset\n, and\n## Property\ntypes) and controlled vocabularies.\nIn general, Masterdata should be\nas generic as possible and as specific as necessary\n. It is encouraged to re-use existing entity types and vocabularies from other groups. For this reason, the use of division-specific information in the codes and labels of entity types (e.g., \"BAM_FBX.Y_ROOM_TEMPERATURE\") should be avoided.\n¶\n## Naming\n## Spaces\nand\n## Projects\n## Spaces\nand\n## Projects\nhave a\ncode/PermID\nand a\ndescription\n, but no additional label or metadata properties. Therefore, the\n## Space\n/\n## Project\ncode and description should contain enough information to make it clear what it is for (both to people outside the group and to people who join the group later).\nOnce created, the code of a\n## Space\n/\n## Project\ncannot\nbe changed.\n¶\n## Space\n/\n## Project\n## Code:\nCan only contain A-Z (uppercase letters), 0-9, '_' (underscore), '-' (hyphen), and '.' (dot).\nWords that would normally be separated by a whitespace, should instead be separated by an underscore (\"_\").\nShould be meaningful.\nShould be in English.\nShould be between 3-30 characters.\nDespite being written in capital letters only, the code of a\n## Space\n/\n## Project\nwill be displayed in the main menu with uppercase letters for every first letter of a new word (as signified by the use of an underscore) followed by lowercase letters.\n¶\n## Example:\n## A\n## Space\nor\n## Project\nwith the code \"TEST.THING_UNDERSCORE-HYPHEN\" will be displayed as \"Test.thing  Underscore-hyphen\" in the main menu.\n¶\n## Space\n/\n## Project\n## Description:\nNot mandatory in openBIS but mandatory for the Data Store.\nShould contain enough detail to be understandable to people outside of the group.\nShould contain 2-50 words.\nShould be in English followed by a double slash (//) and a German translation in the following format: \"English description//Deutsche Beschreibung\"\n¶\n## Defining Entity Types\nWhen defining a new entity type of class\n## Collection\n,\n## Object\nor\n## Dataset\n, please check whether a similar entity type (of the same class) already exists that could be reused.\nIf the new entity type is a specification of an existing, more generic entity type then the new, more specific entity type must include all the Sections and\n## Property\ntypes of the existing entity type. For more information on the concept of inheritance of entity types, see\nhere\n.\nCollection and Dataset types will be deprecated in future openBIS versions. Therefore only Object Types and Controlled Vocabularies are curently defined for the Data Store.\n¶\n## Defining\n## Object\n## Types\nThe creation of new\n## Object\ntypes is one of the main tasks of the\nMasterdata definition process\n.\n¶\n## Object\n## Code:\nOnly capital letters allowed.\nCan only contain A-Z, 0-9, _ and . (dot).\nWords should be separated by an underscore (\"_\").\nShould be meaningful.\nShould be in English.\nShould be between 3-20 characters long.\n¶\n## Object\n## Description:\nNot mandatory in openBIS but mandatory for BAM Data Store.\nShould contain enough details to be understandable to people outside of the group.\nShould be up to 250 characters long.\nShould be in English followed by a German translation in the following format: \"English description//Deutsche Beschreibung\"\n¶\n## Object\n## Generated Code Prefix:\nShould be meaningful.\nAs a convention, we recommend to use the\nfirst 3 letters of the\n## Object\ntype code\n.\n## Example:\nFor the\n## Object\ntype \"Instrument\", the code prefix should be \"INS\". Registered\n## Objects\nof the type \"INSTRUMENT\" will have the code \"INS1\", \"INS2\", etc.\nIf an identical code prefix already exists:\nChoose the first 4-5 letters of the\n## Object\ntype code OR\nIf the\n## Object\ntype is a specification of a an already existing\n## Object\ntype (see above), take the existing generated code prefix, add a dot/period (.) and then another 2-4 character code to make it unique.\n## Example:\nFor a new\n## Object\ntype \"EXPERIMENTAL_STEP.MICROSCOPY\" that is a specification of the existing\n## Object\ntype \"EXPERIMENTAL_STEP\" with the code prefix \"EXP\", the code prefix of the new\n## Object\ntype could be \"EXP.MIC\".\n¶\n## Defining\n## Property\n## Types\n## A\n## Property\nis a metadata field used to describe an entity, i.e.\n## Collection\n, an\n## Object\nor a\n## Dataset\n.\n## Property\ntypes have a\ncode\n, a\nlabel\n, a\ndata type\n, and a\ndescription\n. One and the same\n## Property\ntype can be used for many entity types. When assigning a\n## Property\ntype to an entity type, it can further be defined whether the Property is\nmandatory\nand whether it should be\neditable\nby the user in the ELN-LIMS UI. Additionally, a\n## Dynamic Property Script\ncan be added to the\n## Propert\ny type assignment.\n## All\n## Property\ntypes are\nglobal\nin openBIS which means that they can be assigned to many entity types at the same time. Changes made to a\n## Property\ntype (e.g., changing the label or the description) will thus affect all entity types that the\n## Property\ntype is assigned to and should therefore be considered carefully. The only non-global attributes of a\n## Property\ntype are \"Mandatory\" and \"Editable\" and the\n## Dynamic Property Script\nwhich have to be defined individually for each\n## Property\ntype assignment. More information on changing existing entity types can be found\nhere.\n¶\n## Internal\n## Property\n## Types:\nThere are several internal\n## Property\ntypes in the Data Store (pre-defined by openBIS). Their code begins with a \"$\" sign. Some of them should be reused when creating a new entity Type (e.g., $NAME, $XMLCOMMENTS), others are used internally only (e.g., $ELN_SETTINGS). No changes can be made to these internal\n## Property\ntypes which is why they don't necessarily follow the naming conventions defined below.\nA new\n## Object\ntype should always contain the predefined\n## Property\ntypes $NAME, $XMLCOMMENTS and $ANNOTATIONS_STATE. The latter is not visible in any of the forms, but it is necessary to establish parent-child relationships between\n## Objects\n.\n¶\n## Property\n## Code:\nOnly capital letters allowed.\nCan only contain A-Z, 0-9 and _, -,.\nWords should be separated by an underscore (\"_\").\nShould be meaningful.\nShould be in English.\nShould be between 3-20 characters long.\n¶\n## Property\n## Label:\nShould be meaningful.\nShould be in English.\n¶\n## Property\nData Type:\nA Property type can use one of 11 possible data types:\nData type\n## Description\n## BOOLEAN\nyes or no (checkbox)\n## INTEGER\ninteger number\n## REAL\ndecimal number\n## DATE\ndate field\n## TIMESTAMP\ndate with timestamp\n## VARCHAR\none-line text\n## MULTILINE_VARCHAR\nlong text (it is possible to enable a Rich Text Editor for this type of property)\n## HYPERLINK\n## URL\n## CONTROLLED_VOCABULARY\nlist of predefined (alpha-numeric) values\n## XML\nto be used by Managed Properties and for Spreadsheet components\n## OBJECT\n1-1 connection to a specific Object type\n¶\n## Property Description:\nNot mandatory in openBIS but mandatory for the Data Store.\nShould contain enough details to be understandable to people outside of the group.\nShould be up to 250 characters long.\nShould be in English followed by a German translation in the following format: \"English description//Deutsche Beschreibung\"\n¶\nRepresentation of Values and Units of Measurement of Physical Quantities:\nUnits of measurement (e.g., SI base units) of a physical quantity should be represented as part of the label/description of a\n## Property\ntype and not as an individual\n## Property\ntype. The unit should be specified in square brackets in the\n## Property\nlabel. Where relevant, use the English notation to indicate the decimal place of numbers (period instead of comma). Additionally, the unit can also be part of the code to avoid multiple similarly named\n## Property\ntypes representing different entities.\n## Example:\nTo specify the room temperature measured in °C, create a single\n## Property\ntype called ROOM_TEMP_IN_CELCIUS with the data type REAL, instead of creating two types ROOM_TEMP_VALUE (data type REAL) and ROOM_TEMP_UNIT (data type CONTROLLED_VOCABULARY).\n## Code\n## Mandatory\nShow in edit views\n## Section\nProperty label\nData type\n## Description\n## Metadata\nDynamic script\n## ROOM_TEMP_IN_CELSIUS\n## False\n## TRUE\nFurther information\nRoom temperature [°C]\n## REAL\nRoom temperature in °C//Raumtemperatur in °C\n¶\nAssignment of existing\n## Property\n## Types\nIt is encouraged to reuse/assign already existing\n## Property\ntypes for the definition of new entity types instead of creating new\n## Property\ntypes that are synonymous or similar in meaning to those already in use.\n## Example:\nInstead of creating a new\n## Property\ntype called INSTRUMENT_NAME for an\n## Object\ntype INSTRUMENT, it is recommended to assign the existing\n## Property\ntype $NAME.\n¶\n## Defining Controlled Vocabularies\nA controlled vocabulary is an established list of terms to ensure consistency and uniqueness in the description of a given domain, e.g., a list of room labels, SI units or purity grades. In openBIS controlled vocabularies are one possible data type for metadata\n## Properties\n. The vocabulary itself has a\ncode\nand a\ndescription\nand each term in the vocabulary has a\ncode\n, a\nlabel\n, and a\ndescription\n.\nWhen defining a new controlled vocabulary, please check whether a similar vocabulary already exists.\nAll existing controlled vocabularies and their terms are listed in the Vocabulary Browser under the Utilities main menu of the ELN-LIMS UI.\nYour vocabulary should contain at least three different terms. If your controlled vocabulary consists only of two terms, consider using Boolean values (TRUE/FALSE) instead.\nIt is not possible to choose several terms from the same vocabulary in one metadata\n## Property\n.\n¶\n## Vocabulary Code:\nOnly capital letters allowed.\nCan only contain A-Z, 0-9 and _, -,.\nWords should be separated by an underscore (\"_\").\nShould be meaningful.\nShould be in English.\nShould be between 3-20 characters long.\n¶\n## Vocabulary Description:\nNot mandatory in openBIS but mandatory for the Data Store.\nShould contain enough details to be understandable to people outside of the group.\nShould be up to 250 characters long.\nShould be in English followed by a German translation in the following format: \"English description//Deutsche Beschreibung\"\n¶\n## Vocabulary URL Template:\nSome controlled vocabularies are documented in the web and have unique and persistent identifiers for each term (e.g., a persistent URL or a DOI). In openBIS, it is possible to define a URL template for this type of vocabulary, which represents the vocabulary-specific part of the URL followed by\n${term}\n. The term-specific part of the URL must be identical to the\nterm code\ndefined in openBIS.\nIf vocabulary terms with a URL/DOI are used in the ELN-LIMS UI, these are displayed as hyperlinks.\n## Example:\nFor a controlled vocabulary \"UNITS_OF_MEASURE\" which includes terms from the IUPAC Gold Book, the URL template is\nhttps://doi.org/10.1351/goldbook.${term}\n. The DOI for the term with the label \"degree Celsius\" is\nhttps://doi.org/10.1351/goldbook.D01561\n. Accordingly, the openBIS term code for \"degree Celsius\" must be \"D01561\".\n¶\n## Term Code:\nOnly capital letters allowed.\nCan only contain A-Z, 0-9 and _, -,.\nWords should be separated by an underscore (\"_\").\nShould be meaningful.\nShould be in English.\nShould be between 3-20 characters long.\nWhen using a prefix at the beginning of the code, it recommended to use the same prefix for the codes of all terms of a vocabulary.\n## Example:\nIn the vocabulary \"DFG_DEVICE_CODE\" vocabulary (for DFG Gerätegruppenschlüssel), the prefix could be \"DFG_\" for all terms, e.g., \"DFG_0000\", \"DFG_0005\", \"DFG_0010\" etc.\nShould not contain only numbers, unless the number is part of an external URL/DOI for the term (see below).\n## Example:\nIn the vocabulary “PURITY_GRADE”, for a term with the label “2.0” and the description “gas purity level 2.0 (99.0%)//Gasreinheit 2.0 (99,0 %), the code should not be “2.0” but something more meaningful like “GAS_PUR_2.0”.\nIf the vocabulary makes use of a\nURL template\nto link to definitions of the terms, the term code has to be identical with the term-specific part of the URL/DOI. In this case, it may be necessary to include codes that consist only of numbers.\n¶\n## Term Label:\nShould be meaningful.\nShould be in English.\nCan be up to 128 characters long.\n¶\n## Term Description:\nNot mandatory in openBIS but mandatory for the Data Store.\nShould contain enough details to be understandable to people outside of the group.\nShould be up to 250 characters long.\nShould be in English followed by a German translation in the following format: \"English description//Deutsche Beschreibung\"\n¶\n\"Inheritance\" of Entity Types\nWhen defining a new entity type that is similar to an existing one but requires further specification (i.e., additional\n## Property\ntypes), we make use of the principle of\ninheritance\n. This concept is borrowed from object-oriented programming:\n\"Most object-oriented programming languages have another feature that differentiates them from other data abstraction languages; class inheritance. Each class has a superclass from which it inherits operations and internal structure. A class can add to the operations it inherits or can redefine inherited operations. However, classes cannot delete inherited operations.\" -\nDesigning reusable classes. RE Johnson, B Foote. Journal of object-oriented programming. 1988.\nIn case of the BAM Data Store, entity types (\n## Collection\n,\n## Object\nand\n## Dataset\ntypes) inherit attributes from an entity type of the same class (\n## Collection\ntypes inherit from\n## Collection\ntypes,\n## Object\ntypes inherit from\n## Object\n## types etc.):\nThe new, more specific entity type includes all the Sections,\n## Property\ntypes, and\nDynamic Property and Entity Validation Scripts\nof the existing, more generic entity type.\nThe code of the new entity type contains the name of the original entity type as a prefix followed by a dot/period (.) followed by the specification term as a suffix: GENERIC.SPECIFIC\nLikewise, the generated code prefix of the new entity type contains the code prefix of the original entity type followed by a dot/period (.) followed by the new code prefix: GEN.SPE\nThe attributes of the\n## Property\ntype assignments (\"Mandatory\", \"Editable\") of the existing entity type may be changed but only from FALSE to TRUE (and not the other way round from TRUE to FALSE).\nThe inheritance concept can also be applied across multiple levels of entity types (e.g., GENERIC.SPECIFIC_LEVEL_1.SPECIFIC_LEVEL_2). The same rules apply.\nThe principle of inheritance is not native to openBIS but has been developed for the Data Store which is why the above described rules have to be implemented\nmanually\nwhen defining new entity types.\n## Example:\nThe existing Object type INSTRUMENT (generated code prefix: INS) does not contain the\n## Property\ntypes needed to adequately describe the metadata of a camera. Therefore, a new\n## Object\ntype with the name INSTRUMENT.CAMERA (generated code prefix: INS.CAM) is created.\nThe more specific\n## Object\ntype INSTRUMENT.CAMERA inherits all original Sections (\"General information\", \"BAM information\", \"Details\", \"Comments\") and\n## Property\ntypes (e.g., \"$NAME\", \"DEVICE_MODEL_NAME\", \"BAM_OE\" etc.) from the more generic\n## Object\ntype INSTRUMENT. The attributes (\"Mandatory\", \"Editable\") of the the\n## Property\ntype assignments can be adjusted from FALSE to TRUE (e.g., DEVICE_MODEL_NAME was set as non-mandatory in INSTRUMENT but is defined as mandatory in INSRUMENT.CAMERA). Dynamic Property Scripts stay the same.\nIn addition, the\n## Object\ntype INSTRUMENT.CAMERA contains two new Sections (\"Camera Information\", \"Software information\") with additional\n## Property\ntypes (e.g., \"IMAGE_SENSOR_FRAMERATE, \"LENS_MOUNT_TYPE\", \"FIRMWARE_VERSION\") that are\nnot\nincluded in the\n## Object\ntype INSTRUMENT.\n¶\nMaking Changes to existing Entity Types\nIt is possible to edit entity types after they have been initially created. These changes affect all entities of the type in question that have already been created in the Data Store. For this reason, changes to existing entity types in the Data Store always have to be discussed with the Data Store team and, if applicable, the person who originally created the entity type to ensure metadata consistency.\nIf you want to suggest changes to existing Masterdata please follow the same procedure described for\nThe Process of Masterdata Definition\n.\n¶\nEditing an existing\n## Property\n## Type:\nThis should only be done in case of minor corrections (e.g., correction of typos, additions or clarifications) in the label or the description of the\n## Property\ntype. The meaning of\n## Property\ntypes should never be changed retrospectively when\n## Properties\nof the type are already assigned to entity types that are in use in the Data Store instance.\nThe code of a\n## Property\ntype cannot be changed after its creation nor can a\n## Dynamic Property Script\nbe assigned retrospectively.\n¶\nAssignment of a new\n## Property\nType to an existing Entity Type:\nThe assignment of a\nnon-mandatory\n## Property\ntype\n(either new or existing) is possible but has to be discussed with the Data Store team and, if applicable, the person who originally created the entity type.\nThe assignment of a\nmandatory\n## Property\ntype\n(either new or existing) is only possible if a default value is defined for all entities to which the\n## Property\ntype is assigned. The necessity of assigning a mandatory\n## Property\ntype and the default value have to be discussed with the Data Store team and, if applicable, the person who originally created the entity type.\n¶\nDeletion of a\n## Property\nType Assignment from an existing Entity Type:\nThis is possible only in exceptional cases because removing the assignment will also remove all existing\n## Property\nvalues of this type in the Data Store instance -\ndata will be lost\n! The necessity of the deletion has to be discussed with the Data Store team and, if applicable, the person who originally created the entity type, as well as all users who already registered entities of the type in question.\n¶\nDeletion of an existing\n## Collection\n/\n## Object\n/\n## Dataset\n## Type:\nThis is possible only in exceptional cases because removing the entity type will also remove all existing entities of this type in the Data Store instance -\ndata will be lost\n! The necessity of the deletion has to be discussed with the Data Store team and, if applicable, the person who originally created the entity type, as well as all users who already registered entities of the type in question.", "timestamp": "2025-09-18T09:38:30.469950Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_masterdata_definition_definition_of_masterdata:0", "source": "https://datastore.bam.de/en/masterdata_definition/definition_of_masterdata", "repo": "datastore", "title": "The Process of Masterdata Definition", "section": "The Process of Masterdata Definition", "text": "masterdata_definition\n/\ndefinition_of_masterdata\n# The Process of Masterdata Definition\n## Page Contents\n# The Process of Masterdata Definition\nMasterdata Excel Template examples\nNaming Masterdata Excel Files\nColor-Coding of Masterdata Excel Files\nOrganising Masterdata Definition Files\nAriza de Schellenberger, Angela\nLast Friday at 2:57 PM\n¶\n# The Process of Masterdata Definition\nOnly instance admins can register\n## Masterdata\nin openBIS. Since Data Store Stewards (DSSt) have group admin but not instance admin rights in the\nmain\nData Store instance, they cannot register domain/division-specific Masterdata on their own.\nThey can use one of the Masterdata Excel templates. To create an Object Type e.g.,\n## Instrument\nor a Controlled Vocabulary e.g.,\n## DFG_DEVICE_CODE\n.\nPlease ensure that you follow the\nrules and best practices for Masterdata definition\nand check the Excel files with the\nMasterdata checker\nbefore you make them available to the Data Store team via GitHub repository or via email at\ndatastore@bam.de\n. Information about how to create a new repository in the Github can be found\nhere\n.  You can also create your repository or fork it from\nGitHub BAM research\n.  To Share your GitHub repository with the Data Store team, go to your repository in GitHub, copy the URL from the browser address bar and sent it via email to\ndatastore@bam.de\n.\nThe instance admins of the Data Store team check the Masterdata defined by the divisions and contact the DSSt in case of questions. As soon as the Masterdata for the division is finalised, the instance admins import it into the\nmain\nData Store instance.\nIf you want to propose changes to an existing Object Type or Controlled Vocabulary. You can use one of following options:\nCreate an \"Issue\" in the repository\nbam-masterdata repository\n. Current Object Types and Controlled Vocabularies can be seen within this folder under\ndata model\n.\nDownload the Excel format of the\n## Object Type\nor\n## Controlled Vocabulary\nyou want to modify from the current openBIS Masterdata. Modify it accoring to\nColor-Coding of Masterdata Excel files\nand\nNaming Masterdata Excel Files\n.\n¶\nMasterdata Excel Template examples\nWe provide examples of the Masterdata Excel file to create an Object Type i.e.,\n## Instrument\nand a controlled vocabulary i.e.,\n## DFG_DEVICE_CODE\n.\nThe openBIS Masterdata Excel template uses terms from earlier openBIS versions (which should not be changed):\n\"\n## Object\ntype\" is called \"SAMPLE_TYPE\"\n\"\n## Collection\ntype\" is called \"EXPERIMENT_TYPE\"\nIf you use a German Excel version,  note that the terms TRUE/FALSE (shown in the columns \"Mandatory\" and \"Show in edit views\") are automatically renamed to WAHR/FALSCH and must be changed to English.\n¶\nNaming Masterdata Excel Files\nPlease create a separate Excel file for each entity type/controlled vocabulary.\nThe name of each Masterdata Excel file should include:\nthe entity type:\nobject_type\nor\nvocabulary\nthe code of the entity type/vocabulary should be included while maintaining the exact format (upper-case letters, inlcuding all underscores and dots).\nthe number of the division\nThe different parts of the file name have to be separated by underscores.\n¶\n## Example:\nFor the\n## Object\ntype INSTRUMENT, the Masterdata Excel file should be named\nobject_type_INSTRUMENT_FBX.Y.xlsx\n.\n¶\nColor-Coding of Masterdata Excel Files\nA new entity type typically contains a combination of pre-existing\n## Property\ntypes and newly defined\n## Property\ntypes.\nIn order to easily distinguish between them in the Masterdata Excel file, they should be color-coded to facilitate the Masterdata review process by the Data Store team. This is particularily important when defining an entity type that is a specification of an existing, more generic entity type, and thus \"inherits\" the set of\n## Property\ntypes from the generic one, as described\nhere\n.\n## Property\ntype assignments should be color-coded according to the following rules:\n## New\n## Property\ntypes are marked in\norange\n.\n## Existing\n## Property\ntypes that are already assigned to the more generic entity type are marked in\nblue\n.\n## Existing\n## Property\ntypes that are not assigned to the more generic entity type are marked in\ngreen\n.\n¶\n## Example:\nA user wants to create a new\n## Object\ntype INSTRUMENT.CAMERA that is a specification of the existing generic\n## Object\ntype INSTRUMENT. INSTRUMENT.CAMERA inherits the complete set of\n## Property\ntypes from INSTRUMENT, color-coded in\nblue\n(e.g., MANUFACTURER). The user also defines several new\n## Property\ntypes to describe attributes that are specific to a camera; these are color-coded in\norange\n(e.g., IMAGE_SENSOR_FRAMERATE). In addition, the user assigns an existing\n## Property\ntype (e.g., FIRMWARE_VERSION) that has already been assigned to other\n## Object\ntypes but not to INSTRUMENT. This should be color-coded in\ngreen\n.\n¶\nOrganising Masterdata Definition Files\nEach entity type (except\n## Property\ntypes)/controlled vocabulary must be represented in a separate Excel file for easier organisation and findability.\nPlease make sure that the Excel file also includes all (newly created) dependencies such as new controlled vocabularies, new Dynamic Property and Entity Validation Scripts. Ensure that the individual Excel files and the vocabulary code listed in the Excel file of the object type are consistent.\nIf the Masterdata you generate uses existing vocabulary types (e.g., BAM_OE) or scripts (e.g. date_range_validation.py), they don't need to be sent.", "timestamp": "2025-09-18T09:38:30.478235Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_masterdata_definition_masterdata_checker:0", "source": "https://datastore.bam.de/en/masterdata_definition/masterdata_checker", "repo": "datastore", "title": "How to use Masterdata checker", "section": "How to use Masterdata checker", "text": "masterdata_definition\n/\nmasterdata_checker\n# How to use Masterdata checker\n## Demidova, Caroline\nLast Thursday at 11:44 AM\n¶\n1. Prepare Masterdata Excel file\nGenerate the Excel file (.xlsx) of the Object Type or Controlled Vocabulary according to the\ndefinition of Masterdara\nand\nbest practices\n.\n¶\n2. Open the Masterdata checker and upload the Masterdata Excel file\nThe Masterdata checker can be accessed\nhere\n. Click on the\nDatei auswählen\nand select the Masterdata Excel file, click on the\n## Check Masterdata\ntab to activate the file checker.  Info, warnings, and errors will be listed below under\n## Checker Logs\n¶\n3. Interpretation of the info, warning, and error messages\nERRORs must be\ncorrected by DSSts before the Masterdata Excel file is shared with the Data Store team via GitHub.\n## Info\nand\nwarning\nmessages provide an additional information to users. and do not required any action of the DSSts.\n## ERROR\nmessages indicate mistakes in the Masterdata Excel file, which will prevent functioning of the system,\nmandatory\nto fix before proceeding further. In the error drop-down there will be information displayed about the objective of the mistake, e.g.\n## Code\nis incorect,\n## Property\nis misslabeled etc. In some cases, the specific cell number will be indicated to where mistake is to be found.\nCorrect the error in the Masterdata Excel file and re-upload the file to check if the problem has been fixed.\n## INFO\nprovides general information without requiring any action on the part of the user. It informs the user about the data model used to execute the check; the type of the file uploaded, etc. The last drop-down list shows the name of the files that has just been checked.\n## WARNING\nprovides information about the possible issues or  changes in the system but no action is required. Information about the planned future changes in\nProperty types\ncan be found in the drop-down list of warnings.\nIn case you encounter any bugs or would like to give feedback on using the Masterdata Checker, please send a message to the developers at\nGitHub\n.", "timestamp": "2025-09-18T09:38:30.484068Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_Previous_version_of_Wiki_datastore_stewards_properties-handled-by-scripts:0", "source": "https://datastore.bam.de/en/Previous_version_of_Wiki/datastore/stewards/properties-handled-by-scripts", "repo": "datastore", "title": "Properties handled by Scripts", "section": "Properties handled by Scripts", "text": "Previous_version_of_Wiki\n/\ndatastore\n/\nstewards\n/\nproperties-handled-by-scripts\n# Properties handled by Scripts\nDynamic Property and Entity Validation Scripts\n## Page Contents\nDynamic Property and Entity Validation Scripts\n## Dynamic Property Scripts\n## Entity Validation Scripts\n## Demidova, Caroline\n¶\nDynamic Property and Entity Validation Scripts\nopenBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:\n## Dynamic Property Scripts\nare a mechanism to automatically compute values of\n## Properties\nthat should not/cannot be manually modified by users.\n## Entity Validation Scripts\nare a mechanism to ensure metadata consistency of an entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype).\nBoth types of scripts are defined in the openBIS Admin User Interface (UI) under \"Tools\" -> \"Dynamic Property Plugin\" or \"Entity Validation Plugin\", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.\n¶\n## Dynamic Property Scripts\nIn most cases, values of\n## Properties\nare defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic\n## Properties\nthat should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic\n## Property\n, usually based on the values of one or more\n## Properties\nof the same entity.\nDynamic Property Scripts are part of the\n## Property\ntype assignments of entity types. This means that the script is not always used for a certain\n## Property\ntype. Instead, it is one of the optional characteristics of a\n## Property\ntype that is assigned to a specific entity type: The same\n## Property\ntype can be a dynamic\n## Property\n(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal\n## Property\nthat has to be filled by the user.\n¶\nExample for Dynamic Property Scripts\n## The\n## Object\ntype RECTANGLE includes the following three\n## Property\n## types:\n## RECTANGLE_LENGTH_IN_M\n## [REAL]\n## RECTANGLE_WIDTH_IN_M\n## [REAL]\n## RECTANGLE_AREA_IN_QM\n## [REAL]\nOnly the first two\n## Properties\ncan be edited by users in the ELN-LIMS UI. Once the\n## Object\nis saved, the value of the\n## Property\n## RECTANGLE_AREA_IN_QM\nis automatically computed as the product of the values of\n## RECTANGLE_LENGTH_IN_M\nand\n## RECTANGLE_WIDTH_IN_M\nas defined in the Dynamic Property Script \"rectangle_area\":\ndef\ncalculate\n(\n)\n## :\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nlength\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_LENGTH_IN_M\"\n)\nwidth\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_WIDTH_IN_M\"\n)\narea\n=\nlength\n*\nwidth\nreturn\narea\n## Copy\nWhen assigning the\n## Property\ntype\n## RECTANGLE_AREA_IN_QM\nto the\n## Object\ntype RECTANGLE, the name of the script is entered in the field \"Dynamic Property Plugin\" in the Admin UI:\nWhen using the Excel import option, the name of the script is entered as \"rectangle_area.py\" in the column \"Dynamic script\" in the Masterdata Excel file.\nThe script itself is defined (and can be tested) in the openBIS Admin UI under \"Tools\" -> \"Dynamic Property Plugin\" (see screenshot for Entity Validation Scripts\nabove\n). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).\n¶\nRules & Best Practices for Dynamic Property Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe computation of the value of the dynamic\n## Property\nmust be based on the values of one or more\n## Properties\nof the same entity.\n## Properties\nof other entities must not be accessed.\nAll information needed for the calculation must be included in the script. External resources must not be accessed.\nDynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.\nThe script must be added when first creating the\n## Property\ntype or, in the case of an existing\n## Property\ntype, when assigning it to an entity type. It cannot be added retrospectively after the\n## Property\ntype assignment already exists.\n¶\n## Entity Validation Scripts\nEntity Validation Scripts are defined at the entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype) level.\nOnce an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.\n¶\nExample for Entity Validation Scripts\nAn example of an Entity Validation Script is the\n## Date Range Validation\n(\"EXPERIMENTAL_STEP.date_range_validation\") which checks for an\n## Object\nof the type EXPERIMENTAL_STEP whether the date entered for the\n## Property\n## END.DATE\nis later than the date entered for the\n## Property\n## START.DATE\n.\nIf not, the error message \"End date cannot be before start date!\" is returned and the EXPERIMENTAL_STEP cannot be saved.\n#date_range_validation.py\ndef\ngetRenderedProperty\n(\nentity\n,\nproperty\n)\n## :\nvalue\n=\nentity\n.\nproperty\n(\nproperty\n)\nif\nvalue\nis\nnot\n## None\n## :\nreturn\nvalue\n.\nrenderedValue\n(\n)\ndef\nvalidate\n(\nentity\n,\nisNew\n)\n## :\nstart_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"START_DATE\"\n)\nend_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"END_DATE\"\n)\nif\nstart_date\nis\nnot\n## None\nand\nend_date\nis\nnot\n## None\nand\nstart_date\n>\nend_date\n## :\nreturn\n\"End date cannot be before start date!\"\n## Copy\n¶\nRules & Best Practices for Entity Validation Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe validation must be based on the values of one or more\n## Properties\nof the entity being validated.\n## Properties\nof other entities must not be accessed for validation.\nAll information required for validation must be included in the script. No external resources may be accessed for the validation.\nEntity Validation Scripts must be read-only. No\n## Properties\nof the entity may be added or edited.\nEntity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.", "timestamp": "2025-09-18T09:38:30.491721Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_use_cases:0", "source": "https://datastore.bam.de/en/use_cases", "repo": "datastore", "title": "List of use cases", "section": "List of use cases", "text": "use_cases\n# List of use cases\n## Page Contents\nopenBIS Data Store Use Cases:\nSharing Use Cases in Wiki\nPublication of BAM Use cases\nAriza de Schellenberger, Angela\nLast Monday at 5:01 PM\n¶\nopenBIS Data Store Use Cases:\n¶\nSharing Use Cases in Wiki\nData Store stewards are welcome to share an Use case of your division and make your work visible.\nWe provide a simplified example\nEuVSOP\nas inspiration for how you can illustrate a use case.  In addition, the benefits of the Data Store implementation for the group, project, etc. can be described.\nSimply send us your text in any format (.docx, .txt, etc.) together with screenshots (.jpg, .png) by email (\ndatastore@bam.de\n).  We will implement your example in the wiki.\nIt is not about waiting for the perfect use case, but about exchanging ideas on how research MSE workflows can be mapped in openBIS Data Store. However, if you need your use case reviewed, please us at\ndatastore@bam.de\n.\nEuVSOP\n¶\nPublication of BAM Use cases\nAlternatively, you can also publish an openBIS use case in Zenodo and make it citable:\nQI-Digital/publication Zenodo", "timestamp": "2025-09-18T09:38:30.498237Z", "source_priority": 1, "content_type": "general"}
{"id": "docs:datastore:en_use_cases_EuVSOP:0", "source": "https://datastore.bam.de/en/use_cases/EuVSOP", "repo": "datastore", "title": "Use case: EuVSOP", "section": "Use case: EuVSOP", "text": "use_cases\n/\nEuVSOP\n# Use case: EuVSOP\n## Page Contents\nUse Case 1: Synthesis of europium‑doped VSOP, customized enhancer solution and improved microscopy fluorescence methodology for unambiguous histological detection\nConceptual Data Model\nMap conceptual data model in the Data Store-openBIS data structure\n## Demidova, Caroline\nLast Monday at 9:11 AM\n¶\nUse Case 1: Synthesis of europium‑doped VSOP, customized enhancer solution and improved microscopy fluorescence methodology for unambiguous histological detection\nThe Eu-VSOP project investigates the unambiguous identification of iron oxide nanoparticles -VSOP doped with Europium (III) for flourescence detection in biological samples such as histological tissue sections.\n## Background\n: VSOP are very small iron oxide nanoparticles used in magnetic resonance imaging (MRI). These nanoparticles are studied as an alternative to Gadolium based MRI-contrast agents due to their potentially lower toxicity. The clear detection of EuVSOP in tissue sections enables biodistribution studies.\nNote that the content of this Demo Project is inspired by some scientific open access publications.\n[1]\nSome modifications might be included for illustration of openBIS functions.\n¶\nConceptual Data Model\n## The\nconceptual data model\ndescribes the steps required to map research data in the Data Store and includes a guideline template to detail the following steps for a generic example.\n¶\nDraw the research data Workflow\n¶\nIdentify Entities and Entity Types\n¶\nMap Entities in the openBIS data structure and Connect Entities\n¶\nMap conceptual data model in the Data Store-openBIS data structure\n¶\nRegister data in the Lab Notebook:\nRegister a Project - EuVSOP\nRegister a Collection – Experimental Steps of the type Default Experimental Step\nAt the Collection level, register Objects of the Type -Experimental Step for EuVSOP- Synthesis, HEE Treatment.\nAt the Object level – EuVSOP HEE Treatment, register new Objects of the Type – Experimental Step for: Nanoparticle Iron quantification, Magnetic Characterization and Nanoparticle Size\nUpload datasets to all Experimental Steps\nConnect Experimental Steps: Edit EuVSOP Synthesis to define as Children the Experimental Step - EuVSOP HEE Treatment.\n¶\nRegister items in the Inventory:\nUse inventory spaces defined per default in division´s inventory space (e.g., X.1 Equipment and X.1 Materials).\nRegister a Project-folder - Instruments in the folder (Space) X.1 Equipment\n## Register Instruments\nRegister a Project folder - Consumables in the folder (Space) X.1 Materials\nRegister a Collection – Chemicals\nRegister Objects Chemicals with Batch update registration\n¶\nConnect Inventory and ELN -Experimental Steps:\nEdit Experimental Steps (e.g., EuVSOP Synthesis) to define Objects such as Chemicals and Instruments as Parents.\nVisualize Parent-child connections in the hierarchical tree of each Object (e.g., Experimental Step: EuVSOP Nanoparticle Size).  To improve the visibility of the levels in the hierarchical tree, the displayed types for Chemicals and Instruments are excluded from the hierarchical tree in openBIS.\n## doi:\n10.1186/s12951-017-0301-6\n.\n↩︎", "timestamp": "2025-09-18T09:38:30.504710Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:en_use_cases_QI_digital_additive_manufacturing:0", "source": "https://datastore.bam.de/en/use_cases/QI_digital_additive_manufacturing", "repo": "datastore", "title": "QI-Digital: openBIS Data Model for an Additive Manufacturing", "section": "QI-Digital: openBIS Data Model for an Additive Manufacturing", "text": "use_cases\n/\nQI_digital_additive_manufacturing\n# QI-Digital: openBIS Data Model for an Additive Manufacturing\n## Page Contents\n## Header\n## Demidova, Caroline\nLast Thursday at 1:15 PM\n¶\n## Header\nYour content here", "timestamp": "2025-09-18T09:38:30.508558Z", "source_priority": 1, "content_type": "general"}
{"id": "docs:datastore:index:0", "source": "https://datastore.bam.de/", "repo": "datastore", "title": "Welcome to BAM Data Store", "section": "Welcome to BAM Data Store", "text": "# Welcome to BAM Data Store\n## Page Contents\nWelcome to the Data Store Wiki\n## Wiki Structure:\n## 💡 Concepts\n📖 How-to guides\n## ❓ FAQ\n👥 Use cases\nWhat is the Data Store?\nWhat is openBIS?\nWhat is the Data Store Project?\nWhat is the Data Store rollout process?\n## Demidova, Caroline\nYesterday at 11:44 AM\n¶\nWelcome to the Data Store Wiki\nThis Wiki provides information on the BAM Data Store - the central system for research data management at the Bundesanstalt für Materialforschung und -prüfung (BAM).\nThe Wiki is not intended to replace the openBIS documentation by the ETHZ (\nUser docs\n,\nAdmin docs\n). It provides conscise guidance and should serve as an additional source of openBIS and Data Store documentation for BAM employees.\nSome articles of this Wiki are currently under construction. If you have further questions that are not yet answered here, please contact\ndatastore@bam.de\n.\n¶\n## Wiki Structure:\n¶\n## 💡 Concepts\nExplanation about terms and concepts.\n## Explore Concepts\n¶\n📖 How-to guides\nStep-by-step instructions for openBIS functions.\nGo to Guides\n¶\n## ❓ FAQ\nFrequently asked questions about Data Store and openBIS.\nView FAQ\n¶\n👥 Use cases\nDiscover Use cases of the Data Store.\nDiscover Use cases\n¶\nWhat is the Data Store?\nThe Data Store is the central system for research data management (RDM) at BAM.\nThe Data Store is the central system for research data management (RDM) at BAM.\nIt enables divisions to digitally organize and describe laboratory inventory -such as instruments, samples, standard operating procedures (SOPs), using customizable metadata and linked documentation..\nIntegrated with electronic lab notebook (ELN), the Data Store allows experimental steps to be connected with inventory items, ensuring centralized and traceable documentation of research processes.  This structure linkage supports the FAIR principles\n[1]\n(Findable, Accessible, Interoperable, Reusable), facilitating both internal and external reuse of research data in line with funding bodies.\nBy storing data and metadata in a unified system, the Data Store enhances interdisciplinary collaboration and lays the foundation for advanced data analysis, including big data and Artificial Intelligence (AI) applications.\n¶\nWhat is openBIS?\nopenBIS (open Biology Information System) is the underlying platform of the Data Store.\nIt is an open-source software solution for Research Data Management (RDM) and Electronic Lab Notebook (ELN).\nDeveloped and maintained by the Scientific IT Services (SIS) at ETHZ Zurich (ETHZ), openBIS was originally designed for life sciences\n[2]\n[3]\n, it is now increasingly used materials science and other research domains.\nopenBIS provides a browser-based graphical user interface (GUI) for the managing digital laboratory inventories and documenting experiments in a standardized way.  Data files can be imported into via the GUI or through programming interfaces and linked to inventory items and experimental steps.\nopenBIS also supports integration with external tools and services, such as exporting data to\n## Zenodo\nrepository and analyzing scientific data in\n## Jupyter Notebook\n.\nFor more information on openBIS visit the official website (\nhttps://openbis.ch/\n).\n¶\nWhat is the Data Store Project?\nThe introduction of a RDM system does not happen overnight.\nTo evaluate openBIS suitability, a pilot phase was conducted from 01.12.2020 to 28.02.2022. During this period, five BAM research groups from diverse domains successfully implemented openBIS, confirming its effectiveness for managing data in various materials science domains.\nFollowing the pilot’s success, the Data Store project was approved to establish the system based on openBIS as a central RDM system across all BAM divisions. The project, led by VP.1 eScience and VP.2 Information Technology, began in October 2022 and is scheduled to run for 3.5 years.\nThe initial phase focused on developing the necessary IT infrastructure and preparing for the software rollout, included an analysis of RDM needs across BAM. The rollout of the Data Store began in 2023.\nFor more information about the Data Store project visit the BAM infoportal (\nAbout the Project\n)\n¶\nWhat is the Data Store rollout process?\nThroughout all phases, project management and communication play a crucial role in supporting a smooth and efficient implementation.\nThe actual Data Store rollout began in May 2023 and is being carried out in successive phases, with several divisions being trained at the same time. Lessons learned are collected at the end of each rollout phase to implement improvements in subsequent rollout phases. The order of the rollout is determined based on the interest expressed by division heads in surveys done in December 2022 and February 2025.\nThe current onboarding concept lasts 2 to 3 months for assigned Data Store Stewards (DSSt), including 1 day for division heads and all employees working with research data.\nThe Data Store Stewards are one or two members of the division appointed by the division head. They are ideally permanently employed for sustainability and are familiar with inventory, experimental processes and the (digital) workflows of the division. DSSts are trained to use main functions and customize the group settings.\nAll other division members who handle research data receive introductory training in the use of the system. The head of division takes part in information events and coordinates the completion of the rollout phase together with the DSSts.\nAfter the rollout, the heads of the division and the DSSts, together with the users, are responsible for storing newly generated research data and the continuous implementation of the Data Store within their division.\nMark D. Wilkinson et al. (2016). \"The FAIR Guiding Principles for scientific data management and stewardship\". Scientific Data. 3 (1): 160018. doi:\n## 10.1038/SDATA.2016.18\n.\n↩︎\nAngela Bauch et al. openBIS: a flexible framework for managing and analyzing complex data in biology research. 12, 468 (2011). doi:\n10.1186/1471-2105-12-468\n.\n↩︎\nCaterina Barillari et al. openBIS ELN-LIMS: an open-source database for academic laboratories. Bioinformatics. 32 (4), Feb 2016, 638–640. doi:[10.1093/bioinformatics/btv606].(\nhttps://doi.org/10.1093/bioinformatics/btv606\n).\n↩︎", "timestamp": "2025-09-18T09:38:30.516805Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:masterdata_definition_files_20250730_main_instance_object_types.xlsx:0", "source": "https://datastore.bam.de/masterdata_definition_files/20250730_main_instance_object_types.xlsx", "repo": "datastore", "title": "Properties handled by Scripts", "section": "Properties handled by Scripts", "text": "Previous_version_of_Wiki\n/\ndatastore\n/\nstewards\n/\nproperties-handled-by-scripts\n# Properties handled by Scripts\nDynamic Property and Entity Validation Scripts\n## Page Contents\nDynamic Property and Entity Validation Scripts\n## Dynamic Property Scripts\n## Entity Validation Scripts\n## Demidova, Caroline\n¶\nDynamic Property and Entity Validation Scripts\nopenBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:\n## Dynamic Property Scripts\nare a mechanism to automatically compute values of\n## Properties\nthat should not/cannot be manually modified by users.\n## Entity Validation Scripts\nare a mechanism to ensure metadata consistency of an entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype).\nBoth types of scripts are defined in the openBIS Admin User Interface (UI) under \"Tools\" -> \"Dynamic Property Plugin\" or \"Entity Validation Plugin\", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.\n¶\n## Dynamic Property Scripts\nIn most cases, values of\n## Properties\nare defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic\n## Properties\nthat should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic\n## Property\n, usually based on the values of one or more\n## Properties\nof the same entity.\nDynamic Property Scripts are part of the\n## Property\ntype assignments of entity types. This means that the script is not always used for a certain\n## Property\ntype. Instead, it is one of the optional characteristics of a\n## Property\ntype that is assigned to a specific entity type: The same\n## Property\ntype can be a dynamic\n## Property\n(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal\n## Property\nthat has to be filled by the user.\n¶\nExample for Dynamic Property Scripts\n## The\n## Object\ntype RECTANGLE includes the following three\n## Property\n## types:\n## RECTANGLE_LENGTH_IN_M\n## [REAL]\n## RECTANGLE_WIDTH_IN_M\n## [REAL]\n## RECTANGLE_AREA_IN_QM\n## [REAL]\nOnly the first two\n## Properties\ncan be edited by users in the ELN-LIMS UI. Once the\n## Object\nis saved, the value of the\n## Property\n## RECTANGLE_AREA_IN_QM\nis automatically computed as the product of the values of\n## RECTANGLE_LENGTH_IN_M\nand\n## RECTANGLE_WIDTH_IN_M\nas defined in the Dynamic Property Script \"rectangle_area\":\ndef\ncalculate\n(\n)\n## :\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nlength\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_LENGTH_IN_M\"\n)\nwidth\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_WIDTH_IN_M\"\n)\narea\n=\nlength\n*\nwidth\nreturn\narea\n## Copy\nWhen assigning the\n## Property\ntype\n## RECTANGLE_AREA_IN_QM\nto the\n## Object\ntype RECTANGLE, the name of the script is entered in the field \"Dynamic Property Plugin\" in the Admin UI:\nWhen using the Excel import option, the name of the script is entered as \"rectangle_area.py\" in the column \"Dynamic script\" in the Masterdata Excel file.\nThe script itself is defined (and can be tested) in the openBIS Admin UI under \"Tools\" -> \"Dynamic Property Plugin\" (see screenshot for Entity Validation Scripts\nabove\n). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).\n¶\nRules & Best Practices for Dynamic Property Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe computation of the value of the dynamic\n## Property\nmust be based on the values of one or more\n## Properties\nof the same entity.\n## Properties\nof other entities must not be accessed.\nAll information needed for the calculation must be included in the script. External resources must not be accessed.\nDynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.\nThe script must be added when first creating the\n## Property\ntype or, in the case of an existing\n## Property\ntype, when assigning it to an entity type. It cannot be added retrospectively after the\n## Property\ntype assignment already exists.\n¶\n## Entity Validation Scripts\nEntity Validation Scripts are defined at the entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype) level.\nOnce an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.\n¶\nExample for Entity Validation Scripts\nAn example of an Entity Validation Script is the\n## Date Range Validation\n(\"EXPERIMENTAL_STEP.date_range_validation\") which checks for an\n## Object\nof the type EXPERIMENTAL_STEP whether the date entered for the\n## Property\n## END.DATE\nis later than the date entered for the\n## Property\n## START.DATE\n.\nIf not, the error message \"End date cannot be before start date!\" is returned and the EXPERIMENTAL_STEP cannot be saved.\n#date_range_validation.py\ndef\ngetRenderedProperty\n(\nentity\n,\nproperty\n)\n## :\nvalue\n=\nentity\n.\nproperty\n(\nproperty\n)\nif\nvalue\nis\nnot\n## None\n## :\nreturn\nvalue\n.\nrenderedValue\n(\n)\ndef\nvalidate\n(\nentity\n,\nisNew\n)\n## :\nstart_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"START_DATE\"\n)\nend_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"END_DATE\"\n)\nif\nstart_date\nis\nnot\n## None\nand\nend_date\nis\nnot\n## None\nand\nstart_date\n>\nend_date\n## :\nreturn\n\"End date cannot be before start date!\"\n## Copy\n¶\nRules & Best Practices for Entity Validation Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe validation must be based on the values of one or more\n## Properties\nof the entity being validated.\n## Properties\nof other entities must not be accessed for validation.\nAll information required for validation must be included in the script. No external resources may be accessed for the validation.\nEntity Validation Scripts must be read-only. No\n## Properties\nof the entity may be added or edited.\nEntity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.", "timestamp": "2025-09-18T09:38:30.524668Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:masterdata_definition_files_20250730_main_instance_vocabulary_types.xlsx:0", "source": "https://datastore.bam.de/masterdata_definition_files/20250730_main_instance_vocabulary_types.xlsx", "repo": "datastore", "title": "Properties handled by Scripts", "section": "Properties handled by Scripts", "text": "Previous_version_of_Wiki\n/\ndatastore\n/\nstewards\n/\nproperties-handled-by-scripts\n# Properties handled by Scripts\nDynamic Property and Entity Validation Scripts\n## Page Contents\nDynamic Property and Entity Validation Scripts\n## Dynamic Property Scripts\n## Entity Validation Scripts\n## Demidova, Caroline\n¶\nDynamic Property and Entity Validation Scripts\nopenBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:\n## Dynamic Property Scripts\nare a mechanism to automatically compute values of\n## Properties\nthat should not/cannot be manually modified by users.\n## Entity Validation Scripts\nare a mechanism to ensure metadata consistency of an entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype).\nBoth types of scripts are defined in the openBIS Admin User Interface (UI) under \"Tools\" -> \"Dynamic Property Plugin\" or \"Entity Validation Plugin\", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.\n¶\n## Dynamic Property Scripts\nIn most cases, values of\n## Properties\nare defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic\n## Properties\nthat should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic\n## Property\n, usually based on the values of one or more\n## Properties\nof the same entity.\nDynamic Property Scripts are part of the\n## Property\ntype assignments of entity types. This means that the script is not always used for a certain\n## Property\ntype. Instead, it is one of the optional characteristics of a\n## Property\ntype that is assigned to a specific entity type: The same\n## Property\ntype can be a dynamic\n## Property\n(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal\n## Property\nthat has to be filled by the user.\n¶\nExample for Dynamic Property Scripts\n## The\n## Object\ntype RECTANGLE includes the following three\n## Property\n## types:\n## RECTANGLE_LENGTH_IN_M\n## [REAL]\n## RECTANGLE_WIDTH_IN_M\n## [REAL]\n## RECTANGLE_AREA_IN_QM\n## [REAL]\nOnly the first two\n## Properties\ncan be edited by users in the ELN-LIMS UI. Once the\n## Object\nis saved, the value of the\n## Property\n## RECTANGLE_AREA_IN_QM\nis automatically computed as the product of the values of\n## RECTANGLE_LENGTH_IN_M\nand\n## RECTANGLE_WIDTH_IN_M\nas defined in the Dynamic Property Script \"rectangle_area\":\ndef\ncalculate\n(\n)\n## :\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nlength\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_LENGTH_IN_M\"\n)\nwidth\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_WIDTH_IN_M\"\n)\narea\n=\nlength\n*\nwidth\nreturn\narea\n## Copy\nWhen assigning the\n## Property\ntype\n## RECTANGLE_AREA_IN_QM\nto the\n## Object\ntype RECTANGLE, the name of the script is entered in the field \"Dynamic Property Plugin\" in the Admin UI:\nWhen using the Excel import option, the name of the script is entered as \"rectangle_area.py\" in the column \"Dynamic script\" in the Masterdata Excel file.\nThe script itself is defined (and can be tested) in the openBIS Admin UI under \"Tools\" -> \"Dynamic Property Plugin\" (see screenshot for Entity Validation Scripts\nabove\n). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).\n¶\nRules & Best Practices for Dynamic Property Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe computation of the value of the dynamic\n## Property\nmust be based on the values of one or more\n## Properties\nof the same entity.\n## Properties\nof other entities must not be accessed.\nAll information needed for the calculation must be included in the script. External resources must not be accessed.\nDynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.\nThe script must be added when first creating the\n## Property\ntype or, in the case of an existing\n## Property\ntype, when assigning it to an entity type. It cannot be added retrospectively after the\n## Property\ntype assignment already exists.\n¶\n## Entity Validation Scripts\nEntity Validation Scripts are defined at the entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype) level.\nOnce an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.\n¶\nExample for Entity Validation Scripts\nAn example of an Entity Validation Script is the\n## Date Range Validation\n(\"EXPERIMENTAL_STEP.date_range_validation\") which checks for an\n## Object\nof the type EXPERIMENTAL_STEP whether the date entered for the\n## Property\n## END.DATE\nis later than the date entered for the\n## Property\n## START.DATE\n.\nIf not, the error message \"End date cannot be before start date!\" is returned and the EXPERIMENTAL_STEP cannot be saved.\n#date_range_validation.py\ndef\ngetRenderedProperty\n(\nentity\n,\nproperty\n)\n## :\nvalue\n=\nentity\n.\nproperty\n(\nproperty\n)\nif\nvalue\nis\nnot\n## None\n## :\nreturn\nvalue\n.\nrenderedValue\n(\n)\ndef\nvalidate\n(\nentity\n,\nisNew\n)\n## :\nstart_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"START_DATE\"\n)\nend_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"END_DATE\"\n)\nif\nstart_date\nis\nnot\n## None\nand\nend_date\nis\nnot\n## None\nand\nstart_date\n>\nend_date\n## :\nreturn\n\"End date cannot be before start date!\"\n## Copy\n¶\nRules & Best Practices for Entity Validation Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe validation must be based on the values of one or more\n## Properties\nof the entity being validated.\n## Properties\nof other entities must not be accessed for validation.\nAll information required for validation must be included in the script. No external resources may be accessed for the validation.\nEntity Validation Scripts must be read-only. No\n## Properties\nof the entity may be added or edited.\nEntity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.", "timestamp": "2025-09-18T09:38:30.532839Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:masterdata_definition_files_20250730_masterdata_of_controlled_vocabulary_dfg_device_code.xlsx:0", "source": "https://datastore.bam.de/masterdata_definition_files/20250730_masterdata_of_controlled_vocabulary_dfg_device_code.xlsx", "repo": "datastore", "title": "Properties handled by Scripts", "section": "Properties handled by Scripts", "text": "Previous_version_of_Wiki\n/\ndatastore\n/\nstewards\n/\nproperties-handled-by-scripts\n# Properties handled by Scripts\nDynamic Property and Entity Validation Scripts\n## Page Contents\nDynamic Property and Entity Validation Scripts\n## Dynamic Property Scripts\n## Entity Validation Scripts\n## Demidova, Caroline\n¶\nDynamic Property and Entity Validation Scripts\nopenBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:\n## Dynamic Property Scripts\nare a mechanism to automatically compute values of\n## Properties\nthat should not/cannot be manually modified by users.\n## Entity Validation Scripts\nare a mechanism to ensure metadata consistency of an entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype).\nBoth types of scripts are defined in the openBIS Admin User Interface (UI) under \"Tools\" -> \"Dynamic Property Plugin\" or \"Entity Validation Plugin\", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.\n¶\n## Dynamic Property Scripts\nIn most cases, values of\n## Properties\nare defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic\n## Properties\nthat should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic\n## Property\n, usually based on the values of one or more\n## Properties\nof the same entity.\nDynamic Property Scripts are part of the\n## Property\ntype assignments of entity types. This means that the script is not always used for a certain\n## Property\ntype. Instead, it is one of the optional characteristics of a\n## Property\ntype that is assigned to a specific entity type: The same\n## Property\ntype can be a dynamic\n## Property\n(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal\n## Property\nthat has to be filled by the user.\n¶\nExample for Dynamic Property Scripts\n## The\n## Object\ntype RECTANGLE includes the following three\n## Property\n## types:\n## RECTANGLE_LENGTH_IN_M\n## [REAL]\n## RECTANGLE_WIDTH_IN_M\n## [REAL]\n## RECTANGLE_AREA_IN_QM\n## [REAL]\nOnly the first two\n## Properties\ncan be edited by users in the ELN-LIMS UI. Once the\n## Object\nis saved, the value of the\n## Property\n## RECTANGLE_AREA_IN_QM\nis automatically computed as the product of the values of\n## RECTANGLE_LENGTH_IN_M\nand\n## RECTANGLE_WIDTH_IN_M\nas defined in the Dynamic Property Script \"rectangle_area\":\ndef\ncalculate\n(\n)\n## :\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nlength\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_LENGTH_IN_M\"\n)\nwidth\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_WIDTH_IN_M\"\n)\narea\n=\nlength\n*\nwidth\nreturn\narea\n## Copy\nWhen assigning the\n## Property\ntype\n## RECTANGLE_AREA_IN_QM\nto the\n## Object\ntype RECTANGLE, the name of the script is entered in the field \"Dynamic Property Plugin\" in the Admin UI:\nWhen using the Excel import option, the name of the script is entered as \"rectangle_area.py\" in the column \"Dynamic script\" in the Masterdata Excel file.\nThe script itself is defined (and can be tested) in the openBIS Admin UI under \"Tools\" -> \"Dynamic Property Plugin\" (see screenshot for Entity Validation Scripts\nabove\n). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).\n¶\nRules & Best Practices for Dynamic Property Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe computation of the value of the dynamic\n## Property\nmust be based on the values of one or more\n## Properties\nof the same entity.\n## Properties\nof other entities must not be accessed.\nAll information needed for the calculation must be included in the script. External resources must not be accessed.\nDynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.\nThe script must be added when first creating the\n## Property\ntype or, in the case of an existing\n## Property\ntype, when assigning it to an entity type. It cannot be added retrospectively after the\n## Property\ntype assignment already exists.\n¶\n## Entity Validation Scripts\nEntity Validation Scripts are defined at the entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype) level.\nOnce an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.\n¶\nExample for Entity Validation Scripts\nAn example of an Entity Validation Script is the\n## Date Range Validation\n(\"EXPERIMENTAL_STEP.date_range_validation\") which checks for an\n## Object\nof the type EXPERIMENTAL_STEP whether the date entered for the\n## Property\n## END.DATE\nis later than the date entered for the\n## Property\n## START.DATE\n.\nIf not, the error message \"End date cannot be before start date!\" is returned and the EXPERIMENTAL_STEP cannot be saved.\n#date_range_validation.py\ndef\ngetRenderedProperty\n(\nentity\n,\nproperty\n)\n## :\nvalue\n=\nentity\n.\nproperty\n(\nproperty\n)\nif\nvalue\nis\nnot\n## None\n## :\nreturn\nvalue\n.\nrenderedValue\n(\n)\ndef\nvalidate\n(\nentity\n,\nisNew\n)\n## :\nstart_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"START_DATE\"\n)\nend_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"END_DATE\"\n)\nif\nstart_date\nis\nnot\n## None\nand\nend_date\nis\nnot\n## None\nand\nstart_date\n>\nend_date\n## :\nreturn\n\"End date cannot be before start date!\"\n## Copy\n¶\nRules & Best Practices for Entity Validation Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe validation must be based on the values of one or more\n## Properties\nof the entity being validated.\n## Properties\nof other entities must not be accessed for validation.\nAll information required for validation must be included in the script. No external resources may be accessed for the validation.\nEntity Validation Scripts must be read-only. No\n## Properties\nof the entity may be added or edited.\nEntity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.", "timestamp": "2025-09-18T09:38:30.541458Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:masterdata_definition_files_20250730_masterdata_of_object_type_instrument.xlsx:0", "source": "https://datastore.bam.de/masterdata_definition_files/20250730_masterdata_of_object_type_instrument.xlsx", "repo": "datastore", "title": "Properties handled by Scripts", "section": "Properties handled by Scripts", "text": "Previous_version_of_Wiki\n/\ndatastore\n/\nstewards\n/\nproperties-handled-by-scripts\n# Properties handled by Scripts\nDynamic Property and Entity Validation Scripts\n## Page Contents\nDynamic Property and Entity Validation Scripts\n## Dynamic Property Scripts\n## Entity Validation Scripts\n## Demidova, Caroline\n¶\nDynamic Property and Entity Validation Scripts\nopenBIS offers two different options to include custom Jython/Python scripts as part of the Masterdata:\n## Dynamic Property Scripts\nare a mechanism to automatically compute values of\n## Properties\nthat should not/cannot be manually modified by users.\n## Entity Validation Scripts\nare a mechanism to ensure metadata consistency of an entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype).\nBoth types of scripts are defined in the openBIS Admin User Interface (UI) under \"Tools\" -> \"Dynamic Property Plugin\" or \"Entity Validation Plugin\", respectively. Alternatively, they can be imported as .py files as part of the Masterdata ZIP folder when using the Excel import option.\n¶\n## Dynamic Property Scripts\nIn most cases, values of\n## Properties\nare defined by users in the ELN-LIMS UI or via pyBIS. In contrast, Dynamic Property Scripts can be used to automatically compute values of so-called dynamic\n## Properties\nthat should not/cannot be manually modified by users. The script defines a function that returns a value for a dynamic\n## Property\n, usually based on the values of one or more\n## Properties\nof the same entity.\nDynamic Property Scripts are part of the\n## Property\ntype assignments of entity types. This means that the script is not always used for a certain\n## Property\ntype. Instead, it is one of the optional characteristics of a\n## Property\ntype that is assigned to a specific entity type: The same\n## Property\ntype can be a dynamic\n## Property\n(and have a Dynamic Property Script) for entity type A, while for entity type B, it is a normal\n## Property\nthat has to be filled by the user.\n¶\nExample for Dynamic Property Scripts\n## The\n## Object\ntype RECTANGLE includes the following three\n## Property\n## types:\n## RECTANGLE_LENGTH_IN_M\n## [REAL]\n## RECTANGLE_WIDTH_IN_M\n## [REAL]\n## RECTANGLE_AREA_IN_QM\n## [REAL]\nOnly the first two\n## Properties\ncan be edited by users in the ELN-LIMS UI. Once the\n## Object\nis saved, the value of the\n## Property\n## RECTANGLE_AREA_IN_QM\nis automatically computed as the product of the values of\n## RECTANGLE_LENGTH_IN_M\nand\n## RECTANGLE_WIDTH_IN_M\nas defined in the Dynamic Property Script \"rectangle_area\":\ndef\ncalculate\n(\n)\n## :\n\"\"\"Main script function. The result will be used as the value of appropriate dynamic property.\"\"\"\nlength\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_LENGTH_IN_M\"\n)\nwidth\n=\nentity\n.\npropertyValue\n(\n## \"RECTANGLE_WIDTH_IN_M\"\n)\narea\n=\nlength\n*\nwidth\nreturn\narea\n## Copy\nWhen assigning the\n## Property\ntype\n## RECTANGLE_AREA_IN_QM\nto the\n## Object\ntype RECTANGLE, the name of the script is entered in the field \"Dynamic Property Plugin\" in the Admin UI:\nWhen using the Excel import option, the name of the script is entered as \"rectangle_area.py\" in the column \"Dynamic script\" in the Masterdata Excel file.\nThe script itself is defined (and can be tested) in the openBIS Admin UI under \"Tools\" -> \"Dynamic Property Plugin\" (see screenshot for Entity Validation Scripts\nabove\n). Alternatively, it can be imported as a .py file as part of the ZIP folder that also contains the Masterdata Excel file(s).\n¶\nRules & Best Practices for Dynamic Property Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe computation of the value of the dynamic\n## Property\nmust be based on the values of one or more\n## Properties\nof the same entity.\n## Properties\nof other entities must not be accessed.\nAll information needed for the calculation must be included in the script. External resources must not be accessed.\nDynamic Property Scripts should be simple and should not include any performance-heavy functions, since they are executed whenever the entities to which they are assigned are created or updated.\nThe script must be added when first creating the\n## Property\ntype or, in the case of an existing\n## Property\ntype, when assigning it to an entity type. It cannot be added retrospectively after the\n## Property\ntype assignment already exists.\n¶\n## Entity Validation Scripts\nEntity Validation Scripts are defined at the entity type (\n## Collection\n,\n## Object\n, or\n## Dataset\ntype) level.\nOnce an entity of a certain type is created or modified, the validation is perfomed according to the custom rules defined in the script. If the validation fails, the operation (entity creation or modification) is aborted and an error message is returned.\n¶\nExample for Entity Validation Scripts\nAn example of an Entity Validation Script is the\n## Date Range Validation\n(\"EXPERIMENTAL_STEP.date_range_validation\") which checks for an\n## Object\nof the type EXPERIMENTAL_STEP whether the date entered for the\n## Property\n## END.DATE\nis later than the date entered for the\n## Property\n## START.DATE\n.\nIf not, the error message \"End date cannot be before start date!\" is returned and the EXPERIMENTAL_STEP cannot be saved.\n#date_range_validation.py\ndef\ngetRenderedProperty\n(\nentity\n,\nproperty\n)\n## :\nvalue\n=\nentity\n.\nproperty\n(\nproperty\n)\nif\nvalue\nis\nnot\n## None\n## :\nreturn\nvalue\n.\nrenderedValue\n(\n)\ndef\nvalidate\n(\nentity\n,\nisNew\n)\n## :\nstart_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"START_DATE\"\n)\nend_date\n=\ngetRenderedProperty\n(\nentity\n,\n## \"END_DATE\"\n)\nif\nstart_date\nis\nnot\n## None\nand\nend_date\nis\nnot\n## None\nand\nstart_date\n>\nend_date\n## :\nreturn\n\"End date cannot be before start date!\"\n## Copy\n¶\nRules & Best Practices for Entity Validation Scripts\nThe script must be written in Jython/Python 2.7. No additional modules may be used, only the\n## Python Standard Library\n.\nThe validation must be based on the values of one or more\n## Properties\nof the entity being validated.\n## Properties\nof other entities must not be accessed for validation.\nAll information required for validation must be included in the script. No external resources may be accessed for the validation.\nEntity Validation Scripts must be read-only. No\n## Properties\nof the entity may be added or edited.\nEntity Validation Scripts should be simple and should not contain any performance-heavy functions, since they will be executed every time entities of this type are created or updated.", "timestamp": "2025-09-18T09:38:30.550536Z", "source_priority": 1, "content_type": "code"}
{"id": "docs:datastore:t:0", "source": "https://datastore.bam.de/t", "repo": "datastore", "title": "Tags", "section": "Locale", "text": "Select one or more tags\nSearch within results...\n## Locale\nlocaleAny\n## Order By\n## Title\nSelect one or more tags on the left.", "timestamp": "2025-09-18T09:38:30.555873Z", "source_priority": 1, "content_type": "procedure"}
